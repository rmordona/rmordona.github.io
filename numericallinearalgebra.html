<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Numerical Linear Algebra II | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Numerical Linear Algebra II | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Numerical Linear Algebra II | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linearalgebra.html"/>
<link rel="next" href="numericalcalculus.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#acknowledgment-and-motivations"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#caveat"><i class="fa fa-check"></i><b>0.2</b> Caveat</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i><b>0.3</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.4" data-path="mathematical-notation.html"><a href="mathematical-notation.html#notation"><i class="fa fa-check"></i><b>0.4</b> Notation</a></li>
<li class="chapter" data-level="0.5" data-path="mathematical-notation.html"><a href="mathematical-notation.html#number-system"><i class="fa fa-check"></i><b>0.5</b> Number System</a></li>
<li class="chapter" data-level="0.6" data-path="mathematical-notation.html"><a href="mathematical-notation.html#implementation"><i class="fa fa-check"></i><b>0.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numericalmethods.html"><a href="numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="numericalmethods.html"><a href="numericalmethods.html#closed-form-equation"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="numericalmethods.html"><a href="numericalmethods.html#analytical-and-numerical-solutions"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="numericalmethods.html"><a href="numericalmethods.html#significant-figures"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="numericalmethods.html"><a href="numericalmethods.html#accuracy"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="numericalmethods.html"><a href="numericalmethods.html#precision"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="numericalmethods.html"><a href="numericalmethods.html#stability-and-sensitivity"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="numericalmethods.html"><a href="numericalmethods.html#stiffness-and-implicitness"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="numericalmethods.html"><a href="numericalmethods.html#conditioning-and-posedness"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="linearalgebra.html"><a href="linearalgebra.html#system-of-linear-equations"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="linearalgebra.html"><a href="linearalgebra.html#scalar-vector-and-matrix-tensor"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition-and-multiplication"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="linearalgebra.html"><a href="linearalgebra.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="linearalgebra.html"><a href="linearalgebra.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linearalgebra.html"><a href="linearalgebra.html#magnitude-direction-unit-vectors"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-combination-and-independence"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="linearalgebra.html"><a href="linearalgebra.html#space-span-and-basis"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="linearalgebra.html"><a href="linearalgebra.html#determinants"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="linearalgebra.html"><a href="linearalgebra.html#minors-cofactors-and-adjugate-forms"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="linearalgebra.html"><a href="linearalgebra.html#inverse-form-and-row-echelon-form"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-transformations"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="linearalgebra.html"><a href="linearalgebra.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="linearalgebra.html"><a href="linearalgebra.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="linearalgebra.html"><a href="linearalgebra.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="linearalgebra.html"><a href="linearalgebra.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="linearalgebra.html"><a href="linearalgebra.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="linearalgebra.html"><a href="linearalgebra.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="linearalgebra.html"><a href="linearalgebra.html#rank-and-nullity"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="linearalgebra.html"><a href="linearalgebra.html#singularity-and-triviality"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="linearalgebra.html"><a href="linearalgebra.html#orthogonality-and-orthonormality"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="linearalgebra.html"><a href="linearalgebra.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-reconstruction-using-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="linearalgebra.html"><a href="linearalgebra.html#diagonalizability-of-a-matrix"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="linearalgebra.html"><a href="linearalgebra.html#trace-of-a-square-matrix"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="linearalgebra.html"><a href="linearalgebra.html#algebraic-and-geometric-multiplicity"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="linearalgebra.html"><a href="linearalgebra.html#types-of-matrices"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-factorization"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="linearalgebra.html"><a href="linearalgebra.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="linearalgebra.html"><a href="linearalgebra.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="linearalgebra.html"><a href="linearalgebra.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="linearalgebra.html"><a href="linearalgebra.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="linearalgebra.html"><a href="linearalgebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="linearalgebra.html"><a href="linearalgebra.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="linearalgebra.html"><a href="linearalgebra.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="linearalgebra.html"><a href="linearalgebra.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="linearalgebra.html"><a href="linearalgebra.html#software-libraries"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="linearalgebra.html"><a href="linearalgebra.html#summary"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#iteration-and-convergence"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-root-and-fixed-point-by-iteration"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-solutions-to-systems-of-eqs-by-iteration-ax-b"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialregression"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-polynomial-functions-by-series-expansion"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialinterpolation"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialsmoothing"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-optimization"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#summary-1"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numericalcalculus.html"><a href="numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#introductory-calculus"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-integration"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-differentiation"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-ordinary-differential-equations"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="numericalcalculus.html"><a href="numericalcalculus.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="numericalcalculus.html"><a href="numericalcalculus.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="numericalcalculus.html"><a href="numericalcalculus.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="numericalcalculus.html"><a href="numericalcalculus.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="numericalcalculus.html"><a href="numericalcalculus.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="numericalcalculus.html"><a href="numericalcalculus.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-functional-differential-equations"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-partial-differential-equations"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-fourier-series-and-transform"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#summary-2"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="numericalprobability.html"><a href="numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="numericalprobability.html"><a href="numericalprobability.html#approximation-based-on-random-chances"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="numericalprobability.html"><a href="numericalprobability.html#distribution"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="numericalprobability.html"><a href="numericalprobability.html#mass-and-density"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="numericalprobability.html"><a href="numericalprobability.html#probability"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-density-function-pdf"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-mass-function-pmf"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="numericalprobability.html"><a href="numericalprobability.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="numericalprobability.html"><a href="numericalprobability.html#special-functions"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="numericalprobability.html"><a href="numericalprobability.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="numericalprobability.html"><a href="numericalprobability.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="numericalprobability.html"><a href="numericalprobability.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="numericalprobability.html"><a href="numericalprobability.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="numericalprobability.html"><a href="numericalprobability.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="numericalprobability.html"><a href="numericalprobability.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="numericalprobability.html"><a href="numericalprobability.html#distributiontypes"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="numericalprobability.html"><a href="numericalprobability.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="numericalprobability.html"><a href="numericalprobability.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="numericalprobability.html"><a href="numericalprobability.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="numericalprobability.html"><a href="numericalprobability.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="numericalprobability.html"><a href="numericalprobability.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="numericalprobability.html"><a href="numericalprobability.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="numericalprobability.html"><a href="numericalprobability.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="numericalprobability.html"><a href="numericalprobability.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="numericalprobability.html"><a href="numericalprobability.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="numericalprobability.html"><a href="numericalprobability.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="numericalprobability.html"><a href="numericalprobability.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="numericalprobability.html"><a href="numericalprobability.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="numericalprobability.html"><a href="numericalprobability.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="numericalprobability.html"><a href="numericalprobability.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="numericalprobability.html"><a href="numericalprobability.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="numericalprobability.html"><a href="numericalprobability.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="numericalprobability.html"><a href="numericalprobability.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="numericalprobability.html"><a href="numericalprobability.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="numericalprobability.html"><a href="numericalprobability.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="numericalprobability.html"><a href="numericalprobability.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="numericalprobability.html"><a href="numericalprobability.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="numericalprobability.html"><a href="numericalprobability.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="numericalprobability.html"><a href="numericalprobability.html#summary-3"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="statistics.html"><a href="statistics.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="statistics.html"><a href="statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="statistics.html"><a href="statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="statistics.html"><a href="statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="statistics.html"><a href="statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="statistics.html"><a href="statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="statistics.html"><a href="statistics.html#inferential-statistics"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="statistics.html"><a href="statistics.html#the-significance-of-difference"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="statistics.html"><a href="statistics.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="statistics.html"><a href="statistics.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="statistics.html"><a href="statistics.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="statistics.html"><a href="statistics.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="statistics.html"><a href="statistics.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="statistics.html"><a href="statistics.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="statistics.html"><a href="statistics.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="statistics.html"><a href="statistics.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="statistics.html"><a href="statistics.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="statistics.html"><a href="statistics.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="statistics.html"><a href="statistics.html#post-hoc-analysis"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="statistics.html"><a href="statistics.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="statistics.html"><a href="statistics.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="statistics.html"><a href="statistics.html#multiple-comparison-tests"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="statistics.html"><a href="statistics.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="statistics.html"><a href="statistics.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="statistics.html"><a href="statistics.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="statistics.html"><a href="statistics.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="statistics.html"><a href="statistics.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="statistics.html"><a href="statistics.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="statistics.html"><a href="statistics.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="statistics.html"><a href="statistics.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="statistics.html"><a href="statistics.html#statistical-modeling"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="statistics.html"><a href="statistics.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="statistics.html"><a href="statistics.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="statistics.html"><a href="statistics.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="statistics.html"><a href="statistics.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="statistics.html"><a href="statistics.html#regression-analysis"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="statistics.html"><a href="statistics.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="statistics.html"><a href="statistics.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="statistics.html"><a href="statistics.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="statistics.html"><a href="statistics.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="statistics.html"><a href="statistics.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="statistics.html"><a href="statistics.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="statistics.html"><a href="statistics.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="statistics.html"><a href="statistics.html#the-significance-of-regression"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="statistics.html"><a href="statistics.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="statistics.html"><a href="statistics.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="statistics.html"><a href="statistics.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="statistics.html"><a href="statistics.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="statistics.html"><a href="statistics.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="statistics.html"><a href="statistics.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="statistics.html"><a href="statistics.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="statistics.html"><a href="statistics.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="statistics.html"><a href="statistics.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="statistics.html"><a href="statistics.html#inference-for-regression"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="statistics.html"><a href="statistics.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="statistics.html"><a href="statistics.html#summary-4"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian.html"><a href="bayesian.html#probability-1"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian.html"><a href="bayesian.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian.html"><a href="bayesian.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian.html"><a href="bayesian.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian.html"><a href="bayesian.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="bayesian.html"><a href="bayesian.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian.html"><a href="bayesian.html#probability-rules"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian.html"><a href="bayesian.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian.html"><a href="bayesian.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian.html"><a href="bayesian.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="bayesian.html"><a href="bayesian.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="bayesian.html"><a href="bayesian.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="bayesian.html"><a href="bayesian.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="bayesian.html"><a href="bayesian.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="bayesian.html"><a href="bayesian.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="bayesian.html"><a href="bayesian.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian.html"><a href="bayesian.html#bayes-theorem"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian.html"><a href="bayesian.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian.html"><a href="bayesian.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="bayesian.html"><a href="bayesian.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian.html"><a href="bayesian.html#conjugacy"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian.html"><a href="bayesian.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian.html"><a href="bayesian.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian.html"><a href="bayesian.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="bayesian.html"><a href="bayesian.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="bayesian.html"><a href="bayesian.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="bayesian.html"><a href="bayesian.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="bayesian.html"><a href="bayesian.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="bayesian.html"><a href="bayesian.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="bayesian.html"><a href="bayesian.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="bayesian.html"><a href="bayesian.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="bayesian.html"><a href="bayesian.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="bayesian.html"><a href="bayesian.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian.html"><a href="bayesian.html#information-theory"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian.html"><a href="bayesian.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian.html"><a href="bayesian.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian.html"><a href="bayesian.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian.html"><a href="bayesian.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian.html"><a href="bayesian.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian.html"><a href="bayesian.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian.html"><a href="bayesian.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bayesian.html"><a href="bayesian.html#bayesianinference"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="bayesian.html"><a href="bayesian.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="bayesian.html"><a href="bayesian.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="bayesian.html"><a href="bayesian.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="bayesian.html"><a href="bayesian.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian2.html"><a href="bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-models"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="bayesian2.html"><a href="bayesian2.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="bayesian2.html"><a href="bayesian2.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="bayesian2.html"><a href="bayesian2.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="bayesian2.html"><a href="bayesian2.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="bayesian2.html"><a href="bayesian2.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="bayesian2.html"><a href="bayesian2.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="bayesian2.html"><a href="bayesian2.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="bayesian2.html"><a href="bayesian2.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="bayesian2.html"><a href="bayesian2.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="bayesian2.html"><a href="bayesian2.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="bayesian2.html"><a href="bayesian2.html#simulation-and-sampling"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="bayesian2.html"><a href="bayesian2.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="bayesian2.html"><a href="bayesian2.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="bayesian2.html"><a href="bayesian2.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="bayesian2.html"><a href="bayesian2.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="bayesian2.html"><a href="bayesian2.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="bayesian2.html"><a href="bayesian2.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-analysis"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian2.html"><a href="bayesian2.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian2.html"><a href="bayesian2.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian2.html"><a href="bayesian2.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="bayesian2.html"><a href="bayesian2.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian2.html"><a href="bayesian2.html#summary-5"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="machinelearning1.html"><a href="machinelearning1.html#observation-and-measurement"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="machinelearning1.html"><a href="machinelearning1.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="machinelearning1.html"><a href="machinelearning1.html#input-data"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="machinelearning1.html"><a href="machinelearning1.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="machinelearning1.html"><a href="machinelearning1.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="machinelearning1.html"><a href="machinelearning1.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="machinelearning1.html"><a href="machinelearning1.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="machinelearning1.html"><a href="machinelearning1.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="machinelearning1.html"><a href="machinelearning1.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="machinelearning1.html"><a href="machinelearning1.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="machinelearning1.html"><a href="machinelearning1.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machinelearning1.html"><a href="machinelearning1.html#primitive-methods"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="machinelearning1.html"><a href="machinelearning1.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="machinelearning1.html"><a href="machinelearning1.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="machinelearning1.html"><a href="machinelearning1.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="machinelearning1.html"><a href="machinelearning1.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="machinelearning1.html"><a href="machinelearning1.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="machinelearning1.html"><a href="machinelearning1.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="machinelearning1.html"><a href="machinelearning1.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="machinelearning1.html"><a href="machinelearning1.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="machinelearning1.html"><a href="machinelearning1.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="machinelearning1.html"><a href="machinelearning1.html#distance-metrics"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="machinelearning1.html"><a href="machinelearning1.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="machinelearning1.html"><a href="machinelearning1.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="machinelearning1.html"><a href="machinelearning1.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="machinelearning1.html"><a href="machinelearning1.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="machinelearning1.html"><a href="machinelearning1.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="machinelearning1.html"><a href="machinelearning1.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="machinelearning1.html"><a href="machinelearning1.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="machinelearning1.html"><a href="machinelearning1.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="machinelearning1.html"><a href="machinelearning1.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="machinelearning1.html"><a href="machinelearning1.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="machinelearning1.html"><a href="machinelearning1.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="machinelearning1.html"><a href="machinelearning1.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="machinelearning1.html"><a href="machinelearning1.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="machinelearning1.html"><a href="machinelearning1.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="machinelearning1.html"><a href="machinelearning1.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="machinelearning1.html"><a href="machinelearning1.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="machinelearning1.html"><a href="machinelearning1.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="machinelearning1.html"><a href="machinelearning1.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="machinelearning1.html"><a href="machinelearning1.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="machinelearning1.html"><a href="machinelearning1.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="machinelearning1.html"><a href="machinelearning1.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="machinelearning1.html"><a href="machinelearning1.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="machinelearning1.html"><a href="machinelearning1.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="machinelearning1.html"><a href="machinelearning1.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="machinelearning1.html"><a href="machinelearning1.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="machinelearning1.html"><a href="machinelearning1.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureengineering"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="machinelearning1.html"><a href="machinelearning1.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="machinelearning1.html"><a href="machinelearning1.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="machinelearning1.html"><a href="machinelearning1.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="machinelearning1.html"><a href="machinelearning1.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="machinelearning1.html"><a href="machinelearning1.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="machinelearning1.html"><a href="machinelearning1.html#general-modeling"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="machinelearning1.html"><a href="machinelearning1.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="machinelearning1.html"><a href="machinelearning1.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="machinelearning1.html"><a href="machinelearning1.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="machinelearning1.html"><a href="machinelearning1.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="machinelearning1.html"><a href="machinelearning1.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="machinelearning1.html"><a href="machinelearning1.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="machinelearning1.html"><a href="machinelearning1.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="machinelearning1.html"><a href="machinelearning1.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="machinelearning1.html"><a href="machinelearning1.html#supervised-vs.unsupervised-learning"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="machinelearning1.html"><a href="machinelearning1.html#summary-6"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="machinelearning2.html"><a href="machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="machinelearning2.html"><a href="machinelearning2.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="machinelearning2.html"><a href="machinelearning2.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="machinelearning2.html"><a href="machinelearning2.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="machinelearning2.html"><a href="machinelearning2.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="machinelearning2.html"><a href="machinelearning2.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="machinelearning2.html"><a href="machinelearning2.html#binary-classification-supervised"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="machinelearning2.html"><a href="machinelearning2.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="machinelearning2.html"><a href="machinelearning2.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="machinelearning2.html"><a href="machinelearning2.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="machinelearning2.html"><a href="machinelearning2.html#multi-class-classification-supervised"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="machinelearning2.html"><a href="machinelearning2.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="machinelearning2.html"><a href="machinelearning2.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="machinelearning2.html"><a href="machinelearning2.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="machinelearning2.html"><a href="machinelearning2.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="machinelearning2.html"><a href="machinelearning2.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="machinelearning3.html"><a href="machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="machinelearning3.html"><a href="machinelearning3.html#clustering-unsupervised"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="machinelearning3.html"><a href="machinelearning3.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="machinelearning3.html"><a href="machinelearning3.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="machinelearning3.html"><a href="machinelearning3.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="machinelearning3.html"><a href="machinelearning3.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="machinelearning3.html"><a href="machinelearning3.html#meta-learning"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="machinelearning3.html"><a href="machinelearning3.html#natural-language-processing-nlp"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="machinelearning3.html"><a href="machinelearning3.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="machinelearning3.html"><a href="machinelearning3.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="machinelearning3.html"><a href="machinelearning3.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="machinelearning3.html"><a href="machinelearning3.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="machinelearning3.html"><a href="machinelearning3.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="machinelearning3.html"><a href="machinelearning3.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="machinelearning3.html"><a href="machinelearning3.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="machinelearning3.html"><a href="machinelearning3.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-forecasting"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="machinelearning3.html"><a href="machinelearning3.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="machinelearning3.html"><a href="machinelearning3.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="machinelearning3.html"><a href="machinelearning3.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="machinelearning3.html"><a href="machinelearning3.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="machinelearning3.html"><a href="machinelearning3.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="machinelearning3.html"><a href="machinelearning3.html#recommender-systems"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deeplearning1.html"><a href="deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="deeplearning1.html"><a href="deeplearning1.html#simple-perceptron"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="deeplearning1.html"><a href="deeplearning1.html#adaptive-linear-neuron-adaline"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="deeplearning1.html"><a href="deeplearning1.html#multi-layer-perceptron-mlp"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="deeplearning1.html"><a href="deeplearning1.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="deeplearning1.html"><a href="deeplearning1.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="deeplearning1.html"><a href="deeplearning1.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="deeplearning1.html"><a href="deeplearning1.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="deeplearning1.html"><a href="deeplearning1.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="deeplearning1.html"><a href="deeplearning1.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="deeplearning1.html"><a href="deeplearning1.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="deeplearning1.html"><a href="deeplearning1.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="deeplearning1.html"><a href="deeplearning1.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="deeplearning1.html"><a href="deeplearning1.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="deeplearning1.html"><a href="deeplearning1.html#convolutional-neural-network-cnn"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="deeplearning1.html"><a href="deeplearning1.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="deeplearning1.html"><a href="deeplearning1.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="deeplearning1.html"><a href="deeplearning1.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="deeplearning1.html"><a href="deeplearning1.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="deeplearning1.html"><a href="deeplearning1.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="deeplearning1.html"><a href="deeplearning1.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="deeplearning1.html"><a href="deeplearning1.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="deeplearning1.html"><a href="deeplearning1.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="deeplearning1.html"><a href="deeplearning1.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="deeplearning1.html"><a href="deeplearning1.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="deeplearning1.html"><a href="deeplearning1.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="deeplearning2.html"><a href="deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="deeplearning2.html"><a href="deeplearning2.html#residual-network-resnet"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="deeplearning2.html"><a href="deeplearning2.html#recurrent-neural-network-rnn"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="deeplearning2.html"><a href="deeplearning2.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="deeplearning2.html"><a href="deeplearning2.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="deeplearning2.html"><a href="deeplearning2.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-rnn"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-bidirectional-rnn"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-neural-network-tnn"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="deeplearning2.html"><a href="deeplearning2.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="deeplearning2.html"><a href="deeplearning2.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="deeplearning2.html"><a href="deeplearning2.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="deeplearning2.html"><a href="deeplearning2.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="deeplearning2.html"><a href="deeplearning2.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="deeplearning2.html"><a href="deeplearning2.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="deeplearning2.html"><a href="deeplearning2.html#applications-using-tnn-and-rnn"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="deeplearning2.html"><a href="deeplearning2.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="deeplearning2.html"><a href="deeplearning2.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="deeplearning2.html"><a href="deeplearning2.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="deeplearning2.html"><a href="deeplearning2.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="deeplearning2.html"><a href="deeplearning2.html#generative-adversarial-network-gan"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-reinforcement-network-dqn"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="deeplearning2.html"><a href="deeplearning2.html#summary-8"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="distributedcomputation.html"><a href="distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#integration-and-interoperability"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#ml-pipelines"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-standards"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="distributedcomputation.html"><a href="distributedcomputation.html#general-summary"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>15</b> Appendix</a><ul>
<li class="chapter" data-level="15.1" data-path="appendix.html"><a href="appendix.html#appendix-a"><i class="fa fa-check"></i><b>15.1</b> Appendix A</a><ul>
<li class="chapter" data-level="15.1.1" data-path="appendix.html"><a href="appendix.html#trigonometry"><i class="fa fa-check"></i><b>15.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="15.1.2" data-path="appendix.html"><a href="appendix.html#logarithms"><i class="fa fa-check"></i><b>15.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="15.1.3" data-path="appendix.html"><a href="appendix.html#category-theory"><i class="fa fa-check"></i><b>15.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="appendix.html"><a href="appendix.html#appendix-b"><i class="fa fa-check"></i><b>15.2</b> Appendix B</a><ul>
<li class="chapter" data-level="15.2.1" data-path="appendix.html"><a href="appendix.html#on-random-chances"><i class="fa fa-check"></i><b>15.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="15.2.2" data-path="appendix.html"><a href="appendix.html#on-replacements"><i class="fa fa-check"></i><b>15.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="15.2.3" data-path="appendix.html"><a href="appendix.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>15.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="15.2.4" data-path="appendix.html"><a href="appendix.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>15.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="15.2.5" data-path="appendix.html"><a href="appendix.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>15.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="15.2.6" data-path="appendix.html"><a href="appendix.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>15.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="15.2.7" data-path="appendix.html"><a href="appendix.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>15.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="appendix.html"><a href="appendix.html#appendix-c"><i class="fa fa-check"></i><b>15.3</b> Appendix C</a></li>
<li class="chapter" data-level="15.4" data-path="appendix.html"><a href="appendix.html#appendix-d"><i class="fa fa-check"></i><b>15.4</b> Appendix D</a><ul>
<li class="chapter" data-level="15.4.1" data-path="appendix.html"><a href="appendix.html#lubridate-library"><i class="fa fa-check"></i><b>15.4.1</b> Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="numericallinearalgebra" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 3</span> Numerical Linear Algebra II<a href="numericallinearalgebra.html#numericallinearalgebra" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
            autoNumber: "AMS",
            formatNumber: function (n) {return '3.'+n}
      } 
  }
});
</script>
<p>It can be said that the oldest way of approximating a solution, especially in finding the root value, is the trial-and-error approach. We start with some random false position and then work our way up iteratively to the point of hitting the correct position. This archaic iterative method is described in the classic <strong>Regula-Falsi</strong> method, also called the <strong>False Position</strong> method. Other traditional methods, such as <strong>Archimedes iteration</strong> and <strong>Brent-Salamin iteration</strong> - to name a few - also adopt the same approach. They were developed to find the most accurate value of the famous <strong>PI</strong> (<span class="math inline">\(\pi\)</span>) <span class="citation">(Bailey D.H. <a href="bibliography.html#ref-ref1603d">2021</a>)</span>. In this chapter, we shall show many different Numerical methods rooted in the same idea.</p>
<p>Here, we begin to focus on <strong>Indirect Methods</strong> in the context of Linear Algebra as we reference the great works of Atkinson K. E. <span class="citation">(<a href="bibliography.html#ref-ref288k">1989</a>)</span>, Heath M.T. <span class="citation">(<a href="bibliography.html#ref-ref187m">2002</a>)</span>, Burden R.L. et al. <span class="citation">(<a href="bibliography.html#ref-ref196r">2005</a>)</span>, Press W.H. et al. <span class="citation">(<a href="bibliography.html#ref-ref215w">2007</a>)</span>, and Edwards H. et al. <span class="citation">(<a href="bibliography.html#ref-ref207c">2018</a>)</span> along with other additional references for consistency.</p>
<p>A few places in the discussion may bring about <strong>solving for derivatives</strong>, which is part of <strong>Calculus</strong> covered in the next chapter when we talk about <strong>Numerical Calculus</strong>. Note that it helps to review <strong>Calculus</strong> first for some of the methods discussed in this chapter.</p>
<p>Let us start with the concept of <strong>iteration</strong> and <strong>convergence</strong>.</p>
<div id="iteration-and-convergence" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.1</span> Iteration and Convergence <a href="numericallinearalgebra.html#iteration-and-convergence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We rely on <strong>Iterative</strong> methods in situations where no standard analytical approaches are available to provide an exact result or where analytical techniques are impractical. An iterative method aims to get as close to the actual value or target value as possible; in other words, we expect the approach to lead us to a <strong>Convergence</strong> - it is an approximation method.</p>
<p><strong>Convergence</strong> can be associated with <strong>tolerance</strong>. The key idea is to iterate through steps, re-evaluating the solution until it <strong>converges</strong>, meaning it hits the expected target. Here, we describe the target in terms of tolerance level against the accuracy of the solution. So if the tolerance level is at 99.9% accuracy and we reach the target accuracy, then the iteration ends, and an approximate value of the actual value is delivered as the final solution.</p>
</div>
<div id="approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.2</span> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)<a href="numericallinearalgebra.html#approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In our discussion about matrix decomposition, we aim to find the <strong>Eigenvalues</strong> and corresponding <strong>Eigenvectors</strong> of a matrix. In such an <strong>analytical</strong> process, we rely on evaluating the <strong>determinant</strong> of a <strong>characteristic matrix</strong>, in the form of <span class="math inline">\(det(A - \lambda I) = 0\)</span> to generate a <strong>characteristic polynomial</strong> that leads to a set of solutions for our <strong>Eigenvalues</strong>. It may seem simple and easy to take that approach, but if we begin to perform the same steps against an extensive matrix - which is practically common - then deriving the <strong>characteristic equation</strong> becomes extremely complex and unimaginable. For this reason, we seek other alternatives. <strong>Numerically</strong>, we can use <strong>iterative</strong> methods to approximate <strong>Eigenvalues</strong> and <strong>Eigenvectors</strong>. And that is what we shall cover in this section.</p>
<p>We may accept only a reasonable finite number of loops or iterations for any process involving iteration before we stop the iteration. Therefore, for most discussions around iterations, let us use a tolerance level (our tolerance threshold) such as the value below:</p>

<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1">tol =<span class="st"> </span><span class="fl">1e-5</span></a></code></pre></div>

<p>Before we begin, let us use <strong>eigen(.)</strong> function to generate the actual <strong>Eigenvalues</strong> and <strong>Eigenvectors</strong>, which we will use to validate our solutions later. Our sample matrix and the result of the <strong>eigen(.)</strong> function are shown below:</p>

<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">(<span class="dt">A =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">eigen</span>(A)</a></code></pre></div>
<pre><code>## eigen() decomposition
## $values
## [1] 10.6771903  1.9109438 -0.5881341
## 
## $vectors
##            [,1]        [,2]        [,3]
## [1,] -0.4836669 -0.91825423  0.07759288
## [2,] -0.6128191 -0.05832852 -0.74984844
## [3,] -0.6249152  0.39167200  0.65704388</code></pre>

<div id="power-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.1</span> Power Method <a href="numericallinearalgebra.html#power-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to use iteration to approximate an <strong>Eigenvalue</strong> of a matrix using an initial arbitrary vector until the iteration stops at a tolerance level.</p>
<p>The <strong>Power Method</strong> estimates only the dominant <strong>Eigenvalue</strong> - or the maximum absolute <strong>Eigenvalue</strong> <span class="citation">(Atkinson K. E. <a href="bibliography.html#ref-ref288k">1989</a>; Bai Z. et al <a href="bibliography.html#ref-ref26z">2000</a>)</span>.</p>
<p><span class="math display">\[
|\lambda_1| &gt; |\lambda_2|\ge |\lambda_3| \ge ... \ge |\lambda_n|
\]</span></p>
<p>The restriction imposed by this method is to find only the one largest <strong>dominant Eigenvalue</strong> along with its corresponding <strong>Eigenvector</strong> for a matrix with multiple <strong>Eigenvalues</strong> or even with one <strong>Eigenvalue</strong> but having <strong>Multiplicity</strong> greater than one. Additionally, the condition above applies to <strong>Diagonalizable</strong> matrices in which any column is a linear combination of <strong>Eigenvectors</strong>. Below illustrates the sequence against which we iterate until convergence.</p>
<p><span class="math display">\[
Av_0, A^2v_1, A^3v_2,\ ...\ \ \ \ \ \text{where } v_j \text{ is an Eigenvector}.
\]</span></p>
<p>We normalize the <strong>Eigenvector</strong> at each iteration using the below equation:</p>
<p><span class="math display">\[\begin{align}
v_j =  \frac{v_j} {\|v_j\|_{L2}} = \frac{v_j} {\sqrt{\sum{v_j^2}}}
\end{align}\]</span></p>
<p>For the sake of illustration, we use the <strong>Rayleigh Equation</strong> below to derive the <strong>EigenValue</strong> <span class="citation">(B. N. Parlett <a href="bibliography.html#ref-ref7b">1974</a>)</span>. We also use the value to compute the error.</p>
<p><span class="math display">\[\begin{align}
\lambda = \frac{v^T \cdotp Av}{v^Tv}
\end{align}\]</span></p>
<p>Here, <span class="math inline">\(\lambda\)</span> is a natural approximation of an <strong>Eigenvalue</strong> if <span class="math inline">\(\mathbf{v}\)</span> is close to the actual <strong>Eigenvector</strong>.</p>
<p>That said, let us now introduce the normalized <strong>Power Method</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
v_0 \leftarrow \text{initial arbitrary nonzero vector}\\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ v_j = A \cdotp v_{j-1} \\
\ \ \ \ v_j =  v_j / \|v_j\|_{L2}\ \ \ \leftarrow \text{normalized Eigenvector}\\
\ \ \ \ \lambda_j = v_j^TAv_j / v_j^Tv_j \ \ \ \leftarrow \text{Eigenvalue} \\
\ \ \ \ if\ err(\lambda_j, \lambda_{j-1}) &lt; \text{tolerance then break} \\
end\ loop
\end{array}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\vec{v}}\)</span> is the <strong>approximate eigenvector</strong> and <span class="math inline">\(\lambda\)</span> is the <strong>approximate (dominant) eigenvalue</strong>.</p>
<p>Our naive implementation of normalized <strong>Power Method</strong> in R code shows the following (note that the eigenvector output is scaled):</p>

<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1">power_method &lt;-<span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb28-2" data-line-number="2">    n          =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb28-3" data-line-number="3">    sequence   =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb28-4" data-line-number="4">    vj =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(A)<span class="op">-</span><span class="dv">1</span>)) <span class="co"># initial arbitrary nonzero vector</span></a>
<a class="sourceLine" id="cb28-5" data-line-number="5">    old_evalue =<span class="st"> </span><span class="dv">0</span>; evalue =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb28-6" data-line-number="6">    limit      =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb28-7" data-line-number="7">    tol=<span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb28-8" data-line-number="8">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb28-9" data-line-number="9">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb28-10" data-line-number="10">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb28-11" data-line-number="11">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb28-12" data-line-number="12">            vj       =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>vj</a>
<a class="sourceLine" id="cb28-13" data-line-number="13">            vj =<span class="st"> </span>vj <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(vj<span class="op">^</span><span class="dv">2</span>))  <span class="co"># normalized Eigenvector</span></a>
<a class="sourceLine" id="cb28-14" data-line-number="14">            evalue   =<span class="st"> </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>(A <span class="op">%*%</span><span class="st"> </span>vj))<span class="op">/</span></a>
<a class="sourceLine" id="cb28-15" data-line-number="15"><span class="st">                        </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>vj) <span class="co"># Eigvenvalue</span></a>
<a class="sourceLine" id="cb28-16" data-line-number="16">            err      =<span class="st"> </span>(evalue <span class="op">-</span><span class="st"> </span>old_evalue)<span class="op">/</span>evalue </a>
<a class="sourceLine" id="cb28-17" data-line-number="17">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb28-18" data-line-number="18">            <span class="cf">if</span> (<span class="kw">abs</span>(err) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span>   </a>
<a class="sourceLine" id="cb28-19" data-line-number="19">        }</a>
<a class="sourceLine" id="cb28-20" data-line-number="20">        old_evalue =<span class="st"> </span>evalue </a>
<a class="sourceLine" id="cb28-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb28-22" data-line-number="22">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;J&quot;</span>, <span class="st">&quot;eigenval&quot;</span>, </a>
<a class="sourceLine" id="cb28-23" data-line-number="23">          <span class="kw">paste</span>(<span class="st">&quot;eigenvec&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb28-24" data-line-number="24">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;eigenvalue&quot;</span>=evalue, </a>
<a class="sourceLine" id="cb28-25" data-line-number="25">         <span class="st">&quot;eigenvector&quot;</span>=sequence[<span class="kw">nrow</span>(sequence),<span class="dv">3</span><span class="op">:</span>(<span class="kw">ncol</span>(A)<span class="op">+</span><span class="dv">2</span>)])</a>
<a class="sourceLine" id="cb28-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb28-27" data-line-number="27">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb28-28" data-line-number="28">(<span class="dt">E.normalized =</span> <span class="kw">power_method</span>(A))</a></code></pre></div>
<pre><code>## $Iteration
##       J  eigenval eigenvec1 eigenvec2 eigenvec3        error
##  [1,] 0  0.000000 1.0000000 0.0000000 0.0000000 0.000000e+00
##  [2,] 1  7.857143 0.8017837 0.5345225 0.2672612 1.000000e+00
##  [3,] 2 10.368682 0.5666657 0.5981471 0.5666657 2.422236e-01
##  [4,] 3 10.633900 0.4992259 0.6111735 0.6141991 2.494084e-02
##  [5,] 4 10.669882 0.4864789 0.6125083 0.6230344 3.372238e-03
##  [6,] 5 10.675896 0.4841708 0.6127657 0.6245773 5.633452e-04
##  [7,] 6 10.676959 0.4837571 0.6128095 0.6248548 9.957788e-05
##  [8,] 7 10.677149 0.4836831 0.6128174 0.6249044 1.777917e-05
##  [9,] 8 10.677183 0.4836698 0.6128188 0.6249133 3.180791e-06
## 
## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $eigenvalue
##          [,1]
## [1,] 10.67718
## 
## $eigenvector
## eigenvec1 eigenvec2 eigenvec3 
## 0.4836698 0.6128188 0.6249133</code></pre>

<p>The iteration stops after the tolerance is reached, giving us an <strong>approximate Eigenvalue</strong> of <span class="math inline">\(\lambda\)</span>=10.6771829 with the corresponding <strong>approximate Eigenvector</strong>, v=(0.4836698, 0.6128188, 0.6249133).</p>
</div>
<div id="inverse-power-method-using-lu-decomposition" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.2</span> Inverse Power Method (using LU Decomposition)<a href="numericallinearalgebra.html#inverse-power-method-using-lu-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In contrast to the <strong>Power Method</strong>, the <strong>Inverse Power Method</strong> restricts us to the smallest <strong>Eigenvalue</strong> <span class="citation">(Bai Z. et al. <a href="bibliography.html#ref-ref26z">2000</a>)</span>.</p>
<p>The method is similar to the <strong>Power Method</strong>. We follow the same iteration, except we use an inverse matrix this time. To avoid the cost of calculating the inverse of a matrix, we decompose the matrix into <strong>LU form</strong> and use the decomposed components. In our case, we use <strong>LU decomposition by Doolittle</strong> (See Section <strong>LU Decomposition</strong> in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>)) to extract the <strong>LU</strong> form, then perform substitution to solve for the system.</p>
<p>That said, let us now introduce the <strong>Inverse Power Method</strong> algorithm (a modified version with Doolittle Decomposition):</p>
<p><span class="math display">\[
\begin{array}{l}
v_0 \leftarrow \text{initial arbitrary nonzero vector}\\
LU = lu\_decomposition\_by\_doolittle(A)\\
\text{loop}\ j\ in\ 1:\ ... \\
\ \ \ \ u_y = forward\_sub(L,  v_{j-1} ) \\
\ \ \ \ v_j = backward\_sub(U,  u_y ) \\
\ \ \ \ v_j =  v_j / \|v_j\|_{L2}\ \ \ \leftarrow \text{normalize Eigenvector}\\
\ \ \ \ \lambda_j = v_j^TAv_j / v_j^Tv_j \ \ \ \leftarrow \text{Eigenvalue} \\
\ \ \ \ if\ err(\lambda_j, \lambda_{j-1}) &lt; \text{tolerance then break} \\
\text{end loop}
\end{array}
\]</span></p>
<p>Note in our algorithm that we have not introduced the use of <strong>shift</strong> which we cover in the next method.</p>
<p>Below is a naive implementation of the <strong>Inverse Power Method</strong> in R (note that the eigenvector output is scaled):</p>

<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1">inverse_power_method &lt;-<span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb30-2" data-line-number="2">    LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(A) <span class="co">#derived from Lin Algebra Ch</span></a>
<a class="sourceLine" id="cb30-3" data-line-number="3">    n  =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb30-4" data-line-number="4">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb30-5" data-line-number="5">    vj =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(A)<span class="op">-</span><span class="dv">1</span>)) <span class="co"># initial arbitrary nonzero vector</span></a>
<a class="sourceLine" id="cb30-6" data-line-number="6">    old_evalue =<span class="st"> </span><span class="dv">0</span>; evalue =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb30-7" data-line-number="7">    limit =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb30-8" data-line-number="8">    tol=<span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb30-9" data-line-number="9">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb30-10" data-line-number="10">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb30-11" data-line-number="11">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb30-12" data-line-number="12">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb30-13" data-line-number="13">            uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, vj)</a>
<a class="sourceLine" id="cb30-14" data-line-number="14">            vj =<span class="st"> </span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy)</a>
<a class="sourceLine" id="cb30-15" data-line-number="15">            vj =<span class="st"> </span>vj <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(vj<span class="op">^</span><span class="dv">2</span>)) <span class="co"># Normalize Eigenvector</span></a>
<a class="sourceLine" id="cb30-16" data-line-number="16">            evalue   =<span class="st"> </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>(A <span class="op">%*%</span><span class="st"> </span>vj))<span class="op">/</span></a>
<a class="sourceLine" id="cb30-17" data-line-number="17"><span class="st">                        </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>vj) <span class="co"># Eigvenvalue</span></a>
<a class="sourceLine" id="cb30-18" data-line-number="18">            err =<span class="st"> </span>(evalue <span class="op">-</span><span class="st"> </span>old_evalue)<span class="op">/</span><span class="st"> </span>evalue </a>
<a class="sourceLine" id="cb30-19" data-line-number="19">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb30-20" data-line-number="20">            <span class="cf">if</span> (<span class="kw">abs</span>(err) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span>  </a>
<a class="sourceLine" id="cb30-21" data-line-number="21">        }</a>
<a class="sourceLine" id="cb30-22" data-line-number="22">        old_evalue =<span class="st"> </span>evalue </a>
<a class="sourceLine" id="cb30-23" data-line-number="23">    }</a>
<a class="sourceLine" id="cb30-24" data-line-number="24">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;J&quot;</span>, <span class="st">&quot;eigenval&quot;</span>, </a>
<a class="sourceLine" id="cb30-25" data-line-number="25">          <span class="kw">paste</span>(<span class="st">&quot;eigenvec&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb30-26" data-line-number="26">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;eigenvalue&quot;</span>=evalue, </a>
<a class="sourceLine" id="cb30-27" data-line-number="27">         <span class="st">&quot;eigenvector&quot;</span>=sequence[<span class="kw">nrow</span>(sequence),<span class="dv">3</span><span class="op">:</span>(<span class="kw">ncol</span>(A)<span class="op">+</span><span class="dv">2</span>)])</a>
<a class="sourceLine" id="cb30-28" data-line-number="28">}</a>
<a class="sourceLine" id="cb30-29" data-line-number="29">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb30-30" data-line-number="30">(<span class="dt">E.scaled =</span> <span class="kw">inverse_power_method</span>(A))</a></code></pre></div>
<pre><code>## $Iteration
##        J   eigenval    eigenvec1  eigenvec2  eigenvec3         error
##  [1,]  0  0.0000000  1.000000000  0.0000000  0.0000000  0.000000e+00
##  [2,]  1  0.6976744  0.539163866  0.5391639 -0.6469966  1.000000e+00
##  [3,]  2 -0.5317854  0.361791913 -0.7488251  0.5553085  2.311947e+00
##  [4,]  3 -0.5177750  0.006673751  0.7368606 -0.6760117 -2.705868e-02
##  [5,]  4 -0.6025525  0.104197718 -0.7527992  0.6499509  1.406972e-01
##  [6,]  5 -0.5829676 -0.069460484  0.7488318 -0.6591101 -3.359522e-02
##  [7,]  6 -0.5896563  0.080101260 -0.7501514  0.6563967  1.134351e-02
##  [8,]  7 -0.5876591 -0.076821393  0.7497542 -0.6572420 -3.398616e-03
##  [9,]  8 -0.5882797  0.077830375 -0.7498773  0.6569828  1.054838e-03
## [10,]  9 -0.5880892 -0.077519795  0.7498395 -0.6570627 -3.238135e-04
## [11,] 10 -0.5881479  0.077615379 -0.7498512  0.6570381  9.973976e-05
## [12,] 11 -0.5881298 -0.077585960  0.7498476 -0.6570457 -3.068956e-05
## [13,] 12 -0.5881354  0.077595014 -0.7498487  0.6570433  9.446082e-06
## 
## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $eigenvalue
##            [,1]
## [1,] -0.5881354
## 
## $eigenvector
##   eigenvec1   eigenvec2   eigenvec3 
##  0.07759501 -0.74984870  0.65704333</code></pre>

</div>
<div id="rayleigh-quotient-method-using-lu-decomposition" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.3</span> Rayleigh Quotient Method (using LU Decomposition)<a href="numericallinearalgebra.html#rayleigh-quotient-method-using-lu-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Rayleigh Quotient Method</strong> is a variant of <strong>Inverse Power Method</strong> <span class="citation">(B. N. Parlett <a href="bibliography.html#ref-ref7b">1974</a>)</span>.</p>
<p>Here, the <strong>LU decomposition</strong> takes a shifted matrix using the calculated <strong>Eigenvector</strong>, <span class="math inline">\(\lambda_{j-1}\)</span>, (via Rayleigh Quotient) at every iteration. The shift is expressed as such:</p>
<p><span class="math display">\[
A - \lambda_{j-1} I
\]</span></p>
<p>Here, the calculated <strong>Eigenvector</strong> is called the <strong>shift</strong>, which allows for faster convergence <span class="citation">(Heath M.T. p.176, <a href="bibliography.html#ref-ref187m">2002</a>)</span>.</p>
<p>That said, let us now introduce the <strong>Rayleigh Quotient Iteration</strong> algorithm (a modified version with Doolittle Decomposition):</p>
<p><span class="math display">\[
\begin{array}{l}
v_0 \leftarrow \text{initial arbitrary nonzero vector}\\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ LU = lu\_decomposition\_by\_doolittle(A - \lambda_{j-1} I)\\
\ \ \ \ u_y = forward\_sub(L, v_{j-1}  ) \\
\ \ \ \ v_j = backward\_sub(U,  u_y ) \\
\ \ \ \ v_j =  v_j / \|v_j\|_{L2}\ \ \ \leftarrow \text{normalized Eigenvector}\\
\ \ \ \ \lambda_j = v_j^TAv_j / v_j^Tv_j \ \ \ \leftarrow \text{Eigenvalue} \\
\ \ \ \ if\ err(v_j, v_{j-1}) &lt; \text{tolerance then break} \\
end\ loop
\end{array}
\]</span></p>
<p>Below is a naive implementation of <strong>Rayleigh Quotient Method</strong> in R (note that the eigenvector output is scaled):</p>

<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1">rayleighquotient_method &lt;-<span class="cf">function</span>(A, eigenvector) {</a>
<a class="sourceLine" id="cb32-2" data-line-number="2">    n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb32-3" data-line-number="3">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb32-4" data-line-number="4">    vj =<span class="st"> </span>eigenvector <span class="co"># initial arbitrary nonzero vector</span></a>
<a class="sourceLine" id="cb32-5" data-line-number="5">    old_evalue =<span class="st"> </span><span class="dv">0</span>; evalue =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb32-6" data-line-number="6">    limit =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb32-7" data-line-number="7">    tol=<span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb32-8" data-line-number="8">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb32-9" data-line-number="9">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb32-10" data-line-number="10">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb32-11" data-line-number="11">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb32-12" data-line-number="12">            B  =<span class="st"> </span>A <span class="op">-</span><span class="st"> </span><span class="kw">c</span>(evalue) <span class="op">*</span><span class="st"> </span><span class="kw">diag</span>(n)</a>
<a class="sourceLine" id="cb32-13" data-line-number="13">            LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(B) <span class="co"># from Lin Algebra Ch</span></a>
<a class="sourceLine" id="cb32-14" data-line-number="14">            uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, vj)</a>
<a class="sourceLine" id="cb32-15" data-line-number="15">            vj =<span class="st"> </span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy)</a>
<a class="sourceLine" id="cb32-16" data-line-number="16">            vj =<span class="st"> </span>vj <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(vj<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb32-17" data-line-number="17">            evalue =<span class="st"> </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>(A <span class="op">%*%</span><span class="st"> </span>vj)) <span class="op">/</span><span class="st"> </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>vj)</a>
<a class="sourceLine" id="cb32-18" data-line-number="18">            err =<span class="st"> </span>(evalue <span class="op">-</span><span class="st"> </span>old_evalue)<span class="op">/</span><span class="st"> </span>evalue </a>
<a class="sourceLine" id="cb32-19" data-line-number="19">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb32-20" data-line-number="20">            <span class="cf">if</span> (<span class="kw">abs</span>(err) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span>  </a>
<a class="sourceLine" id="cb32-21" data-line-number="21">        }</a>
<a class="sourceLine" id="cb32-22" data-line-number="22">        old_evalue =<span class="st"> </span>evalue </a>
<a class="sourceLine" id="cb32-23" data-line-number="23">    }</a>
<a class="sourceLine" id="cb32-24" data-line-number="24">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;J&quot;</span>, <span class="st">&quot;eigenval&quot;</span>, </a>
<a class="sourceLine" id="cb32-25" data-line-number="25">          <span class="kw">paste</span>(<span class="st">&quot;eigenvec&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb32-26" data-line-number="26">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;eigenvalue&quot;</span>=evalue, </a>
<a class="sourceLine" id="cb32-27" data-line-number="27">         <span class="st">&quot;eigenvector&quot;</span>=sequence[<span class="kw">nrow</span>(sequence),<span class="dv">3</span><span class="op">:</span>(<span class="kw">ncol</span>(A)<span class="op">+</span><span class="dv">2</span>)])</a>
<a class="sourceLine" id="cb32-28" data-line-number="28">}</a>
<a class="sourceLine" id="cb32-29" data-line-number="29">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb32-30" data-line-number="30">neigenvector =<span class="st"> </span>E.scaled<span class="op">$</span>eigenvector   </a>
<a class="sourceLine" id="cb32-31" data-line-number="31"><span class="co">#eigenvector = E.normalized$eigenvector  </span></a>
<a class="sourceLine" id="cb32-32" data-line-number="32"><span class="kw">rayleighquotient_method</span>(A,neigenvector )</a></code></pre></div>
<pre><code>## $Iteration
##      J   eigenval   eigenvec1  eigenvec2  eigenvec3        error
## [1,] 0  0.0000000  0.07759501 -0.7498487  0.6570433 0.000000e+00
## [2,] 1 -0.5881337 -0.07759223  0.7498484 -0.6570441 1.000000e+00
## [3,] 2 -0.5881341  0.07759288 -0.7498484  0.6570439 6.841798e-07
## 
## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $eigenvalue
##            [,1]
## [1,] -0.5881341
## 
## $eigenvector
##   eigenvec1   eigenvec2   eigenvec3 
##  0.07759288 -0.74984844  0.65704388</code></pre>

<p>Given an actual <strong>Eigenvector</strong>, we can use the <strong>Rayleigh Quotient Method</strong> to discover the corresponding unknown <strong>Eigenvalue</strong>. More importantly, the method allows discovering interior <strong>Eigenvalues</strong>, which is an advantage over <strong>Power Method</strong>, which only discovers the largest <strong>Eigenvalue</strong> and over <strong>Inverse Power Method</strong>, which only finds the smallest <strong>Eigenvalue</strong>.</p>
</div>
<div id="qr-method-using-qr-decomposition-by-givens" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.4</span> QR Method (using QR Decomposition by Givens)<a href="numericallinearalgebra.html#qr-method-using-qr-decomposition-by-givens" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us now look into the <strong>QR Method</strong>, which allows us to find the list of <strong>Eigenvalues</strong> of a given matrix.</p>
<p>In <strong>Linear Algebra</strong>, we have discussed how to decompose a matrix into its <strong>QR form</strong>. We use <strong>QR decomposition by Givens</strong> in the algorithm to derive all the <strong>Eigenvalues</strong>.</p>
<p><span class="math display">\[
\begin{array}{l}
loop\ j\ in\ 1:\ ... \\
\ \ \ \ QR = qr\_decomposition\_by\_givens(A_{j-1})\\
\ \ \ \ A_j = Q^T A_{j-1} Q\\
\ \ \ \ \lambda_j = diag(Q) \\
\ \ \ \ if\ err(\|\lambda_j\|_{L2}, \|\lambda_{j-1}\|_{L2}) &lt; \text{tolerance then break} \\
end\ loop
\end{array}
\]</span></p>
<p>We implement the <strong>QR Method</strong> in R code like so:</p>

<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1">qr_method &lt;-<span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb34-2" data-line-number="2">    n          =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb34-3" data-line-number="3">    sequence   =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb34-4" data-line-number="4">    old_evalue =<span class="st"> </span>evalues =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb34-5" data-line-number="5">    limit      =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb34-6" data-line-number="6">    tol        =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb34-7" data-line-number="7">    A_ =<span class="st"> </span>A</a>
<a class="sourceLine" id="cb34-8" data-line-number="8">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb34-9" data-line-number="9">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb34-10" data-line-number="10">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalues, err))</a>
<a class="sourceLine" id="cb34-11" data-line-number="11">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb34-12" data-line-number="12">            QR =<span class="st"> </span><span class="kw">qr_decomposition_by_givens</span>(A) <span class="co"># from Lin Algebra Ch</span></a>
<a class="sourceLine" id="cb34-13" data-line-number="13">            A        =<span class="st"> </span><span class="kw">t</span>(QR<span class="op">$</span>Q) <span class="op">%*%</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>QR<span class="op">$</span>Q</a>
<a class="sourceLine" id="cb34-14" data-line-number="14">            evalues  =<span class="st"> </span><span class="kw">diag</span>(QR<span class="op">$</span>R)</a>
<a class="sourceLine" id="cb34-15" data-line-number="15">            ls_ev    =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(evalues<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb34-16" data-line-number="16">            ls_ov    =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(old_evalues<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb34-17" data-line-number="17">            err      =<span class="st"> </span>(ls_ev <span class="op">-</span><span class="st"> </span>ls_ov) <span class="op">/</span><span class="st"> </span>ls_ev</a>
<a class="sourceLine" id="cb34-18" data-line-number="18">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalues,  err))</a>
<a class="sourceLine" id="cb34-19" data-line-number="19">            <span class="cf">if</span> (<span class="kw">abs</span>(err) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span>  </a>
<a class="sourceLine" id="cb34-20" data-line-number="20">        }</a>
<a class="sourceLine" id="cb34-21" data-line-number="21">        old_evalues =<span class="st"> </span>evalues </a>
<a class="sourceLine" id="cb34-22" data-line-number="22">    }</a>
<a class="sourceLine" id="cb34-23" data-line-number="23">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;J&quot;</span>, <span class="kw">paste</span>(<span class="st">&quot;eigenval&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  </a>
<a class="sourceLine" id="cb34-24" data-line-number="24">                           <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb34-25" data-line-number="25">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;matrix&quot;</span>=<span class="st"> </span>A_, <span class="st">&quot;eigenvalues&quot;</span>=evalues)</a>
<a class="sourceLine" id="cb34-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb34-27" data-line-number="27">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb34-28" data-line-number="28"><span class="kw">qr_method</span>(A)</a></code></pre></div>
<pre><code>## $Iteration
##       J eigenval1 eigenval2  eigenval3        error
##  [1,] 0  0.000000  0.000000  0.0000000 0.000000e+00
##  [2,] 1  3.741657  3.927922 -0.8164966 1.000000e+00
##  [3,] 2  8.489489  2.227705 -0.6345152 3.765876e-01
##  [4,] 3 10.404981  1.993459 -0.5785391 1.706118e-01
##  [5,] 4 10.635189  1.907776 -0.5914371 1.950956e-02
##  [6,] 5 10.669923  1.915441 -0.5871527 3.247539e-03
##  [7,] 6 10.675897  1.910184 -0.5884393 4.617532e-04
##  [8,] 7 10.676959  1.911289 -0.5880405 1.120004e-04
##  [9,] 8 10.677149  1.910858 -0.5881629 1.078783e-05
## [10,] 9 10.677183  1.910974 -0.5881252 4.771086e-06
## 
## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $eigenvalues
## [1] 10.6771829  1.9109740 -0.5881252</code></pre>

<p>As long as <strong>QR Method</strong> yields all the <strong>Eigenvalues</strong>, this saves us from composing the <strong>characteristic polynomials</strong> using determinants. From here, we plug the individual <strong>Eigenvalues</strong> into the original matrix and form each corresponding <strong>characteristic matrix</strong>. Then, we derive the <strong>RREF</strong> of each <strong>characteristic matrix</strong> and perform a substitution to get the corresponding <strong>Eigenvectors</strong>.</p>
<p>Note that <strong>QR iteration</strong> may not necessarily converge for <strong>Eigenvalues</strong> that are negative or for matrices with complex numbers. For this, workarounds such as <strong>conjugate shifts</strong> are being used.</p>
<p>Also, note that other <strong>QR decomposition</strong> such as <strong>Householder</strong> can be used and evaluated, though best to play around with the â-signâ in <strong>Householder</strong> as that tends to vary the signs of <strong>Eigenvalues</strong> <span class="citation">(Anley E. F. <a href="bibliography.html#ref-ref39e">2016</a>)</span>.</p>
</div>
<div id="jacobi-eigenvalue-method-using-jacobi-rotation" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.5</span> Jacobi Eigenvalue Method (using Jacobi Rotation)<a href="numericallinearalgebra.html#jacobi-eigenvalue-method-using-jacobi-rotation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Jacobi Eigenvalue Method</strong> reduces a <strong>real symmetric</strong> matrix into its <strong>diagonal</strong> form. This reduction is an orthogonalization transformation called <strong>Jacobi rotation</strong> based on <strong>Givens rotation</strong> <span class="citation">(Heath M.T. <a href="bibliography.html#ref-ref187m">2002</a>; Burden R.L. et al. <a href="bibliography.html#ref-ref196r">2005</a>)</span>.</p>
<p><span class="math display">\[
G = 
\left[
\begin{array}{rr}
cos &amp; -sin \\
sin &amp; cos
\end{array}
\right]
\]</span></p>
<p>Recall that <strong>Givens rotation</strong> is used in <strong>QR decomposition</strong> to transform a matrix into its upper triangular form by annihilating its lower triangular region, excluding the diagonal entries. In the same fashion, the <strong>Jacobi Eigenvalue Method</strong> uses <strong>Jacobi rotation</strong> to annihilate non-diagonal entries, leaving a diagonal with <strong>Eigenvalues</strong> as entries.</p>
<p>To illustrate, here is a simple symmetric matrix:</p>
<p><span class="math display">\[
A = 
\left[
\begin{array}{rrrrr}
a_{ii} &amp; a_{ij}\\
a_{ji} &amp; a_{jj}
\end{array}
\right] =
\left[
\begin{array}{rrrrr}
9 &amp; 1\\
1 &amp; 9
\end{array}
\right]
\]</span></p>
<p>First, we compute for the angle - <span class="math inline">\(\theta\)</span> - that we need to plug into our <strong>cos</strong> and <strong>sin</strong>:</p>
<p><span class="math display">\[\begin{align*}
tan 2\theta {}&amp;= \frac{2A_{ij}}{(A_{jj} - A_{ii})} 
= \frac{2(1)}{9 - 9)}  = \infty \\
\theta &amp;= \frac{1}{2} arctan(\infty) = -0.7853982
\end{align*}\]</span></p>
<p>thus, we get:</p>
<p><span class="math display">\[\begin{align*}
cos (\theta) {}&amp;= cos(-0.7853982) = 0.7071068 \\
sin (\theta) &amp;= sin(-0.7853982) = -0.7071068
\end{align*}\]</span></p>
<p>We then construct the <strong>Jacobi rotation matrix</strong>:</p>
<p><span class="math display">\[
J(i,j) = 
\left[
\begin{array}{rr}
cos &amp; sin \\
-sin &amp; cos
\end{array}
\right] = 
\left[
\begin{array}{rrrrr}
0.7071608 &amp; -0.7071608 \\
0.7071608 &amp; 0.7071608
\end{array}
\right]
\]</span></p>
<p>Then, using <strong>Jacobi rotation matrix</strong>, we transform matrix <span class="math inline">\(A\)</span>, into a <strong>diagonal matrix</strong>, <span class="math inline">\(A&#39;\)</span>:</p>
<p><span class="math display">\[\begin{align*}
A&#39; = J^TAJ {}&amp;= 
\left[
\begin{array}{rrrrr}
cos &amp; -sin \\
sin &amp; cos
\end{array}
\right]_{J^T}
\left[
\begin{array}{rrrrr}
9 &amp; 1 \\
1 &amp; 9
\end{array}
\right]_A
\left[
\begin{array}{rrrrr}
cos &amp; sin \\
-sin &amp; cos
\end{array}
\right]_J \\
&amp;=
\left[
\begin{array}{rrrrr}
0.7071608 &amp; 0.7071608 \\
-0.7071608 &amp; 0.7071608
\end{array}
\right]
\left[
\begin{array}{rrrrr}
9 &amp; 1 \\
1 &amp; 9
\end{array}
\right]
\left[
\begin{array}{rrrrr}
0.7071608 &amp; -0.7071608 \\
0.7071608 &amp; 0.7071608
\end{array}
\right] \\
A&#39; = D &amp;=
\left[
\begin{array}{rrrrr}
10 &amp; 0 \\
0 &amp; 8
\end{array}
\right]
\end{align*}\]</span></p>
<p><strong>In general</strong>, the algorithm is described simply by the following equation:</p>
<p><span class="math display">\[\begin{align}
A&#39; = J^TAJ\ \leftarrow \text{iterate until}\ A&#39; = D \label{eqn:eqnnumber6}
\end{align}\]</span></p>
<p>We iterate until <span class="math inline">\(A&#39;\)</span> transforms into a diagonal matrix.</p>
<p>Here <strong>J</strong> is the <strong>Jacobi rotation matrix</strong>:</p>
<p><span class="math display">\[
J(i,j) = 
\left[
\begin{array}{rrrrr}
1 &amp; . &amp; . &amp; . &amp; . \\
. &amp; c_{ii} &amp; . &amp; s_{ji} &amp; . \\
. &amp; . &amp; 1 &amp; . &amp; . \\
. &amp; -s_{ij} &amp; . &amp; c_{jj} &amp; . \\
. &amp; . &amp; . &amp; . &amp; 1 \\
\end{array}
\right]
\]</span></p>
<p>To construct the <strong>Jacobi rotation matrix</strong>, we need to identify or select a region in the matrix using <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> indices. Below are six choices we can make to choose a region from which to base our <strong>rotation matrix</strong>.</p>
<p><span class="math display">\[
\begin{array}{l}
\left[\begin{array}{ccccc}
\square  &amp; \square  &amp; . &amp; .\\
\square  &amp; \square  &amp;. &amp; .\\
.  &amp; . &amp; . &amp; .\\
. &amp; .  &amp; . &amp; .
\end{array}\right]_{i=1, j=2}
\left[\begin{array}{ccccc}
.  &amp; . &amp; . &amp; .\\
. &amp; \square  &amp; \square &amp; . \\
. &amp; \square &amp; \square &amp; . \\
.  &amp; . &amp; . &amp; .
\end{array}\right]_{i=2,j=2}
\left[\begin{array}{ccccc}
.  &amp; . &amp; . &amp; .\\
.  &amp; . &amp; . &amp; .\\
. &amp;. &amp; \square &amp; \square \\
. &amp; . &amp; \square &amp; \square  \\
\end{array}\right]_{i=3,j=4} \\
\\
\left[\begin{array}{ccccc}
\square  &amp; \square &amp; \square &amp;.\\
\square &amp; . &amp;   \square &amp; .\\
\square &amp; \square&amp;  \square &amp; . \\
.  &amp; . &amp; . &amp; .
\end{array}\right]_{i=1,j=3}
\left[\begin{array}{ccccc}
.  &amp; . &amp; . &amp; .\\
.&amp; \square &amp; \square &amp; \square \\
. &amp; \square  &amp; . &amp;   \square \\
. &amp; \square &amp; \square &amp;  \square  \\
\end{array}\right]_{i=2,j=4}
\left[\begin{array}{ccccc}
\square &amp; \square &amp; \square &amp; \square  \\
\square  &amp; . &amp; . &amp;  \square\\
\square &amp; .  &amp; . &amp; \square\\
\square &amp; \square&amp; \square &amp;  \square \\
\end{array}\right]_{i=1,j=4}
\end{array}
\]</span></p>
<p>For example, if we are to select a region with <span class="math inline">\(i=1, j=3\)</span>, then our <strong>Jacobi rotation matrix</strong> becomes:</p>
<p><span class="math display">\[
J(i=1,j=3) = 
\left[
\begin{array}{rrrrr}
c_{ii} &amp; . &amp; s_{ji} &amp; . \\ 
. &amp; 1 &amp; . &amp; . \\
-s_{ij} &amp; . &amp; c_{jj} &amp; . \\
. &amp; . &amp; . &amp; 1
\end{array}
\right]_{i=1, j=3}
\]</span></p>
<p>The most simple selection is based on searching for a region with the largest off-diagonal entry:</p>
<p><span class="math display">\[
max\{A\} = max_{i&lt;j}\{\|A_{ij}\|\}
\]</span></p>
<p>This allows for faster convergence. The <strong>stop</strong> is when the max is less or equal to a tolerance level (e.g., in our case <span class="math inline">\(\text{1e-5}\)</span>). Unfortunately, it becomes a challenge to perform a search against a matrix with extreme high-dimension. In that case, we can use the <strong>Cyclic Jacobi Method</strong>. We leave this method to the reader to investigate. In our illustration below, we use a simple selection algorithm for the region for the <strong>Jacobi rotation matrix</strong>.</p>
<p>Now, assume a selected region with indices <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, our next step is then to compute for both <span class="math inline">\(c = cos\)</span> and <span class="math inline">\(s=sin\)</span>:</p>
<p><span class="math display">\[\begin{align*}
tan 2\theta {}&amp;= \left(\frac{2A_{ij}}{A_{jj}-A_{ii}}\right) \\
\theta  &amp;= \frac{1}{2} arctan\left(\frac{2A_{ij}}{A_{jj}-A_{ii}}\right) \\
c  &amp;= cos(\theta) \\
s  &amp;= sin(\theta) \\
\end{align*}\]</span></p>
<p>For a more stable solution, we can also perform trigonometric manipulation to compute for <strong>cos</strong> and <strong>sin</strong> given <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[\begin{align*}
 \\
t {}&amp;= \frac{sign(\theta)}{|\theta| + \sqrt{\theta^2 + 1}}\ \ \ \leftarrow \ \ \ t^2 + 2\theta t - 1 = 0\\
c &amp;= \frac{1}{\sqrt{t^2 + 1}} \\
s &amp;= ct
\end{align*}\]</span></p>
<p>From there, we construct the <span class="math inline">\(J\)</span> matrix and use that to approximate a <strong>diagonal</strong> matrix using Equation <span class="math inline">\(\ref{eqn:eqnnumber6}\)</span> or perhaps to validate, we can use the following list of equations to construct <span class="math inline">\(A&#39;\)</span>:</p>
<p><span class="math display">\[\begin{align}
A&#39;_{ii} {}&amp;= c^2A_{ii} - 2csA_{ij} + s^2A_{jj} \\
A&#39;_{jj} &amp;= s^2A_{ii} + 2csA_{ij} + c^2A_{jj} \\
A&#39;_{ij} &amp;= A&#39;_{ji} = (c^2 - s^2)A_{ij} + cs(A_{ii} - A_{jj}) = 0\\
A&#39;_{ik} &amp;= A&#39;_{ki} = cA_{ki} - sA_{kj} \rightarrow for\ k\ \neq i\ and\ k \neq j \\
A&#39;_{jk} &amp;= A&#39;_{kj} = sA_{ki} + cA_{kj} \rightarrow for\ k\ \neq i\ and\ k \neq j  
\end{align}\]</span></p>
<p>Note that the diagonal form may not readily be apparent the first time we compute for <span class="math inline">\(A&#39;\)</span>. Hence, it may take a few iterations to get an <strong>approximation</strong> of a diagonal form in which the entries are the <strong>Eigenvalues</strong> of the symmetric matrix. For example:</p>
<p><span class="math display">\[\begin{align*}
A_1 {}&amp;= J^T_0A_0J_0 \rightarrow choose\ i,j\ to\ construct\ J_0\\
A_2 &amp;= J_1^TA_1J_1 \rightarrow choose\ i,j\ to\ construct\ J_1\\
&amp;\vdots \\
D &amp;= J_k^TA_kJ_k \rightarrow choose\ i,j\ to\ construct\ J_k 
\end{align*}\]</span></p>
<p>After every step in the <strong>Jacobi iteration</strong> method, the off-diagonal elements, <span class="math inline">\(A&#39;_{ij}\)</span> and <span class="math inline">\(A&#39;_{ji}\)</span>, are annihilated (turn into zeros).</p>
<p>We illustrate using the following symmetric matrix:</p>
<p><span class="math display">\[
A = 
\left[
\begin{array}{rrrr}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 9 &amp; 6 &amp; 3 \\
3 &amp; 6 &amp; 9 &amp; 2 \\
4 &amp; 3 &amp; 2 &amp; 1
\end{array}
\right]
\]</span></p>
<p><strong>For the first Iteration</strong>, let us formulate our first <span class="math inline">\(J_1\)</span> matrix and <span class="math inline">\(A_1\)</span> matrix. We see six being a max off-diagonal entry in <span class="math inline">\(A_{i=2,j=3}\)</span>. The corresponding <strong>Jacobi rotation matrix</strong> is:</p>
<p><span class="math display">\[
J_1 = 
\left[
\begin{array}{rrrr}
1 &amp; . &amp; . &amp; . \\
. &amp; 0.7071068 &amp; 0.7071068 &amp; . \\
. &amp; -0.7071068 &amp; 0.7071068 &amp; . \\
. &amp; . &amp; . &amp; 1
\end{array}
\right]
\]</span></p>
<p>We end up with matrix <span class="math inline">\(A_1\)</span>:</p>

<p><span class="math display">\[
A_1 = J_1^TAJ_1 = 
\left[
\begin{array}{rrrr}
1.0000000 &amp; -7.071068e-01 &amp; 3.535534e+00 &amp; 4.0000000 \\
-0.7071068 &amp; 3.000000e+00 &amp; 1.110223e-15 &amp; 0.7071068 \\
3.5355339 &amp; 8.881784e-16 &amp; 1.500000e+01 &amp; 3.5355339 \\
4.0000000 &amp; 7.071068e-01 &amp; 3.535534e+00 &amp; 1.0000000
\end{array}
\right]
\]</span>
</p>
<p><strong>For the second Iteration</strong>, we construct our next <span class="math inline">\(J_2\)</span> matrix and <span class="math inline">\(A_2\)</span> matrix. We see four being a max off-diagonal entry in <span class="math inline">\(A_{i=1,j=4}\)</span>. The corresponding <strong>Jacobi rotation matrix</strong> is:</p>
<p><span class="math display">\[
J_2 = 
\left[
\begin{array}{rrrr}
0.7071068  &amp; . &amp; . &amp; 0.7071068  \\
. &amp; 1 &amp; . &amp; . \\
. &amp; . &amp; 1 &amp; . \\
-0.7071068  &amp; . &amp; . &amp; 0.7071068 
\end{array}
\right]
\]</span></p>
<p>We end up with matrix <span class="math inline">\(A_2\)</span>:</p>

<p><span class="math display">\[
A_2 = J_2^TA_1J_2 = 
\left[
\begin{array}{rrrr}
-3.000000e+00 &amp; -1.000000e+00 &amp; 4.440892e-16 &amp; 4.440892e-16 \\
-1.000000e+00 &amp; 3.000000e+00 &amp; 1.110223e-15 &amp; 5.551115e-16 \\
4.440892e-16 &amp; 8.881784e-16 &amp; 1.500000e+01 &amp; 5.000000e+00 \\
8.881784e-16 &amp; 5.551115e-16 &amp; 5.000000e+00 &amp; 5.000000e+00
\end{array}
\right]
\]</span>
</p>
<p>We continue until we hit the fifth iteration. The <strong>stop</strong> is when the search for max off-diagonal entry ends at zero.</p>
<p>We end up with the final matrix <span class="math inline">\(A_5\)</span>:</p>
<p><span class="math display">\[
A_5 = J_5^TA_4J_5 = 
\left[
\begin{array}{rrrr}
-3.162278 &amp; . &amp; . &amp; . \\
. &amp; 3.162278 &amp; . &amp; . \\
. &amp; . &amp; 17.07107 &amp; . \\
. &amp; . &amp; . &amp; 2.928932
\end{array}
\right]
\]</span></p>
<p>Hence, the approximate <strong>Eigenvalues</strong> are (sorted in descending order):</p>
<p><span class="math display">\[
\tilde \lambda_1=17.07107 \ \ \ \ 
\tilde \lambda_2=3.162278\ \ \ \ \ 
\tilde \lambda_3=2.928932\ \ \ \ \ 
\tilde \lambda_4=-3.162278
\]</span>
It is notable to mention that there are applications of the algorithm in which we may not require to seek all of the eigenvalues. For example, suppose a matrix is extremely large. It may be enough to get the first few of them using a different <strong>stopping</strong> criterion versus just evaluating a max off-diagonal entry.</p>
<p>Finally, we illustrate a naive implementation of the <strong>Jacobi Eigenvalue Method</strong> algorithm in R code:</p>

<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1">offdiagonal_maxsearch &lt;-<span class="st"> </span><span class="cf">function</span>(A, tol) {</a>
<a class="sourceLine" id="cb36-2" data-line-number="2">    n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb36-3" data-line-number="3">    m =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb36-4" data-line-number="4">    max =<span class="st"> </span>tol</a>
<a class="sourceLine" id="cb36-5" data-line-number="5">    index =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb36-6" data-line-number="6">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb36-7" data-line-number="7">        <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb36-8" data-line-number="8">            <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>j) {</a>
<a class="sourceLine" id="cb36-9" data-line-number="9">                <span class="cf">if</span> ( max <span class="op">&lt;</span><span class="st"> </span><span class="kw">abs</span>( A[i,j]) ) {</a>
<a class="sourceLine" id="cb36-10" data-line-number="10">                    <span class="cf">if</span> (i <span class="op">&lt;</span><span class="st"> </span>j) {</a>
<a class="sourceLine" id="cb36-11" data-line-number="11">                        max =<span class="st"> </span><span class="kw">abs</span>( A[i,j] )</a>
<a class="sourceLine" id="cb36-12" data-line-number="12">                        index =<span class="st"> </span><span class="kw">c</span>(i,j)</a>
<a class="sourceLine" id="cb36-13" data-line-number="13">                    }</a>
<a class="sourceLine" id="cb36-14" data-line-number="14">                }</a>
<a class="sourceLine" id="cb36-15" data-line-number="15">            }</a>
<a class="sourceLine" id="cb36-16" data-line-number="16">        }</a>
<a class="sourceLine" id="cb36-17" data-line-number="17">    }</a>
<a class="sourceLine" id="cb36-18" data-line-number="18">    i =<span class="st"> </span><span class="kw">min</span>(index)</a>
<a class="sourceLine" id="cb36-19" data-line-number="19">    j =<span class="st"> </span><span class="kw">max</span>(index)</a>
<a class="sourceLine" id="cb36-20" data-line-number="20">    <span class="kw">return</span>( <span class="kw">list</span>(<span class="st">&quot;max&quot;</span>=max, <span class="st">&quot;i&quot;</span>=i, <span class="st">&quot;j&quot;</span>=j) )</a>
<a class="sourceLine" id="cb36-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb36-22" data-line-number="22">rotation_matrix &lt;-<span class="st"> </span><span class="cf">function</span>(i,j, A) {</a>
<a class="sourceLine" id="cb36-23" data-line-number="23">    R =<span class="st">  </span><span class="kw">diag</span>(<span class="kw">ncol</span>(A))</a>
<a class="sourceLine" id="cb36-24" data-line-number="24">    theta =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">atan</span>( <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>A[i,j] <span class="op">/</span><span class="st"> </span>( A[j,j] <span class="op">-</span><span class="st"> </span>A[i,i]))</a>
<a class="sourceLine" id="cb36-25" data-line-number="25">    cos =<span class="st"> </span><span class="kw">cos</span>(theta) </a>
<a class="sourceLine" id="cb36-26" data-line-number="26">    sin =<span class="st"> </span><span class="kw">sin</span>(theta)</a>
<a class="sourceLine" id="cb36-27" data-line-number="27">    R[i,i] =<span class="st"> </span>cos; R[i,j] =<span class="st"> </span>sin; R[j,i] =<span class="st"> </span><span class="op">-</span>sin; R[j,j] =<span class="st"> </span>cos</a>
<a class="sourceLine" id="cb36-28" data-line-number="28">    <span class="kw">list</span>(<span class="st">&quot;J&quot;</span>=R, <span class="st">&quot;c&quot;</span>=cos, <span class="st">&quot;s&quot;</span>=sin)</a>
<a class="sourceLine" id="cb36-29" data-line-number="29">}</a>
<a class="sourceLine" id="cb36-30" data-line-number="30">jacobi_eigenvalue_method &lt;-<span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb36-31" data-line-number="31">    iterate =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb36-32" data-line-number="32">    tol =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb36-33" data-line-number="33">    limit=<span class="dv">100</span></a>
<a class="sourceLine" id="cb36-34" data-line-number="34">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb36-35" data-line-number="35">      S =<span class="st"> </span><span class="kw">offdiagonal_maxsearch</span>(A, tol)</a>
<a class="sourceLine" id="cb36-36" data-line-number="36">      <span class="cf">if</span> (S<span class="op">$</span>max <span class="op">&lt;=</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb36-37" data-line-number="37">      R =<span class="st"> </span><span class="kw">rotation_matrix</span>(S<span class="op">$</span>i,S<span class="op">$</span>j, A)</a>
<a class="sourceLine" id="cb36-38" data-line-number="38">      A =<span class="st"> </span><span class="kw">t</span>(R<span class="op">$</span>J) <span class="op">%*%</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>R<span class="op">$</span>J</a>
<a class="sourceLine" id="cb36-39" data-line-number="39">      iterate =<span class="st"> </span>k</a>
<a class="sourceLine" id="cb36-40" data-line-number="40">    }</a>
<a class="sourceLine" id="cb36-41" data-line-number="41">    <span class="kw">list</span>(<span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;iterate&quot;</span>=k, </a>
<a class="sourceLine" id="cb36-42" data-line-number="42">         <span class="st">&quot;eigenvalues&quot;</span>=<span class="kw">sort</span>(<span class="kw">diag</span>(A),  <span class="dt">decreasing=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb36-43" data-line-number="43">}</a>
<a class="sourceLine" id="cb36-44" data-line-number="44">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,  <span class="dv">2</span>,<span class="dv">9</span>,<span class="dv">6</span>,<span class="dv">3</span>,  <span class="dv">3</span>,<span class="dv">6</span>,<span class="dv">9</span>,<span class="dv">2</span>,  <span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>), <span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb36-45" data-line-number="45"><span class="kw">jacobi_eigenvalue_method</span>(A)</a></code></pre></div>
<pre><code>## $matrix
##               [,1]         [,2]         [,3]          [,4]
## [1,] -3.162278e+00 0.000000e+00 7.710670e-16  2.513307e-16
## [2,]  0.000000e+00 3.162278e+00 1.129214e-15  4.835791e-17
## [3,]  9.059580e-16 8.994983e-16 1.707107e+01 -8.881784e-16
## [4,]  6.699290e-16 6.651312e-17 0.000000e+00  2.928932e+00
## 
## $iterate
## [1] 5
## 
## $eigenvalues
## [1] 17.071068  3.162278  2.928932 -3.162278</code></pre>

<p>We validate the result using <strong>eigen(.)</strong> function like so:</p>

<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="kw">eigen</span>(A)<span class="op">$</span>values</a></code></pre></div>
<pre><code>## [1] 17.071068  3.162278  2.928932 -3.162278</code></pre>

</div>
<div id="arnoldi-method-using-gram-schmidt-in-krylov-subspace" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.6</span> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) <a href="numericallinearalgebra.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Arnoldi Method</strong> is one of the orthogonal projection methods of finding the approximation of both <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong>. To understand the idea behind this method, let us discuss what we are projecting orthogonally using Figure <a href="numericallinearalgebra.html#fig:krylovspace">3.1</a>. The left side portion of the figure covers generalized minimal residual (GMRES) and Conjugate Gradient(CG) for solving systems of linear equations. But here, our discussion is about the right side portion, which covers solving for <strong>Eigenvalue</strong> problems <span class="citation">(Heath M.T. <a href="bibliography.html#ref-ref187m">2002</a>; Sleijpen G. <a href="bibliography.html#ref-ref61g">2014</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:krylovspace"></span>
<img src="krylovspace.png" alt="Orthogonal Projection unto Krylov Space" width="90%" />
<p class="caption">
Figure 3.1: Orthogonal Projection unto Krylov Space
</p>
</div>
<p>Recall the <strong>Eigen</strong> equation (<span class="math inline">\(A \cdotp \mathbf{\vec{v}} = \lambda \times \mathbf{\vec{v}}\)</span>) under Section <strong>Eigenvectors and Eigenvalues</strong> in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>). Here, we emphasis the actual <strong>Eigenvectors</strong> denoted as <strong>v</strong> and <strong>Eigenvalues</strong>, <span class="math inline">\(\lambda\)</span>. Equivalently, we can designate a notation denoting approximation of <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong>:</p>
<p><span class="math display">\[\begin{align}
A \tilde u = \tilde \lambda \tilde u 
\end{align}\]</span></p>
<p>Then we can find a subspace, <strong>K</strong>, into which we can project <span class="math inline">\(A \tilde u\)</span> such that the equation becomes:</p>
<p><span class="math display">\[\begin{align}
Proj_{(K)}(A \tilde u)\ \ \ \ \ \rightarrow \ \ \ \  (A \tilde u - \tilde \lambda \tilde u) \perp K
\end{align}\]</span></p>
<p>Here, we introduce the <strong>Krylov subspace</strong> , K, such that <span class="math inline">\((A \tilde u - \tilde \lambda \tilde u)\)</span> is orthogonal to K. For more intuition about <strong>Krylov subspace</strong>, it may also help to review Section <strong>Krylov Method</strong>.</p>
<p>Given a system of linear equations in matrix form, <span class="math inline">\(Ax = b\)</span>, the <strong>Krylov subspace</strong> is a subspace spanned by {Ab + AAb + AAAb + â¦ + AAAâ¦Ab} , which can be expressed this way (m-th order of Krylov sequence):</p>
<p><span class="math display">\[\begin{align}
K_m(A,b) = span\ \{b,\ Ab,\ A^2b,\ A^3b,\ ...,\ A^{m-1}b\} 
\end{align}\]</span></p>
<p>In the same manner, in this particular case for eigenvalue problems, <strong>Krylov subspace</strong> is spanned by a linear combination like so (using an orthogonal basis, <strong>Q</strong>):</p>
<p><span class="math display">\[\begin{align}
K_m(A,q) =  span\ \{ q_1,\ Aq_2,\ A^2q_3, ...,\ A^{m-1}q_m \}
\end{align}\]</span></p>
<p>We derive a change of <strong>basis</strong> called the <strong>Arnoldi basis</strong> from the subspace, <strong>K</strong>. And we let this basis be an orthogonal matrix denoted as <strong>Q</strong>:</p>
<p><span class="math display">\[\begin{align}
Q_{basis} = \{ \ q_1,\ q_2,\ q_3, ...,\ q_{m}\ \} 
\end{align}\]</span></p>
<p>See <strong>Krylov Methods</strong> and <strong>GMRES</strong> in a few sections ahead, covering an example of the construction of the Q basis for the <strong>K</strong> space.</p>
<p>The <strong>basis</strong> is derived by <strong>Arnoldi Method</strong>, decomposing an <span class="math inline">\(n \times n\)</span> square matrix, <strong>A</strong>, into the form <span class="math inline">\(QHQ^T\)</span>. The algorithm also shows the combined <span class="math inline">\(AQ\)</span> equal to a combined upper Hessenberg matrix, <strong>H</strong>, and a basis matrix, <strong>Q</strong>, expressed as:</p>
<p><span class="math display">\[\begin{align}
AQ_m = Q_{m+1}H_m \ \ \ \ \ \rightarrow \ \ \ \ \ \ (AQ - QH) \perp K
\end{align}\]</span></p>
<p>We show the equation in matrix forms:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrrrr}
&amp; &amp;   \\
&amp; &amp;  \\
 &amp; A_{mxm} &amp; \\
&amp; &amp;  \\
&amp; &amp;  \\
\end{array}
\right]
\left[
\begin{array}{rrrr}
&amp; &amp; \\
&amp; &amp; \\
q1 &amp; ... &amp;  q_m \\
&amp; &amp; \\
&amp; &amp; \\
\end{array}
\right] =
\left[
\begin{array}{rrrr}
&amp; &amp; \\
&amp; &amp; \\
q1 &amp; ... &amp; q_{m+1} \\
&amp; &amp; \\
&amp; &amp; \\
\end{array}
\right]
\left[
\begin{array}{llll}
h_{1,1} &amp; h_{1,2} &amp;  \ldots &amp; h_{1,m}  \\
\alpha_{2,1} &amp; h_{2,2} &amp;  \ldots &amp; h_{2,m} \\
. &amp;\alpha_{3,2} &amp;  \ldots &amp; h_{3,m}  \\
. &amp; . &amp;  \ddots &amp; \vdots \\
. &amp; . &amp;  . &amp; \alpha_{m+1,m}  \\
\end{array}
\right]
\]</span>
</p>
<p>Based on the equation, we can derive the equation for any column. For example, for the 2nd column and 3rd columns, we can use the following equations to compute for <span class="math inline">\(Aq_2\)</span> and <span class="math inline">\(Aq_3\)</span> respectively:</p>
<p><span class="math display">\[
Aq_2 =  h_{1,2}q_1 +  h_{2,2}q_2 + \alpha_{3,2}q_3\ \ \ \ \ \ \ \ \ \
Aq_3 =  h_{1,3}q_1 +  h_{2,3}q_2 + h_{3,3}q_3 + \alpha_{4,3}q_4
\]</span></p>
<p>where <span class="math inline">\(\alpha_{j+1,j} = \| Aq_j - \sum_k(h_{k,j}q_j) \|_{L2}\)</span>.</p>
<p>On the other hand, using the <strong>Arnoldi basis</strong>, <strong>Q</strong>, we can also generate the <strong>upper Hessenberg</strong> matrix like so:</p>
<p><span class="math display">\[\begin{align}
H_m = Q_{m}^TAQ_m \label{eqn:eqnnumber7}
\end{align}\]</span></p>
<p>This can be interpreted as <strong>H</strong> being the projection of <strong>A</strong> onto <strong>K</strong>.</p>
<p>Now, given <span class="math inline">\(Q\)</span> and the approximation of <strong>Eigenvalues</strong> and <strong>Eigenvectors</strong>, we can perform a mathematical conversion:</p>
<p><span class="math display">\[\begin{align}
A \tilde u = \tilde \lambda \tilde u\ \rightarrow\ \ \ \ Q^TAQ \tilde u = \lambda Q^TQ \tilde u \rightarrow\ \ \ \ H \tilde u = \tilde \lambda \tilde u 
\end{align}\]</span></p>
<p>This pair <span class="math inline">\(\tilde \lambda \tilde u\)</span> is called the <strong>Ritz</strong> pair. We can find the <strong>Ritz value</strong>, <span class="math inline">\(\tilde \lambda\)</span>, and <strong>Ritz vector</strong>, <span class="math inline">\(\tilde u\)</span>, using <strong>QR Method</strong> to solve the equation below (recall the equation <span class="math inline">\((A - \lambda I) \cdotp v = 0\)</span> under Section <strong>Eigenvectors and Eigenvalues</strong> in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>)):</p>
<p><span class="math display">\[
(H - \tilde \lambda I)\tilde u = 0
\]</span></p>
<p>That said, let us now review the following <strong>Arnoldi Method</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
v \leftarrow \text{initial arbitrary nonzero vector} \\
q_1 = v / \|v\|_{L2}\\
loop\ j\ in\ 1:m \\
\ \ \ \ v = Aq_j\\
\ \ \ \ loop\ k\ in\ 1:j \rightarrow \text{Gram-Schmidt} \\
\ \ \ \ \ \ \ \ h_{kj} = q_k^T v\\
\ \ \ \ \ \ \ \ v = v - h_{kj}q_k \\
\ \ \ \ end\ loop \\
\ \ \ \ h_{j+1,j} = \|v\|_{L2}\\
\ \ \ \ if\ (h_{j+1,j} &lt; tol\ )\ break \\
\ \ \ \ q_{j+1} = v / h_{j+1,j}\\
end\ loop
\end{array}
\]</span></p>
<p>We follow this with a naive implementation of <strong>Arnoldi Method</strong> in R code:</p>

<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1">arnoldi_method &lt;-<span class="st"> </span><span class="cf">function</span>(A, v) {</a>
<a class="sourceLine" id="cb40-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb40-3" data-line-number="3">  m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb40-4" data-line-number="4">  q =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">*</span>(m<span class="op">+</span><span class="dv">1</span>)), n, <span class="dt">byrow=</span><span class="ot">TRUE</span> ) <span class="co"># Q n x n+1</span></a>
<a class="sourceLine" id="cb40-5" data-line-number="5">  h =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, (m<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>m), (m<span class="op">+</span><span class="dv">1</span>), <span class="dt">byrow=</span><span class="ot">TRUE</span>) <span class="co"># H n+1 x n </span></a>
<a class="sourceLine" id="cb40-6" data-line-number="6">  q[,<span class="dv">1</span>] =<span class="st"> </span>v <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(v<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb40-7" data-line-number="7">  tol =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb40-8" data-line-number="8">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb40-9" data-line-number="9">    v =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>q[,j]</a>
<a class="sourceLine" id="cb40-10" data-line-number="10">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>j) { <span class="co"># Gram-Schmidt Orthogonalization</span></a>
<a class="sourceLine" id="cb40-11" data-line-number="11">        h[k,j] =<span class="st"> </span><span class="kw">t</span>(q[,k]) <span class="op">%*%</span><span class="st"> </span>v</a>
<a class="sourceLine" id="cb40-12" data-line-number="12">        v =<span class="st"> </span>v <span class="op">-</span><span class="st">  </span>h[k,j] <span class="op">*</span><span class="st"> </span>q[,k] <span class="co"># subtracting projection</span></a>
<a class="sourceLine" id="cb40-13" data-line-number="13">    }</a>
<a class="sourceLine" id="cb40-14" data-line-number="14">    h[j<span class="op">+</span><span class="dv">1</span>,j] =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(v<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb40-15" data-line-number="15">    <span class="cf">if</span> (<span class="kw">abs</span>( h[j<span class="op">+</span><span class="dv">1</span>,j]) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb40-16" data-line-number="16">    q[,j<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span>v <span class="op">/</span><span class="st"> </span>h[j<span class="op">+</span><span class="dv">1</span>,j]</a>
<a class="sourceLine" id="cb40-17" data-line-number="17">  }</a>
<a class="sourceLine" id="cb40-18" data-line-number="18">  h_m =<span class="st"> </span>h[<span class="dv">1</span><span class="op">:</span>m,<span class="dv">1</span><span class="op">:</span>m]; q_m =<span class="st"> </span>q[<span class="dv">1</span><span class="op">:</span>m,<span class="dv">1</span><span class="op">:</span>m]</a>
<a class="sourceLine" id="cb40-19" data-line-number="19">  <span class="kw">list</span>(<span class="st">&quot;Q&quot;</span> =<span class="st"> </span>q_m , <span class="st">&quot;H&quot;</span> =<span class="st"> </span>h_m,  </a>
<a class="sourceLine" id="cb40-20" data-line-number="20">       <span class="st">&quot;AQ&quot;</span> =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>q_m, <span class="st">&quot;QH&quot;</span> =<span class="st"> </span>q <span class="op">%*%</span><span class="st"> </span>h, </a>
<a class="sourceLine" id="cb40-21" data-line-number="21">       <span class="st">&quot;H=QtAQ&quot;</span> =<span class="st"> </span><span class="kw">t</span>(q_m) <span class="op">%*%</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>q_m,</a>
<a class="sourceLine" id="cb40-22" data-line-number="22">       <span class="st">&quot;A=QHQt&quot;</span>=<span class="st"> </span>q_m <span class="op">%*%</span><span class="st"> </span>h_m <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(q_m) )</a>
<a class="sourceLine" id="cb40-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb40-24" data-line-number="24">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb40-25" data-line-number="25">b=<span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">5</span>,<span class="dv">6</span>)</a>
<a class="sourceLine" id="cb40-26" data-line-number="26">(<span class="dt">arnoldi =</span> <span class="kw">arnoldi_method</span>(A, b))</a></code></pre></div>
<pre><code>## $Q
##           [,1]        [,2]       [,3]
## [1,] 0.6092077 -0.65018500 -0.4540104
## [2,] 0.5076731  0.75958120 -0.4065765
## [3,] 0.6092077  0.01720066  0.7928241
## 
## $H
##           [,1]     [,2]      [,3]
## [1,] 10.123711 3.127356 1.5019617
## [2,]  1.521378 1.194132 1.2436289
## [3,]  0.000000 1.649767 0.6821563
## 
## $AQ
##          [,1]      [,2]       [,3]
## [1,] 5.178265 0.3797906 -0.2032882
## [2,] 6.295146 1.8239581  1.4297940
## [3,] 6.193612 3.2337243  1.4772279
## 
## $QH
##          [,1]      [,2]       [,3]
## [1,] 5.178265 0.3797906 -0.2032882
## [2,] 6.295146 1.8239581  1.4297940
## [3,] 6.193612 3.2337243  1.4772279
## 
## $`H=QtAQ`
##              [,1]     [,2]      [,3]
## [1,] 1.012371e+01 3.127356 1.5019617
## [2,] 1.521378e+00 1.194132 1.2436289
## [3,] 3.941292e-15 1.649767 0.6821563
## 
## $`A=QHQt`
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1">(<span class="dt">eigenvalues =</span> <span class="kw">qr_method</span>(arnoldi<span class="op">$</span>H)<span class="op">$</span>eigenvalues)</a></code></pre></div>
<pre><code>## [1] 10.6771883  1.9107948 -0.5881801</code></pre>

<p>We validate the result using <strong>eigen(.)</strong> function like so:</p>

<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" data-line-number="1"><span class="kw">eigen</span>(A)<span class="op">$</span>values</a></code></pre></div>
<pre><code>## [1] 10.6771903  1.9109438 -0.5881341</code></pre>

<p>We can easily validate using the following equations (See also Equation <span class="math inline">\(\ref{eqn:eqnnumber7}\)</span>) below:</p>
<p><span class="math display">\[\begin{align}
H = Q^TAQ\ \ \ \ \ \ \ \ A=QHQ^T 
\end{align}\]</span></p>
<p>It is notable to mention that the <strong>Arnoldi Method</strong> tends to converge towards the largest <strong>Eigenvector</strong> similar to that of the <strong>Power Method</strong> and so is ill-conditioned. To work around that, <strong>Arnoldi Method</strong> uses <strong>Gram Schmidt orthogonalization</strong>, resulting in a more stable convergence, arising in a better conditioned Hessenberg matrix. Other literature may illustrate using other orthogonalization methods in place of <strong>Gram-Schmidt</strong> orthogonalization.</p>
</div>
<div id="lanczos-method-using-gram-schmidt-in-krylov-subspace" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.7</span> Lanczos Method (using Gram-Schmidt in Krylov Subspace)<a href="numericallinearalgebra.html#lanczos-method-using-gram-schmidt-in-krylov-subspace" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Lanczos Method</strong> method <span class="citation">(Lanczos, C. <a href="bibliography.html#ref-ref299c">1950</a>)</span> is useful for <strong>Symmetric Positive Definite (SPD) matrices</strong> and <strong>Tridiagonal Hermitian</strong> types of matrices, including highly sparse matrices. This method is also an orthogonal projection method similar to <strong>Arnoldi Method</strong> but it handles <strong>Hermitian matrices</strong>.  </p>
<p>We illustrate the method by starting with a <strong>Tridiagonal Hermitian matrix</strong> with the (super/sub)diagonal entries denoted by <span class="math inline">\(\beta\)</span>, and the main diagonal entries denoted by <span class="math inline">\(\alpha\)</span>.</p>
<p><span class="math display">\[
A = \left[
\begin{array}{rrrrr}
\alpha_1 &amp; \beta_1 &amp; . &amp; . &amp; . \\
\beta_1 &amp; \alpha_2 &amp; \beta_2 &amp; . &amp; . \\
. &amp; \beta_2 &amp;  \alpha_3  &amp; \ddots &amp; . \\
. &amp; . &amp; \ddots &amp; \ddots &amp; \beta_{j-1} \\
. &amp; . &amp; . &amp; \beta_{j-1} &amp; \alpha_j \\
\end{array}
\right]
\]</span></p>
<p>Now recall the equation below from <strong>Arnoldi</strong> method:</p>
<p><span class="math display">\[\begin{align}
AQ_m = Q_{m+1}H_m \ \ \ \ \ \rightarrow \ \ \ \ \ \ (AQ - QH) \perp K 
\end{align}\]</span></p>
<p>In <strong>Lanczos</strong> method, because we deal with <strong>SPD</strong> matrix, we have the following equation instead:</p>
<p><span class="math display">\[\begin{align}
AQ_m = Q_{m}T_m \ \ \ \ \ \rightarrow \ \ \ \ \ \ (AQ - QT) \perp K
\end{align}\]</span></p>
<p>where <span class="math inline">\(T\)</span> is an <strong>SPD Tridiagonal</strong> matrix.</p>
<p>Because we deal with a symmetric tridiagonal matrix, <span class="math inline">\(T\)</span>, and an orthogonal matrix <span class="math inline">\(Q^TQ = I\)</span>, we can therefore use the 3-term recurrence <span class="citation">(Parlett B. N. <a href="bibliography.html#ref-ref72b">1994</a>)</span>:</p>
<p><span class="math display">\[\begin{align*}
Aq_j {}&amp;= \beta_{(j-1)}q_{(j-1)} + \alpha_{(j)} q_{(j)} + \beta_{(j)} q_{(j+1)}
\end{align*}\]</span></p>
<p>Here follows the <strong>Lanczos Method</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
\mathbf{\text{Standard Lanczos}}\\
\\
\beta_0 = 0, q_0 = 0  \leftarrow \text{initial zero } \\
b \leftarrow \text{initial arbitrary nonzero vector} \\
q_1 = b / \|b\|_{L2}\\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ r = Aq_j \\
\ \ \ \ \alpha_j = q_j^T r \\
\ \ \ \ r = r -  \alpha_j q_j - \beta_{(j)}q_{(j-1)} \\ 
\ \ \ \ \beta_j = \|r\|_{L2} \\
\ \ \ \ if\ (\beta_j &lt; tol)\ break \\
\ \ \ \ q_{j+1} = r / \beta_j\\
end\ loop
\end{array}
\left|
\begin{array}{l}
\mathbf{\text{Chris Paige Proposal}}\\
\\
\beta_0 = 0, q_0 = 0  \leftarrow \text{initial zero } \\
b \leftarrow \text{initial arbitrary nonzero vector} \\
q_1 = b / \|b\|_{L2}\\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ r = Aq_j - \beta_{(j-1)}q_{(j-1)}\\
\ \ \ \ \alpha_j = q_j^T r \\
\ \ \ \ r = r -  \alpha_j q_j  \\ 
\ \ \ \ \beta_j = \|r\|_{L2} \\
\ \ \ \ if\ (\beta_j &lt; tol)\ break \\
\ \ \ \ q_{j+1} = r / \beta_j\\
end\ loop
\end{array}
\right.
\]</span></p>
<p>Note that, at each iteration in the vanilla Lanczos method, the equation <span class="math inline">\(r = Aq_j\)</span> creates a new basis vector and the equation <span class="math inline">\(r = r - \alpha_j q_j - \beta_{(j)}q_{(j-1)}\)</span> subtracts the new vector from its projection. The subtraction is a remedy for the loss of orthogonality for the Lanczos Method infinite precision <span class="citation">(Hoppe T., <a href="bibliography.html#ref-ref123t">n.d.</a>; Hoppe T., <a href="bibliography.html#ref-ref131t">n.d.</a>; Qianqian Yang <a href="bibliography.html#ref-ref1127y">2019</a>)</span></p>
<p>To illustrate, let us use our naive implementation of the <strong>Lanczos Method</strong> algorithm based on Paigeâs Proposal <span class="citation">(<a href="bibliography.html#ref-ref92c">1975</a>)</span>:</p>

<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb46-2" data-line-number="2">lanczos_method &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb46-3" data-line-number="3">  n      =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb46-4" data-line-number="4">  m     =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb46-5" data-line-number="5">  q     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, m<span class="op">*</span>n), m, <span class="dt">byrow=</span><span class="ot">TRUE</span> ) <span class="co"># Sparse Matrix for Q</span></a>
<a class="sourceLine" id="cb46-6" data-line-number="6">  beta  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb46-7" data-line-number="7">  alpha =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb46-8" data-line-number="8">  b     =<span class="st"> </span><span class="kw">rnorm</span>(n)  <span class="co"># initialize randomly</span></a>
<a class="sourceLine" id="cb46-9" data-line-number="9">  q[,<span class="dv">1</span>] =<span class="st"> </span>b <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(b<span class="op">^</span><span class="dv">2</span>)) <span class="co"># normalize</span></a>
<a class="sourceLine" id="cb46-10" data-line-number="10">  tol   =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb46-11" data-line-number="11">  prev.beta =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb46-12" data-line-number="12">  prev.q =<span class="st"> </span>q[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb46-13" data-line-number="13">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n)) {</a>
<a class="sourceLine" id="cb46-14" data-line-number="14">    r         =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>q[,j] <span class="op">-</span><span class="st"> </span>prev.beta <span class="op">*</span><span class="st"> </span>prev.q</a>
<a class="sourceLine" id="cb46-15" data-line-number="15">    alpha[j]  =<span class="st"> </span><span class="kw">t</span>(q[,j]) <span class="op">%*%</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb46-16" data-line-number="16">    r =<span class="st"> </span>r <span class="op">-</span><span class="st"> </span>alpha[j] <span class="op">*</span><span class="st"> </span>q[,j]  <span class="co"># subtract from its projection</span></a>
<a class="sourceLine" id="cb46-17" data-line-number="17">    beta[j]   =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(r<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb46-18" data-line-number="18">    <span class="cf">if</span> (j <span class="op">&gt;=</span>n <span class="op">||</span><span class="st"> </span><span class="kw">abs</span>(beta[j]) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb46-19" data-line-number="19">    q[,j<span class="op">+</span><span class="dv">1</span>]   =<span class="st"> </span>r <span class="op">/</span><span class="st"> </span>beta[j]   <span class="co"># then scale</span></a>
<a class="sourceLine" id="cb46-20" data-line-number="20">    prev.beta =<span class="st"> </span>beta[j]</a>
<a class="sourceLine" id="cb46-21" data-line-number="21">    prev.q    =<span class="st"> </span>q[,j]</a>
<a class="sourceLine" id="cb46-22" data-line-number="22">  }</a>
<a class="sourceLine" id="cb46-23" data-line-number="23">  T =<span class="st"> </span><span class="kw">t</span>(q) <span class="op">%*%</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>q</a>
<a class="sourceLine" id="cb46-24" data-line-number="24">  T[<span class="kw">which</span>(<span class="kw">abs</span>(T) <span class="op">&lt;</span><span class="st"> </span>tol) ] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb46-25" data-line-number="25">  <span class="kw">list</span>(<span class="st">&quot;matrix&quot;</span>=<span class="st"> </span>A , <span class="st">&quot;T&quot;</span>=<span class="st"> </span>T, <span class="st">&quot;Q&quot;</span> =<span class="st"> </span>q , <span class="st">&quot;beta&quot;</span> =<span class="st"> </span>beta, <span class="st">&quot;alpha&quot;</span> =<span class="st"> </span>alpha )</a>
<a class="sourceLine" id="cb46-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb46-27" data-line-number="27">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">5</span>, <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">5</span>),<span class="dv">5</span>,</a>
<a class="sourceLine" id="cb46-28" data-line-number="28">           <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb46-29" data-line-number="29">(<span class="dt">lanczos =</span> <span class="kw">lanczos_method</span>(A))</a></code></pre></div>
<pre><code>## $matrix
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    2    0    0    0
## [2,]    2    3    3    0    0
## [3,]    0    3    3    4    0
## [4,]    0    0    4    4    5
## [5,]    0    0    0    5    5
## 
## $T
##          [,1]     [,2]     [,3]      [,4]     [,5]
## [1,] 1.726379 5.939271 0.000000 0.0000000 0.000000
## [2,] 5.939271 5.223253 2.542124 0.0000000 0.000000
## [3,] 0.000000 2.542124 6.710107 1.2006169 0.000000
## [4,] 0.000000 0.000000 1.200617 0.8125495 1.296137
## [5,] 0.000000 0.000000 0.000000 1.2961366 1.527711
## 
## $Q
##             [,1]        [,2]       [,3]        [,4]        [,5]
## [1,] -0.32230088  0.07123367  0.3943700 -0.85660737 -0.04176164
## [2,]  0.09448167 -0.30542846  0.7022612  0.28996500 -0.56615912
## [3,] -0.42991811  0.50829179  0.4628082  0.39608594  0.43096916
## [4,]  0.82074751  0.16736559  0.3444932 -0.15551579  0.39433690
## [5,]  0.16952670  0.78438985 -0.1357740 -0.03278521 -0.58007225
## 
## $beta
## [1] 5.939271e+00 2.542124e+00 1.200617e+00 1.296137e+00 4.913656e-14
## 
## $alpha
## [1] 1.7263794 5.2232530 6.7101074 0.8125495 1.5277108</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" data-line-number="1">(<span class="dt">eigenvalues =</span> <span class="kw">qr_method</span>(lanczos<span class="op">$</span>T)<span class="op">$</span>eigenvalues)</a></code></pre></div>
<pre><code>## [1] 10.7729509  6.1028726  2.9759549  2.3984001  0.2983397</code></pre>

<p>We validate the result (in decreasing order using) <strong>eigen(.)</strong> function like so:</p>

<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" data-line-number="1"><span class="kw">sort</span>(<span class="kw">abs</span>(<span class="kw">eigen</span>(lanczos<span class="op">$</span>T)<span class="op">$</span>values), <span class="dt">decreasing=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 10.7729806  6.1028984  2.9759370  2.3983977  0.2983397</code></pre>

<p>We can easily use <span class="math inline">\(Q\)</span> to construct a tridiagonal symmetric matrix:</p>
<p><span class="math display">\[\begin{align}
T = Q^TAQ
\end{align}\]</span></p>
<p>We can derive a tridiagonal symmetric matrix, <span class="math inline">\(T = Q^TAQ\)</span>, from <span class="math inline">\(Q\)</span> to then find the approximate <strong>Eigenvalues</strong>, and <strong>Eigenvectors</strong> - the <strong>Ritz</strong> pair.</p>
<p>Like <strong>Arnoldi Method</strong>, we use <strong>QR Method</strong> to generate the <strong>Eigenvalues</strong> but this time using the <strong>tridiagonal symmetric matrix</strong> derived from <strong>Lanczos Method</strong>.</p>
<p>Given <span class="math inline">\(Q = (q_1, q_2, ..., q_j)\)</span>, <strong>Laczos basis</strong>, where <span class="math inline">\(Q\)</span> is an orthogonal matrix derived from <strong>Lanczos Method</strong>, we can then perform the following equations.</p>
<p><span class="math display">\[\begin{align}
Av = \lambda v\ \rightarrow\ \ \ \ Q^TAQv = \lambda Q^TQv \rightarrow\ \ \ \ Tv = \lambda v
\end{align}\]</span></p>
<p>Improvements of the <strong>Lanczos</strong> method have evolved through the years. We leave readers to investigate <strong>Lanczos algorithm for SVD</strong> and <strong>Randomized Block Lanczos</strong> <span class="citation">(Yuan Q. et al <a href="bibliography.html#ref-ref71y">2018</a>)</span>.</p>
<p>Other recent methods such as <strong>Jacobi-Davidson Method </strong> uses <strong>Ritz-Galerkin</strong> procedure instead of <strong>Krylov subspace</strong>. And like <strong>Power Method</strong>, the <strong>Jacobi-Davidson</strong> method converges to the largest <strong>Eigenvalue</strong>.</p>
</div>
<div id="fine-tuning-of-iteration-and-convergence" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.8</span> Fine-Tuning of Iteration and Convergence<a href="numericallinearalgebra.html#fine-tuning-of-iteration-and-convergence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before we close the topic around iteration and convergence for <strong>Eigenvalue problems</strong>, it is notable, for further reading, to introduce procedures that are essential for a more stable and faster convergence.</p>
<ul>
<li><strong>Restarting</strong></li>
</ul>
<p>Restarting is simply just restarting the iteration in the middle of it after a certain threshold - usually when we reach a predetermined set of vectors. This allows us to use a new set of initial vectors at restart.</p>
<ul>
<li><strong>Shifting</strong></li>
</ul>
<p>Shifts are commonly seen in <strong>Iteration</strong> methods in the form of:</p>
<p><span class="math display">\[\begin{align}
(A - \alpha I)
\end{align}\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> shifts a matrix. We use shift in <strong>Eigenvalue problems</strong> to get an approximate <strong>Eigenvalue</strong> much closer to one actual <strong>Eigenvalue</strong> than to another in the case in which a matrix has multiple (interior) <strong>Eigenvalues</strong>. While we have not covered <strong>shifting</strong>, the methods we previously discussed can implement shifting, e.g. <strong>Inverse Power Method with shift</strong>, <strong>QR Method with shift</strong>, etc.</p>
<ul>
<li><strong>Preconditioning</strong> </li>
</ul>
<p><strong>Preconditioning</strong> transforms a system of equations to a new form as a way to reduce or eliminate the system from being ill-conditioned. A common equation with <strong>preconditioner</strong> is applied on systems of equations where <span class="math inline">\(M^{-1}\)</span> denotes a <strong>preconditioner</strong>:</p>
<p><span class="math display">\[\begin{align}
Ax = b \rightarrow M^{-1}Ax = M^{-1}b 
\end{align}\]</span></p>
<p>There are three ways to use preconditioners:</p>
<ul>
<li>Left preconditioning</li>
</ul>
<p><span class="math display">\[\begin{align}
Ax = b \rightarrow M^{-1}Ax = M^{-1}b 
\end{align}\]</span></p>
<ul>
<li>Right preconditioning</li>
</ul>
<p><span class="math display">\[\begin{align}
Ax = b \rightarrow AM^{-1}u = b \ \ \  where\ u = Mx
\end{align}\]</span></p>
<ul>
<li>Split or Symmetric preconditioning</li>
</ul>
<p><span class="math display">\[\begin{align}
Ax = b \rightarrow M^{-1}AM^{-1}u = M^{-1}b  \ \ \ where\ u = Mx 
\end{align}\]</span></p>
<p>A simple preconditioner is in the form of diagonal of a matrix:</p>
<p>e.g.</p>
<p><span class="math display">\[\begin{align}
M^{-1} = diag(A)
\end{align}\]</span></p>
<p>We leave readers to investigate other preconditioners.</p>
<p>We also leave readers to investigate <strong>Deflation</strong> and <strong>Augmentation</strong>, which are techniques used to optimize convergence (in the context of <strong>Krylov subspace</strong>). For <strong>Deflation</strong>, we have <strong>Hotellingâs Deflation</strong> and <strong>Wielandt deflation</strong> to be familiar.</p>
</div>
</div>
<div id="approximating-root-and-fixed-point-by-iteration" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.3</span> Approximating Root and Fixed-Point by Iteration<a href="numericallinearalgebra.html#approximating-root-and-fixed-point-by-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have covered iteration and convergence for <strong>Eigenvalue problems</strong> to this section. We now change course away from solving for the <strong>Eigenvalues</strong> and instead, move to a course that starts with <strong>Root-Finding</strong> problems and <strong>Fixed-Point</strong> problems using iterative methods of approximating the intersection of a <strong>curve</strong>. Afterwhich, we will cover iterative methods of solving <strong>systems of equations</strong>.</p>
<div id="root-finding-method-fx-0" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.1</span> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) <a href="numericallinearalgebra.html#root-finding-method-fx-0" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before we extend the idea of solving systems of equations for vector <span class="math inline">\(\mathbf{\vec{x}}\)</span>, let us first have a quick overview of <strong>Root-Finding</strong> and <strong>Fixed-Point finding</strong>, which can be used to approximate a <strong>root</strong>. There is a subtle difference between <strong>finding a Fixed Point</strong> and <strong>Root-Finding</strong>. See Figure <a href="numericallinearalgebra.html#fig:rootfinding">3.2</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rootfinding"></span>
<img src="rootfinding.png" alt="Root Finding vs Fixed-Point" width="80%" />
<p class="caption">
Figure 3.2: Root Finding vs Fixed-Point
</p>
</div>
<p>In <strong>Root-Finding Method</strong>, we are solving for <strong>x</strong> using the following equation <span class="citation">(Driscoll T. A. <a href="bibliography.html#ref-ref536w">2020</a>)</span>:</p>
<p><span class="math display">\[
f(x) = 0\ \ \ \ \ \rightarrow\ \ \ \ \ \ y = 0
\]</span></p>
<p>This is solving for <strong>x</strong> to find the zeroes of function <strong>f</strong>.</p>
<p>In <strong>Fixed-point Method</strong>, we are solving for <strong>x</strong> using the following equation <span class="citation">(Driscoll T. A. <a href="bibliography.html#ref-ref536w">2020</a>)</span>:</p>
<p><span class="math display">\[
f(x) = x\ \ \ \ \ \rightarrow\ \ \ \ \ \ y = x
\]</span></p>
<p>We illustrate the <strong>Fixed-Point</strong> method in the next section.</p>
<p>Here, for <strong>Root-Finding</strong>, consider a third-degree polynomial equation where <span class="math inline">\(f(x)=0\)</span>:</p>
<p><span class="math display">\[\begin{align*}
f(x) = \frac{1}{2}x^3 - x = 0\\
\end{align*}\]</span></p>
<p>Solving for <strong>x</strong>, we get <strong>three roots (three solutions)</strong>:</p>
<p><span class="math display">\[
x =0,\ \ \ \ \ x = - \sqrt{2},\ \ \ \ \ \ \ x = \sqrt{2}
\]</span></p>
<p>We show iterative method in solving for <strong>roots</strong> next.</p>
</div>
<div id="fixed-point-method-fx-x" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.2</span> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) <a href="numericallinearalgebra.html#fixed-point-method-fx-x" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Fixed-point Method</strong> solves for <strong>x</strong> iteratively where <strong>x</strong> is the <strong>Fixed-Point</strong> output of a function that is equal to its input; expresssed as <span class="math inline">\(f(x) = x\)</span>.</p>
<p>Here, we also can interpret it this way:</p>
<p><span class="math display">\[
f(x) = y = x
\]</span></p>
<p>The method iterates until convergence:</p>
<p><span class="math display">\[\begin{align}
x_{k+1} = g(x_k)\ \ \ \ \leftarrow\ \ \text{iterates until } \mathbf{ab}s(x_k - x_{k+1}) &lt; tol
\end{align}\]</span></p>
<p>To use the <strong>Fixed-Point method</strong> for <strong>Root-Finding</strong>, we need to convert the equation, <span class="math inline">\(f(x)=0\)</span> into <span class="math inline">\((fx)=x\)</span>. To do that, suppose we have the following equation</p>
<p><span class="math display">\[
f(x) = f(x) = x^3 + x^2 - 1 = y,\ \ \ \ \ where\ y=0
\]</span>
We need to assume y = x</p>
<p>Therefore:</p>
<p><span class="math display">\[
 f(x) = x^3 + x^2 - 1 = x\ \ \ \ \ \leftarrow \text{root-find form}
\]</span></p>
<p>We now implement the fixed-point iteration in R code. Let us first construct an helper function for plotting the graphs:</p>

<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb52-1" data-line-number="1">plot_par &lt;-<span class="st"> </span><span class="cf">function</span>(f) {</a>
<a class="sourceLine" id="cb52-2" data-line-number="2">    <span class="kw">par</span>(<span class="dt">pty=</span><span class="st">&quot;m&quot;</span>)</a>
<a class="sourceLine" id="cb52-3" data-line-number="3">    xl =<span class="st"> </span><span class="dv">-1</span>; yl =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb52-4" data-line-number="4">    <span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(xl,yl), <span class="dt">ylim=</span><span class="kw">range</span>(xl,yl), </a>
<a class="sourceLine" id="cb52-5" data-line-number="5">         <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>)</a>
<a class="sourceLine" id="cb52-6" data-line-number="6">    <span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb52-7" data-line-number="7">    <span class="kw">lines</span>(<span class="kw">c</span>(xl,yl), <span class="kw">c</span>(xl,yl), <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb52-8" data-line-number="8">    <span class="kw">curve</span>( <span class="kw">f</span>(x), xl, yl,  <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">lwd=</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb52-9" data-line-number="9">    <span class="kw">points</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>),<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>), <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb52-10" data-line-number="10">    <span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">4</span>), <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="fl">3.2</span>), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb52-11" data-line-number="11">    x_loc =<span class="st"> </span><span class="kw">c</span>(<span class="fl">3.5</span>, <span class="fl">-0.5</span>, <span class="fl">0.15</span>, <span class="fl">1.2</span> )</a>
<a class="sourceLine" id="cb52-12" data-line-number="12">    y_loc =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">-0.2</span>, <span class="fl">1.5</span> )</a>
<a class="sourceLine" id="cb52-13" data-line-number="13">    labels=<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;x=4 (initial)&quot;</span>, <span class="st">&quot;f(x) = .2x^2&quot;</span>, <span class="st">&quot;(x, g(x))&quot;</span>, <span class="st">&quot;y=x&quot;</span>)</a>
<a class="sourceLine" id="cb52-14" data-line-number="14">    <span class="kw">text</span>(x_loc, y_loc,  <span class="dt">labels=</span>labels, <span class="dt">offset=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;darkgreen&quot;</span> )</a>
<a class="sourceLine" id="cb52-15" data-line-number="15">}</a></code></pre></div>

<p>Here is a naive implementation of fixed-point method in R code:</p>

<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" data-line-number="1">polynomial &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="fl">0.2</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb53-2" data-line-number="2">fixed_point &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb53-3" data-line-number="3">    limit=<span class="dv">100</span></a>
<a class="sourceLine" id="cb53-4" data-line-number="4">    tol =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb53-5" data-line-number="5">    g =<span class="st"> </span>polynomial</a>
<a class="sourceLine" id="cb53-6" data-line-number="6">    x_ =<span class="st"> </span><span class="kw">g</span>(x)</a>
<a class="sourceLine" id="cb53-7" data-line-number="7">    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb53-8" data-line-number="8">        <span class="kw">plot_iteration</span>(x, x_, n)</a>
<a class="sourceLine" id="cb53-9" data-line-number="9">        x_ =<span class="st"> </span>x</a>
<a class="sourceLine" id="cb53-10" data-line-number="10">        x =<span class="st"> </span><span class="kw">g</span>(x)  <span class="co"># fixed-point formula</span></a>
<a class="sourceLine" id="cb53-11" data-line-number="11">        <span class="cf">if</span> (<span class="kw">abs</span>(x <span class="op">-</span><span class="st"> </span>x_) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb53-12" data-line-number="12">    }</a>
<a class="sourceLine" id="cb53-13" data-line-number="13">    <span class="kw">list</span>(<span class="st">&quot;x&quot;</span>=<span class="kw">round</span>(x_), <span class="st">&quot;y&quot;</span>=<span class="kw">round</span>(x))</a>
<a class="sourceLine" id="cb53-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb53-15" data-line-number="15">plot_iteration &lt;-<span class="st"> </span><span class="cf">function</span>(x,y,n) {</a>
<a class="sourceLine" id="cb53-16" data-line-number="16">    g =<span class="st"> </span>polynomial</a>
<a class="sourceLine" id="cb53-17" data-line-number="17">    <span class="cf">if</span> (n<span class="op">&gt;</span><span class="dv">1</span>) { </a>
<a class="sourceLine" id="cb53-18" data-line-number="18">        <span class="kw">lines</span>(<span class="kw">c</span>(x,x), <span class="kw">c</span>(x, <span class="kw">g</span>(x)), <span class="dt">col=</span><span class="st">&quot;brown&quot;</span> )</a>
<a class="sourceLine" id="cb53-19" data-line-number="19">        <span class="kw">lines</span>(<span class="kw">c</span>(x,y), <span class="kw">c</span>(x, <span class="kw">g</span>(y)), <span class="dt">col=</span><span class="st">&quot;brown&quot;</span> )    </a>
<a class="sourceLine" id="cb53-20" data-line-number="20">    }</a>
<a class="sourceLine" id="cb53-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb53-22" data-line-number="22"><span class="kw">plot_par</span>(polynomial)</a>
<a class="sourceLine" id="cb53-23" data-line-number="23">root =<span class="st"> </span><span class="kw">fixed_point</span>(<span class="dv">4</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fixedpoint"></span>
<img src="embed0002.png" alt="Fixed-Point Iteration" width="90%" />
<p class="caption">
Figure 3.3: Fixed-Point Iteration
</p>
</div>

</div>
<div id="bisection-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.3</span> Bisection Method <a href="numericallinearalgebra.html#bisection-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Bisection Method</strong> is a classic way of solving for the root of a continuous function. It iterates until convergence using the following equation <span class="citation">(Ehiwario J.C. <a href="bibliography.html#ref-ref121e">2014</a>)</span>.</p>
<p><span class="math display">\[\begin{align}
x_{k+1} = \frac{a_k + b_k}{2}
\end{align}\]</span></p>
<p>If <span class="math inline">\(abs(x_{k+1})\)</span> &lt; tolerance level, then we found the root.</p>
<p>However, if <span class="math inline">\(sign(x_{i+1}) = sign(a_k)\)</span>, then we replace <span class="math inline">\(a_k\)</span> with <span class="math inline">\(x_{k+1}\)</span>. And if <span class="math inline">\(sign(x_{i+1}) = sign(b_k)\)</span>, then we replace <span class="math inline">\(b_k\)</span> with <span class="math inline">\(x_{k+1}\)</span>.</p>
<p>We then repeat evaluating the equation to find the root or stop if we reach the tolerance level.</p>
<p>Here is a naive implementation of <strong>Bisection Method</strong> in R code:</p>

<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb54-1" data-line-number="1">f &lt;-<span class="st"> </span><span class="cf">function</span>(x)  {  <span class="kw">tanh</span>(x<span class="op">*</span>pi<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">4</span>))   } </a>
<a class="sourceLine" id="cb54-2" data-line-number="2">bisection &lt;-<span class="st"> </span><span class="cf">function</span>(a, b) {</a>
<a class="sourceLine" id="cb54-3" data-line-number="3">  limit=<span class="dv">20</span></a>
<a class="sourceLine" id="cb54-4" data-line-number="4">  tol =<span class="st"> </span><span class="fl">1e-3</span></a>
<a class="sourceLine" id="cb54-5" data-line-number="5">  <span class="kw">points</span>(<span class="kw">c</span>(a,b), <span class="kw">c</span>(<span class="kw">f</span>(a),<span class="kw">f</span>(b)), <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>) </a>
<a class="sourceLine" id="cb54-6" data-line-number="6">  <span class="kw">text</span>(<span class="kw">c</span>(a,b), <span class="kw">c</span>(<span class="kw">f</span>(a) <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="kw">f</span>(b) <span class="op">-</span><span class="st"> </span><span class="fl">0.1</span>), <span class="dt">label=</span><span class="kw">c</span>(a,b), </a>
<a class="sourceLine" id="cb54-7" data-line-number="7">       <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb54-8" data-line-number="8">  f.x =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb54-9" data-line-number="9">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb54-10" data-line-number="10">      x =<span class="st"> </span>( a <span class="op">+</span><span class="st"> </span>b ) <span class="op">/</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb54-11" data-line-number="11">      y =<span class="st"> </span><span class="kw">f</span>(x)</a>
<a class="sourceLine" id="cb54-12" data-line-number="12">      <span class="cf">if</span> (<span class="kw">abs</span>(y) <span class="op">&lt;</span><span class="st"> </span>tol) {</a>
<a class="sourceLine" id="cb54-13" data-line-number="13">        <span class="kw">points</span>(<span class="kw">c</span>(x), <span class="kw">c</span>(y), <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>) </a>
<a class="sourceLine" id="cb54-14" data-line-number="14">        <span class="kw">return</span>(x)</a>
<a class="sourceLine" id="cb54-15" data-line-number="15">      }</a>
<a class="sourceLine" id="cb54-16" data-line-number="16">      <span class="cf">if</span> (<span class="kw">sign</span>(a) <span class="op">==</span><span class="st"> </span><span class="kw">sign</span>(x)) { a =<span class="st"> </span>x } <span class="cf">else</span> { b =<span class="st"> </span>x }</a>
<a class="sourceLine" id="cb54-17" data-line-number="17">      <span class="kw">points</span>(<span class="kw">c</span>(x), <span class="kw">c</span>(y), <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>) </a>
<a class="sourceLine" id="cb54-18" data-line-number="18">  }</a>
<a class="sourceLine" id="cb54-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb54-20" data-line-number="20"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">2</span><span class="op">:</span><span class="dv">2</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span><span class="op">:</span><span class="dv">1</span>), </a>
<a class="sourceLine" id="cb54-21" data-line-number="21">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>,</a>
<a class="sourceLine" id="cb54-22" data-line-number="22">     <span class="dt">main=</span><span class="st">&quot;Bisection Iteration&quot;</span>)</a>
<a class="sourceLine" id="cb54-23" data-line-number="23"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb54-24" data-line-number="24"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb54-25" data-line-number="25"><span class="kw">curve</span>( <span class="kw">f</span>(x),  <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">lwd=</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb54-26" data-line-number="26">(<span class="dt">root =</span> <span class="kw">bisection</span>(<span class="op">-</span><span class="fl">1.9</span>, <span class="fl">1.5</span>))</a></code></pre></div>
<pre><code>## [1] 4.882813e-05</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bisection"></span>
<img src="embed0003.png" alt="Bisection Iteration" width="70%" />
<p class="caption">
Figure 3.4: Bisection Iteration
</p>
</div>

</div>
<div id="newton-raphson-method-using-the-tangent-line" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.4</span> Newton-Raphson Method (using the Tangent Line)<a href="numericallinearalgebra.html#newton-raphson-method-using-the-tangent-line" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Newton-Raphson</strong> method iterates until convergence using the following equation <span class="citation">(Ypma T. J. <a href="bibliography.html#ref-ref103y">1995</a>)</span>.</p>
<p><span class="math display">\[\begin{align}
x_{k+1} = x_k - \frac{f(x_k)}{f&#39;(x_k)} 
\end{align}\]</span></p>
<p>The equation is derived by <strong>approximation</strong> using first-order <strong>Taylor Series</strong> given a <strong>differentiable function</strong>, <span class="math inline">\(f(x) = 0\)</span> (note that we can also use <span class="math inline">\(f(x) \approx 0\)</span> since here we are approximating the root):</p>
<p><span class="math display">\[\begin{align}
f(x) \approx f(x_k) + f&#39;(x_k)(x_{k+1} - x_k) = 0 
\end{align}\]</span></p>
<p>This iterative method uses a tangent line to the curve, and where the tangent line intersects, the x-axis becomes the next <strong>x</strong>.</p>
<p>Here is a naive implementation of <strong>Newton-Raphson</strong> in R code:</p>

<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" data-line-number="1">polynomial &lt;-<span class="st"> </span><span class="cf">function</span>(x)  { <span class="fl">0.2</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb56-2" data-line-number="2">dpolynomial &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="fl">0.4</span> <span class="op">*</span><span class="st"> </span>x }</a>
<a class="sourceLine" id="cb56-3" data-line-number="3">newton_raphson &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb56-4" data-line-number="4">    limit=<span class="dv">100</span></a>
<a class="sourceLine" id="cb56-5" data-line-number="5">    tol =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb56-6" data-line-number="6">    g =<span class="st"> </span>polynomial</a>
<a class="sourceLine" id="cb56-7" data-line-number="7">    dg =<span class="st"> </span>dpolynomial</a>
<a class="sourceLine" id="cb56-8" data-line-number="8">    x_ =<span class="st"> </span>x <span class="op">-</span><span class="st"> </span><span class="kw">g</span>(x)<span class="op">/</span><span class="kw">dg</span>(x)</a>
<a class="sourceLine" id="cb56-9" data-line-number="9">    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb56-10" data-line-number="10">        <span class="kw">plot_iteration</span>(x_, x, n)</a>
<a class="sourceLine" id="cb56-11" data-line-number="11">        x =<span class="st"> </span>x_</a>
<a class="sourceLine" id="cb56-12" data-line-number="12">        x_ =<span class="st"> </span>x <span class="op">-</span><span class="st"> </span><span class="kw">g</span>(x)<span class="op">/</span><span class="kw">dg</span>(x) <span class="co"># Newton-Raphson formula</span></a>
<a class="sourceLine" id="cb56-13" data-line-number="13">        <span class="cf">if</span> (<span class="kw">abs</span>(x <span class="op">-</span><span class="st"> </span>x_) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb56-14" data-line-number="14">    }</a>
<a class="sourceLine" id="cb56-15" data-line-number="15">    <span class="kw">list</span>(<span class="st">&quot;x&quot;</span>=<span class="kw">round</span>(x_), <span class="st">&quot;y&quot;</span>=<span class="kw">round</span>(x))</a>
<a class="sourceLine" id="cb56-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb56-17" data-line-number="17">plot_iteration &lt;-<span class="st"> </span><span class="cf">function</span>(x_, x, n) {</a>
<a class="sourceLine" id="cb56-18" data-line-number="18">    f =<span class="st"> </span>polynomial</a>
<a class="sourceLine" id="cb56-19" data-line-number="19">    slope =<span class="st"> </span><span class="kw">dpolynomial</span>(x)</a>
<a class="sourceLine" id="cb56-20" data-line-number="20">    y1 =<span class="st"> </span>slope <span class="op">*</span><span class="st"> </span>( x_ )</a>
<a class="sourceLine" id="cb56-21" data-line-number="21">    <span class="kw">lines</span>(<span class="kw">c</span>(x_, x), <span class="kw">c</span>(<span class="dv">0</span>, y1), <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb56-22" data-line-number="22">    <span class="kw">points</span>(x ,<span class="kw">f</span>(x), <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb56-23" data-line-number="23">    <span class="kw">lines</span>(<span class="kw">c</span>(x_, x_), <span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">f</span>(x_)), <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb56-24" data-line-number="24">}</a>
<a class="sourceLine" id="cb56-25" data-line-number="25"><span class="kw">plot_par</span>(polynomial)</a>
<a class="sourceLine" id="cb56-26" data-line-number="26"><span class="kw">text</span>(<span class="dv">2</span>, <span class="fl">-0.1</span>, <span class="dt">labels=</span><span class="st">&quot;tangent&quot;</span>, <span class="dt">offset=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;darkgreen&quot;</span>)</a>
<a class="sourceLine" id="cb56-27" data-line-number="27">root =<span class="st"> </span><span class="kw">newton_raphson</span>(<span class="dv">4</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:newtonraphson"></span>
<img src="embed0004.png" alt="Newton-Raphson Iteration" width="70%" />
<p class="caption">
Figure 3.5: Newton-Raphson Iteration
</p>
</div>

</div>
<div id="secant-method-using-the-secant-line" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.5</span> Secant Method (using the Secant Line)<a href="numericallinearalgebra.html#secant-method-using-the-secant-line" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Secant</strong> method iterates until convergence using the following equation <span class="citation">(Ehiwario J.C. <a href="bibliography.html#ref-ref121e">2014</a>)</span>.</p>
<p><span class="math display">\[\begin{align}
x_{k+1} = x_k - \frac{f(x_k)(x_k - x_{k-1})}{f(x_k)-f(x_{k-1})}
\ \ \ \leftarrow \ \ \ \ \ f&#39;(x) \approx \frac{ f(x) - f(x+h)}{ x - (x + h)}
\end{align}\]</span></p>
<p>This iterative method is similar to <strong>Newton method</strong>, but here it uses the <strong>secant</strong> line instead, and where the <strong>secant</strong> line intersects, the x-axis becomes the next <strong>x</strong>.</p>
<p>Here is a naive implementation of the secant method in R code:</p>

<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1">polynomial &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="fl">0.2</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb57-2" data-line-number="2">g &lt;-<span class="st"> </span><span class="cf">function</span>(x, x_) {</a>
<a class="sourceLine" id="cb57-3" data-line-number="3">    f =<span class="st"> </span>polynomial</a>
<a class="sourceLine" id="cb57-4" data-line-number="4">    gx_ =<span class="st"> </span>x <span class="op">-</span><span class="st"> </span>( <span class="kw">f</span>(x) <span class="op">*</span><span class="st">  </span>(x <span class="op">-</span><span class="st"> </span>x_) ) <span class="op">/</span><span class="st">  </span>( <span class="kw">f</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">f</span>(x_)) <span class="co"># secant formula</span></a>
<a class="sourceLine" id="cb57-5" data-line-number="5">    <span class="kw">list</span>(<span class="st">&quot;x&quot;</span>=gx_, <span class="st">&quot;y&quot;</span>=<span class="dv">0</span>, <span class="st">&quot;x_&quot;</span>=x, <span class="st">&quot;y_&quot;</span>=<span class="kw">f</span>(x), <span class="st">&quot;ox_&quot;</span>=x_, <span class="st">&quot;oy_&quot;</span>=<span class="kw">f</span>(x_))</a>
<a class="sourceLine" id="cb57-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb57-7" data-line-number="7">secant &lt;-<span class="st"> </span><span class="cf">function</span>(x,x_) {</a>
<a class="sourceLine" id="cb57-8" data-line-number="8">    limit=<span class="dv">100</span></a>
<a class="sourceLine" id="cb57-9" data-line-number="9">    tol =<span class="st"> </span><span class="fl">1e-2</span></a>
<a class="sourceLine" id="cb57-10" data-line-number="10">    g_ =<span class="st"> </span><span class="kw">g</span>(x, x_)</a>
<a class="sourceLine" id="cb57-11" data-line-number="11">    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb57-12" data-line-number="12">        <span class="kw">plot_iteration</span>(g_, n)</a>
<a class="sourceLine" id="cb57-13" data-line-number="13">        g_ =<span class="st"> </span><span class="kw">g</span>(g_<span class="op">$</span>x, g_<span class="op">$</span>x_)</a>
<a class="sourceLine" id="cb57-14" data-line-number="14">        <span class="cf">if</span> (<span class="kw">abs</span>(g_<span class="op">$</span>x <span class="op">-</span><span class="st"> </span>g_<span class="op">$</span>x_) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb57-15" data-line-number="15">    }</a>
<a class="sourceLine" id="cb57-16" data-line-number="16">    <span class="kw">list</span>(<span class="st">&quot;x&quot;</span>=<span class="kw">round</span>(g_<span class="op">$</span>x), <span class="st">&quot;y&quot;</span>=<span class="kw">round</span>(g_<span class="op">$</span>y))</a>
<a class="sourceLine" id="cb57-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb57-18" data-line-number="18">plot_iteration &lt;-<span class="st"> </span><span class="cf">function</span>(g_,n) {</a>
<a class="sourceLine" id="cb57-19" data-line-number="19">    <span class="kw">lines</span>(<span class="kw">c</span>(g_<span class="op">$</span>x, g_<span class="op">$</span>ox_), <span class="kw">c</span>(g_<span class="op">$</span>y,g_<span class="op">$</span>oy_ ), <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb57-20" data-line-number="20">    <span class="kw">points</span>(g_<span class="op">$</span>ox_,g_<span class="op">$</span>oy_, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb57-21" data-line-number="21">    <span class="kw">lines</span>(<span class="kw">c</span>(g_<span class="op">$</span>x, g_<span class="op">$</span>x), <span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">polynomial</span>(g_<span class="op">$</span>x)), <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb57-22" data-line-number="22">}</a>
<a class="sourceLine" id="cb57-23" data-line-number="23"><span class="kw">plot_par</span>(polynomial)</a>
<a class="sourceLine" id="cb57-24" data-line-number="24"><span class="kw">text</span>(<span class="fl">1.8</span>, <span class="fl">-0.1</span>, <span class="dt">labels=</span><span class="st">&quot;secant&quot;</span>, <span class="dt">offset=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;darkgreen&quot;</span>)</a>
<a class="sourceLine" id="cb57-25" data-line-number="25">root =<span class="st"> </span><span class="kw">secant</span>(<span class="dv">3</span>,<span class="dv">4</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:secantitration"></span>
<img src="embed0005.png" alt="Secant Iteration" width="70%" />
<p class="caption">
Figure 3.6: Secant Iteration
</p>
</div>

</div>
</div>
<div id="approximating-solutions-to-systems-of-eqs-by-iteration-ax-b" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.4</span> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)<a href="numericallinearalgebra.html#approximating-solutions-to-systems-of-eqs-by-iteration-ax-b" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the motivations of linear algebra is to solve for Systems of Polynomial Equations (whether linear or non-linear) denoted as:</p>
<p><span class="math display">\[\begin{align}
A \mathbf{\vec{x}} = b \label{eqn:eqnnumber1}
\end{align}\]</span></p>
<p>We can express simple systems of linear equations geometrically in terms of lines - first-degree polynomial equations. For example, in Figure <a href="numericallinearalgebra.html#fig:linearsystem">3.7</a>, it is easy to show that if two lines (two linear equations) intersect, it means that there is a point of intersection; and thus <strong>there is one solution to the system</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linearsystem"></span>
<img src="linear_system.png" alt="System of Linear Equations" width="90%" />
<p class="caption">
Figure 3.7: System of Linear Equations
</p>
</div>
<p>It is also easy to show that if two lines (two linear equations) are in parallel, they will never intersect. There is no point of intersection, and thus <strong>there is no solution to the system</strong>.</p>
<p>On the other hand, we can show that if two lines (two linear equations) are in parallel and along the same path, it means that every point in both lines hits an intersection, and thus <strong>there is an infinite number of solutions to the system</strong>.</p>
<p>That said, let us look at what we describe here as <strong>solution</strong> - the point of intersection. We can derive the point of intersection by translating the equation into matrix form and then solve for the <strong>unknown coordinates (x,y)</strong> by reducing the matrix into its <strong>reduced echelon</strong> form (using <strong>Gaussian Elimination</strong> and <strong>Lu Factorization</strong>):</p>
<p><span class="math display">\[
\left(\begin{array}{l}  
y = -x + 4\\
y = x
\end{array}\right)
\rightarrow
\left(\begin{array}{r}
x + y = 4\\
-x + y = 0
\end{array}\right)
\]</span></p>
<p>That gives us:</p>
<p><span class="math display">\[
A\mathbf{\vec{x}} = b
\rightarrow
\left[\begin{array}{rr}  
1 &amp; 1 \\
-1 &amp; 1
 \end{array}\right] 
 \left[\begin{array}{r}  
x \\
y
 \end{array}\right] =
 \left[\begin{array}{r}  
4 \\ 0
 \end{array}\right]
\]</span></p>
<p>in <strong>reduced echelon</strong> form:</p>
<p><span class="math display">\[
\left[\begin{array}{rr}  
1 &amp; 0 \\
0 &amp; 1
 \end{array}\left|\begin{array}{r}2 \\ 2\end{array}\right.\right]
\]</span></p>
<p>Thus by RREF method, we are able to solve for the exact values for the vector <span class="math inline">\(\mathbf{\vec{x}}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
x = 2\\
y = 2
\end{align*}\]</span></p>
<p>Note that vector <span class="math inline">\(\mathbf{\vec{x}}\)</span> has two components: x and y. Alternatively, we can use <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> to represent the two components.</p>
<p>That is an example of the most elementary way of solving a set of polynomial equations for vector <span class="math inline">\(\mathbf{\vec{x}}\)</span>.</p>
<p>When we say <strong>solve for the solution</strong>, we mean to solve for vector <span class="math inline">\(\mathbf{\vec{x}}\)</span>. If <span class="math inline">\(\mathbf{\vec{x}}\)</span> happens to be a vector variable, then we need to find the value of each entry in the vector. That requires us to perform a simple mathematical manipulation:</p>
<p><span class="math display">\[\begin{align}
\mathbf{\vec{x}} = A^{-1}b \ \ \ \ \ \leftarrow A\mathbf{\vec{x}} = b
\end{align}\]</span></p>
<p>For a system of equations translated to a matrix with extremely high dimensionality, computing for the inverse of a matrix is not practical. In Chapter , we introduce methods such as <strong>LU decomposition</strong> and <strong>Gauss elimination</strong> as alternatives.</p>
<p>Recall the below R code, which we use in Chapter  under <strong>LU decomposition</strong>:</p>

<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" data-line-number="1">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb58-2" data-line-number="2">b =<span class="st"> </span><span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">5</span>,<span class="dv">6</span>)</a>
<a class="sourceLine" id="cb58-3" data-line-number="3">LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(A)  <span class="co"># from Lin Algebra Ch</span></a>
<a class="sourceLine" id="cb58-4" data-line-number="4">uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, b) <span class="co"># REF/RREF section in LinAgb chapter</span></a>
<a class="sourceLine" id="cb58-5" data-line-number="5">x =<span class="st"> </span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy) <span class="co"># REF/RREF section in LinAgb chapter</span></a>
<a class="sourceLine" id="cb58-6" data-line-number="6"></a>
<a class="sourceLine" id="cb58-7" data-line-number="7">A <span class="co"># the matrix (system of equations)</span></a></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    5    5
## [2,]    2    4    5
## [3,]    3    3    3</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb60-1" data-line-number="1">x <span class="co"># the solution</span></a></code></pre></div>
<pre><code>## [1]  1  2 -1</code></pre>

<div id="krylovmethods" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.1</span> Krylov Methods<a href="numericallinearalgebra.html#krylovmethods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Krylov Methods</strong> are iterative methods that make use of a special subspace called <strong>Krylov subspace</strong> denoted as:  </p>
<p><span class="math display">\[\begin{align}
K_m(A,v) = \{\ v,\ Av,\ A^2v,\ A^3v,\ ...\ ,\ A^{m-1}v\ \}
\end{align}\]</span></p>
<p>where m = dimension of K.</p>
<p>Here, we reference the works of Heath M.T. <span class="citation">(<a href="bibliography.html#ref-ref187m">2002</a>)</span>, An D. <span class="citation">(<a href="bibliography.html#ref-ref166d">2009</a>)</span>, Driscoll T. <span class="citation">(<a href="bibliography.html#ref-ref175t">2012</a>)</span>, and Sleijpen G. <span class="citation">(<a href="bibliography.html#ref-ref61g">2014</a>)</span>.</p>
<p>Recall Figure <a href="numericallinearalgebra.html#fig:krylovspace">3.1</a>. The right side illustrates solving for eigenvalues. And the left side illustrates a linear system of solving for <span class="math inline">\(x\)</span>. In Figure <a href="numericallinearalgebra.html#fig:krylovmethod">3.8</a>, we show the same methods; but we focus now on <strong>GMRES</strong> and <strong>CG</strong> in the next sections.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:krylovmethod"></span>
<img src="krylovmethod.png" alt="Krylov SubSpace methods" width="70%" />
<p class="caption">
Figure 3.8: Krylov SubSpace methods
</p>
</div>
<p>Though <strong>Krylov</strong> methods are also used to deal with <strong>Eigen problems</strong> (e.g., <strong>Arnoldi and Lanczos</strong> methods), here we continue to focus on solving systems of equations for <span class="math inline">\(\mathbf{\vec{x}}\)</span> in the form of:</p>
<p><span class="math display">\[
A\mathbf{\vec{x}} = b \rightarrow\ \ \ \ \ \mathbf{\vec{x}} = A^{-1}b
\]</span>
Now instead of using analytical methods to compute for the <strong>inverse of A</strong>, we approximate by using <strong>Power Series</strong>, forming the polynomial equation below:</p>
<p><span class="math display">\[\begin{align}
A^{-1} \approx p_j(A) =  \frac{1}{c_0} \sum_{j=0}^{m-1}(c_{j+1}) A^j, \ where\ c_0 \neq 0 
\end{align}\]</span></p>
<p>We use â<span class="math inline">\(\approx\)</span>â notation to indicate that we are numerically approximating <span class="math inline">\(\mathbf{A^{-1}}\)</span> rather than using direct solvers. Thus we get:</p>
<p><span class="math display">\[\begin{align}
\mathbf{\hat{x}} = A^{-1}b \approx  p_m(A)b
\end{align}\]</span></p>
<p>And in its expanded form:</p>
<p><span class="math display">\[\begin{align}
\mathbf{\hat{x}} \approx  p_m(A)b = c_0b + c_1Ab +  c_2A^2b +  c_3A^3b +\ ...\ +  c_mA^{m-1} b
\end{align}\]</span></p>
<p>What we have is a <strong>linear combination</strong> that spans a subspace, <strong>K</strong>:</p>
<p><span class="math display">\[\begin{align}
\mathbf{\hat{x}} \approx  p_m(A)b = K_m(A,b)
\end{align}\]</span></p>
<p>We call this subspace <strong>K</strong> the <strong>Krylov subspace</strong> - the span of vectors in multiples of the powers of matrix <strong>A</strong> as shown.</p>
<p>For example (consider <span class="math inline">\(\mathbf{\vec{b}} = \mathbf{\vec{v_0}}\)</span>):</p>
<p><span class="math display">\[\begin{align}
A(b) &amp;= Ab &amp; &amp;\rightarrow \text{where} (\mathbf{Ab}) \text{ is a vector denoted as } \mathbf{\vec{v_1}} \\
A(Ab) &amp;= A^2b &amp; &amp;\rightarrow \text{where} (\mathbf{A^2b}) \text{ is a vector denoted as } \mathbf{\vec{v_2}} \\
A(A^2b) &amp;= A^3b &amp; &amp;\rightarrow \text{where} (\mathbf{A^3b}) \text{ is a vector denoted as } \mathbf{\vec{v_3}} \\
&amp; &amp; &amp;\vdots \nonumber \\
A(A^{m-1}b) &amp;= A^{m-1}b &amp; &amp;\rightarrow \text{where} (\mathbf{A^{m-1}b}) \text{  is a vector denoted as  } \mathbf{\vec{v_m}} 
\end{align}\]</span></p>
<p>That creates a subspace that spans the following:</p>
<p><span class="math display">\[\begin{align}
K_m(A,b) = span\{ b, Ab, A^2b, A^3b, ..., A^{m-1}b \} 
= \{ \mathbf{\vec{v_0}}, \mathbf{\vec{v_1}}, \mathbf{\vec{v_2}}, \mathbf{\vec{v_3}}, ..., \mathbf{\vec{v_m}} \}
\end{align}\]</span></p>
<p>Because we intend to approximate for <span class="math inline">\(\mathbf{\vec{x}}\)</span>, we can readily say that <span class="math inline">\(\mathbf{\vec{x}}\)</span> is nothing more than a <strong>linear combination</strong> of (almost) <strong>linearly dependent</strong> vectors in <strong>K</strong> subspace. In other words, the approximate solution of <span class="math inline">\(\mathbf{\vec{x}}\)</span> can be anywhere in this subspace - to put it plainly, the solution spans the subspace of K, granting the system of equations is non-singular.</p>
<p><span class="math display">\[
\mathbf{\vec{x}} \in K_m(A,b) \rightarrow \ \ \ \ \mathbf{\vec{x}} \in \{ 
\mathbf{\vec{v_0}}, \mathbf{\vec{v_1}}, \mathbf{\vec{v_2}}, \mathbf{\vec{v_3}}, ..., \mathbf{\vec{v_m}} \}
\rightarrow\ \ \ \ \ \ \mathbf{\vec{x}} \in V_m
\]</span></p>
<p>If the system is singular, there is a chance that the solution may not lie in the space, granting there is even a solution.</p>
<p>Granting there exists a solution, and that our approximate solution <span class="math inline">\(\mathbf{\hat{x}}\)</span> is in the <strong>K</strong> space, <span class="math inline">\(\mathbf{\hat{x}} \in K_m(A,b)\)</span>; it then means:</p>
<p><span class="math display">\[\begin{align}
\mathbf{\vec{x}} = A^{-1}b\ \ \ \ \rightarrow\ \ \ \ \mathbf{\hat{x}} \approx K_m\mathbf{\beta}\ \ \ \ \ \ given\ \ \ \ 
\mathbf{\hat{x}} \in K_m(A,b)
\end{align}\]</span></p>
<p>Here, the <strong>solution</strong> denoted as <span class="math inline">\(\mathbf{\hat{x}}\)</span> is an approximation at the <strong>mth</strong> iteration for the <span class="math inline">\(\mathbf{K_m}\)</span> matrix and <span class="math inline">\(\beta\)</span>. The <span class="math inline">\(\beta\)</span> value is derived using the following equation:</p>
<p><span class="math display">\[\begin{align}
\beta = ((K_m)^T (K_m))^{-1} (K_m)^T b
\end{align}\]</span></p>
<p>It is not readily apparent that solving for <span class="math inline">\(\mathbf{\hat{x}}\)</span> by iterative approximation gives us the closest solution. For that reason, we minimize the norm of the residual:</p>
<p>We just need to use the following minimal-residual equation:</p>
<p><span class="math display">\[\begin{align}
{f(x)}_{min} = \| b - A\mathbf{\vec{x}} \|_{L2} &lt; tol
\end{align}\]</span></p>
<p>and replace <span class="math inline">\(\mathbf{\vec{x}}\)</span> with the approximate <span class="math inline">\(\mathbf{\hat{x}} \approx K_n\beta_n\)</span>:</p>
<p><span class="math display">\[\begin{align}
{f(\beta)}_{min} = \| b - A(K_m\beta) \|_{L2} &lt; tol
\end{align}\]</span></p>
<p>To illustrate, given a matrix <strong>A</strong> and <strong>b</strong>:</p>
<p><span class="math display">\[
\mathbf{\vec{x}} = 
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array}
\right]_{A}^{-1} 
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b}
\]</span></p>
<p><strong>First</strong>, initialize our <strong>Krylov matrix</strong>: </p>
<p><span class="math display">\[
K = b\ \ \ \ \rightarrow\ \ \ \ \ K_{0}(A,b) = \{\ b\ \} \rightarrow 
\left\{ \left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} \right\}
\]</span></p>
<p><strong>Then we iterate:</strong> At <span class="math inline">\(m=1\)</span>, we perform the following:</p>
<ul>
<li>multiply the last element of <strong>K</strong> by <strong>A</strong>, e.g., <span class="math inline">\(A \cdotp b\)</span>, adding a new linear combination into the space.</li>
</ul>
<p><span class="math display">\[
K_{1}(A,b) = \{\ b,\ Ab\ \} \rightarrow
K_1 =
\left\{ 
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} 
\left[ \begin{array}{rrr} 51 \\ 62 \\ 61 \end{array} \right]_{Ab} 
\right\}
\]</span></p>
<ul>
<li>compute for <span class="math inline">\(\beta\)</span>:</li>
</ul>

<p><span class="math display">\[\begin{align*}
\beta {}&amp;= ((K_1)^T (K_1))^{-1} (K_1)^T b = 
\left(
\left[ \begin{array}{rrr} 6 &amp; 51 \\ 5 &amp; 62 \\ 6 &amp; 61\end{array} \right]_{K_1}^T
\left[ \begin{array}{rrr} 6 &amp; 51 \\ 5 &amp; 62 \\ 6 &amp; 61\end{array} \right]_{K_1}
\right)^{-1}
\left[ \begin{array}{rrr} 6 &amp; 51 \\ 5 &amp; 62 \\ 6 &amp; 61\end{array} \right]_{K_1}^T
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} \\
\\
\beta &amp;= 0.0965965
\end{align*}\]</span>
</p>
<ul>
<li>compute for first approximate of <span class="math inline">\(\mathbf{\hat{x}}\)</span>:</li>
</ul>
<p><span class="math display">\[
\mathbf{\hat{x}} = K_1 \cdotp \beta =
\left[ \begin{array}{rrr} 6 &amp; 51 \\ 5 &amp; 62 \\ 6 &amp; 61\end{array} \right]_{K_1}
(0.0965965)_{\beta} =
\left[ \begin{array}{rrr} 0.5795790 \\ 0.4829825 \\ 0.5795790 \end{array} \right]
\]</span></p>
<ul>
<li>Compute for residual:</li>
</ul>
<p><span class="math display">\[\begin{align*}
r_{min} = \| b - A\mathbf{\hat{x}} \|_{L2} =  
\left\|
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} -
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array}
\right]_{A}
\left[ \begin{array}{rrr} 0.5795790 \\ 0.4829825 \\ 0.5795790 \end{array} \right]_{\mathbf{\hat{x}}}
\right\|_{L2} = 1.463639
\end{align*}\]</span></p>
<ul>
<li>check if residual reaches tolerance level (e.g. <strong>1e-5</strong>):</li>
</ul>
<p><span class="math display">\[
if\ (\ (r_{min} = 1.463639) &lt; tol\ ) \text{ stop iteration}
\]</span></p>
<ul>
<li>Otherwise, continue for next iteration at n = 2 â¦</li>
</ul>
<p><strong>We continue to iterate:</strong> At <span class="math inline">\(m=2\)</span>, we perform the following:</p>
<ul>
<li>multiply last element of <strong>K</strong> by <strong>A</strong>, e.g., <span class="math inline">\(A \cdotp Ab\)</span>, adding a new linear combination into the space.</li>
</ul>
<p><span class="math display">\[
K_{2}(A,b) = \{\ b,\ Ab,\ A^2b\ \} \rightarrow
K_2 =
\left\{ 
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} 
\left[ \begin{array}{rrr} 51 \\ 62 \\ 61 \end{array} \right]_{Ab} 
\left[ \begin{array}{rrr} 522 \\ 655 \\ 666 \end{array} \right]_{A^2b} 
\right\}
\]</span></p>
<ul>
<li>compute for <span class="math inline">\(\beta\)</span>:</li>
</ul>
<p><span class="math display">\[\begin{align*}
\beta {}&amp;= ((K_2)^T (K_2))^{-1} (K_2)^T b = 
\left[ \begin{array}{rrr} 0.32604333 \\ -0.02162618 \end{array} \right]
\end{align*}\]</span></p>
<ul>
<li>compute for second approximate of <span class="math inline">\(\mathbf{\hat{x}}\)</span>:</li>
</ul>
<p><span class="math display">\[
\mathbf{\hat{x}} = K_2 \cdotp \beta =
\left[ \begin{array}{rrr} 6 &amp; 51 &amp; 522 \\ 5 &amp; 62 &amp; 655 \\ 6 &amp; 61 &amp; 666\end{array} \right]_{K_2}
\left[ \begin{array}{rrr} 0.32604333 \\ -0.02162618 \end{array} \right]_{\beta} =
\left[ \begin{array}{rrr} 0.8533248 \\ 0.2893935 \\ 0.6370630 \end{array} \right]
\]</span></p>
<ul>
<li>Compute for residual:</li>
</ul>
<p><span class="math display">\[\begin{align*}
r_{min} = \| b - A\mathbf{\hat{x}} \|_{L2} =  
\left\|
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} -
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array}
\right]_{A}
\left[ \begin{array}{rrr} 0.8533248 \\ 0.2893935 \\ 0.6370630 \end{array} \right]_{\mathbf{\hat{x}}}
\right\|_{L2} = 1.342609
\end{align*}\]</span></p>
<ul>
<li>check if residual reaches tolerance level (e.g. <strong>1e-5</strong>):</li>
</ul>
<p><span class="math display">\[
if\ (\ (r_{min} = 1.342609) &lt; tol\ ) \text{ stop iteration}
\]</span></p>
<ul>
<li>Otherwise, continue for next iteration at n = 3 â¦</li>
</ul>
<p><strong>After the third iteration</strong>, we get the approximate solution <span class="math inline">\(\mathbf{\hat{x}} = \left[ \begin{array}{rrr} 1 &amp; 2 &amp; -1 \end{array} \right]^T\)</span></p>
<p>hitting the following residual:</p>
<p><span class="math display">\[
r_{min} = \text{8.683085e-12}
\]</span></p>
<p>Let us review a naive implementation of a simple Krylov method in R code:</p>

<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1">naive_krylov_method &lt;-<span class="st"> </span><span class="cf">function</span>(A, b) {</a>
<a class="sourceLine" id="cb62-2" data-line-number="2">    n =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb62-3" data-line-number="3">    K =<span class="st"> </span>b <span class="co">#  start with K(A,b) = {b}</span></a>
<a class="sourceLine" id="cb62-4" data-line-number="4">    limit=<span class="dv">100</span></a>
<a class="sourceLine" id="cb62-5" data-line-number="5">    tol =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb62-6" data-line-number="6">    <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb62-7" data-line-number="7">        v =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>K</a>
<a class="sourceLine" id="cb62-8" data-line-number="8">        beta =<span class="st"> </span><span class="kw">solve</span>( <span class="kw">t</span>(v) <span class="op">%*%</span><span class="st"> </span>v, <span class="kw">t</span>(v) <span class="op">%*%</span><span class="st"> </span>b) </a>
<a class="sourceLine" id="cb62-9" data-line-number="9">        appr_x =<span class="st">   </span>K <span class="op">%*%</span><span class="st"> </span>beta</a>
<a class="sourceLine" id="cb62-10" data-line-number="10">        min_r =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((b <span class="op">-</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>appr_x)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb62-11" data-line-number="11">        <span class="cf">if</span> (min_r <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb62-12" data-line-number="12">        <span class="co"># keep building krylov space, K(A,b) = {b,Ab,...}</span></a>
<a class="sourceLine" id="cb62-13" data-line-number="13">        K  =<span class="st">  </span><span class="kw">matrix</span>( <span class="kw">cbind</span>(K, v[,<span class="kw">c</span>(m)]),  n )</a>
<a class="sourceLine" id="cb62-14" data-line-number="14">    }</a>
<a class="sourceLine" id="cb62-15" data-line-number="15">    <span class="kw">list</span>(<span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;b&quot;</span>=<span class="kw">c</span>(b), <span class="st">&quot;x&quot;</span> =<span class="st"> </span><span class="kw">c</span>(x), <span class="st">&quot;residual&quot;</span> =<span class="st"> </span>min_r)</a>
<a class="sourceLine" id="cb62-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb62-17" data-line-number="17">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb62-18" data-line-number="18">x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb62-19" data-line-number="19">b &lt;-<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>x</a>
<a class="sourceLine" id="cb62-20" data-line-number="20"><span class="kw">naive_krylov_method</span>(A,b)</a></code></pre></div>
<pre><code>## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $b
## [1] 6 5 6
## 
## $x
## [1]  1  2 -1
## 
## $residual
## [1] 8.683085e-12</code></pre>

<p>It is notable to mention that the <strong>Krylov matrix</strong> is a linear combination of multiples of <strong>A</strong> and a fixed <strong>b</strong>; in that respect, the <strong>K</strong> matrix becomes ill-conditioned. We can apply preconditioners <span class="math inline">\(M^{-1}\)</span> to <strong>A</strong> to make it well-conditioned.</p>
<p>Successive computation of the normalized residual appears to converge but then eventually diverges further away from zero. That is because the <strong>K space</strong> starts to accumulate vectors that are parallel to each other.</p>
<p>We need an orthonormal basis that spans the <strong>Krylov space</strong>. In the next section, we discuss GMRES, which offers that approach.</p>
</div>
<div id="gmres-generalized-minimal-residual" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.2</span> GMRES (Generalized Minimal Residual)  <a href="numericallinearalgebra.html#gmres-generalized-minimal-residual" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Relevant to <strong>Krylov methods</strong>, let us cover the <strong>GMRES</strong> method developed by Yousef Saad and Martin Schultz <span class="citation">(<a href="bibliography.html#ref-ref297y">1986</a>)</span>, which is used in solving nonsymmetric systems of equations. It may help to recall Figure <a href="numericallinearalgebra.html#fig:krylovspace">3.1</a> - particularly, to review the left figure. As we explain the figure further, there are two important notes to emphasize.</p>
<p><span class="math inline">\(\mathbf{Ax}\)</span> as projection:</p>
<p><span class="math display">\[\begin{align}
Ax \parallel K_m(A,b) 
\end{align}\]</span></p>
<p><span class="math inline">\(\mathbf{(b - Ax)}\)</span> as orthogonal projection:</p>
<p><span class="math display">\[\begin{align}
b - Ax \perp K_m(A,b)  
\end{align}\]</span></p>
<p>On the one hand, <span class="math inline">\(\mathbf{Ax}\)</span> is the projection unto K and thus is parallel to K. On the other hand, <span class="math inline">\(\mathbf{(b - Ax)}\)</span> is the orthogonal projection unto K. And of course, <span class="math inline">\(\mathbf{b}\)</span> is being projected.</p>
<p>We can also denote the orthogonal relationship as:</p>
<p><span class="math display">\[\begin{align}
r \perp K_m(A,r) \ \ \ \ \ where\ (r = b - Ax)
\end{align}\]</span></p>
<p>Now, recall the following equation from <strong>Arnoldi</strong> iteration:</p>
<p><span class="math display">\[
AQ_m = Q_mH_m\ \ \leftarrow \ \ \text{Ignore last row of H where } H_{m+1,m}=0\ \ \ \  \therefore\ Q_{m,m} \leftarrow Q_{m,m+1}
\]</span></p>
<p>Here, our <strong>GMRES</strong> method uses the orthogonal projection to construct the <span class="math inline">\(Q\)</span> basis for the <strong>K space</strong>. For example, given an initial arbitrary approximation for <span class="math inline">\(x^0\)</span>:</p>
<p><span class="math display">\[\begin{align}
r^0 = b - Ax^0\ \ \ \ \rightarrow\ \ \ \ \ \ \ q = \frac{r^0}{\| r^0 \|_{L2}}
\end{align}\]</span></p>
<p>we then build our <strong>K space</strong> out of <span class="math inline">\(q\)</span>:</p>
<p><span class="math display">\[\begin{align}
K_m(A,q) = span\ \{\ q,\ Aq,\ A^2q,\ A^3q,\ ..., A^{m-1}q \} 
= \{\ q_1,\ q_2,\ q_3,\ ...,\ q_m\ \}
\end{align}\]</span></p>
<p>As such, we get an orthonormal basis in the form of <span class="math inline">\(Q\)</span>:</p>
<p><span class="math display">\[\begin{align}
Q_{basis} = \{\ q_1,\ q_2,\ q_3,\ ...,\ q_m\ \}
\end{align}\]</span></p>
<p>We use <strong>Arnoldi</strong> method to iterate and build our <span class="math inline">\(Q\)</span> basis (with <strong>Gram-Schmid</strong> orthogonalization - see also <strong>Matrix manipulation</strong> in <strong>Linear Algebra</strong> chapter for the orthogonal projections):</p>
<p><span class="math display">\[\begin{align}
q_1 {}&amp;= \frac{r_0}{\| r_0 \|_{L2}} = q\\
q_2 &amp;= \frac{Aq_1 - ( h_{1,1}q_1 ) }{\| Aq_1 - ( h_{1,1}q_1 )  \|} \\
q_3 &amp;= \frac{Aq_2 - ( h_{1,2}q_1 + h_{2,2}q_2 ) }{\| Aq_2 - ( h_{1,2}q_1 + h_{2,2}q_2 ) \|} \\
\vdots \nonumber \\
q_{j+1} &amp;= \frac{Aq_j - ( h_{1,j}q_1 + h_{2,j}q_2 + ... + h_{j,j}q_j ) }
{\| Aq_{j} - ( h_{1,j}q_j + h_{2,j}q_2 + ... + h_{j,j}q_j ) \|}  
\end{align}\]</span></p>
<p>Now, if there is such a target approximate solution vector <span class="math inline">\(\mathbf{\hat{x}}\)</span> that lies in the <strong>K space</strong> such that <span class="math inline">\(\mathbf{\hat{x}} \in K_m(A, q)\)</span>, then <span class="math inline">\(\mathbf{\hat{x}}\)</span> must be a linear combination and thus we can write <span class="math inline">\(\mathbf{\hat{x}}\)</span> as such:</p>
<p><span class="math display">\[
\mathbf{\hat{x}} = c_1q_1 + c_2q_2  + ... + c_mq_m = 
\left[
\begin{array}{rrrrr}
q_{1,1} &amp; q_{1,2} &amp; ... &amp; q_{1,m} \\
q_{2,1} &amp; q_{2,2} &amp; ... &amp; q_{2,m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
q_{n,1} &amp; q_{n,2} &amp; ... &amp; q_{n,m} \\
\end{array}
\right]_{Q_m}
\left[
\begin{array}{rrrrr}
c_1 \\ c_2 \\ \vdots \\ c_m
\end{array}
\right]_\beta = Q_m\beta
\]</span></p>
<p>Out of which, we formulate our equation for the solution:</p>
<p><span class="math display">\[\begin{align}
x = x^0 +  Q_m\beta\ \ \ \ \leftarrow\ \ \ \ x = x^0 + \mathbf{\hat{x}}
\end{align}\]</span></p>
<p>which is derived based on the following mathematical manipulation:</p>
<p><span class="math display">\[
\begin{array}{llll}
Ax   &amp;= b        &amp;\leftarrow&amp; \text{equation for our target solution} \\
Ax^0 &amp;= b - r^0  &amp;\leftarrow&amp; \text{equation for our initial solution}  
\end{array}
\]</span></p>
<p>Because our initial solution is not the actual solution, therefore, we expect some residual <span class="math inline">\(\mathbf{r^0}\)</span></p>
<p>Cancelling out <span class="math inline">\(\mathbf{\vec{b}}\)</span>, we then get:</p>

<p><span class="math display">\[\begin{align}
A(x - x^0) = r^0\ \ \ \ {}&amp;\rightarrow x - x^0 \in K_m(A, r^0) \\
&amp;\rightarrow x \in x^0 + K_m(A, r^0) \\
&amp;\rightarrow x = x^0 + Q_m\beta 
\end{align}\]</span>
</p>
<p>The <span class="math inline">\(\beta\)</span> is a minimizer. Before we compute for <span class="math inline">\(\beta\)</span>, let us consider a few equations first using the <span class="math inline">\(Q\)</span> basis. We know that:</p>
<p><span class="math display">\[\begin{align}
r^0 =  q \| r^0 \|_{L2}\ \ \ \leftarrow \ \ \ \ \ \ q = \frac{r^0}{\| r^0 \|_{L2}}
\end{align}\]</span></p>
<p>We also know that <span class="math inline">\(q\)</span> is the first vector in the <span class="math inline">\(Q\)</span> basis, and can thus be written as such:</p>
<p><span class="math display">\[\begin{align}
q = Q_{m}e_1 \in R^n
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
 e_1 = (1,0,0, ..., 0)\ \ \ \leftarrow \text{use to extract first column}
\]</span></p>
<p>Therefore:</p>

<p><span class="math display">\[\begin{align}
r^0 {}&amp;=  \| r^0 \|_{L2}\ q \\
r^0 {}&amp;=  \| r^0 \|_{L2}\ Q_{m}e_1 \\
Q_{m}^Tr^0 &amp;=  \| r^0 \|_{L2}\ Q_{m}^TQ_{m}\ e_1 \\
Q_{m}^Tr^0  &amp;=  \| r^0 \|_{L2}e_1
\end{align}\]</span>
</p>
<p>Now, to compute for <span class="math inline">\(\beta\)</span>, we perform a few mathematical manipulation.</p>

<p><span class="math display">\[\begin{align}
b - Ax {}&amp;= 0 &amp; {}&amp; \leftarrow Ax = b \\
b - A(x^0 + Q_{m}\beta ) &amp;= 0 &amp; &amp; \leftarrow x = x^0 + Q_{m}\beta \\
b - Ax^0 - AQ_{m}\beta &amp;= 0  \\
r^0 -  AQ_{m}\beta &amp;= 0 &amp; &amp; \leftarrow r^0 = b - Ax^0 \\
r^0 -  Q_{m}H_{m}\beta &amp;= 0 &amp; &amp; \leftarrow AQ_{m} = Q_{m}H_{m} \\
Q_{m}H_{m}\beta &amp;= r^0  &amp; &amp; \\
\beta &amp;= H_{m}^{-1}Q_{m}^Tr^0 &amp; &amp; \leftarrow Q_{m}^T = Q_{m}^{-1}\ \ \ \{orthogonal\}  \\
\beta &amp;=  H_{m}^{-1}\| r^0\|_{L2} e1 &amp; &amp; \leftarrow Q_{m}^T r^0 = \| r^0\|_{L2}e1
\end{align}\]</span>
</p>
<p><strong>Finally</strong>, let us now review the <strong>GMRES</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
x^0 \leftarrow \text{initial arbitrary nonzero vector} \\
r^0 = b - Ax^0 \\
&lt;Q,H&gt; = arnoldi\_iteration(A, r^0) \\
\rho = \| r^0 \|_{L2} \in R \\
\beta = ( H^{-1} \times \rho ) \cdotp e1 \in R^n \\
x = x^0 + Q \cdotp \beta
\end{array}
\]</span></p>
<p>Below is a naive implementation of <strong>GMRES</strong> in R code:</p>

<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1">gmres &lt;-<span class="st"> </span><span class="cf">function</span>(A, b) {</a>
<a class="sourceLine" id="cb64-2" data-line-number="2">    n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb64-3" data-line-number="3">    e1 =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="dv">-1</span>))</a>
<a class="sourceLine" id="cb64-4" data-line-number="4">    x0 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, n)</a>
<a class="sourceLine" id="cb64-5" data-line-number="5">    r0 =<span class="st"> </span>b <span class="op">-</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>x0</a>
<a class="sourceLine" id="cb64-6" data-line-number="6">    arnoldi =<span class="st"> </span><span class="kw">arnoldi_method</span>(A, r0 )</a>
<a class="sourceLine" id="cb64-7" data-line-number="7">    rho =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>( ( r0 )<span class="op">^</span><span class="dv">2</span> )) <span class="co"># minimize normal residual</span></a>
<a class="sourceLine" id="cb64-8" data-line-number="8">    beta =<span class="st"> </span>(<span class="kw">solve</span>(arnoldi<span class="op">$</span>H) <span class="op">*</span><span class="st"> </span>rho) <span class="op">%*%</span><span class="st"> </span>e1</a>
<a class="sourceLine" id="cb64-9" data-line-number="9">    x =<span class="st"> </span>x0 <span class="op">+</span><span class="st"> </span>arnoldi<span class="op">$</span>Q <span class="op">%*%</span><span class="st"> </span>beta</a>
<a class="sourceLine" id="cb64-10" data-line-number="10">    <span class="kw">list</span>(<span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;b&quot;</span>=<span class="kw">c</span>(b), <span class="st">&quot;x&quot;</span>=<span class="kw">c</span>(x))</a>
<a class="sourceLine" id="cb64-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb64-12" data-line-number="12">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb64-13" data-line-number="13">b =<span class="st"> </span><span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">5</span>,<span class="dv">6</span>)</a>
<a class="sourceLine" id="cb64-14" data-line-number="14"><span class="kw">gmres</span>(A, b)</a></code></pre></div>
<pre><code>## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $b
## [1] 6 5 6
## 
## $x
## [1]  1  2 -1</code></pre>

<p>There are other variations (or modifications) to the original <strong>GMRES</strong> method - introduced by Saad Schultz (1986). We leave them for the readers to investigate.</p>
<p>When it comes to iterative methods in solving for systems of equations, especially suitable for large sparse matrices, there are two other <strong>Krylov-based</strong> iterative methods to mention: <strong>Generalized Minimal Residual (GMRES)</strong> method and <strong>Conjugate Gradient (CG)</strong> method.</p>
<p>The <strong>GMRES</strong> method allows for solving systems of equations whose corresponding matrices are highly sparse. Two other Krylov-based iterative methods are illustrated next.</p>
</div>
<div id="conjugate-gradient-method-cg" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.3</span> Conjugate Gradient Method (CG)  <a href="numericallinearalgebra.html#conjugate-gradient-method-cg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Conjugate Gradient</strong> method is suitable for <strong>symmetric positive definite (SPD)</strong> matrices.</p>
<p>There are two ways to look at this.</p>
<p><strong>First way to look at this: </strong> <strong>CG</strong> method is a <strong>Krylov-base</strong> method.</p>
<p>The idea is that if <span class="math inline">\(\mathbf{x}\)</span> is the true solution, then it follows that:</p>
<p><span class="math display">\[
Ax - b = 0
\]</span></p>
<p>But because we are clueless about what the actual value is, we can only approximate for <span class="math inline">\(\mathbf{x}\)</span> iteratively starting with an initial <strong>guess</strong> - call it <span class="math inline">\(\mathbf{x_0}\)</span>. In effect, an approximation yields a residual - call it <span class="math inline">\(\mathbf{r_0}\)</span>. Thus, we modify the equation slightly to account for the residual:</p>
<p><span class="math display">\[
Ax_0 - b= r_0
\]</span></p>
<p>Let us take those two equations to derive an equation that incorporates our approximation, and that manifests a <strong>Krylov subspace</strong>:</p>
<p><span class="math display">\[\begin{align}
Ax {}&amp;= b,\ \ \ \ Ax_0 = b - r_0 \\
Ax - Ax_0 &amp;= b - (b - r_0) \\
A(x - x_0) &amp;= r_0 \\
(x - x_0) &amp;= A^{-1} r_0 \\
x &amp;= x_0 + A^{-1} r_0 \\
x &amp;= x_0 + K_m\beta\ \ \ \leftarrow\ \ \ \ if\ x \in x_0 + K_m(A, r_0)\
\end{align}\]</span></p>
<p>The derivation looks familiar because that is discussed in the <strong>GMRES</strong> method. It comes down to how <strong>GMRES</strong> and <strong>CG</strong> handle the minimizer differently.</p>
<p>In <strong>GMRES</strong> method, the norm of the residual <span class="math inline">\(\| \mathbf{r_k}\|\)</span> is minimized.</p>
<p><span class="math display">\[\begin{align}
arg\ min_{x \in K_k} \|Ax_k - b = r_k \|_{L2}
\end{align}\]</span></p>
<p>In <strong>CG</strong> method, the norm of the <strong>error energy</strong> <span class="math inline">\(\| \mathbf{e_k} = x - xk\|\)</span> - also called <strong>A-norm</strong> - is minimized.</p>
<p><span class="math display">\[\begin{align}
arg\ min_{x \in K_k} \|x - x_k   \|_A \equiv (x - x_k )^TA(x - x_k)  
\end{align}\]</span></p>
<p>That also becomes apparent when we discuss <strong>steepest descent</strong> where <strong>direction</strong> is optimal if it is orthogonal to the tangent line (See Figure <a href="numericallinearalgebra.html#fig:gradientconjugacy">3.9</a>). We will cover more of that statement later.</p>
<p>See <strong>Lanczos</strong> algorithm as a reference to derive <strong>Krylov-based CG</strong> method. It resembles the algorithm we discuss next for a second way of looking at the <strong>CG</strong> method.</p>
<p><strong>Second way to look at this: </strong> If we take the <strong>CG</strong> method as a modified <strong>Steepest Descent</strong> method, let us first understand the concept behind the <strong>Steepest Descent</strong> method. The plotted graph in Figure <a href="numericallinearalgebra.html#fig:gradientconjugacy">3.9</a> may help us to get an intuition around the <strong>Steepest Descent</strong> method and the <strong>CG</strong> method. Note that the figure assumes an optimal direction orthogonal to the tangent line. In practice, <strong>Steepest Descent</strong> tends toward a crooked path.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gradientconjugacy"></span>
<img src="embed0006.png" alt="Gradient and Conjugacy" width="70%" />
<p class="caption">
Figure 3.9: Gradient and Conjugacy
</p>
</div>

<p>As shown, the <strong>black arrows</strong> denoted by <span class="math inline">\((\ d_0,\ d_1,\ d_2,\ d_3,\ d_4\ )\)</span> are the vectors called <strong>gradients</strong> - each <strong>gradient</strong> represents the direction (or the path) of travel from a starting point (black dot) to a target point (red dot). A single vector with partial derivative elements is a <strong>gradient</strong> with a <span class="math inline">\(\nabla\)</span> symbol. For example:</p>
<p><span class="math display">\[\begin{align}
\nabla f(x) = f&#39;(x) =
\left[
\begin{array}{rrrrr}
\frac{ \partial f }{\partial x_1 } &amp; \frac{ \partial f }{\partial x_2 } &amp; ... &amp;
\frac{ \partial f }{\partial x_n }
\end{array}
\right]^T\ \ \ \ \leftarrow \text{ gradient of } f(x) \in R^n \label{eqn:eqnnumber2}
\end{align}\]</span></p>
<p>Here, for the sake of explanation, <strong>gradient</strong> <span class="math inline">\(\nabla f(x_n)\)</span> equals the <strong>direction</strong> <span class="math inline">\(d_n\)</span>. Other methods may manipulate the <strong>direction</strong> however for better optimization which becomes apparent later.</p>
<p><span class="math display">\[\begin{align}
\nabla f(x_n) = d_n
\end{align}\]</span></p>
<p>In the figure, each <strong>gradient</strong> obeys <strong>conjugacy</strong> between two contour lines if the direction of its path is orthogonal to the curves (contour lines) that it intersects. For example, there are nine contour lines labeled consecutively (5, 10, 15, 20, 25, 30). The point (in black) at which we start our travel intersects at contour line 25. We make a <span class="math inline">\(90^\circ\)</span> (orthogonal) hop to the next contour line at 20. In other words, the direction is orthogonal to the tangent line that follows the slope of the curvatures of the contour lines.</p>
<p>The length of a gradient (e.g. <span class="math inline">\(d_n\)</span>) is denoted as <span class="math inline">\(\mathbf{\alpha_n}\)</span>. We also call the length a <strong>stepsize</strong> (or <strong>steplength</strong>). It can be seen that a combination of the direction (the gradient) and steplength, <span class="math inline">\(\alpha_nd_n\)</span>, represents a <strong>step</strong> to the next sub-solution <span class="math inline">\(\mathbf{x_{k+1}}\)</span> - where <span class="math inline">\(\mathbf{x_k}\)</span> is the current position - which is one step closer towards the final destination. It can be expressed in an equation called the <strong>line search method</strong>:</p>
<p><span class="math display">\[\begin{align}
x_{k+1} = x_k + \alpha_{k}d_{k}
\end{align}\]</span></p>
<p>Note that the graph in Figure <a href="numericallinearalgebra.html#fig:gradientconjugacy">3.9</a> is quite tuned to try to reach an explanation around the concept of <strong>gradient</strong> and <strong>conjugacy</strong> - however, <strong>steepest descent</strong> and <strong>conjugate gradient</strong> do not necessarily walk the path that lands orthogonal unto each contour lines. Two directions are used to walk the path: the <strong>gradient direction</strong> and the <strong>conjugate direction</strong>.</p>
<p>The <strong>steepest descent</strong> walks the path in a <strong>gradient direction</strong>. Mathematically, it is expressed as:</p>
<p><span class="math display">\[\begin{align}
x_{k+1} = x_{k} + \mathbf{\alpha_k}\nabla f(x_k) 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\nabla f(x_k)\)</span> is the gradient direction.</p>
<p>The method iterates all the way until it reaches a <strong>critical point</strong> - the solution <span class="math inline">\(\mathbf{x^*}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{x_1} =  x_{0} + \mathbf{\alpha_0}\nabla f(x_0),\ \ \ \ \ \ \ \ \ \
\mathbf{x_2} =  x_{1} + \mathbf{\alpha_1}\nabla f(x_1),\ \ \ \ \ \ \ \ \ \
\mathbf{x_3} =  x_{2} + \mathbf{\alpha_2}\nabla f(x_2)
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathbf{x^*} &amp;= (( x_{0} + \mathbf{\alpha_0} \nabla f(x_0)) + \mathbf{\alpha_1} \nabla f(x_1)) + \mathbf{\alpha_2} \nabla f(x_2) \\
\mathbf{x^*} &amp;= x_{0} + \sum_k^m \mathbf{\alpha_k} \nabla f(x_k)
\end{align}\]</span></p>
<p>It is notable to talk about <strong>gradient descent</strong> as a side note. The difference between <strong>gradient descent</strong> and <strong>steepest descent</strong> is the use of the <strong>stepsize</strong>. In <strong>gradient descent</strong>, the <strong>stepsize</strong> is initialized to a fixed scalar value and then used repeatedly in the iteration:</p>
<p><span class="math display">\[\begin{align}
\alpha {}&amp;=  &lt;initial\ value&gt; \nonumber \\
\mathbf{x^*} &amp;= x_{0} + \sum_k^m \mathbf{\alpha} \nabla f(x_k)  
\end{align}\]</span></p>
<p>As for <strong>conjugate descent</strong>, the method follows a <strong>gradient direction</strong> as the first step, similar to <strong>steepest descent</strong>; after which, the rest of the steps follow a <strong>conjugate direction</strong>.</p>
<p><span class="math display">\[\begin{align}
d_k = \begin{cases}
\nabla (f{x_k}) &amp; if\ k=0\ \ \ \ \ \ \leftarrow \text{gradient direction}\\ 
\nabla (f{x_k}) + \beta_k d_{k-1} &amp;  if\ k &gt;= 1\ \ \  \leftarrow \text{conjugate direction}
\end{cases} \label{eqn:eqnnumber700}
\end{align}\]</span></p>
<p>Here, a <strong>conjugate direction</strong> is a direction in which two vectors, u and v, are A-orthogonal (or conjugate) which is a <strong>conjugate property</strong> expressed as:</p>
<p><span class="math display">\[\begin{align}
&lt;u,v&gt;_A = u^TAv = 0, \ \ \ \rightarrow\ \ \ \ \  d_{k}^TAd_{k+1} = 0 
\end{align}\]</span></p>
<p>Set that aside for a moment. In terms of deriving <span class="math inline">\(\alpha_k\)</span>, let us first have a brief description of <strong>conjugate gradient</strong>. The <strong>conjugate gradient</strong> method was introduced by <strong>Hestenes and Stiefel (1952)</strong> with the intent to minimize a non-linear quadratic function: </p>
<p><span class="math display">\[\begin{align}
arg\ min_{x \in R^n} \ f(x) = \frac{1}{2}x^TAx - b^Tx + c.
\end{align}\]</span></p>
<p>But by minimizing the function - solving for the <strong>gradient</strong> - it thus also ends up <strong>linearizing</strong> the equation; and, in effect, solving for a linear equation:</p>
<p><span class="math display">\[\begin{align}
\nabla f(x) = f&#39;(x) = Ax - b
\end{align}\]</span></p>
<p>Additionally, because we deal with approximation, it is notable to mention here that in the <strong>conjugate gradient</strong> method, the first step takes <strong>gradient</strong> not only as the <strong>gradient direction</strong> but also as the <strong>residual</strong>; after which, the rest of the steps take the <strong>conjugate gradient</strong> as the <strong>residual</strong>:</p>
<p><span class="math display">\[\begin{align}
r_k = \begin{cases} 
\nabla f(x_k) &amp; if\ k=0\ \ \ \ \ \ \ \leftarrow \text{gradient}\\ 
\nabla f(x_{k-1}) - \alpha_{k-1} A d_{k-1} &amp;  if\ k &gt;= 1\ \ \ \ \leftarrow \text{conjugate gradient}
\end{cases} \label{eqn:eqnnumber701}
\end{align}\]</span></p>
<p>Furthermore, we notice the emergence of <strong>alpha</strong> <span class="math inline">\(\alpha\)</span> and <strong>beta</strong> <span class="math inline">\(\beta\)</span> symbols. It is essential to mention that there are choices for solving <span class="math inline">\(\beta_k\)</span> (<strong>conjugate gradient parameter</strong>) and <span class="math inline">\(\alpha_k\)</span> (<strong>stepsize</strong>); though, we leave these choices for the readers to further investigate <span class="citation">(Hager W. W. and Zhang H. <a href="bibliography.html#ref-ref306w">2005</a>)</span>:</p>
<ul>
<li>Hestenes-Stiefel (1952)</li>
<li>Fletcher-Reeves (1964)</li>
<li>Polak-Ribiere-Polyak (1969)</li>
<li>Liu-Storey (1991)</li>
<li>Dai-Yuan (1999)</li>
<li>Hager-Zhang (2005)</li>
</ul>
<p>For example, <strong>Polak-Riebre</strong> method computes for <strong>stepsize</strong> <span class="math inline">\(\alpha_k\)</span> using the following equation: </p>
<p><span class="math display">\[\begin{align}
\alpha_k 
= \frac{\nabla f_(x_k)^Td_k}{d_k^TAd_k}
 =\frac{r_k^Td_k}{d_k^TAd_k}
\ \ \ \ \ \rightarrow\ \ \ \  \ where\ \nabla f(x_k)  = r_k = Ax_k - b 
\end{align}\]</span></p>
<p>In our case, we use the <strong>Fletcher-Reeves</strong> method to compute for <span class="math inline">\(\alpha_k\)</span> expressed as such (with no complete derivation included): </p>
<p><span class="math display">\[\begin{align}
\alpha_k = \frac{d_k^Td_k}{d_k^TAd_k}
\end{align}\]</span></p>
<p>As for <span class="math inline">\(\beta_k \in R\)</span>, we need this <strong>conjugate gradient parameter</strong> to compute for the next <strong>conjugate direction</strong> <span class="math inline">\(d_{k+1}\)</span>:</p>
<p><span class="math display">\[\begin{align}
d_{k+1} = r_{k} + \beta_kd_k
\end{align}\]</span></p>
<p>For that, we also use the <strong>Fletcher-Reeves</strong> method to compute for <span class="math inline">\(\beta_k\)</span> (with no complete derivation included):</p>
<p><span class="math display">\[\begin{align}
\beta_k = \frac{r_{k+1}^TAd_k}{d_k^TAd_k} = \frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k}
\end{align}\]</span></p>
<p>Overall, suppose we are to find our way to the final destination (<strong>the critical point</strong>) - the final approximate solution, which is <span class="math inline">\(\mathbf{x^*} \in R^n\)</span>. In that case, we need a linear combination of all the paths (which is linearly independent, granting A-orthogonality is zero). In our case, our target is to reach zero - our solution for <span class="math inline">\(\mathbf{x^*}\)</span>.</p>
<p><span class="math display">\[\begin{align}
x^* &amp;= x_0 + \sum_{k=1}^m \alpha_{k-1}d_{k-1} \\
&amp;= x_0 + \alpha_0d_0 + \alpha_1d_1 + \alpha_2d_2 + \alpha_3d_3 + ... +  \alpha_{m-1}d_{m-1} = 0 \nonumber
\end{align}\]</span></p>
<p>As final touch, we <strong>minimize</strong> gradient, <span class="math inline">\(r_k = \nabla f = 0\)</span>, which we use to gauge for convergence. The <strong>goal</strong> is to reach tolerance level (e.g.Â tol=1e-5) which is zero by using the norm of the gradient:</p>
<p><span class="math display">\[
\| r_k  \| &lt; tol
\]</span></p>
<p>With all that explained, let us now illustrate <strong>CG</strong> using <strong>SPD</strong> matrix <strong>A</strong> and vector <strong>b</strong>:</p>
<p><span class="math display">\[
Ax = b\ \ \ \ \rightarrow\ \ \ \
\left[
\begin{array}{rrrr}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 9 &amp; 6 &amp; 3 \\
3 &amp; 6 &amp; 9 &amp; 2 \\
4 &amp; 3 &amp; 2 &amp; 1
\end{array}
\right]_A
\left[\begin{array}{r} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]_x =
\left[\begin{array}{r} 30 \\ 50 \\ 50 \\ 20 \end{array}\right]_b
\]</span>
<strong>First</strong>, initialize <span class="math inline">\(x = x^0\)</span>:</p>
<p><span class="math display">\[
x_0 = \left[\begin{array}{rrrr} 1 &amp; 2 &amp; 1 &amp; 1\end{array}\right]^T
\]</span></p>
<p><strong>Second</strong>, compute for the initial direction <span class="math inline">\(\mathbf{d_0}\)</span> - the <strong>first gradient direction of the steepest descent</strong> equivalent to the <strong>first residual</strong>:</p>
<p><span class="math display">\[
r_0 = d_0 = b - Ax_0 = 
\left[\begin{array}{r} 30 \\ 50 \\ 50 \\ 20 \end{array}\right]_b -
\left[
\begin{array}{rrrr}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 9 &amp; 6 &amp; 3 \\
3 &amp; 6 &amp; 9 &amp; 2 \\
4 &amp; 3 &amp; 2 &amp; 1
\end{array}
\right]_A
\left[\begin{array}{r} 1 \\ 2 \\ 1 \\ 1 \end{array}\right]_{x_0} =
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0}
\]</span>
where <span class="math inline">\(d_0\)</span> is the <strong>gradient direction</strong>; meaning, we come from a zero starting point (the black dot in Figure <a href="numericallinearalgebra.html#fig:gradientconjugacy">3.9</a>).</p>
<p><strong>Third</strong>, start the iteration by computing for <strong>stepsize</strong> <span class="math inline">\(\mathbf{\alpha_0}\)</span> using <strong>Fletcher-Reeves</strong> equation:</p>
<p><span class="math display">\[
\alpha_0 = \frac{r_0^Tr_0}{d_0^TAd_0} = 
\frac{
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0}^T
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0}
}
{
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{d_0}^T
\left[
\begin{array}{rrrr}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 9 &amp; 6 &amp; 3 \\
3 &amp; 6 &amp; 9 &amp; 2 \\
4 &amp; 3 &amp; 2 &amp; 1
\end{array}
\right]_A
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{d_0}
} =
\frac{1390}{22240}_{\alpha_0}
\]</span>
<strong>Fourth</strong>, then compute for the solution <span class="math inline">\(\mathbf{x_1}\)</span>:</p>
<p><span class="math display">\[
x_1 = x_0 + \alpha_0 d_0 = 
\left[\begin{array}{r} 1 \\ 2 \\ 1 \\ 1 \end{array}\right]_{x_0} +
\frac{1390}{22240}_{\alpha_0}
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{d_0} = 
\left[\begin{array}{r} 2.1250 \\ 3.3125 \\ 2.5000 \\ 1.4375 \end{array}\right]_{x_1}
\]</span></p>
<p><strong>Fifth</strong>, compute for the next <strong>gradient</strong> (or residual) <span class="math inline">\(\mathbf{r_1}\)</span>:</p>
<p><span class="math display">\[
r_1 = r_0 - \alpha_0 A d_0 = 
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0} -
\frac{1390}{22240}
\left[
\begin{array}{rrrr}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 9 &amp; 6 &amp; 3 \\
3 &amp; 6 &amp; 9 &amp; 2 \\
4 &amp; 3 &amp; 2 &amp; 1
\end{array}
\right]_A
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{d_0} = 
\left[\begin{array}{r} 8.000 \\ -3.375 \\ -1.625 \\ -4.875 \end{array}\right]_{r_1}
\]</span></p>
<p><strong>Sixth</strong>, compute for convergence (expecting the gradient, <span class="math inline">\(r_1\)</span>, to become zero):</p>
<p><span class="math display">\[
if\ (\ \| r_1\|_{L2}\ &lt;\ tol\ )\, then\ it\ converges\  
\]</span></p>
<p><strong>Seventh</strong>, compute for <strong>conjugate gradient parameter</strong> <span class="math inline">\(\beta_0\)</span>:</p>
<p><span class="math display">\[
\beta_0 = \frac{r_1^Tr_1}{r_0^Tr_0} = 
\frac{
\left[\begin{array}{r} 8.000 \\ -3.375 \\ -1.625 \\ -4.875 \end{array}\right]_{r_1}^T
\left[\begin{array}{r} 8.000 \\ -3.375 \\ -1.625 \\ -4.875 \end{array}\right]_{r_1}
}
{
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0}^T
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0}
} =
0.07323516_{\beta_0}
\]</span></p>
<p><strong>Eight</strong>, then compute for the next direction <span class="math inline">\(\mathbf{d_1}\)</span> - this is a <strong>conjugate direction</strong>:</p>
<p><span class="math display">\[
d_1 = r_1 + \beta_0 d_0 = 
\left[\begin{array}{r} 8.000 \\ -3.375 \\ -1.625 \\ -4.875 \end{array}\right]_{r_1} +
0.07323516_{\beta_0}
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{d_0} = 
\left[\begin{array}{r} 9.3182329 \\ -1.8370616 \\ 0.1326439 \\ -4.3623539 \end{array}\right]_{d_1}
\]</span></p>
<p>From here, we need to <strong>iterate</strong> by repeating from <strong>third</strong> step until <span class="math inline">\(x_{k+1}\)</span> converges (<span class="math inline">\(e_{k+1}\)</span> &lt; tol).</p>
<p><strong>Finally</strong>, after convergence, our solution for <strong>x</strong> is expected to be: <span class="math inline">\(\left[\begin{array}{rrrr} 1 &amp; 2 &amp; 3 &amp; 4 \end{array}\right]_{x}^T\)</span></p>
<p>The steps above follow the <strong>Conjugate Gradient</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
x_0 \leftarrow \text{initial arbitrary nonzero vector} \\
r_0 = d_0 = b - Ax_0 \\
loop\ k\ in\ 1:\ ... \\
\ \ \ \ \ \alpha_k = \frac{r_k^Tr_k}{r_k^TAr_k}\\
\ \ \ \ \ x_{k+1} = x_k + \alpha_k d_k  \\
\ \ \ \ \ r_{k+1} = r_k - \alpha_k A d_k  \\
\ \ \ \ \ if\ (\ \| r_{k+1} \|_{L2}\ &lt;\ tol\ )\ break \\
\ \ \ \ \ \beta_k = \frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k} \\
\ \ \ \ \ d_{k+1} = r_{k+1} + \beta_k r_k \\
end\ loop
\end{array}
\]</span></p>
<p>We now show a naive implementation of <strong>Conjugate Gradient</strong> in R code:</p>

<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" data-line-number="1">conjugate_gradient &lt;-<span class="st"> </span><span class="cf">function</span>(A, x0, b) {</a>
<a class="sourceLine" id="cb66-2" data-line-number="2">    n =<span class="st"> </span><span class="kw">length</span>(x0)</a>
<a class="sourceLine" id="cb66-3" data-line-number="3">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb66-4" data-line-number="4">    limit =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb66-5" data-line-number="5">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb66-6" data-line-number="6">    r =<span class="st"> </span>d =<span class="st"> </span>b <span class="op">-</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>(<span class="dt">x=</span>x0)</a>
<a class="sourceLine" id="cb66-7" data-line-number="7">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb66-8" data-line-number="8">        <span class="cf">if</span> (k<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb66-9" data-line-number="9">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(k, x, err))</a>
<a class="sourceLine" id="cb66-10" data-line-number="10">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb66-11" data-line-number="11">            q =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>d</a>
<a class="sourceLine" id="cb66-12" data-line-number="12">            r_k =<span class="st"> </span><span class="kw">c</span>( <span class="kw">t</span>(r) <span class="op">%*%</span><span class="st"> </span>r )</a>
<a class="sourceLine" id="cb66-13" data-line-number="13">            alpha =<span class="st"> </span><span class="kw">c</span>( r_k <span class="op">/</span><span class="st"> </span>( <span class="kw">t</span>(d) <span class="op">%*%</span><span class="st"> </span>q ) )</a>
<a class="sourceLine" id="cb66-14" data-line-number="14">            x =<span class="st"> </span>x <span class="op">+</span><span class="st"> </span>alpha <span class="op">*</span><span class="st"> </span>d</a>
<a class="sourceLine" id="cb66-15" data-line-number="15">            r =<span class="st"> </span>r <span class="op">-</span><span class="st"> </span>alpha <span class="op">*</span><span class="st"> </span>q</a>
<a class="sourceLine" id="cb66-16" data-line-number="16">            err =<span class="st">  </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((r)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb66-17" data-line-number="17">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(k, x,  err))</a>
<a class="sourceLine" id="cb66-18" data-line-number="18">            <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb66-19" data-line-number="19">            beta =<span class="st"> </span><span class="kw">c</span>( ( <span class="kw">t</span>(r) <span class="op">%*%</span><span class="st"> </span>r ) <span class="op">/</span><span class="st"> </span>r_k )</a>
<a class="sourceLine" id="cb66-20" data-line-number="20">            d =<span class="st"> </span>r <span class="op">+</span><span class="st"> </span>beta <span class="op">*</span><span class="st"> </span>d</a>
<a class="sourceLine" id="cb66-21" data-line-number="21">        }</a>
<a class="sourceLine" id="cb66-22" data-line-number="22">   }</a>
<a class="sourceLine" id="cb66-23" data-line-number="23">   <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;K&quot;</span>, <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n),<span class="st">&quot;&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>), </a>
<a class="sourceLine" id="cb66-24" data-line-number="24">                          <span class="st">&quot;error&quot;</span>)     </a>
<a class="sourceLine" id="cb66-25" data-line-number="25">   <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;matrix&quot;</span>=A, </a>
<a class="sourceLine" id="cb66-26" data-line-number="26">        <span class="st">&quot;b&quot;</span>=<span class="kw">c</span>(b), <span class="st">&quot;x&quot;</span>=<span class="kw">c</span>(x),  <span class="st">&quot;error&quot;</span>=err)</a>
<a class="sourceLine" id="cb66-27" data-line-number="27">}</a>
<a class="sourceLine" id="cb66-28" data-line-number="28">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>, <span class="dv">2</span>,<span class="dv">9</span>,<span class="dv">6</span>,<span class="dv">3</span>, <span class="dv">3</span>,<span class="dv">6</span>, <span class="dv">9</span>,<span class="dv">2</span>, <span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>), <span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb66-29" data-line-number="29">x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb66-30" data-line-number="30">b =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>x  <span class="co"># (30, 50, 50, 20)</span></a>
<a class="sourceLine" id="cb66-31" data-line-number="31"><span class="kw">conjugate_gradient</span>(A,  <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>), b)</a></code></pre></div>
<pre><code>## $Iteration
##      K         x1       x2       x3       x4        error
## [1,] 0  1.0000000 2.000000 1.000000 1.000000 0.000000e+00
## [2,] 1  2.1250000 3.312500 2.500000 1.437500 1.008944e+01
## [3,] 2 -2.4570919 4.215846 2.434774 3.582618 1.269644e+01
## [4,] 3  0.9623681 1.963518 3.074545 3.945714 3.247722e-01
## [5,] 4  1.0000000 2.000000 3.000000 4.000000 3.947232e-14
## 
## $matrix
##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3    4
## [2,]    2    9    6    3
## [3,]    3    6    9    2
## [4,]    4    3    2    1
## 
## $b
## [1] 30 50 50 20
## 
## $x
## [1] 1 2 3 4
## 
## $error
## [1] 3.947232e-14</code></pre>

<p>Note that the closest approximate value is already achieved after the 4th iteration, for which it can be noticed that an SPD matrix <span class="math inline">\(A \in R^{nxn}\)</span> yields at most <span class="math inline">\(nth\)</span> iteration.</p>
<p>For further reading, please consider investigating other <strong>Krylov-subspace</strong> methods for solving <span class="math inline">\(Ax = b\)</span>:</p>
<ul>
<li>LSQR - Least Squares QR-factorization Method</li>
<li>MINRES - Minimal Residual Method</li>
<li>SYMMLQ - Symmetric LQ Method</li>
<li>BiCG - Bi-Conjugate Gradient</li>
<li>QMR - Quasi-minimal Residual Method</li>
<li>FOM - Full Orthogonal Method</li>
</ul>
<p>Now, let us discuss <strong>non-Krylov-subspace</strong> methods for solving <span class="math inline">\(Ax=b\)</span>, starting with <strong>Jacobi and Gauss-Seidel Method</strong>.</p>
</div>
<div id="jacobi-and-gauss-seidel-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.4</span> Jacobi and Gauss-Seidel Method <a href="numericallinearalgebra.html#jacobi-and-gauss-seidel-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If analytical methods cannot solve a system of polynomial equations, we <strong>use iterative methods to approximate the solution</strong>. The <strong>Jacobi</strong> method and <strong>Gauss-Seidel</strong> method - also known as the <strong>Liebmann</strong> method - are such iterative methods of solving systems of equations.</p>
<p>Given a <strong>system of linear equations</strong> and its expanded form below (See Equation <span class="math inline">\(\ref{eqn:eqnnumber1}\)</span>) <span class="citation">(Dr.Â S. Karunanithi et al. <a href="bibliography.html#ref-ref49d">2018</a>; Saha M. and Chakrabarty J. <a href="bibliography.html#ref-ref53m">2018</a>)</span>:</p>
<p><span class="math display">\[\begin{align*}
a_{1,1}x_1 +  a_{1,2}x_2 + a_{1,3}x_3 +\ ...\ + a_{1,n}x_n  {}&amp;= b_1\\
a_{2,1}x_1 +  a_{2,2}x_2 + a_{2,3}x_3 +\ ...\ + a_{2,n}x_n  &amp;= b_2\\
\vdots \\
a_{n,1}x_1 +  a_{n,2}x_2 + a_{n,3}x_3 +\ ...\ + a_{n,n}x_n  &amp;= b_n,
\end{align*}\]</span></p>
<p>Both methods iteratively solve for <strong>x</strong> by separation of variables. Here we separate <span class="math inline">\(\mathbf{x_i}\)</span> to the left side of the equation. For example:</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= ( b_1 - a_{1,2}x_2 - a_{1,3}x_3 -\ ...\ - a_{1,n}x_n)/a_{1,1}\\
x_2 &amp;= ( b_2 - a_{2,1}x_1 - a_{2,3}x_3 -\ ...\ - a_{2,n}x_n)/a_{2,2}\\
\vdots \\
x_3 &amp;= (b_n - a_{n,1}x_1 -  a_{n,2}x_2 -\ ...\ - a_{n,n}x_n)/a_{n,3}\\
\end{align*}\]</span></p>
<p>To illustrate, let us use the following equations (square matrix):</p>
<p><span class="math display">\[\begin{align*}
9x_1 + 1x_2 + 2x_3 {}&amp;= 6\\
3x_1 + 4x_2 + 1x_3 &amp;= 5\\
3x_1 + 1x_2 + 28x_3 &amp;= 6\\
\end{align*}\]</span></p>
<p><strong>First</strong>, let us separate the <strong>x</strong> variables such that we get the following:</p>
<p><span class="math display">\[\begin{align*}
x1 {}&amp;= (6 - 1x_2 - 2x_3)/9 = 6/9 - 1/9x_2 - 2/9x_3  \\
x2 &amp;= (5 - 3x_1 - 1x_3)/4 = 5/4 - 3/4x_1 - 1/4x_3  \\
x3 &amp;= (6 - 3x_1 - 1x_2)/28 = 6/28 - 3/28x_1 - 1/28x_2  \\
\end{align*}\]</span></p>
<p>That order of the equation will be used throughout the iteration.</p>
<p><strong>Second</strong>, we assume the following initialization: <span class="math inline">\(x_1=0, x_2=0, x_3=0\)</span>.</p>
<p>Now, in the <strong>Jacobi Method</strong>, we perform the following iteration:</p>
<p><span class="math display">\[\begin{align*}
x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (0) - (0) = 6/9\\
x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3  = 5/4 - 3/4(0) - 1/4(0) = 5/4\\
x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(0) - 1/28(0) = 6/28\\
\end{align*}\]</span></p>
<p>Then, we go through the second iteration with the following values: <span class="math inline">\(x_1=6/9, x_2=5/4, x_3=6/28\)</span></p>
<p><span class="math display">\[\begin{align*}
x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (5/4) - (6/28) = 168/211\\
x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3  = 5/4 - 3/4(6/9) - 1/4(6/28) = 39/56\\
x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(6/9) - 1/28(5/4) = 11/112\\
\end{align*}\]</span></p>
<p>We then continue with the iteration using the new <strong>x</strong> values until convergence.</p>
<p>However, in the <strong>Gauss-Seidel Method</strong>, we perform the following iteration instead:</p>
<p><span class="math display">\[\begin{align*}
x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (0) - (0) = 6/9\\
x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3  = 5/4 - 3/4(6/9) - 1/4(0) = 3/4\\
x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(6/9) - 1/28(3/4) = 13/112\\
\end{align*}\]</span></p>
<p>Then, we go through the second iteration with the following values: <span class="math inline">\(x_1=6/9, x_2=3/4, x_3=13/112\)</span></p>
<p><span class="math display">\[\begin{align*}
x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (3/4) - (13/112) = 0.73765\\
x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3  = 5/4 - 3/4(0.73765) - 1/4(13/112) = 0.66774\\
x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(0.73765) - 1/28(0.66774) = 0.11140\\
\end{align*}\]</span></p>
<p>We then continue with the iteration using the new <strong>x</strong> values until convergence.</p>
<p>Notice that the <strong>Jacobi</strong> method uses the values of the previous iteration while the <strong>Gauss-Seidel</strong> method uses new values immediately within the iteration.</p>
<p>Here are the algorithms for the two methods:</p>
<p><span class="math display">\[
\begin{array}{l}
\mathbf{Jacobi}\\
===============\\
loop \\
\ \ \ \ \phi  = x\\
\ \ \ \ loop\ i\ in\ 1..n\\
\ \ \ \ \ \ \ \ s = 0\\
\ \ \ \ \ \ \ \ loop\ k\ in\ 1..n\\
\ \ \ \ \ \ \ \ \ \ \ \ if ( i \ne k):\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s = s + A_{ik} * x_k \\
\ \ \ \ \ \ \ \ end\ loop\\
\ \ \ \ \ \ \ \ \phi_i = (b_i - s) / A_{ii}\\
\ \ \ \ end\ loop\\
\ \ \ \ x = \phi \\
\ \ \ \ if\ \|Ax - b \| &lt; tol\ then\ break\\
end\ loop
\end{array}
\left|
\begin{array}{l}
\mathbf{\text{Gauss-Seidel}}\\
===============\\
loop \\
\ \ \ \ loop\ i\ in\ 1..n\\
\ \ \ \ \ \ \ \ s = 0\\
\ \ \ \ \ \ \ \ loop\ k\ in\ 1..n\\
\ \ \ \ \ \ \ \ \ \ \ \ if ( i \ne k):\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s = s + A_{ik} * x_k \\
\ \ \ \ \ \ \ \ end\ loop\\
\ \ \ \ \ \ \ \ x_i = (b_i - s) / A_{ii}\\
\ \ \ \ end\ loop\\
\ \ \ \ if\ \|Ax - b \| &lt; tol\ then\ break\\
end\ loop\\
 \\
 \\
 \end{array}
\right.
\]</span></p>
<p>Note that both methods do not always result in convergence. There are matrices that do not converge using the methods. One way to identify them is using <strong>spectral radius</strong> - denoted as <span class="math inline">\(\rho(A)\)</span> - the maximum absolute eigenvalue of a matrix (system of linear equations). The approximate solution has a higher chance of convergence if the <strong>spectral radius</strong> is less than 0.70.</p>
<p><span class="math display">\[
\rho(R/D) = \rho(D^{-1}R)&lt; 0.70
\]</span></p>
<p>where <strong>D</strong> is the diagonal of A, and <span class="math inline">\(\mathbf{R=(L + U)}\)</span> is the non-diagonal of A. We normalize A using its diagonal entries to test if the matrix is diagonally dominant.</p>
<p>Note that we choose to use 0.70 as our level of tolerance instead of 1.0 for smaller iterations for the sake of illustration. One may use 1.00, resulting in more iterations but may still converge.</p>
<p>For other convergence criteria, also investigate <strong>Stein-Rosenberg</strong> theorem (e.g., convergence condition for rectangular matrices). </p>
<p>Here is a naive implementation of the <strong>Jacobi</strong> and <strong>Gauss-Seidel</strong> method with the convergence condition (Note that we include a new method called <strong>SOR</strong> which we discuss in the next section):</p>

<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1"><span class="co"># maximum absolute eigenvalue of matrix</span></a>
<a class="sourceLine" id="cb68-2" data-line-number="2">spectral_radius &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb68-3" data-line-number="3">    <span class="kw">max</span> ( <span class="kw">abs</span>( <span class="kw">eigen</span>(A)<span class="op">$</span>values ) )</a>
<a class="sourceLine" id="cb68-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb68-5" data-line-number="5">convergence_condition &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb68-6" data-line-number="6">    n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb68-7" data-line-number="7">    d =<span class="st"> </span><span class="kw">diag</span>(A)</a>
<a class="sourceLine" id="cb68-8" data-line-number="8">    D =<span class="st"> </span>d <span class="op">*</span><span class="st"> </span><span class="kw">diag</span>(n)</a>
<a class="sourceLine" id="cb68-9" data-line-number="9">    R =<span class="st"> </span>A <span class="op">-</span><span class="st"> </span>D </a>
<a class="sourceLine" id="cb68-10" data-line-number="10">    <span class="kw">max</span> ( <span class="kw">abs</span>( <span class="kw">eigen</span>(<span class="kw">solve</span>(D) <span class="op">%*%</span><span class="st"> </span>R)<span class="op">$</span>values ) )</a>
<a class="sourceLine" id="cb68-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb68-12" data-line-number="12">jacobi &lt;-<span class="cf">function</span>(x0, A, b) {</a>
<a class="sourceLine" id="cb68-13" data-line-number="13">    x =<span class="st"> </span>x0</a>
<a class="sourceLine" id="cb68-14" data-line-number="14">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb68-15" data-line-number="15">    limit =<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb68-16" data-line-number="16">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-17" data-line-number="17">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb68-18" data-line-number="18">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb68-19" data-line-number="19">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb68-20" data-line-number="20">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-21" data-line-number="21">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb68-22" data-line-number="22">          x_ =<span class="st"> </span>x</a>
<a class="sourceLine" id="cb68-23" data-line-number="23">          <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-24" data-line-number="24">              s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-25" data-line-number="25">              <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-26" data-line-number="26">                  <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>k) {</a>
<a class="sourceLine" id="cb68-27" data-line-number="27">                      s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span>A[i,k] <span class="op">*</span><span class="st"> </span>x[k] </a>
<a class="sourceLine" id="cb68-28" data-line-number="28">                  }</a>
<a class="sourceLine" id="cb68-29" data-line-number="29">              }</a>
<a class="sourceLine" id="cb68-30" data-line-number="30">              x_[i] =<span class="st"> </span>(b[i] <span class="op">-</span><span class="st"> </span>s) <span class="op">/</span><span class="st"> </span>A[i,i]</a>
<a class="sourceLine" id="cb68-31" data-line-number="31">          }</a>
<a class="sourceLine" id="cb68-32" data-line-number="32">          x =<span class="st"> </span>x_</a>
<a class="sourceLine" id="cb68-33" data-line-number="33">          err =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((A <span class="op">%*%</span><span class="st"> </span>x <span class="op">-</span><span class="st"> </span>b)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb68-34" data-line-number="34">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-35" data-line-number="35">          <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol ) <span class="cf">break</span></a>
<a class="sourceLine" id="cb68-36" data-line-number="36">        }</a>
<a class="sourceLine" id="cb68-37" data-line-number="37">    }</a>
<a class="sourceLine" id="cb68-38" data-line-number="38">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;N&quot;</span>, </a>
<a class="sourceLine" id="cb68-39" data-line-number="39">          <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb68-40" data-line-number="40">    <span class="kw">list</span>(<span class="st">&quot;Jacobi Iteration&quot;</span>=sequence, <span class="st">&quot;initial&quot;</span>=x0,  <span class="st">&quot;x&quot;</span>=x) </a>
<a class="sourceLine" id="cb68-41" data-line-number="41">}</a>
<a class="sourceLine" id="cb68-42" data-line-number="42">gauss_seidel &lt;-<span class="cf">function</span>(x0, A, b) {</a>
<a class="sourceLine" id="cb68-43" data-line-number="43">    x =<span class="st"> </span>x0</a>
<a class="sourceLine" id="cb68-44" data-line-number="44">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb68-45" data-line-number="45">    limit =<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb68-46" data-line-number="46">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-47" data-line-number="47">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb68-48" data-line-number="48">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb68-49" data-line-number="49">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb68-50" data-line-number="50">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-51" data-line-number="51">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb68-52" data-line-number="52">          <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-53" data-line-number="53">              s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-54" data-line-number="54">              <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-55" data-line-number="55">                  <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>k) {</a>
<a class="sourceLine" id="cb68-56" data-line-number="56">                      s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span>A[i,k] <span class="op">*</span><span class="st"> </span>x[k] </a>
<a class="sourceLine" id="cb68-57" data-line-number="57">                  }</a>
<a class="sourceLine" id="cb68-58" data-line-number="58">              }</a>
<a class="sourceLine" id="cb68-59" data-line-number="59">              x[i] =<span class="st"> </span>(b[i] <span class="op">-</span><span class="st"> </span>s) <span class="op">/</span><span class="st"> </span>A[i,i]</a>
<a class="sourceLine" id="cb68-60" data-line-number="60">          }</a>
<a class="sourceLine" id="cb68-61" data-line-number="61">          err =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((A <span class="op">%*%</span><span class="st"> </span>x <span class="op">-</span><span class="st"> </span>b)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb68-62" data-line-number="62">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-63" data-line-number="63">          <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol ) <span class="cf">break</span></a>
<a class="sourceLine" id="cb68-64" data-line-number="64">        }</a>
<a class="sourceLine" id="cb68-65" data-line-number="65">    }</a>
<a class="sourceLine" id="cb68-66" data-line-number="66">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;N&quot;</span>, </a>
<a class="sourceLine" id="cb68-67" data-line-number="67">          <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb68-68" data-line-number="68">    <span class="kw">list</span>(<span class="st">&quot;Gauss-Seidel Iteration&quot;</span>=sequence, <span class="st">&quot;initial&quot;</span>=x0,  <span class="st">&quot;x&quot;</span>=x) </a>
<a class="sourceLine" id="cb68-69" data-line-number="69">}</a>
<a class="sourceLine" id="cb68-70" data-line-number="70">sor &lt;-<span class="cf">function</span>(x0, A, b,w) {</a>
<a class="sourceLine" id="cb68-71" data-line-number="71">    x =<span class="st"> </span>x0</a>
<a class="sourceLine" id="cb68-72" data-line-number="72">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb68-73" data-line-number="73">    limit =<span class="st"> </span><span class="dv">150</span></a>
<a class="sourceLine" id="cb68-74" data-line-number="74">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-75" data-line-number="75">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb68-76" data-line-number="76">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb68-77" data-line-number="77">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb68-78" data-line-number="78">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-79" data-line-number="79">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb68-80" data-line-number="80">          <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-81" data-line-number="81">              s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-82" data-line-number="82">              <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-83" data-line-number="83">                  <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>k) {</a>
<a class="sourceLine" id="cb68-84" data-line-number="84">                      s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span>A[i,k] <span class="op">*</span><span class="st"> </span>x[k] </a>
<a class="sourceLine" id="cb68-85" data-line-number="85">                  }</a>
<a class="sourceLine" id="cb68-86" data-line-number="86">              }</a>
<a class="sourceLine" id="cb68-87" data-line-number="87">              x[i] =<span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>w ) <span class="op">*</span><span class="st"> </span>x[i] <span class="op">+</span><span class="st">  </span>(b[i] <span class="op">-</span><span class="st"> </span>s) <span class="op">*</span><span class="st"> </span>( w <span class="op">/</span><span class="st"> </span>A[i,i] )</a>
<a class="sourceLine" id="cb68-88" data-line-number="88">          }</a>
<a class="sourceLine" id="cb68-89" data-line-number="89">          err =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((A <span class="op">%*%</span><span class="st"> </span>x <span class="op">-</span><span class="st"> </span>b)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb68-90" data-line-number="90">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-91" data-line-number="91">          <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol ) <span class="cf">break</span></a>
<a class="sourceLine" id="cb68-92" data-line-number="92">        }</a>
<a class="sourceLine" id="cb68-93" data-line-number="93">    }</a>
<a class="sourceLine" id="cb68-94" data-line-number="94">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;N&quot;</span>, </a>
<a class="sourceLine" id="cb68-95" data-line-number="95">          <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb68-96" data-line-number="96">    <span class="kw">list</span>(<span class="st">&quot;SOR Iteration&quot;</span>=sequence, <span class="st">&quot;initial&quot;</span>=x0,  <span class="st">&quot;x&quot;</span>=x) </a>
<a class="sourceLine" id="cb68-97" data-line-number="97">}</a>
<a class="sourceLine" id="cb68-98" data-line-number="98">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">1</span>,<span class="dv">2</span>, <span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">1</span>, <span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">28</span>),<span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb68-99" data-line-number="99">b =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb68-100" data-line-number="100"><span class="cf">if</span> ( <span class="kw">convergence_condition</span>(A) <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.70</span> ) {</a>
<a class="sourceLine" id="cb68-101" data-line-number="101">    J =<span class="st"> </span><span class="kw">jacobi</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), A, b)</a>
<a class="sourceLine" id="cb68-102" data-line-number="102">    G =<span class="st"> </span><span class="kw">gauss_seidel</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), A, b)</a>
<a class="sourceLine" id="cb68-103" data-line-number="103">    S =<span class="st"> </span><span class="kw">sor</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), A, b, <span class="dv">1</span>) <span class="co"># becomes gauss_seidel if w=1</span></a>
<a class="sourceLine" id="cb68-104" data-line-number="104">    <span class="kw">print</span>(J)</a>
<a class="sourceLine" id="cb68-105" data-line-number="105">    <span class="kw">print</span>(G)</a>
<a class="sourceLine" id="cb68-106" data-line-number="106">    <span class="kw">print</span>(S)</a>
<a class="sourceLine" id="cb68-107" data-line-number="107">} <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb68-108" data-line-number="108">    <span class="kw">print</span>(<span class="st">&quot;Spectral Radius of Matrix &gt; 0.70&quot;</span>)</a>
<a class="sourceLine" id="cb68-109" data-line-number="109">}</a></code></pre></div>
<pre><code>## $`Jacobi Iteration`
##        N         x1        x2         x3        error
##  [1,]  0 0.00000000 0.0000000 0.00000000 0.000000e+00
##  [2,]  1 0.11111111 0.5000000 0.10714286 1.182653e+00
##  [3,]  2 0.03174603 0.3898810 0.07738095 4.709345e-01
##  [4,]  3 0.05059524 0.4568452 0.08981718 1.686652e-01
##  [5,]  4 0.04039116 0.4395993 0.08540604 6.478116e-02
##  [6,]  5 0.04328763 0.4483551 0.08711526 2.367872e-02
##  [7,]  6 0.04193493 0.4457555 0.08649221 9.001587e-03
##  [8,]  7 0.04236223 0.4469257 0.08672999 3.321377e-03
##  [9,]  8 0.04217936 0.4465458 0.08664241 1.255004e-03
## [10,]  9 0.04224104 0.4467049 0.08667557 4.656032e-04
## [11,] 10 0.04221600 0.4466503 0.08666329 1.752602e-04
## [12,] 11 0.04222479 0.4466722 0.08666792 6.523372e-05
## [13,] 12 0.04222133 0.4466644 0.08666619 2.449648e-05
## [14,] 13 0.04222258 0.4466675 0.08666684 9.135815e-06
## 
## $initial
## [1] 0 0 0
## 
## $x
## [1] 0.04222258 0.44666745 0.08666684
## 
## $`Gauss-Seidel Iteration`
##      N         x1        x2         x3        error
## [1,] 0 0.00000000 0.0000000 0.00000000 0.000000e+00
## [2,] 1 0.11111111 0.4166667 0.08035714 5.829460e-01
## [3,] 2 0.04695767 0.4446925 0.08622980 4.020236e-02
## [4,] 3 0.04253866 0.4465386 0.08663734 2.692186e-03
## [5,] 4 0.04224297 0.4466584 0.08666474 1.768140e-04
## [6,] 5 0.04222357 0.4466661 0.08666654 1.145822e-05
## [7,] 6 0.04222231 0.4466666 0.08666666 7.355387e-07
## 
## $initial
## [1] 0 0 0
## 
## $x
## [1] 0.04222231 0.44666663 0.08666666
## 
## $`SOR Iteration`
##      N         x1        x2         x3        error
## [1,] 0 0.00000000 0.0000000 0.00000000 0.000000e+00
## [2,] 1 0.11111111 0.4166667 0.08035714 5.829460e-01
## [3,] 2 0.04695767 0.4446925 0.08622980 4.020236e-02
## [4,] 3 0.04253866 0.4465386 0.08663734 2.692186e-03
## [5,] 4 0.04224297 0.4466584 0.08666474 1.768140e-04
## [6,] 5 0.04222357 0.4466661 0.08666654 1.145822e-05
## [7,] 6 0.04222231 0.4466666 0.08666666 7.355387e-07
## 
## $initial
## [1] 0 0 0
## 
## $x
## [1] 0.04222231 0.44666663 0.08666666</code></pre>

</div>
<div id="successive-over-relaxation-sor-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.5</span> Successive Over-Relaxation (SOR) Method  <a href="numericallinearalgebra.html#successive-over-relaxation-sor-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Successive Over-relaxation Method (SOR)</strong> improves over the <strong>Gauss-Seidel</strong> method <span class="citation">(Dr.Â S. Karunanithi et al. <a href="bibliography.html#ref-ref49d">2018</a>; Saha M. and Chakrabarty J. <a href="bibliography.html#ref-ref53m">2018</a>)</span>.</p>
<p>In <strong>SOR</strong>, we introduce a <strong>relaxation factor (w)</strong> used to slow down or speed up convergence. If <span class="math inline">\(\mathbf{w=1}\)</span>, then <strong>SOR</strong> is reduced to <strong>Gauss-Seidel</strong> method.</p>
<p>Here, we make a slight change in the equation used by the <strong>Gauss-Seidel</strong> method:</p>
<p>For example:</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= (1 - \omega) x_1 + ( b_1 - a_{1,2}x_2 - a_{1,3}x_3 -\ ...\ - a_{1,n}x_n) \frac{\omega}{a_{1,1}}\\
x_2 &amp;= (1 - \omega) x_2 + ( b_2 - a_{2,1}x_1 - a_{2,3}x_3 -\ ...\ - a_{2,n}x_n)\frac{\omega}{a_{2,2}}\\
\vdots \\
x_3 &amp;= (1 - \omega) x_3 + (b_n - a_{n,1}x_1 -  a_{n,2}x_2 -\ ...\ - a_{n,n}x_n)\frac{\omega}{a_{n,3}}\\
\end{align*}\]</span></p>
<p>And we use the same <strong>algorithm</strong> as the <strong>Gauss-Seidel</strong> method with slight modification in the equations used to approximate <span class="math inline">\(\mathbf{x_i}\)</span>:</p>
<p><span class="math display">\[
\begin{array}{l}
\mathbf{\text{Successive Over-Relaxation (SOR)}}\\
======================\\
loop \\
\ \ \ \ loop\ i\ in\ 1..n\\
\ \ \ \ \ \ \ \ s = 0\\
\ \ \ \ \ \ \ \ loop\ k\ in\ 1..n\\
\ \ \ \ \ \ \ \ \ \ \ \ if ( i \ne k):\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s = s + A_{ik} * x_k \\
\ \ \ \ \ \ \ \ end\ loop\\
\ \ \ \ \ \ \ \ x_i = (1-\omega) * x_i  + (b_i - s) * (w / A_{ii})\\
\ \ \ \ end\ loop\\
\ \ \ \ if\ \|Ax - b \| &lt; tol\ then\ break\\
end\ loop\\
 \\
\end{array}
\]</span>
See also the R implementation covered in previous section.</p>
</div>
<div id="newtons-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.6</span> Newtonâs Method <a href="numericallinearalgebra.html#newtons-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like <strong>Gauss-Seidel</strong> method, the <strong>Newtonâs Method</strong> is also an iterative approximation. We use the following iterative equation (where x is a multivariate, e.g. <span class="math inline">\(\mathbf{x} \in (x_1, x_2)\)</span>):</p>
<p><span class="math display">\[\begin{align}
x_{k+1} = x_k - J_k^{-1} f(x_k)
\end{align}\]</span></p>
<p>The equation is derived by <strong>approximation</strong> using first-order <strong>Taylor Series</strong> given a <strong>partial differentiable function</strong> where we force the function to zero to extract the <strong>roots</strong>, <span class="math inline">\(f(x) = 0\)</span> (note that we can also use <span class="math inline">\(f(x) \approx 0\)</span> since here we are approximating the roots):</p>
<p><span class="math display">\[\begin{align}
f(x) \approx f(x_k) + J_k(x_{k+1} - x_k) = 0,\ \ \ \ where\ \Delta x = \hat{x} - x\ and\ \hat{x}\ =\ x_{k+1}  
\end{align}\]</span></p>
<p>Later, we show how <strong>Jacobian matrix</strong>, J, can be illustrated using two non-linear <strong>differentiable</strong> functions:</p>
<p><span class="math display">\[\begin{align}
f(\mathbf{\vec{x}}) = \{\ \ \ f_1(\mathbf{\vec{x}}),\ \ f_2(\mathbf{\vec{x}})\ \ \}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{\vec{x}}\)</span> is a multivariate vector.</p>
<p>Now, when it comes to approximation, we can readily say that <span class="math inline">\(\mathbf{\hat{x}}\)</span> is the approximation for the actual value of <strong>x</strong> and that <span class="math inline">\(\Delta x\)</span> is the difference (or delta change) between the approximate and the true value. Therefore, we can denote this as such:</p>
<p><span class="math display">\[\begin{align}
\hat{x} = x + \Delta x, \ \ \ \ \ where\ \Delta x\ \text{is the delta change}
\end{align}\]</span></p>
<p>As we point out, the vector <span class="math inline">\(\mathbf{\hat{x}}\)</span> can otherwise be represented as multivariate <span class="math inline">\(\{ \mathbf{x_1, x_2, x_3, ..., x_n}\}\)</span> and given we deal with approximation, as an example, we, therefore, can denote the equation this way (with two-variable multivariate):</p>
<p><span class="math display">\[\begin{align}
f(\mathbf{\vec{x}} + \Delta \vec{x}) 
= \{\ f_1(x_1 + \Delta x_1, x_2 + \Delta x_2),
      \ f_2(x_1 + \Delta x_1, x_2 + \Delta x_2)\ \}
\end{align}\]</span></p>
<p>We can use <strong>Taylor series</strong> to approximate two-variable multivariate function <span class="math inline">\(f_i(\mathbf{\vec{x}})\)</span>, performing <strong>partial derivatives</strong> with respect to each variable:</p>
<p><span class="math display">\[\begin{align}
f_i(\mathbf{\vec{x}} + \Delta \vec{x}) 
{}&amp;= f_i(x_1 + \Delta x_1,x_2 + \Delta x_2  ) \nonumber \\
&amp;+ \frac{\partial f_i}{\partial x_1}(x_1 + \Delta x_1,x_2 + \Delta x_2) \Delta x_1 \nonumber  \\
&amp;+ \frac{\partial f_i}{\partial x_2}(x_1 + \Delta x_1,x_2 + \Delta x_2) \Delta x_2 +  R_n(\vec{x} + \Delta \vec{x}) 
\end{align}\]</span></p>
<p>It can also be written this way:</p>
<p><span class="math display">\[\begin{align}
f_i(\mathbf{\hat{x}}) = f_i(\hat{x_1},\hat{x_2} ) + \frac{\partial f_i}{\partial x_1}(\hat{x_1},\hat{x_2} ) \Delta x_1
+ \frac{\partial f_i}{\partial x_2}(\hat{x_1},\hat{x_2} ) \Delta x_2 + R_n(\hat{x})
\end{align}\]</span></p>
<p>or this way:</p>
<p><span class="math display">\[\begin{align}
f_i(\mathbf{\hat{x}}) = f_i(\hat{x_1},\hat{x_2} ) + \frac{\partial f_i}{\partial x_1}(\hat{x_1},\hat{x_2} ) (\hat{x_1} - x)
+ \frac{\partial f_i}{\partial x_2}(\hat{x_1},\hat{x_2} ) (\hat{x_2} - x) + R_n(\hat{x})
\end{align}\]</span></p>
<p>where the <strong>remainder term</strong> is denoted as:</p>
<p><span class="math display">\[\begin{align}
R_n(\hat{x}) = \frac{\partial f_i}{\partial x_n}(\xi ) (\hat{x_n} - x)
\end{align}\]</span></p>
<p>For two non-linear equations, e.g.Â two differential functions, we expand <span class="math inline">\(f(\mathbf{\hat{x}}) = \{f_1(\mathbf{\hat{x}}), f_2(\mathbf{\hat{x}}) \}\)</span> to show the two functions (as an approximate given we removed the <strong>remainder terms</strong>):</p>
<p><span class="math display">\[\begin{align}
f_1(\mathbf{\hat{x}}) \approx f_1(\hat{x_1},\hat{x_2} ) + \frac{\partial f_1}{\partial x_1}(\hat{x_1},\hat{x_2} ) (\hat{x_1} - x)
+ \frac{\partial f_1}{\partial x_2}(\hat{x_1},\hat{x_2} ) (\hat{x_2} - x) \\
f_2(\mathbf{\hat{x}}) \approx f_2(\hat{x_1},\hat{x_2} ) + \frac{\partial f_2}{\partial x_1}(\hat{x_1},\hat{x_2} ) (\hat{x_1} - x)
+ \frac{\partial f_2}{\partial x_2}(\hat{x_1},\hat{x_2} ) (\hat{x_2} - x)  
\end{align}\]</span></p>
<p>We can translate the partial derivatives into a <strong>Jacobian matrix</strong>. Note that we could have also considered including the <strong>second-order</strong> in the <strong>Taylor Series</strong> and hence be able to translate the derivatives into a <strong>Hessian matrix</strong>. Now, to illustrate <strong>Newtonâs method</strong>, we settle with <strong>first-order</strong>.</p>
<p>Let us illustrate the <strong>Newtonâs method</strong>.</p>
<p><strong>First</strong>, perform partial derivatives with respect to each unknown variable and build the <strong>Jacobian matrix</strong>:</p>
<p><span class="math display">\[\begin{align*}
f_1(\mathbf{\vec{x}}) {}&amp;=  3x_1^2 + x_2^2 - 4x_1  &amp; 
\ \ \frac{\partial f_1}{\partial x_1} {}&amp;= 6x_1 - 4,&amp;
\ \ \frac{\partial f_1}{\partial x_2} {}&amp;= 2x_2 + 0\\
f_2(\mathbf{\vec{x}}) &amp;= 9x_1^2 + x_2^2 - 2x_2 &amp;
\ \ \frac{\partial f_2}{\partial x_1} &amp;= 18x_1 + 0, &amp;
\ \ \frac{\partial f_2}{\partial x_2} &amp;= 2x_2 - 2  \\
\end{align*}\]</span></p>
<p>We translate the partial derivatives into <strong>Jacobian matrix</strong> form:</p>
<p><span class="math display">\[
J(\mathbf{\vec{x}}) =
\left[
\begin{array}{ccc}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} \\
\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2}\
\end{array}
\right] = 
\left[
\begin{array}{rr}
6x_1 - 4 &amp; 2x_2 + 0\\
18x_1 + 0  &amp; 2x_2 - 2
\end{array}
\right]
\]</span></p>
<p><strong>Second</strong>: compute for the functions and their partial derivatives given an initial arbitrary nonzero vector:</p>
<p>Assume initial values for vector <span class="math inline">\(\mathbf{\vec{x}}^0\)</span> (note that we are using a superscript for the vector to not confuse with the <strong>x</strong> variable indexes:</p>
<p><span class="math display">\[
\mathbf{\vec{x}}^0 = \left[\begin{array}{rrr} 1 &amp; 1 \end{array}\right]^T
\]</span>
We get result for <span class="math inline">\(f(\mathbf{\vec{x}}^0)\)</span> and <span class="math inline">\(J(\mathbf{\vec{x}}^0)\)</span>:</p>

<p><span class="math display">\[
f(\mathbf{\vec{x}}^0) =
\left[
\begin{array}{ccc}
3x_1^2 + x_2^2 - 4x_1 \\
9x_1^2 + x_2^2 - 2x_2
\end{array}
\right] = 
\left[
\begin{array}{ccc}
0 \\
8 
\end{array}
\right]\ \ \ \ \ \ \ \ 
J(\mathbf{\vec{x}}^0) =
\left[
\begin{array}{ccc}
6x_1 - 4 &amp; 2x_2 + 0 \\
18x_1 + 0  &amp; 2x_2 - 2
\end{array}
\right] = 
\left[
\begin{array}{ccc}
2 &amp; 2 \\
18 &amp; 0
\end{array}
\right]
\]</span>
</p>
<p><strong>Third</strong>, solve for the next <span class="math inline">\(\mathbf{\vec{x}}^{k+1}\)</span> using the below equation:</p>
<p><span class="math display">\[\begin{align}
\mathbf{\vec{x}}^{k+1} {}&amp;= \mathbf{\vec{x}}^k - J_f(\mathbf{\vec{x}}^k)^{-1} f(\mathbf{\vec{x}}^k) \label{eqn:eqnnumber200} \\
&amp;= \mathbf{\vec{x}}^0 - J(\mathbf{\vec{x}}^0)^{-1}f(\mathbf{\vec{x}}^0) \label{eqn:eqnnumber201} \\
&amp;= \left[
\begin{array}{ccc}
1 \\
1 
\end{array}
\right] - \left[
\begin{array}{ccc}
2 &amp; 2 \\
18 &amp; 0
\end{array}
\right]^{-1}
\left[
\begin{array}{ccc}
0 \\
8 
\end{array}
\right] \nonumber \\
\mathbf{\vec{x}}^{1} &amp;= \left[
\begin{array}{r}
0.556 \\
1.444
\end{array}
\right] \nonumber
\end{align}\]</span></p>
<p><strong>Fourth</strong>, <span class="math inline">\(\mathbf{\vec{x}}^{1}\)</span> becomes the next <span class="math inline">\(\mathbf{\vec{x}}^k\)</span>. From there, iterate until convergence (e.g., tolerance is reached):</p>
<p><span class="math display">\[\begin{align}
\mathbf{\vec{x}}^{k+1} {}&amp;= \mathbf{\vec{x}}^k - J_f(\mathbf{\vec{x}}^k)^{-1} f(\mathbf{\vec{x}}^k)  \\
&amp;= \mathbf{\vec{x}}^1 - J(\mathbf{\vec{x}}^1)^{-1}f(\mathbf{\vec{x}}^1) 
\end{align}\]</span></p>
<p>We use the following tolerance and convergence criterion:</p>
<p><span class="math display">\[\begin{align}
\left|\sqrt{\sum(x^k)^2} - \sqrt{\sum(x^{k+1})^2}\right| &lt; 1e{-5}
\end{align}\]</span></p>
<p>Convergence is reached with the following result:</p>
<p><span class="math display">\[
x_1 \approx \frac{1}{3}, \ \ \ \ \ \ x_2 \approx 1\ \ \ \ \ \  \text {if } x^0 = [1,1] 
\]</span></p>
<p>and:</p>
<p><span class="math display">\[
x_1 \approx 0, \ \ \ \ \ \ x_2 \approx 0\ \ \ \ \ \  \text {if } x^0 = [-1,1] 
\]</span></p>
<p>The steps above illustrates the <strong>Newtonâs Method</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
x^0 \leftarrow \text{initial arbitrary nonzero vector} \\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ \Delta x^k = -J(x^k)^{-1} f(x^k) \ \ \ \ use\ LU\ decomposition\ for\ J^{-1}\\
\ \ \ \ x^{k+1} = x^k + \Delta x^k \\
end\ loop
\end{array}
\]</span></p>
<p>We show the naive implementation of the <strong>Newtonâs Method</strong> in R code:</p>

<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1">f1 &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">3</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">4</span><span class="op">*</span>x[<span class="dv">1</span>] }</a>
<a class="sourceLine" id="cb70-2" data-line-number="2">f2 &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">9</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>] }</a>
<a class="sourceLine" id="cb70-3" data-line-number="3">J &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb70-4" data-line-number="4">    <span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb70-5" data-line-number="5">        <span class="kw">c</span>( <span class="dv">6</span><span class="op">*</span>x[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">4</span>, <span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>], <span class="dv">18</span><span class="op">*</span>x[<span class="dv">1</span>] , <span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb70-6" data-line-number="6">        <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb70-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb70-8" data-line-number="8">F &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">c</span>( <span class="kw">f1</span>(x), <span class="kw">f2</span>(x) ) }</a>
<a class="sourceLine" id="cb70-9" data-line-number="9">newton &lt;-<span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb70-10" data-line-number="10">    x0 =<span class="st"> </span>x</a>
<a class="sourceLine" id="cb70-11" data-line-number="11">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb70-12" data-line-number="12">    limit =<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb70-13" data-line-number="13">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb70-14" data-line-number="14">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb70-15" data-line-number="15">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb70-16" data-line-number="16">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb70-17" data-line-number="17">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb70-18" data-line-number="18">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb70-19" data-line-number="19">          x_ =<span class="st"> </span>x</a>
<a class="sourceLine" id="cb70-20" data-line-number="20">          LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(<span class="kw">J</span>(x))</a>
<a class="sourceLine" id="cb70-21" data-line-number="21">          uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, <span class="kw">F</span>(x))</a>
<a class="sourceLine" id="cb70-22" data-line-number="22">          delta_x =<span class="st"> </span><span class="op">-</span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy)</a>
<a class="sourceLine" id="cb70-23" data-line-number="23">          x =<span class="st"> </span>x <span class="op">+</span><span class="st"> </span>delta_x</a>
<a class="sourceLine" id="cb70-24" data-line-number="24">          <span class="co">#x = x - solve(J(x)) %*% F(x) # alternative</span></a>
<a class="sourceLine" id="cb70-25" data-line-number="25">          a =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(x_<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb70-26" data-line-number="26">          b =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb70-27" data-line-number="27">          err =<span class="st"> </span><span class="kw">abs</span>(a<span class="op">-</span>b)</a>
<a class="sourceLine" id="cb70-28" data-line-number="28">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb70-29" data-line-number="29">          <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol ) <span class="cf">break</span></a>
<a class="sourceLine" id="cb70-30" data-line-number="30">        }</a>
<a class="sourceLine" id="cb70-31" data-line-number="31">    }</a>
<a class="sourceLine" id="cb70-32" data-line-number="32">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;N&quot;</span>, </a>
<a class="sourceLine" id="cb70-33" data-line-number="33">          <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb70-34" data-line-number="34">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;initial&quot;</span>=x0,  <span class="st">&quot;x&quot;</span>=x) </a>
<a class="sourceLine" id="cb70-35" data-line-number="35">}</a>
<a class="sourceLine" id="cb70-36" data-line-number="36"><span class="kw">newton</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</a></code></pre></div>
<pre><code>## $Iteration
##      N        x1       x2        error
## [1,] 0 1.0000000 1.000000 0.000000e+00
## [2,] 1 0.5555556 1.444444 1.333851e-01
## [3,] 2 0.3858180 1.131770 3.518735e-01
## [4,] 3 0.3388188 1.015405 1.252831e-01
## [5,] 4 0.3334154 1.000241 1.609513e-02
## [6,] 5 0.3333334 1.000000 2.543819e-04
## [7,] 6 0.3333333 1.000000 6.209382e-08
## 
## $initial
## [1] 1 1
## 
## $x
## [1] 0.3333333 1.0000000</code></pre>

<p>The implementation of <strong>Newtonâs Method</strong> gives us the result of multivariate <strong>x</strong>:</p>
<p><span class="math display">\[
x_1 = 1/3\ \ \ \ \ \ \ \ \ x_2 = 1\ \ \ \ \ \ where\ x^{(0)} = (1,1)
\]</span></p>
<p>Here is another example:</p>

<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1"><span class="kw">newton</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</a></code></pre></div>
<pre><code>## $Iteration
##      N            x1            x2        error
## [1,] 0 -1.000000e+00  1.000000e+00 0.000000e+00
## [2,] 1 -5.555556e-01 -7.777778e-01 4.583996e-01
## [3,] 2 -1.721440e-02 -9.029734e-01 5.267645e-02
## [4,] 3 -1.082059e-01 -2.061246e-01 6.703374e-01
## [5,] 4 -1.212666e-02 -5.150582e-02 1.798860e-01
## [6,] 5 -7.137358e-04 -1.816708e-03 5.096225e-02
## [7,] 6 -1.202316e-06 -3.927739e-06 1.947775e-03
## [8,] 7 -4.940920e-12 -1.421849e-11 4.107624e-06
## 
## $initial
## [1] -1  1
## 
## $x
## [1] -4.940920e-12 -1.421849e-11</code></pre>

<p>The result of multivariate <strong>x</strong> gives us:</p>
<p><span class="math display">\[
x_1 = 0\ \ \ \ \ \ \ \ \ x_2 = 0\ \ \ \ \ \ where\ x^{(0)} = (-1,1)
\]</span></p>
</div>
<div id="broydens-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.7</span> Broydenâs Method <a href="numericallinearalgebra.html#broydens-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Broydenâs Method</strong> is a <strong>Quasi-Newton method</strong> - it is an enhancement to the <strong>Newton</strong> method by avoiding the repeated evaluation of the Jacobian matrix at every iteration <span class="citation">(Jarlebring E. <a href="bibliography.html#ref-ref316e">2018</a>)</span>.</p>
<p>Given the following equation:</p>
<p><span class="math display">\[\begin{align}
f(x) \approx A_{k+1} \Delta x = f(x_{k+1}) - f(x), \ \ \ \ where\ \Delta x = x_{k+1} - x,  
\end{align}\]</span></p>
<p>we derive the <strong>Broydenâs formula</strong>:</p>
<p><span class="math display">\[\begin{align}
A_{k+1} = A_x + \frac{(y_k - A_k \Delta x)(\Delta x)^T}{\| \Delta x \|_{L2}^2},\ \ \ \ where\ y_k = f(x_{k+1}) - f(x).
\end{align}\]</span></p>
<p>The steps above illustrates the <strong>Broydenâs method</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
x^0 \leftarrow \text{initial arbitrary nonzero vector} \\
A^0 = J_f(x^0) \\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ \Delta x  = -(A^k)^{-1} f(x^k) \ \ \ \ use\ LU\ decomposition\ for\ A^{-1}\\
\ \ \ \ x_{k+1} = x^k + \Delta x \\
\ \ \ \ y^k = f(x^{k+1}) - f(x^k)\\
\ \ \ \ r^k = y^k - A^k\Delta x \\
\ \ \ \ A^{k+1} = A^k + r^k(\Delta x )^T/\| \Delta x \|_{L2}^2  \\
end\ loop
\end{array}
\]</span></p>
<p>We show the naive implementation of <strong>Broydenâs Method</strong> in R code:</p>

<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1">f1 &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">3</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">4</span><span class="op">*</span>x[<span class="dv">1</span>] }</a>
<a class="sourceLine" id="cb74-2" data-line-number="2">f2 &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">9</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>] }</a>
<a class="sourceLine" id="cb74-3" data-line-number="3">J &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb74-4" data-line-number="4">    <span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb74-5" data-line-number="5">        <span class="kw">c</span>( <span class="dv">6</span><span class="op">*</span>x[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">4</span>, <span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>],<span class="dv">18</span><span class="op">*</span>x[<span class="dv">1</span>] , <span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb74-6" data-line-number="6">        <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb74-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb74-8" data-line-number="8">F &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">c</span>( <span class="kw">f1</span>(x), <span class="kw">f2</span>(x) ) }</a>
<a class="sourceLine" id="cb74-9" data-line-number="9">broyden &lt;-<span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb74-10" data-line-number="10">    x0 =<span class="st"> </span>x</a>
<a class="sourceLine" id="cb74-11" data-line-number="11">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb74-12" data-line-number="12">    limit =<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb74-13" data-line-number="13">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb74-14" data-line-number="14">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb74-15" data-line-number="15">    A =<span class="st"> </span><span class="kw">J</span>(x)</a>
<a class="sourceLine" id="cb74-16" data-line-number="16">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb74-17" data-line-number="17">        <span class="cf">if</span> (i<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb74-18" data-line-number="18">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(i, x, err))</a>
<a class="sourceLine" id="cb74-19" data-line-number="19">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb74-20" data-line-number="20">          x_ =<span class="st"> </span>x</a>
<a class="sourceLine" id="cb74-21" data-line-number="21">          LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(A)</a>
<a class="sourceLine" id="cb74-22" data-line-number="22">          uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, <span class="kw">F</span>(x))</a>
<a class="sourceLine" id="cb74-23" data-line-number="23">          delta_x =<span class="st"> </span><span class="op">-</span><span class="st"> </span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy)</a>
<a class="sourceLine" id="cb74-24" data-line-number="24">          x =<span class="st"> </span>x <span class="op">+</span><span class="st"> </span>delta_x</a>
<a class="sourceLine" id="cb74-25" data-line-number="25">          y =<span class="st"> </span><span class="kw">F</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">F</span>(x_)</a>
<a class="sourceLine" id="cb74-26" data-line-number="26">          r =<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>delta_x</a>
<a class="sourceLine" id="cb74-27" data-line-number="27">          A =<span class="st"> </span>A <span class="op">+</span><span class="st"> </span>(r <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(delta_x)) <span class="op">/</span>( <span class="kw">sqrt</span>(<span class="kw">sum</span>(delta_x<span class="op">^</span><span class="dv">2</span>)) )<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb74-28" data-line-number="28">          a =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(x_<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb74-29" data-line-number="29">          b =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb74-30" data-line-number="30">          err =<span class="st"> </span><span class="kw">abs</span>(a<span class="op">-</span>b)</a>
<a class="sourceLine" id="cb74-31" data-line-number="31">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(i, x, err))</a>
<a class="sourceLine" id="cb74-32" data-line-number="32">          <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol ) <span class="cf">break</span></a>
<a class="sourceLine" id="cb74-33" data-line-number="33">        }</a>
<a class="sourceLine" id="cb74-34" data-line-number="34">    }</a>
<a class="sourceLine" id="cb74-35" data-line-number="35">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;N&quot;</span>, </a>
<a class="sourceLine" id="cb74-36" data-line-number="36">          <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb74-37" data-line-number="37">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;initial&quot;</span>=x0,  <span class="st">&quot;x&quot;</span>=x) </a>
<a class="sourceLine" id="cb74-38" data-line-number="38">}</a>
<a class="sourceLine" id="cb74-39" data-line-number="39"><span class="kw">broyden</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</a></code></pre></div>
<pre><code>## $Iteration
##        N        x1        x2        error
##  [1,]  0 1.0000000 1.0000000 0.000000e+00
##  [2,]  1 0.5555556 1.4444444 1.333851e-01
##  [3,]  2 0.4639175 1.2061856 2.552740e-01
##  [4,]  3 0.3870957 1.1129019 1.140235e-01
##  [5,]  4 0.3215272 0.9879433 1.393538e-01
##  [6,]  5 0.3269287 0.9877312 1.482875e-03
##  [7,]  6 0.4564969 1.2256526 2.674740e-01
##  [8,]  7 0.3320245 0.9966953 2.573608e-01
##  [9,]  8 0.3330608 0.9992606 2.761418e-03
## [10,]  9 0.3333392 1.0000118 8.006343e-04
## [11,] 10 0.3333336 1.0000005 1.248732e-05
## [12,] 11 0.3333333 1.0000000 5.371434e-07
## 
## $initial
## [1] 1 1
## 
## $x
## [1] 0.3333333 1.0000000</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" data-line-number="1"><span class="kw">broyden</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</a></code></pre></div>
<pre><code>## $Iteration
##        N            x1            x2        error
##  [1,]  0 -1.000000e+00  1.000000e+00 0.000000e+00
##  [2,]  1 -5.555556e-01 -7.777778e-01 4.583996e-01
##  [3,]  2 -1.616267e-01 -1.502607e+00 5.554606e-01
##  [4,]  3  2.120387e-01 -1.478334e+00 1.781104e-02
##  [5,]  4  4.681942e-02 -7.533436e-01 7.386664e-01
##  [6,]  5 -1.218198e-01 -3.135692e-01 4.183959e-01
##  [7,]  6 -4.877114e-02 -4.873996e-02 2.674504e-01
##  [8,]  7 -4.399305e-03 -1.188260e-02 5.627993e-02
##  [9,]  8 -5.441345e-04 -2.080117e-03 1.052073e-02
## [10,]  9 -1.332127e-05 -1.809338e-05 2.127641e-03
## [11,] 10  1.432844e-07 -3.547711e-07 2.208574e-05
## [12,] 11 -5.747636e-09  1.149681e-08 3.697599e-07
## 
## $initial
## [1] -1  1
## 
## $x
## [1] -5.747636e-09  1.149681e-08</code></pre>

</div>
<div id="bfgs-broyden-fletcher-goldfarb-shanno-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.8</span> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method <a href="numericallinearalgebra.html#bfgs-broyden-fletcher-goldfarb-shanno-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>BFGS</strong> method takes the <strong>Broydenâs Method</strong> even further with the following equation:</p>
<p><span class="math display">\[\begin{align}
A_{k+1} = A_x +  \frac{y_k(y_k)^T}{(y_k)^T \Delta x} - \frac{A_k \Delta x(\Delta x)^T A_k}{(\Delta x)^TA_k \Delta x},\ \ \ \ where\ y_k = f(x_{k+1}) - f(x)
\end{align}\]</span></p>
<p>We leave the method and equation to the readers to investigate.</p>
</div>
</div>
<div id="polynomialregression" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.5</span> Approximating Polynomial Functions by Regression<a href="numericallinearalgebra.html#polynomialregression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use Figure <a href="numericallinearalgebra.html#fig:fitting">3.10</a> to illustrate a point in this section. Note that while our discussion in this section is about <strong>regression</strong> - fitting a line, the line being fitted can serve as a model for prediction, which we show in the left graph. We defer the subject of inference and cover that in the <strong>Statistical Computation</strong> chapter and <strong>Computational Learning</strong> chapter. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fitting"></span>
<img src="fitting.png" alt="Linear Regression" width="100%" />
<p class="caption">
Figure 3.10: Linear Regression
</p>
</div>
<p>In the previous section, we deal with linear systems of first-degree polynomials. Geometrically, each equation represents a line in a cartesian plane. Therefore, solving the system means searching for the intersections. In other words, the <strong>common unknown</strong> is a set of coordinates of (x,y) at the intersection - that is, the <strong>solution set (roots)</strong>.</p>
<p>In this section, however, it is the opposite. Geometrically, we are given a bunch of dots (points) instead of a set of lines (equations) in the cartesian plane. For example, in Figure <a href="numericallinearalgebra.html#fig:fitting">3.10</a>, the first graph illustrates a set of arbitrary points in the system. These data points are usually sourced from a table like so:</p>
<p><span class="math display">\[
  \left[\begin{array}{r}x \\ y \end{array}\left|\begin{array}{rrrrrrrrrrr}  
0.6 &amp; 0.7 &amp; 1.3 &amp; 1.6 &amp; 1.8 &amp; 1.8 &amp; 2.2 &amp; 2.5 &amp; 2.5 &amp; 3.2 &amp; 3.6\\
1.5 &amp; 1.9 &amp; 2.3 &amp; 2.2 &amp; 2.8 &amp; 3.2 &amp; 3.1 &amp; 3.4 &amp; 3.8 &amp; 4.3 &amp; 4.5
 \end{array}\right.\right]
\]</span>
Note that these data points do not represent <strong>the solution set</strong>; instead, they are merely arbitrary points, perhaps a set of observations sampled from a population.</p>
<p>Therefore, instead of looking for intersections (roots) of linear equations, we are looking for a good linear model represented as a line that fits through a given set of data points. Geometrically, we look for a linear pattern that we can draw through the center of a scattered plot.</p>
<p>To do that, we use <strong>Least-Squares</strong>.</p>
<div id="least-squares" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.5.1</span> Least-Squares <a href="numericallinearalgebra.html#least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Figure <a href="numericallinearalgebra.html#fig:leastsquares1">3.11</a>, we use the vertical <strong>absolute</strong> distance instead of the distance orthogonal to the line to compute for the <strong>Least-Squares</strong>. That is because it may be more convenient to use the vertical distance given it is, after all, proportional to the orthogonal distance when computed; and at the same time, because the difference between <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y\)</span> fall vertically at a given x-axis.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:leastsquares1"></span>
<img src="least_squares.png" alt="Least Squares" width="40%" />
<p class="caption">
Figure 3.11: Least Squares
</p>
</div>
<p>The <strong>vertical distance</strong> represents the error, <span class="math inline">\(\epsilon_i\)</span>, which is computed based on the following:</p>
<p>A simple linear equation (where <strong>i</strong> in 1..<strong>n</strong> and <strong>n</strong> is number of data points):</p>
<p><span class="math display">\[\begin{align}
y_i {}&amp;= \beta_0 + \beta_i x_i   \\
\hat{y_i} {}&amp;= \beta_0 + \beta_i x_i  + \epsilon_i\ \rightarrow\ \ \ \ 
\epsilon_i  = \left| \hat{y_i} - \left(\beta_0 + \beta_i x_i \right) \right|
\\
\nonumber \\
&amp;therefore\ \ \ \epsilon_i = |y_i - \hat{y}_i | \nonumber
\end{align}\]</span></p>
<p>A multilinear equation (where <strong>m</strong> is number of independent variables):</p>
<p><span class="math display">\[\begin{align}
y_i {}&amp;= \beta_0 + \sum_{j=1}^m \beta_ix_{i,j} \\
\hat{y_i} &amp;= \beta_0 +  \sum_{j=1}^m \beta_i x_{i,j} + \epsilon_i\ \rightarrow\ \ \ \ 
\epsilon_i = \left| \hat{y_i} - \left(\beta_0 +  \beta_{i} x_{i,j}\right) \right|
\\
\nonumber \\
&amp;therefore \ \ \ \epsilon_i = |y_i - \hat{y}_i| \nonumber
\end{align}\]</span></p>
<p>We sum the square of the errors by using the following function - we call this the <strong>Ordinary Least Squares (OLS)</strong>.  </p>
<p><span class="math display">\[\begin{align}
RSS(y, \hat{y}) = \text{residual sum square} 
{}&amp;= \sum_{i=1}^n | y_i - \hat{y}_i |^2  = \sum_{i=1}^n |\epsilon_i|^2 \\
\text{1st degree multilinear} &amp;= \sum_{i=1}^n  \left| y_i - \left(\beta_0 + \sum_{j=1}^m \beta_i x_{i,j} \right) \right| ^2 \\
\text{general equation} &amp;=  \left| y - \hat{A}x \right| ^2
\end{align}\]</span></p>
<p>where <strong>A</strong> is a matrix of <strong>coefficients</strong> (<span class="math inline">\(\beta\)</span>s). It can be a Vandermonde matrix for ordered higher degree polynomials or multivariate.</p>
<p>If we randomly draw ten lines across a cartesian plane, we end up with a list of ten <strong>RSS</strong>. We then choose the line with the smallest <strong>RSS</strong> and conclude that our chosen line is our model. Such a model may not necessarily reflect the <strong>best fit</strong>, however. We might need to draw more lines - a hundred times perhaps - to be more confident that we get an even smaller <strong>RSS</strong>. In other words, we aim to find the most minimum <strong>RSS</strong> - we call this <strong>minimizing the loss function</strong>. We minimize the function with the following equation:    </p>
<p><span class="math display">\[\begin{align}
\hat{\beta} = \underset{\beta}{\mathrm{argmin}}\ RSS(y, \hat{y})
\ \ \ \ \ where\ \hat{y} = \hat{A}x
\end{align}\]</span></p>
<p>To minimize the objective function, we can get a simpler normalized equation by applying <strong>partial derivatives</strong> of the objective function with respect to the individual <span class="math inline">\(\beta\)</span> coefficients and solve for systems of equation (the derivatives). We do not cover the derivative step here, but it leads to the following equations that help us generate the <span class="math inline">\(\beta\)</span> <strong>coefficients</strong> corresponding to the <strong>Least-Square</strong>.</p>
<p>For first-degree polynomials, there are a few derived equations to generate the <strong>coefficients</strong>, but we can choose one of the two formulas we cover here.</p>
<p>Here is the first option:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta}_1
 &amp;=  \frac{n\sum_{i=1}^n{(x_iy_i)} - \sum_{i=1}^n{x_i}\sum_{i=1}^n{y_i}}{n\sum_{i=1}^n{(x_i^2)} - (\sum_{i=1}^n{x_i})^2}   \\
\hat{\beta}_0 &amp;= \frac{\sum_{i=1}^n{y_i} - \hat{\beta}_1\sum_{i=1}^n{x_i}}{n}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>n</strong> is the size of the data (e.g.Â number of data points)</li>
</ul>

<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1">x =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.6</span>,<span class="fl">0.7</span>,<span class="fl">1.3</span>,<span class="fl">1.6</span>,<span class="fl">1.8</span>,<span class="fl">1.8</span>,<span class="fl">2.2</span>,<span class="fl">2.5</span>,<span class="fl">2.5</span>,<span class="fl">3.2</span>,<span class="fl">3.6</span>)</a>
<a class="sourceLine" id="cb78-2" data-line-number="2">y =<span class="st"> </span><span class="kw">c</span>(<span class="fl">1.5</span>,<span class="fl">1.9</span>,<span class="fl">2.3</span>,<span class="fl">2.2</span>,<span class="fl">2.8</span>,<span class="fl">3.2</span>,<span class="fl">3.1</span>,<span class="fl">3.4</span>,<span class="fl">3.8</span>,<span class="fl">4.3</span>,<span class="fl">4.5</span>)</a>
<a class="sourceLine" id="cb78-3" data-line-number="3">n=<span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb78-4" data-line-number="4">b1 &lt;-<span class="st"> </span><span class="cf">function</span>(x,y) { </a>
<a class="sourceLine" id="cb78-5" data-line-number="5">    ( n<span class="op">*</span><span class="kw">sum</span>(x <span class="op">*</span><span class="st"> </span>y) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(x) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(y) ) <span class="op">/</span><span class="st"> </span>(n<span class="op">*</span><span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(x)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb78-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb78-7" data-line-number="7">b0 &lt;-<span class="st"> </span><span class="cf">function</span>(x,y) {</a>
<a class="sourceLine" id="cb78-8" data-line-number="8">    (<span class="kw">sum</span>(y) <span class="op">-</span><span class="st"> </span><span class="kw">b1</span>(x,y) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(x)) <span class="op">/</span><span class="st"> </span>n</a>
<a class="sourceLine" id="cb78-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb78-10" data-line-number="10"><span class="kw">c</span>(<span class="st">&quot;beta_hat_0&quot;</span> =<span class="st"> </span><span class="kw">b0</span>(x,y), <span class="st">&quot;beta_hat_1&quot;</span> =<span class="st"> </span><span class="kw">b1</span>(x,y))</a></code></pre></div>
<pre><code>## beta_hat_0 beta_hat_1 
##   1.017374   1.000408</code></pre>

<p>or, alternatively, we can use the second option:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta}_1 
 &amp;=  \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2} \\
\hat{\beta}_0 &amp;= \bar{y} - \hat{\beta}_1 \bar{x}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\bar{x}\)</span> is the mean of all x</li>
<li><span class="math inline">\(\bar{y}\)</span> is the mean of all y</li>
</ul>

<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" data-line-number="1">b1 &lt;-<span class="st"> </span><span class="cf">function</span>(x,y) { </a>
<a class="sourceLine" id="cb80-2" data-line-number="2">    <span class="kw">sum</span>( (x <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x)) <span class="op">*</span><span class="st"> </span>( y <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y))) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span> ((x<span class="op">-</span><span class="kw">mean</span>(x))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb80-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb80-4" data-line-number="4">b0 &lt;-<span class="st"> </span><span class="cf">function</span>(x,y) {</a>
<a class="sourceLine" id="cb80-5" data-line-number="5">    <span class="kw">mean</span>(y) <span class="op">-</span><span class="st"> </span><span class="kw">b1</span>(x,y) <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(x)</a>
<a class="sourceLine" id="cb80-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb80-7" data-line-number="7"><span class="kw">c</span>(<span class="st">&quot;beta_hat_0&quot;</span> =<span class="st"> </span><span class="kw">b0</span>(x,y), <span class="st">&quot;beta_hat_1&quot;</span> =<span class="st"> </span><span class="kw">b1</span>(x,y))</a></code></pre></div>
<pre><code>## beta_hat_0 beta_hat_1 
##   1.017374   1.000408</code></pre>

<p>For multilinear equations, e.g. <span class="math inline">\(\{\ x_1,\ x_2,\ x_3,\ ...,\ x_m\ \}\)</span>, or higher order polynomials, e.g. <span class="math inline">\(\{\ x^1,\ x^2, ...,\ \ x^n\ \}\)</span>, we use a more general matrix equation (where A is the matrix). See <strong>linear algebra</strong> chapter:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta} = (A^T \cdot A)^{-1} \cdot A^T \cdot y\ \ \ \ \leftarrow\ \ \ \ \ \ y = A\hat{\beta} \label{eqn:eqnnumber3}
\end{align}\]</span></p>
<p>The formula offers a <strong>mathematically convenient</strong> way to compute the coefficients using matrices and is derived from the following:</p>
<p><span class="math display">\[\begin{align}
A\hat{\beta} {}&amp;= y \\
A^T \cdot A \hat{\beta} &amp;= A^T \cdot y \\
\hat{\beta} &amp;= (A^T \cdot A)^{-1} \cdot A^T \cdot y
\end{align}\]</span></p>
<p>Both equations lead to the same set of <span class="math inline">\(\hat{\beta}\)</span>s - at least for the simple equation.</p>
</div>
<div id="linear-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.5.2</span> Linear Regression <a href="numericallinearalgebra.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us illustrate <strong>linear regression</strong> by using <strong>Ordinary Least Square (OLS)</strong>.  </p>
<p><strong>First</strong>, we translate our data points above into a set of linear equations using the following formula:</p>
<p><span class="math display">\[\begin{align}
y = \hat{\beta}_0 + \hat{\beta}_1 x\ \ \ \ \leftarrow \ \  y = mx + c\ \ \ \ \text{(from the slope formula)}
\end{align}\]</span></p>
<p>We build a system of linear equations using the slope formula and the data points like so:</p>
<p><span class="math display">\[
\begin{array}{lll}
1.5 = \hat{\beta}_0 + \hat{\beta}_1 \times 0.6  &amp; &amp; 3.1 = \hat{\beta}_0 + \hat{\beta}_1 \times 2.2 \\
1.9 = \hat{\beta}_0 + \hat{\beta}_1 \times 0.7 &amp; &amp; 3.4 = \hat{\beta}_0 + \hat{\beta}_1 \times 2.5 \\
2.3 = \hat{\beta}_0 + \hat{\beta}_1 \times 1.3 &amp; &amp; 3.8 = \hat{\beta}_0 + \hat{\beta}_1 \times 2.5 \\
2.2 = \hat{\beta}_0 + \hat{\beta}_1 \times 1.6 &amp; &amp; 4.3 = \hat{\beta}_0 + \hat{\beta}_1 \times 3.2 \\
2.8 = \hat{\beta}_0 + \hat{\beta}_1 \times 1.8 &amp; &amp; 4.5 = \hat{\beta}_0 + \hat{\beta}_1 \times 3.6 \\
3.2 = \hat{\beta}_0 + \hat{\beta}_1 \times 1.8 
\end{array} 
\]</span></p>
<p><strong>Second</strong>, we construct the <strong>A matrix</strong> based on the above <strong>linear of equations</strong>.</p>
<p><span class="math display">\[
Vandermonde(A) = \left[\begin{array}{rr} 
1 &amp; x_1 \\
1 &amp; x_2 \\
1 &amp; x_3 \\
1 &amp; ...\\
1 &amp; x_{11} \\
\end{array}\right] =
\left[\begin{array}{rr} 
1 &amp; 0.6 \\
1 &amp; 0.7 \\
1 &amp; 1.3 \\
1 &amp; ...\\
1 &amp; 3.6 \\
\end{array}\right]
\]</span></p>
<p><strong>Third</strong>, we also construct our vector of <strong>coefficients</strong>, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Here, we are looking for the <strong>unknown coefficients</strong> denoted as <span class="math inline">\(\beta = \{m, c\}\)</span> where m=<strong>unknown slope</strong> and c=<strong>unknown intercept</strong>:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta} = \left[\begin{array}{c} c\\m \end{array}\right] = \left[\begin{array}{c} \hat{\beta}_0\\ \hat{\beta}_1 \end{array}\right] \label{eqn:eqnnumber4}
\end{align}\]</span></p>
<p><strong>Fourth</strong>, now let us solve for the <strong>coefficients</strong> using the following equation:</p>
<p><span class="math display">\[\begin{align}
A\hat{\beta} = y
\end{align}\]</span></p>
<p>Let us replace the equation with the vandermonde matrix (A) and coefficient vector (<span class="math inline">\(\beta\)</span>).</p>
<p><span class="math display">\[
A\hat{\beta} = y 
\rightarrow
 \ \ \ \ \ 
\left[\begin{array}{c}
\begin{array}{rr} 
1 &amp; 0.6 \\ 1 &amp; 0.7 \\ 1 &amp; 1.3 \\ 1 &amp; 1.6 \\ 1&amp; 1.8 \\ 
1 &amp; 1.8 \\ 1 &amp; 2.2 \\ 1 &amp; 2.5 \\ 1 &amp; 2.5 \\ 1 &amp; 3.2 \\ 1 &amp; 3.6
\end{array}
\end{array}\right]_A
\left[\begin{array}{c} \hat{\beta}_0 \\ \hat{\beta}_1 \end{array}\right]_{\hat{\beta}}  =
\left[\begin{array}{c}
\begin{array}{r}
1.5 \\ 1.9 \\ 2.3 \\ 2.2 \\ 2.8 \\ 3.2 \\ 3.1 \\ 3.4 \\ 3.8 \\ 4.3 \\ 4.5
\end{array}
\end{array}\right]_y
\]</span></p>
<p><strong>Finally</strong>, let us implement the following equation in R code using Equation <span class="math inline">\(\ref{eqn:eqnnumber3}\)</span>.</p>

<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" data-line-number="1">intercept =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">length</span>(x))</a>
<a class="sourceLine" id="cb82-2" data-line-number="2"><span class="co"># Our A matrix</span></a>
<a class="sourceLine" id="cb82-3" data-line-number="3">A =<span class="st"> </span><span class="kw">matrix</span>( <span class="dt">data =</span> <span class="kw">c</span>(intercept, x),  <span class="dt">nrow =</span> <span class="dv">11</span>, <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span> )</a>
<a class="sourceLine" id="cb82-4" data-line-number="4"><span class="co"># Our coefficients</span></a>
<a class="sourceLine" id="cb82-5" data-line-number="5">B =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>A) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>y </a>
<a class="sourceLine" id="cb82-6" data-line-number="6"><span class="kw">c</span>(<span class="st">&quot;beta_hat_0&quot;</span> =<span class="st"> </span>B[<span class="dv">1</span>], <span class="st">&quot;beta_hat_1&quot;</span> =<span class="st"> </span>B[<span class="dv">2</span>])</a></code></pre></div>
<pre><code>## beta_hat_0 beta_hat_1 
##   1.017374   1.000408</code></pre>

<p>Our coefficients, <span class="math inline">\(\mathbf{\beta}s\)</span>, for the observed data are: <span class="math inline">\(\beta_0\)</span> = 1.0173736 and <span class="math inline">\(\beta_1\)</span> = 1.0004078.</p>
<p>A plot of the linear regression model is shown in Figure <a href="numericallinearalgebra.html#fig:simpleregression">3.12</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simpleregression"></span>
<img src="embed0007.png" alt="Simple Linear Regression" width="70%" />
<p class="caption">
Figure 3.12: Simple Linear Regression
</p>
</div>

<p>Our <strong>regression model</strong> is expressed as:</p>
<p>y = 1.0173736 + 1.0004078x</p>
<p>We can use the equation as a model to solve for values of y given any new values of x.</p>
</div>
<div id="higherdegreepolynomials" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.5.3</span> Higher Degree Polynomials<a href="numericallinearalgebra.html#higherdegreepolynomials" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now cover curves in this section. We know that the equation of a straight line is expressed by a polynomial with 1st-degree terms. This section introduces polynomials having higher degree terms that describe a curve.</p>
<p>For example, the following equations describe <strong>Legendre polynomials</strong> - one of the <strong>orthogonal polynomials</strong>: </p>
<ul>
<li><strong>Linear</strong>: <span class="math inline">\(P_1(x)\)</span> = x</li>
<li><strong>Quadratic</strong>: <span class="math inline">\(P_2(x)\)</span> = <span class="math inline">\(\frac{1}{2}(3x^2 -1)\)</span></li>
<li><strong>Cubic</strong>: <span class="math inline">\(P_3(x)\)</span> = <span class="math inline">\(\frac{1}{2}(5x^3 -3x)\)</span></li>
<li><strong>Quartic</strong>: <span class="math inline">\(P_4(x)\)</span> = <span class="math inline">\(\frac{1}{8}(35x^4 -30x^2 + 3)\)</span></li>
<li><strong>Quintic</strong>: <span class="math inline">\(P_5(x)\)</span> = <span class="math inline">\(\frac{1}{8}(63x^5 -70x^3 + 15x)\)</span></li>
<li><strong>Sextic</strong>: <span class="math inline">\(P_6(x)\)</span> = <span class="math inline">\(\frac{1}{16}(231x^6 -315x^4 + 105x^2 - 5)\)</span></li>
<li><strong>Septic</strong>: <span class="math inline">\(P_7(x)\)</span> = <span class="math inline">\(\frac{1}{16}(429x^7 -693x^5 + 315x^3 -35x)\)</span></li>
</ul>
<p>We demonstrate <strong>Legendre polynomials</strong> in R to give us the following Figure <a href="numericallinearalgebra.html#fig:legendre">3.13</a>:</p>

<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1"> x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">.2</span>) </a>
<a class="sourceLine" id="cb84-2" data-line-number="2"></a>
<a class="sourceLine" id="cb84-3" data-line-number="3">linear &lt;-<span class="st"> </span><span class="cf">function</span>(t) { t <span class="op">*</span><span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb84-4" data-line-number="4">quadratic &lt;-<span class="st"> </span><span class="cf">function</span>(t) { <span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>( <span class="dv">3</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span> ) }</a>
<a class="sourceLine" id="cb84-5" data-line-number="5">cubic &lt;-<span class="st"> </span><span class="cf">function</span>(t) { <span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">5</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">3</span><span class="op">*</span>t ) }</a>
<a class="sourceLine" id="cb84-6" data-line-number="6">quartic &lt;-<span class="st"> </span><span class="cf">function</span>(t) { <span class="dv">1</span><span class="op">/</span><span class="dv">8</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">35</span><span class="op">*</span>t<span class="op">^</span><span class="dv">4</span> <span class="op">-</span><span class="st"> </span><span class="dv">30</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">3</span>) }</a>
<a class="sourceLine" id="cb84-7" data-line-number="7">quintic &lt;-<span class="st"> </span><span class="cf">function</span>(t) { <span class="dv">1</span><span class="op">/</span><span class="dv">8</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">63</span><span class="op">*</span>t<span class="op">^</span><span class="dv">5</span> <span class="op">-</span><span class="st"> </span><span class="dv">70</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">15</span><span class="op">*</span>t ) }</a>
<a class="sourceLine" id="cb84-8" data-line-number="8">sextic &lt;-<span class="st"> </span><span class="cf">function</span>(t) { <span class="dv">1</span><span class="op">/</span><span class="dv">16</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">231</span><span class="op">*</span>t<span class="op">^</span><span class="dv">6</span> <span class="op">-</span><span class="st"> </span><span class="dv">315</span><span class="op">*</span>t<span class="op">^</span><span class="dv">4</span> <span class="op">+</span><span class="st"> </span><span class="dv">105</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span>  <span class="op">-</span><span class="st"> </span><span class="dv">5</span> ) }</a>
<a class="sourceLine" id="cb84-9" data-line-number="9">septic &lt;-<span class="st"> </span><span class="cf">function</span>(t) { <span class="dv">1</span><span class="op">/</span><span class="dv">16</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">429</span><span class="op">*</span>t<span class="op">^</span><span class="dv">7</span> <span class="op">-</span><span class="st"> </span><span class="dv">693</span><span class="op">*</span>t<span class="op">^</span><span class="dv">5</span> <span class="op">+</span><span class="st"> </span><span class="dv">315</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">35</span><span class="op">*</span>t ) }</a>
<a class="sourceLine" id="cb84-10" data-line-number="10"><span class="kw">par</span>(<span class="dt">pty=</span><span class="st">&quot;m&quot;</span>)</a>
<a class="sourceLine" id="cb84-11" data-line-number="11"><span class="kw">curve</span>( <span class="kw">linear</span>(x), <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>,  </a>
<a class="sourceLine" id="cb84-12" data-line-number="12">      <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>)</a>
<a class="sourceLine" id="cb84-13" data-line-number="13"><span class="kw">curve</span>( <span class="kw">quadratic</span>(x), <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>,<span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb84-14" data-line-number="14"><span class="kw">curve</span>( <span class="kw">cubic</span>(x), <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb84-15" data-line-number="15"><span class="kw">curve</span>( <span class="kw">quartic</span>(x), <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb84-16" data-line-number="16"><span class="kw">curve</span>( <span class="kw">quintic</span>(x), <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb84-17" data-line-number="17"><span class="co">#curve( sextic(x), -1, 1, col=&quot;green&quot;, add=TRUE)</span></a>
<a class="sourceLine" id="cb84-18" data-line-number="18"><span class="co">#curve( septic(x), -1, 1, col=&quot;brown&quot;, add=TRUE)</span></a>
<a class="sourceLine" id="cb84-19" data-line-number="19"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb84-20" data-line-number="20">x_loc =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.75</span>, <span class="fl">0.00</span>, <span class="fl">-0.45</span>, <span class="fl">0.00</span>, <span class="fl">0.55</span>)</a>
<a class="sourceLine" id="cb84-21" data-line-number="21">y_loc =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.9</span>, <span class="fl">-0.6</span>,  <span class="fl">0.52</span>, <span class="fl">0.45</span>, <span class="fl">0.22</span>)</a>
<a class="sourceLine" id="cb84-22" data-line-number="22">labels=<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;line&quot;</span>, <span class="st">&quot;quadratic&quot;</span>, <span class="st">&quot;cubic&quot;</span>, <span class="st">&quot;quartic&quot;</span>, <span class="st">&quot;quintic&quot;</span>)</a>
<a class="sourceLine" id="cb84-23" data-line-number="23"><span class="kw">text</span>(x_loc, y_loc,  <span class="dt">labels=</span>labels, <span class="dt">offset=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;darkgreen&quot;</span> )</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:legendre"></span>
<img src="embed0008.png" alt="Legendre Polynomial" width="70%" />
<p class="caption">
Figure 3.13: Legendre Polynomial
</p>
</div>

<p>See more discussion of <strong>Legendre polynomials</strong> under Section <strong>Gaussian Quadrature</strong> in Chapter <strong>4</strong> (<strong>Numerical Calculus</strong>), including the formula used to derive the polynomials.</p>
<p>We leave readers to investigate <strong>Chebyshev polynomials</strong> in the first and second kinds. </p>
<p>For example:</p>
<p>First kind:</p>
<p><span class="math display">\[\begin{align*}
T_0(x) {}&amp;= 1 \\
T_1(x) &amp;= x \\
T_2(x) &amp;= 2x^2 - 1 \\
T_3(x) &amp;= 4x^3 - 3x \\ 
T_4(x) &amp;= 8x^4 - 8x^2 + 1  \\
T_5(x) &amp;= 16x^5 - 20x^3 + 5x  \\
\end{align*}\]</span></p>
<p>Second kind:</p>
<p><span class="math display">\[\begin{align*}
U_0(x) {}&amp;= 1 \\
U_1(x) &amp;= 2x \\
U_2(x) &amp;= 4x^2 - 1 \\
U_3(x) &amp;= 8x^3 - 4x \\ 
U_4(x) &amp;= 16x^4 - 12x^2 + 1  \\
U_5(x) &amp;= 32x^5 - 32x^3 + 6x  \\
\end{align*}\]</span></p>
<p>Both <strong>Legendre polynomials</strong> and <strong>Chebyshev polynomials</strong> belong to <strong>a class of polynomials</strong> called <strong>Orthogonal polynomials</strong>, which are expressed using the following generalized equation:</p>
<p><span class="math display">\[\begin{align}
\int_{a}^{b} P_m(x)P_n(x)W(x)dx = 0,\ where\ n=0,1,2,..,m-1
\end{align}\]</span></p>
<p><strong>Orthogonal polynomials</strong> require that <span class="math inline">\(P_m(x)\)</span> and <span class="math inline">\(P_n(x)\)</span> are orthogonal, <span class="math inline">\(\langle P_m,P_n \rangle = 0\)</span>, with respect to a <strong>weight</strong> function, <span class="math inline">\(W(x) \geq 0\)</span>. This book does not cover properties of <strong>Orthogonal polynomials</strong>; however, it helps to be familiar with other orthogonal polynomials such as:</p>
<ul>
<li>Hermite Polynomials</li>
<li>Jacobi Polynomials</li>
<li>Laguerre Polynomials</li>
</ul>
<p>One crucial point to emphasize is the difference of <strong>slope</strong> between a curve and a line. The <strong>slope</strong> of a line is constant anywhere - any point in a line. On the other hand, the <strong>slope</strong> of a curve can vary from point to point depending on the curvature. That obviously makes curves non-linear. The importance of a <strong>slope</strong> in a curvature becomes more apparent once we cover <strong>piecewise polynomials</strong>.</p>
<p>In the previous section, the coordinates of (x,y) are given to us. In Figure <a href="numericallinearalgebra.html#fig:polynomial">3.14</a>, the first graph illustrates a set of arbitrary points in the system. Such random points form a <strong>collinear</strong> pattern resembling a straight line. However, this is not always the case. Some datasets paint all sorts of patterns, including curve lines. Such datasets do not come short of unique problems and, at the same time, possibly unique solutions.</p>
<p>Let us use Figure <a href="numericallinearalgebra.html#fig:polynomial">3.14</a> to illustrate a point in this section and introduce a few solutions to polynomial equations.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:polynomial"></span>
<img src="polynomial.png" alt="Polynomial and Interpolation" width="90%" />
<p class="caption">
Figure 3.14: Polynomial and Interpolation
</p>
</div>
</div>
<div id="non-linear-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.5.4</span> Non-Linear Regression <a href="numericallinearalgebra.html#non-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Performing regression to fit a line - a <strong>linear regression</strong> - is no different a concept from a curve. Generally, we extend the concept of a <strong>linear regression</strong>, for which a <strong>line</strong> is in the <strong>order</strong> of a <strong>linear 1st-degree polynomial</strong>, into <strong>polynomial regression</strong>, for which a <strong>curve</strong> is in the <strong>order</strong> of a <strong>non-linear higher-degree polynomial</strong>.</p>
<p>Note here that we emphasize <strong>higher-degree polynomial</strong> equations as <strong>non-linear</strong>. In this section, we do not cover other more complex <strong>non-linear</strong> equations such as <strong>ODEs/PDEs</strong> or equations with interactions and a combination of logarithms or exponentials. We cover <strong>ODE</strong> and <strong>PDE</strong> in the next chapter.</p>
<p>We have already discussed <strong>line fitting</strong> in <strong>Linear 1st-degree polynomials</strong>. We extend that concept and use the term <strong>curve fitting</strong> or <strong>polynomial fitting</strong> in both <strong>linear first-degree polynomials</strong> and <strong>non-linear higher-degree polynomials</strong>.</p>
<p>Here, we approximate the placement of a curve through a given set of data points. Depending on the dataset, there may be a need to employ a 2nd-degree polynomial (a quadratic line) or a 3rd-degree polynomial (a cubic line) to fit against the data. In a case where a dataset cannot sufficiently accommodate a 2nd-degree or a 3rd-degree polynomial, a higher degree terms such as 4th-degree (quartic), 5th-degree (quintic), and so on to construct the polynomial may be required at times. However, there are consequences to using higher-degree polynomials. One of them is losing the generality of a model because the more the curve fits perfectly against a given dataset, the further away we can adapt the model to fit against any other datasets. This is called <strong>over-fitting</strong>.</p>
<p>Now, fitting a curve calls for the same matrix equation but is more generalized:</p>
<p><span class="math display">\[
Ax = b
\]</span></p>
<p>Recall a simple slope formula, (<span class="math inline">\(y = mx + c\)</span>), and its generalized form:</p>
<p><span class="math display">\[\begin{align}
\hat{y} = \sum_{i=0}^n c_ix_i + e\ \rightarrow\ \ \ \ \hat{y} = c_0 + c_1x_1 + c_2x_2 +,...+, c_{n-1}x_{n-1} + c_{n}x_n + \epsilon 
\end{align}\]</span></p>
<p>Here, we extend that to show that polynomial terms are now in higher order: <span class="math inline">\(O(x^{n})\)</span>.</p>
<p><span class="math display">\[\begin{align}
\hat{y} = \sum_{i=0}^\infty c_ix_i + e\ \rightarrow\ \ \ \ \hat{y} = c_0 + c_1x^1 + c_2x^2 + c_3x^3 ,...+, c_{n-2}x^{n-1}+ O(x^{n}) + \epsilon 
\end{align}\]</span></p>
<p>The benefit is higher precision. Because higher order of <strong>x</strong> implies increasing the degree of precision, graphically, the <strong>line</strong> starts to <strong>curve</strong> towards every data point, as we see shortly.</p>
<p>Let us illustrate <strong>polynomial regression</strong> by first introducing an arbitrary set of data points as shown below:</p>
<p><span class="math display">\[
  \left[\begin{array}{r}x \\ y \end{array}\left|\begin{array}{rrrrrrrrrrr}  
1.1 &amp; 1.5 &amp; 2.5 &amp; 2.0 &amp; 3.0 &amp; 3.5 &amp; 4.0 &amp; 4.3 &amp; 5.0 &amp; 5.2\\
6.8 &amp; 5.0 &amp; 3.2 &amp; 4.0 &amp; 5.0 &amp; 6.0 &amp; 6.3 &amp; 6.0 &amp; 3.0 &amp; 3.5
 \end{array}\right.\right]
\]</span></p>
<p>Now, our intent is to fit a <strong>curve</strong>, which requires us to find all the <strong>coefficients</strong> that we can <strong>use to construct</strong> the <strong>polynomial</strong> that best fit the dataset. This is similar to finding the <strong>m</strong> slope and <strong>c</strong> intercept for a simple <strong>line-fitting</strong>, <span class="math inline">\(y = mx + c\)</span>, which we illustrated previously.</p>
<p>Using the same formula, <span class="math inline">\(A\beta = y\)</span>, let us first construct a Vandermonde matrix. To do that, we start with a system of polynomial equations using the data points like so:</p>

<p><span class="math display">\[
\begin{array}{lll}
6.8 = c_0 + c_1\times 1.1 + c_2\times 1.1^2 + c_3\times 1.1^3 + c_4\times 1.1^4 + ... +  O(x^k)\\
5.0 = c_0 + c_1\times 1.5 + c_2\times 1.5^2 + c_3\times 1.5^3 + c_4\times 1.5^4 + ... +  O(x^k) \\
3.2 = c_0 + c_1\times 2.5 + c_2\times 2.5^2 + c_3\times 2.5^3 + c_4\times 2.5^4 + ... +  O(x^k) \\
4.0 = c_0 + c_1\times 2.0 + c_2\times 2.0^2 + c_3\times 2.0^3 + c_4\times 2.0^4 + ... +  O(x^k) \\
5.0 = c_0 + c_1\times 3.0 + c_2\times 3.0^2 + c_3\times 3.0^3 + c_4\times 3.0^4 + ... +  O(x^k) \\
6.0 = c_0 + c_1\times 3.5 + c_2\times 3.5^2 + c_3\times 3.5^3 + c_4\times 3.5^4 + ... +  O(x^k)\\
6.3 = c_0 + c_1\times 4.0 + c_2\times 4.0^2 + c_3\times 4.0^3 + c_4\times 4.0^4 + ... +  O(x^k) \\
6.0 = c_0 + c_1\times 4.3 + c_2\times 4.3^2 + c_3\times 4.3^3 + c_4\times 4.3^4 + ... +  O(x^k) \\
3.0 = c_0 + c_1\times 5.0 + c_2\times 5.0^2 + c_3\times 5.0^3 + c_4\times 5.0^4 + ... +  O(x^k) \\
3.5 = c_0 + c_1\times 5.2 + c_2\times 5.2^2 + c_3\times 5.2^3 + c_4\times 5.2^4 + ... +  O(x^k) \\
\end{array}
\]</span>
</p>
<p>We then translate that stack of equations into a <strong>Vandermonde matrix</strong> of the kth-order. In this case, let us assume 4th order.</p>
<p><span class="math display">\[
Vandermonde(A) = \left[\begin{array}{rrrrrrr} 
1 &amp; x_1 &amp; x_1^2 &amp; x_1^3 &amp; x_1^4\\
1 &amp; x_2 &amp; x_2^2 &amp; x_2^3 &amp; x_2^4 \\
1 &amp; x_3 &amp; x_3^2 &amp; x_3^3 &amp; x_3^4 \\
1 &amp; ... &amp; ... &amp; ... &amp; ...\\
1 &amp; x_{10} &amp; x_{10}^2 &amp; x_{10}^3 &amp; x_{10}^4 \\
\end{array}\right] =
\left[\begin{array}{rrrrrr} 
1 &amp; 1.1 &amp; 1.1^2 &amp; 1.1^3 &amp; 1.1^4\\
1 &amp; 1.5 &amp; 1.5^2 &amp; 1.5^3 &amp; 1.5^4 \\
1 &amp; 2.5 &amp; 2.5^2 &amp; 2.5^3 &amp; 2.5^4 \\
1 &amp; ... &amp; ... &amp; ... &amp; ...\\
1 &amp; 5.2  &amp; 5.2^2 &amp; 5.2^3 &amp; 5.2^4\\
\end{array}\right]
\]</span></p>
<p>Here, we are looking for the <strong>unknown coefficients</strong> denoted as <span class="math inline">\(\hat{\beta} = \{c_0, c_1, c_2, c_3, c_4\}\)</span>:</p>
<p><span class="math display">\[
\hat{\beta} \approx \left[
\begin{array}{rrrr} 
c_0 \\ c_1 \\ c_2 \\ c_3 \\ c_4  
\end{array}\right]
\]</span></p>
<p>So what we get is the following:</p>

<p><span class="math display">\[
A\hat{\beta} \approx y 
\rightarrow
A\left[\begin{array}{ccc} c_0 \\ c_1 \\ c_2 \\ c_3 \\ c_4   \end{array}\right] \approx y
\ \  \rightarrow \ \ 
\left[\begin{array}{c}
\begin{array}{rrrrrr}  
1 &amp; 1.1 &amp; 1.1^2 &amp; 1.1^3 &amp; 1.1^4 \\ 
1 &amp; 1.5 &amp; 1.5^2 &amp; 1.5^3 &amp; 1.5^4  \\ 
1 &amp; 2.5 &amp; 2.5^2 &amp; 2.5^3 &amp; 2.5^4  \\ 
1 &amp; 2.0 &amp; 2.0^2 &amp; 2.0^3 &amp; 2.0^4  \\ 
1 &amp; 3.0 &amp; 3.0^2 &amp; 3.0^3 &amp; 3.0^4  \\ 
1 &amp; 3.5 &amp; 3.5^2 &amp; 3.5^3 &amp; 3.5^4  \\ 
1 &amp; 4.0 &amp; 4.0^2 &amp; 4.0^3 &amp; 4.0^4  \\ 
1 &amp; 4.3 &amp; 4.3^2 &amp; 4.3^3 &amp; 4.3^4  \\ 
1 &amp; 5.0 &amp; 5.0^2 &amp; 5.0^3 &amp; 5.0^4  \\ 
1 &amp; 5.2 &amp; 5.2^2 &amp; 5.2^3 &amp; 5.2^4  \\
\end{array}
\end{array}\right]_A
\left[\begin{array}{c} c_0 \\ c_1 \\ c_2 \\ c_3 \\ c_4 \end{array}\right]  \approx
\left[\begin{array}{c}
\begin{array}{r}
6.8 \\ 5.0 \\ 3.2 \\ 4.0 \\ 5.0 \\ 6.0 \\ 6.3 \\ 6.0 \\ 3.0 \\ 3.5
\end{array}
\end{array}\right]_y
\]</span>
</p>
<p>To compute for the <span class="math inline">\(\hat{\beta}\)</span> with the <span class="math inline">\(c_i\)</span> coefficients, we use Equation <span class="math inline">\(\ref{eqn:eqnnumber3}\)</span> as before. That gives us the following approximate <strong>coefficients</strong>, <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\hat{\beta} \approx \left[\begin{array}{c} c_0 \\ c_1 \\ c_2 \\ c_3 \\ c_4 \end{array}\right] =
\left[\begin{array}{r} 25.190 \\ -26.955 \\ 11.366 \\ -1.813 \\ 0.090 \end{array}\right]
\]</span></p>
<p>With that approximate <strong>coefficients</strong>, we can finally construct our <strong>fit</strong>.</p>
<p><span class="math display">\[
P(x) = 25.190 -26.955x + 11.366x^2 -1.813x^3 + 0.090x^4
\]</span>
That is our <strong>polynomial</strong> that best fits the given dataset at 4th degree.</p>
<p>We implement the case above in R code to show other higher-order polynomials and how each one fits the dataset. It turns out that there are four polynomials, 3rd-order, 4th-order, 5th-order, and 6th order, that seem to fit the dataset. They all seem to follow the same pattern inside the range between the first and last data points but tend to be diverse towards other patterns outside the range. For example, the 5th-order polynomial assumes an upward start but then predicts that the next data point is in the upward direction. The 3rd-order and 4th-order polynomials, however, tend towards a downward direction. We somehow need to validate which one may best fit, but it requires extra new unforeseen datasets. In Chapter <strong>9</strong> (<strong>Computational Learning I</strong>), we will be covering <strong>cross-validation</strong> and other means to validate a model. Before that, our <strong>polynomial regression</strong> works only against the currently available dataset.</p>
<p>Here is a naive implementation of the entire case in R code. We introduce different orders of polynomials and fit each one. It can be seen that the polynomial of the 6th degree seems to fit better than the others:</p>

<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1"><span class="co"># arbitrary data</span></a>
<a class="sourceLine" id="cb85-2" data-line-number="2">x =<span class="st"> </span><span class="kw">c</span>(<span class="fl">1.1</span>, <span class="fl">1.5</span>, <span class="fl">2.5</span>, <span class="fl">2.0</span>, <span class="fl">3.0</span>, <span class="fl">3.5</span>, <span class="fl">4.0</span>, <span class="fl">4.3</span>, <span class="fl">5.0</span>, <span class="fl">5.2</span>)</a>
<a class="sourceLine" id="cb85-3" data-line-number="3">y =<span class="st"> </span><span class="kw">c</span>(<span class="fl">6.8</span>, <span class="fl">5.0</span>, <span class="fl">3.2</span>, <span class="fl">4.0</span>, <span class="fl">5.0</span>, <span class="fl">6.0</span>, <span class="fl">6.3</span>, <span class="fl">6.0</span>, <span class="fl">3.0</span>, <span class="fl">3.5</span>)</a>
<a class="sourceLine" id="cb85-4" data-line-number="4">polynomial &lt;-<span class="st"> </span><span class="cf">function</span>(x, beta) {</a>
<a class="sourceLine" id="cb85-5" data-line-number="5">    n=<span class="kw">length</span>(beta) </a>
<a class="sourceLine" id="cb85-6" data-line-number="6">    y =<span class="st"> </span>beta[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb85-7" data-line-number="7">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>n) { y =<span class="st"> </span>y <span class="op">+</span><span class="st"> </span>beta[i] <span class="op">*</span><span class="st"> </span>x<span class="op">^</span>(i<span class="dv">-1</span>)   }</a>
<a class="sourceLine" id="cb85-8" data-line-number="8">    y</a>
<a class="sourceLine" id="cb85-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb85-10" data-line-number="10">lu_factoring &lt;-<span class="st"> </span><span class="cf">function</span>(A, y) {</a>
<a class="sourceLine" id="cb85-11" data-line-number="11">    LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(A)</a>
<a class="sourceLine" id="cb85-12" data-line-number="12">    uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, y)</a>
<a class="sourceLine" id="cb85-13" data-line-number="13">    x =<span class="st"> </span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy)</a>
<a class="sourceLine" id="cb85-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb85-15" data-line-number="15">show_curve &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, order, ilwd,  col) {</a>
<a class="sourceLine" id="cb85-16" data-line-number="16">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb85-17" data-line-number="17">    lty =<span class="st"> </span><span class="dv">2</span>; lwd=<span class="dv">1</span></a>
<a class="sourceLine" id="cb85-18" data-line-number="18">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(order)) {</a>
<a class="sourceLine" id="cb85-19" data-line-number="19">        p =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,n)</a>
<a class="sourceLine" id="cb85-20" data-line-number="20">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>order[i]) { p =<span class="st"> </span><span class="kw">c</span>(p, x<span class="op">^</span>k) }</a>
<a class="sourceLine" id="cb85-21" data-line-number="21">        A =<span class="st"> </span><span class="kw">matrix</span>(p, n, <span class="dt">byrow=</span><span class="ot">FALSE</span>)  <span class="co"># build vandermonde matrix</span></a>
<a class="sourceLine" id="cb85-22" data-line-number="22">        beta =<span class="st"> </span><span class="kw">lu_factoring</span>(<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>A,<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>y ) </a>
<a class="sourceLine" id="cb85-23" data-line-number="23">        <span class="cf">if</span> (ilwd[i] <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) { lty =<span class="st"> </span><span class="dv">1</span>; lwd=<span class="dv">2</span> }</a>
<a class="sourceLine" id="cb85-24" data-line-number="24">        <span class="kw">curve</span>( <span class="kw">polynomial</span>(x, beta), <span class="dv">0</span>, <span class="dv">6</span>,  <span class="dt">col=</span>col[i], </a>
<a class="sourceLine" id="cb85-25" data-line-number="25">              <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">lty=</span>lty, <span class="dt">lwd=</span>lwd)</a>
<a class="sourceLine" id="cb85-26" data-line-number="26">    }</a>
<a class="sourceLine" id="cb85-27" data-line-number="27">}</a>
<a class="sourceLine" id="cb85-28" data-line-number="28"><span class="kw">par</span>(<span class="dt">pty=</span><span class="st">&quot;m&quot;</span>)</a>
<a class="sourceLine" id="cb85-29" data-line-number="29"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">6</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">8</span>), </a>
<a class="sourceLine" id="cb85-30" data-line-number="30">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>)</a>
<a class="sourceLine" id="cb85-31" data-line-number="31"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb85-32" data-line-number="32"><span class="kw">show_curve</span>(x, y, <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>), </a>
<a class="sourceLine" id="cb85-33" data-line-number="33">                <span class="kw">c</span>(<span class="ot">FALSE</span>, <span class="ot">FALSE</span>, <span class="ot">FALSE</span>, <span class="ot">FALSE</span>, <span class="ot">FALSE</span>, <span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb85-34" data-line-number="34">                 <span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;lightgreen&quot;</span>, <span class="st">&quot;green&quot;</span>, </a>
<a class="sourceLine" id="cb85-35" data-line-number="35">                   <span class="st">&quot;dodgerblue&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;red&quot;</span>))</a>
<a class="sourceLine" id="cb85-36" data-line-number="36"><span class="kw">points</span>(x,y, <span class="dt">col=</span><span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb85-37" data-line-number="37">x_loc =<span class="st"> </span><span class="kw">c</span>(<span class="fl">3.8</span>, <span class="fl">2.5</span>, <span class="fl">5.2</span>, <span class="fl">5.7</span>,  <span class="fl">0.75</span>, <span class="fl">4.0</span>)</a>
<a class="sourceLine" id="cb85-38" data-line-number="38">y_loc =<span class="st"> </span><span class="kw">c</span>(<span class="fl">4.5</span>, <span class="fl">5.5</span>, <span class="fl">0.52</span>, <span class="fl">2.0</span>, <span class="fl">2.5</span>, <span class="fl">6.8</span>)</a>
<a class="sourceLine" id="cb85-39" data-line-number="39">labels=<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;line&quot;</span>, <span class="st">&quot;quadratic&quot;</span>, <span class="st">&quot;cubic&quot;</span>, <span class="st">&quot;quartic&quot;</span>, <span class="st">&quot;quintic&quot;</span>, <span class="st">&quot;sextic&quot;</span>)</a>
<a class="sourceLine" id="cb85-40" data-line-number="40"><span class="kw">text</span>(x_loc, y_loc,  <span class="dt">labels=</span>labels, <span class="dt">offset=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;darkgreen&quot;</span> )</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:polyregression"></span>
<img src="embed0009.png" alt="Polynomial Regression" width="70%" />
<p class="caption">
Figure 3.15: Polynomial Regression
</p>
</div>

<p>Additional <strong>Polynomial regression</strong> techniques are available in the <strong>Polynomial Smoothing</strong> section. Also, see the <strong>Local Linear Regression</strong> section that uses <strong>smoothing</strong> techniques.</p>
</div>
</div>
<div id="approximating-polynomial-functions-by-series-expansion" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.6</span> Approximating Polynomial Functions by Series Expansion <a href="numericallinearalgebra.html#approximating-polynomial-functions-by-series-expansion" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There is a difference between <strong>Polynomial Regression</strong> and <strong>Polynomial Approximation</strong>. The former performs <strong>curve fitting</strong> against an arbitrary dataset. The latter performs <strong>curve fitting</strong> against an arbitrary polynomial function.</p>
<p>While both are considered an approximation, they differ in what they are approximating.</p>
<p>This section covers three known series that we can use to perform <strong>curve fitting</strong> to a function - otherwise also regarded as <strong>Polynomial approximation</strong>. It can also be considered <strong>Function Optimization</strong>.</p>
<p>Here, we approximate a given polynomial function by:</p>
<ul>
<li>using <strong>Maclaurin series</strong> </li>
</ul>
<p><span class="math display">\[\begin{align}
p(x) = \sum_{n=0}^\infty \left( \frac{f^{(n)}(0)x^n}{n!}\right)
\end{align}\]</span></p>
<p>which can be expanded into this:</p>
<p><span class="math display">\[\begin{align}
p(x) = f(0) + f&#39;(0)\frac{x^1}{1!} + f&#39;&#39;(0)\frac{x^2}{2!} + f&#39;&#39;&#39;(0)\frac{x^3}{3!} 
  +\ ...\ + f^{(n)}(0)\frac{x^n}{n!} +\ ...
\end{align}\]</span></p>
<ul>
<li>using <strong>Taylor series</strong> </li>
</ul>
<p><span class="math display">\[\begin{align}
p(a) = \sum_{n=0}^\infty \left( \frac{f^{(n)}(a)(z)^n}{n!}\right)
\end{align}\]</span></p>
<p>which can be expanded into this:</p>
<p><span class="math display">\[\begin{align}
{}&amp;Given:\ z\ =\ x\ -\ a
\nonumber \\ \nonumber \\
&amp;p(a) = f(a) + f&#39;(a)\frac{z^1}{1!} + f&#39;&#39;(a)\frac{z^2}{2!} + f&#39;&#39;&#39;(a)\frac{z^3}{3!} 
  +\ ...\ + f^{(n)}(a)\frac{z^n}{n!} +\ ...
\end{align}\]</span></p>
<ul>
<li>using <strong>Fourier series</strong></li>
</ul>
<p><span class="math display">\[\begin{align}
p(a) = \sum_{n=0}^\infty \left( \frac{f^{(n)}(a)(z)^n}{n!}\right)
\end{align}\]</span></p>
<p>which can be expanded into this:</p>
<p><span class="math display">\[\begin{align}
{}&amp;Given:\ z\ = e^{i\theta},\ \ \ \ where\ e^{i\theta} = cos\theta + i\cdotp sin\theta
\nonumber \\ \nonumber \\
&amp;p(a) = f(a) + f&#39;(a)\frac{z^1}{1!} + f&#39;&#39;(a)\frac{z^2}{2!} + f&#39;&#39;&#39;(a)\frac{z^3}{3!} 
  +\ ...\ + f^{(n)}(a)\frac{z^n}{n!} +\ ...
\end{align}\]</span></p>
<p>We illustrate <strong>curve-fitting</strong> using one of the series above. Let us try to approximate the following curve using <strong>Taylor Series</strong>:</p>
<p><span class="math display">\[
f(x) = 2x^3 + 5x^2 - 9x + 3,\ \ \ \ \ where\ \ x=2
\]</span></p>
<p><strong>First</strong>, let us compute for the <strong>derivatives</strong> of the function, f(x):</p>
<p><span class="math display">\[\begin{align*}
f(x) {}&amp;= 2x^3 + 5x^2 - 9x + 3 &amp;\rightarrow f(2) {}&amp;= 21\\
f&#39;(x) &amp;= 6x^2 + 10x - 9 &amp;\rightarrow f&#39;(2) &amp;=  35\\
f&#39;&#39;(x) &amp;= 12x + 10 &amp;\rightarrow f&#39;&#39;(2) &amp;= 34 \\
f&#39;&#39;&#39;(x) &amp;= 12 &amp;\rightarrow f&#39;&#39;&#39;(2) &amp;=  12\\
f^{(4)}(x) &amp;= 0 &amp;\rightarrow f^{(4)}(2) &amp;= 0 \\
\end{align*}\]</span></p>
<p><strong>Second</strong>, using <strong>Taylor Series</strong>, we get:</p>
<p><span class="math display">\[\begin{align}
p(x) {}&amp;= \sum_{n=0}^\infty \left( \frac{f^{(n)}(2)(x-2)^n}{n!}\right) \\
p(x) &amp;= f(2) + f&#39;(2)\frac{z^1}{1!} + f&#39;&#39;(2)\frac{z^2}{2!} + f&#39;&#39;&#39;(2)\frac{z^3}{3!}, \ \ where \ z = x - 2 \\
p(x) &amp;= 21 + 35\frac{z^1}{1} + 34\frac{z^2}{2} + 12\frac{z^3}{6} \nonumber \\
p(x) &amp;= 21 + 35z + 17z^2 + 2z^3 \nonumber  \\
p(x) &amp;= 21 + 35(x-2) + 17(x-2)^2 + 2(x-2)^3  
\end{align}\]</span></p>
<p>We then can use the derived polynomial, <span class="math inline">\(p(x)\)</span>, to approximate the actual curve.</p>
<p>We discuss more of <strong>Fourier Series</strong> to derive methods in Chapter <strong>4</strong> (<strong>Numerical Calculus</strong>). <strong>Taylor series</strong> is covered more under <strong>Laplace Approximation</strong> Section in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>).</p>
</div>
<div id="polynomialinterpolation" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.7</span> Approximating Polynomial Functions by Interpolation<a href="numericallinearalgebra.html#polynomialinterpolation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Interpolation</strong> is a method used to numerically approximate an <strong>unknown</strong> data point that is either missing or hidden in a range of a given dataset such that it allows us to refurbish the <strong>missing</strong> data back into the range using the approximated data point. In some cases, this allows us to smoothen the curvature of a curve; in other cases, it allows us to merely fill the gap in a dataset - especially when completing data in tabular form. </p>
<p>In hindsight, it is essential to understand the difference between <strong>Polynomial Regression</strong> and <strong>Polynomial Interpolation</strong>. If a dataset resembles a curvy pattern but is scattered in a way that we cannot get a curve to pass through the data points, what we can do is approximate the closest curve that we can fit through the data points - this is fitting a curve - this is <strong>regression</strong>. And if a dataset resembles a curvy pattern and we can get a curve to pass through all the data points, then we can use <strong>Interpolation</strong> to add more points through the curve.</p>
<p>Both methods yield a <strong>polynomial expression</strong> that we can use as a model to predict the value of <strong>y</strong> given the value of <strong>x</strong> in any context that represents both <strong>y</strong> and <strong>x</strong>. For example, predicting the weather (y) given the condition of air (x). See Figure <a href="numericallinearalgebra.html#fig:regressinterpolate">3.16</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:regressinterpolate"></span>
<img src="regression_interpolation.png" alt="Regression vs Interpolation" width="70%" />
<p class="caption">
Figure 3.16: Regression vs Interpolation
</p>
</div>
<p>Additionally, in this section, we will be discussing in detail <strong>spline regression (or curve fitting)</strong> and <strong>spline interpolation</strong> as a natural next topic for <strong>polynomial discussion</strong>.</p>
<p>There are a few methods of interpolation we introduce in this section:</p>
<div id="polynomial-interpolation" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.1</span> Polynomial interpolation <a href="numericallinearalgebra.html#polynomial-interpolation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The goal is to be able to construct a polynomial equation that can help us to obtain the <strong>unknown</strong> data points within the dataset.</p>
<p>Given a set of data points (the initial range of dataset), determine the coefficients required to formulate the following polynomial equation:</p>
<p><span class="math display">\[\begin{align}
P(x) = a_o + a_1x + a_2x^2 + a_3x^3 + ... + a_nx^{n-1}
\end{align}\]</span></p>
<p>First, the set of data points (dataset) could be arbitrary (mostly derived from an observation of an event). Here, a dataset is as simple as a list of x-y coordinates like so: (1,1), (3,4), (3,14).</p>
<p>Second, to explain the case, we use <strong>Runge</strong> function to generate our sample data points. Our strategy is to build a uniform sequence of <strong>x</strong> values between [-1,1].</p>
<p>In R code, here is the sequence of <strong>x</strong> values with a fixed interval of 0.25:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1">n =<span class="st"> </span><span class="dv">8</span></a>
<a class="sourceLine" id="cb86-2" data-line-number="2"><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>, <span class="dt">length.out =</span> n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] -1.00 -0.75 -0.50 -0.25  0.00  0.25  0.50  0.75  1.00</code></pre>
<p>Third, we feed the generated sequence of <strong>x</strong> values to the <strong>Runge</strong> function to give us our <strong>y</strong> values:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1">n =<span class="st"> </span><span class="dv">8</span></a>
<a class="sourceLine" id="cb88-2" data-line-number="2">x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>, <span class="dt">length.out =</span> n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb88-3" data-line-number="3">runge &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">25</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span>) }</a>
<a class="sourceLine" id="cb88-4" data-line-number="4">(<span class="dt">y =</span> <span class="kw">round</span>(<span class="kw">runge</span>(x),<span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 0.04 0.07 0.14 0.39 1.00 0.39 0.14 0.07 0.04</code></pre>
<p>So now we are able to obtain our data set:</p>
<p><span class="math display">\[
  \left[\begin{array}{r}x \\ y\end{array}\left|\begin{array}{rrrrrrrrrrr}  
-1.00 &amp; -0.75 &amp; -0.50 &amp; -0.25 &amp; 0.00 &amp; 0.25 &amp; 0.50 &amp; 0.75 &amp; 1.00\\
 0.04 &amp; 0.07 &amp; 0.14 &amp; 0.39 &amp; 1.00 &amp; 0.39 &amp; 0.14 &amp; 0.07 &amp; 0.04
 \end{array}\right.\right]
\]</span></p>
<p>Our goal here is to formulate a polynomial using the dataset we obtained from the <strong>Runge</strong> function that will eventually help us perform an interpolation.</p>
<p>Figure <a href="numericallinearalgebra.html#fig:polynomialinterpolate">3.17</a> shows a list of polynomial interpolations with different degrees. It can be observed that those interpolations do not seem to fit the Runge curve perfectly. For example, the polynomial with degree 10 perfectly fits the peak of the Runge curve, but then the oscillation gets larger (worse) at the edges, suggesting an extremely inaccurate interpolation. On the other hand, the polynomial of degree 9 seems to demonstrate being closer to the Runge curve, but still not quite.</p>

<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1">runge &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">25</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span>) }</a>
<a class="sourceLine" id="cb90-2" data-line-number="2">uniform &lt;-<span class="st"> </span><span class="cf">function</span>(n) { x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, n, <span class="dv">1</span>); <span class="dv">2</span><span class="op">*</span>x <span class="op">/</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb90-3" data-line-number="3">chebyshev &lt;-<span class="st"> </span><span class="cf">function</span>(n) { x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, pi, <span class="dt">length.out=</span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>); <span class="op">-</span><span class="kw">cos</span>(x)  }</a>
<a class="sourceLine" id="cb90-4" data-line-number="4">vandermonde &lt;-<span class="st"> </span><span class="cf">function</span>(xi, n) {</a>
<a class="sourceLine" id="cb90-5" data-line-number="5">    m &lt;-<span class="st"> </span><span class="kw">matrix</span>(, <span class="dt">nrow=</span><span class="dv">0</span>, <span class="dt">ncol=</span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb90-6" data-line-number="6">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb90-7" data-line-number="7">        v =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb90-8" data-line-number="8">        <span class="cf">for</span> (p <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb90-9" data-line-number="9">            v[p <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]  =<span class="st">  </span>xi[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]<span class="op">^</span>p</a>
<a class="sourceLine" id="cb90-10" data-line-number="10">        }</a>
<a class="sourceLine" id="cb90-11" data-line-number="11">        m &lt;-<span class="st"> </span><span class="kw">rbind</span>(m, v)</a>
<a class="sourceLine" id="cb90-12" data-line-number="12">    }</a>
<a class="sourceLine" id="cb90-13" data-line-number="13">    m</a>
<a class="sourceLine" id="cb90-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb90-15" data-line-number="15">polynomial &lt;-<span class="st"> </span><span class="cf">function</span>(coeffs, t, degree) {</a>
<a class="sourceLine" id="cb90-16" data-line-number="16">    p =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb90-17" data-line-number="17">    <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>degree) {</a>
<a class="sourceLine" id="cb90-18" data-line-number="18">        p =<span class="st"> </span>p <span class="op">+</span><span class="st"> </span>coeffs[d <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>t<span class="op">^</span>d</a>
<a class="sourceLine" id="cb90-19" data-line-number="19">    }</a>
<a class="sourceLine" id="cb90-20" data-line-number="20">    p</a>
<a class="sourceLine" id="cb90-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb90-22" data-line-number="22">interpolate &lt;-<span class="st"> </span><span class="cf">function</span>(degree, col, <span class="dt">plot=</span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb90-23" data-line-number="23">  n=degree</a>
<a class="sourceLine" id="cb90-24" data-line-number="24">  x =<span class="st"> </span><span class="kw">uniform</span>(n)</a>
<a class="sourceLine" id="cb90-25" data-line-number="25">  A =<span class="st"> </span><span class="kw">vandermonde</span>(x, n)</a>
<a class="sourceLine" id="cb90-26" data-line-number="26">  coeffs =<span class="st"> </span><span class="kw">solve</span>(A,  <span class="kw">runge</span>(x))</a>
<a class="sourceLine" id="cb90-27" data-line-number="27">  yi =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb90-28" data-line-number="28">  n =<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb90-29" data-line-number="29">  x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span>n)</a>
<a class="sourceLine" id="cb90-30" data-line-number="30">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb90-31" data-line-number="31">      yi[i] =<span class="st"> </span><span class="kw">polynomial</span>(coeffs, x[i], degree)</a>
<a class="sourceLine" id="cb90-32" data-line-number="32">  }</a>
<a class="sourceLine" id="cb90-33" data-line-number="33">  <span class="cf">if</span> (plot <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb90-34" data-line-number="34">    <span class="kw">lines</span>(x, yi ,  <span class="dt">col=</span>col, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb90-35" data-line-number="35">  }</a>
<a class="sourceLine" id="cb90-36" data-line-number="36">  yi</a>
<a class="sourceLine" id="cb90-37" data-line-number="37">}</a>
<a class="sourceLine" id="cb90-38" data-line-number="38"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>), </a>
<a class="sourceLine" id="cb90-39" data-line-number="39">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>)</a>
<a class="sourceLine" id="cb90-40" data-line-number="40"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb90-41" data-line-number="41"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb90-42" data-line-number="42">y =<span class="st"> </span><span class="kw">interpolate</span>(<span class="dt">degree=</span><span class="dv">5</span>, <span class="st">&quot;dodgerblue&quot;</span>, <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb90-43" data-line-number="43">y =<span class="st"> </span><span class="kw">interpolate</span>(<span class="dt">degree=</span><span class="dv">9</span>, <span class="st">&quot;lightgreen&quot;</span>, <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb90-44" data-line-number="44">y =<span class="st"> </span><span class="kw">interpolate</span>(<span class="dt">degree=</span><span class="dv">10</span>, <span class="st">&quot;lightblue&quot;</span>, <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb90-45" data-line-number="45"><span class="kw">curve</span>(<span class="kw">runge</span>(x), <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>,  <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb90-46" data-line-number="46"><span class="kw">legend</span>(<span class="op">-</span><span class="fl">0.4</span>, <span class="dv">2</span>, </a>
<a class="sourceLine" id="cb90-47" data-line-number="47">   <span class="kw">c</span>( <span class="st">&quot;runge function&quot;</span>, </a>
<a class="sourceLine" id="cb90-48" data-line-number="48">      <span class="st">&quot;polynomial interpolation (degree=5)&quot;</span>, </a>
<a class="sourceLine" id="cb90-49" data-line-number="49">      <span class="st">&quot;polynomial interpolation (degree=9)&quot;</span>,</a>
<a class="sourceLine" id="cb90-50" data-line-number="50">      <span class="st">&quot;polynomial interpolation (degree=10)&quot;</span></a>
<a class="sourceLine" id="cb90-51" data-line-number="51">            ),</a>
<a class="sourceLine" id="cb90-52" data-line-number="52"><span class="dt">fill=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;dodgerblue&quot;</span>, <span class="st">&quot;lightgreen&quot;</span>, <span class="st">&quot;lightblue&quot;</span>, <span class="st">&quot;lightgrey&quot;</span>), </a>
<a class="sourceLine" id="cb90-53" data-line-number="53">     <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:polynomialinterpolate"></span>
<img src="embed0010.png" alt="Polynomial Interpolation" width="70%" />
<p class="caption">
Figure 3.17: Polynomial Interpolation
</p>
</div>

<p>At a higher degree of 10, the oscillation gets larger (and worse), suggesting an inaccurate interpolation. For example, below, the interpolated data point at the oscillation peak is at (-0.9296482, 1.9258452). And it is way beyond the Runge curve.</p>

<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1">n =<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb91-2" data-line-number="2">x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span>n)</a>
<a class="sourceLine" id="cb91-3" data-line-number="3">y =<span class="st"> </span><span class="kw">interpolate</span>( <span class="dt">degree=</span><span class="dv">10</span>, <span class="st">&quot;lightblue&quot;</span>)</a>
<a class="sourceLine" id="cb91-4" data-line-number="4">(<span class="dt">x_y =</span> <span class="kw">list</span>( <span class="st">&quot;x&quot;</span> =<span class="st"> </span>x[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>], <span class="st">&quot;y&quot;</span> =<span class="st"> </span>y[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]))</a></code></pre></div>
<pre><code>## $x
##  [1] -1.0000000 -0.9899497 -0.9798995 -0.9698492 -0.9597990 -0.9497487
##  [7] -0.9396985 -0.9296482 -0.9195980 -0.9095477
## 
## $y
##  [1] 0.03846154 0.72903685 1.23455801 1.58523834 1.80773572 1.92546758
##  [7] 1.95890471 1.92584519 1.84166913 1.71957548</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1"><span class="kw">c</span>(x_y<span class="op">$</span>x[<span class="dv">8</span>], x_y<span class="op">$</span>y[<span class="dv">8</span>])</a></code></pre></div>
<pre><code>## [1] -0.9296482  1.9258452</code></pre>

<p>Try to run the R code using the Chebyshev interval and see what happens.</p>
<p>Now let us look at other interpolation methods to see if we obtain a better result.</p>
</div>
<div id="lagrange-interpolation" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.2</span> Lagrange interpolation <a href="numericallinearalgebra.html#lagrange-interpolation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This interpolation uses the following equation:</p>
<p><span class="math display">\[\begin{align}
L(x) = \sum_{j=0}^k y_j l_j(x)
\end{align}\]</span></p>
<p>with the <strong>Lagrange polynomial</strong> formula as our <strong>basis function</strong> <span class="citation">(Berrut JP and Trefethen L. N. <a href="bibliography.html#ref-ref336r">2012</a>; Das B. and Chakrabarty D. <a href="bibliography.html#ref-ref326b">2016</a>)</span>: </p>
<p><span class="math display">\[\begin{align}
l_j(x) 
= \prod_{i = 0,\ i \ne j}^k \frac{x - x_i}{x_j - x_i}
= \frac{(x - x_0)}{(x_j - x_0)} 
\frac{(x - x_1)}{(x_j - x_1)}  ...
\frac{(x - x_k)}{(x_j - x_k)} 
\end{align}\]</span></p>
<p>where k = number of data points.</p>
<p>Similar to <strong>Polynomial</strong> interpolation, the <strong>Lagrange</strong> interpolation demonstrates an also observed oscillation at the end of the curves.</p>
<p>In expanded form, we get the following equation (where <span class="math inline">\(i \ne j\)</span>):</p>
<p><span class="math display">\[\begin{align}
L(x) &amp;= y_0 + y_{j=1}\left(\frac{(x-x_{i=0})}{(x_{j=1}-x_{i=0})} ...\frac{(x-x_{i=k})}{(x_{j=1}-x_{i=k})}\right) + ... \nonumber \\
&amp; + y_{j=k}\left(\frac{(x-x_{i=0})}{(x_{j=k}-x_{i=0})} ...\frac{(x-x_{i=k})}{(x_{j=k}-x_{i=k})}\right)
\end{align}\]</span></p>
<p>As long as <span class="math inline">\(i \ne j\)</span> is a constraint, then <span class="math inline">\((x_{j=k}-x_{i=k})\)</span> will not happen.</p>
<p>The R code and result of <strong>Lagrange</strong> interpolation is demonstrated in Figure <a href="numericallinearalgebra.html#fig:lagrange">3.18</a> using Chebyshev interval. Try to run the R code but using uniform interval. Compare the result against Figure <a href="numericalmethods.html#fig:runge">1.1</a>. Also, for other functions, try to use <span class="math inline">\(exp(x)\)</span> instead of <span class="math inline">\(runge(x)\)</span> for experiment.</p>

<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" data-line-number="1">runge &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">25</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span>) }</a>
<a class="sourceLine" id="cb95-2" data-line-number="2">uniform &lt;-<span class="st"> </span><span class="cf">function</span>(n) { x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, n , <span class="dv">1</span> ); <span class="dv">2</span><span class="op">*</span>x <span class="op">/</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb95-3" data-line-number="3">chebyshev &lt;-<span class="st"> </span><span class="cf">function</span>(n) { x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, pi, <span class="dt">length.out=</span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span> ); <span class="op">-</span><span class="kw">cos</span>(x)}</a>
<a class="sourceLine" id="cb95-4" data-line-number="4">lagrange &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, new_x) {</a>
<a class="sourceLine" id="cb95-5" data-line-number="5">    p =<span class="st"> </span><span class="kw">rbind</span>(x, y)</a>
<a class="sourceLine" id="cb95-6" data-line-number="6">    k =<span class="st"> </span><span class="kw">ncol</span>(p)</a>
<a class="sourceLine" id="cb95-7" data-line-number="7">    Lx =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb95-8" data-line-number="8">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {</a>
<a class="sourceLine" id="cb95-9" data-line-number="9">        xj =<span class="st"> </span>p[,j][<span class="st">&quot;x&quot;</span>]</a>
<a class="sourceLine" id="cb95-10" data-line-number="10">        yj =<span class="st"> </span>p[,j][<span class="st">&quot;y&quot;</span>]</a>
<a class="sourceLine" id="cb95-11" data-line-number="11">        lj_x =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb95-12" data-line-number="12">        <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {</a>
<a class="sourceLine" id="cb95-13" data-line-number="13">            <span class="cf">if</span> (j <span class="op">!=</span><span class="st"> </span>i) {</a>
<a class="sourceLine" id="cb95-14" data-line-number="14">                xi =<span class="st"> </span>p[,i][<span class="st">&quot;x&quot;</span>]</a>
<a class="sourceLine" id="cb95-15" data-line-number="15">                lj_x =<span class="st"> </span>lj_x <span class="op">*</span><span class="st"> </span>(new_x <span class="op">-</span><span class="st"> </span>xi) <span class="op">/</span><span class="st"> </span>(xj <span class="op">-</span><span class="st"> </span>xi)</a>
<a class="sourceLine" id="cb95-16" data-line-number="16">            }</a>
<a class="sourceLine" id="cb95-17" data-line-number="17">        }</a>
<a class="sourceLine" id="cb95-18" data-line-number="18">        Lx =<span class="st"> </span>Lx <span class="op">+</span><span class="st"> </span>yj <span class="op">*</span><span class="st"> </span>lj_x</a>
<a class="sourceLine" id="cb95-19" data-line-number="19">    }</a>
<a class="sourceLine" id="cb95-20" data-line-number="20">    Lx</a>
<a class="sourceLine" id="cb95-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb95-22" data-line-number="22">interpolate &lt;-<span class="st"> </span><span class="cf">function</span>(x, y) {</a>
<a class="sourceLine" id="cb95-23" data-line-number="23">    x_n =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">200</span>)</a>
<a class="sourceLine" id="cb95-24" data-line-number="24">    order =<span class="st"> </span><span class="kw">c</span>( <span class="dv">11</span>, <span class="dv">13</span>, <span class="dv">15</span>)</a>
<a class="sourceLine" id="cb95-25" data-line-number="25">    col =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;dodgerblue&quot;</span>,  <span class="st">&quot;lightgreen&quot;</span>, <span class="st">&quot;lightblue&quot;</span>, <span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb95-26" data-line-number="26">    l =<span class="st"> </span><span class="kw">length</span>(order)</a>
<a class="sourceLine" id="cb95-27" data-line-number="27">    xn =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb95-28" data-line-number="28">    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>l) {</a>
<a class="sourceLine" id="cb95-29" data-line-number="29">        <span class="co">#x = uniform(order[n] ) </span></a>
<a class="sourceLine" id="cb95-30" data-line-number="30">        x =<span class="st"> </span><span class="kw">chebyshev</span>(order[n])</a>
<a class="sourceLine" id="cb95-31" data-line-number="31">        y_n =<span class="st"> </span><span class="kw">lagrange</span>(x, <span class="kw">runge</span>(x), x_n)</a>
<a class="sourceLine" id="cb95-32" data-line-number="32">        <span class="kw">lines</span>(x_n, y_n, <span class="dt">col=</span>col[n], <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb95-33" data-line-number="33">        <span class="cf">if</span> (n<span class="op">==</span><span class="dv">1</span>) {  xn =<span class="st"> </span>x }</a>
<a class="sourceLine" id="cb95-34" data-line-number="34">    }</a>
<a class="sourceLine" id="cb95-35" data-line-number="35">    xn</a>
<a class="sourceLine" id="cb95-36" data-line-number="36">}</a>
<a class="sourceLine" id="cb95-37" data-line-number="37">x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">20</span>) </a>
<a class="sourceLine" id="cb95-38" data-line-number="38">y =<span class="st"> </span><span class="kw">runge</span>(x)</a>
<a class="sourceLine" id="cb95-39" data-line-number="39"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>), </a>
<a class="sourceLine" id="cb95-40" data-line-number="40">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>)</a>
<a class="sourceLine" id="cb95-41" data-line-number="41"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb95-42" data-line-number="42"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb95-43" data-line-number="43">xn =<span class="st"> </span><span class="kw">interpolate</span>(x, y)</a>
<a class="sourceLine" id="cb95-44" data-line-number="44"><span class="kw">curve</span>(<span class="kw">runge</span>(x), <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>,  <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb95-45" data-line-number="45"><span class="kw">points</span>(xn, <span class="kw">runge</span>(xn), <span class="dt">col=</span><span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb95-46" data-line-number="46"><span class="kw">legend</span>(<span class="op">-</span><span class="fl">0.4</span>, <span class="dv">2</span>, </a>
<a class="sourceLine" id="cb95-47" data-line-number="47">  <span class="kw">c</span>( <span class="st">&quot;runge function&quot;</span>, </a>
<a class="sourceLine" id="cb95-48" data-line-number="48">      <span class="st">&quot;lagrange interpolation (n=12)&quot;</span>,</a>
<a class="sourceLine" id="cb95-49" data-line-number="49">      <span class="st">&quot;lagrange interpolation (n=14)&quot;</span>,  </a>
<a class="sourceLine" id="cb95-50" data-line-number="50">      <span class="st">&quot;lagrange interpolation (n=16)&quot;</span>),</a>
<a class="sourceLine" id="cb95-51" data-line-number="51"><span class="dt">fill=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;dodgerblue&quot;</span>,  <span class="st">&quot;lightgreen&quot;</span>, <span class="st">&quot;lightblue&quot;</span>, <span class="st">&quot;lightgrey&quot;</span>), </a>
<a class="sourceLine" id="cb95-52" data-line-number="52">     <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lagrange"></span>
<img src="embed0011.png" alt="Lagrange Interpolation (Chebyshev Interval)" width="70%" />
<p class="caption">
Figure 3.18: Lagrange Interpolation (Chebyshev Interval)
</p>
</div>

<p>We leave readers to also investigate a variant formulation of the Lagrange interpolation using <strong>Barycentric formula</strong> <span class="citation">(Berrut JP and Trefethen L. N. <a href="bibliography.html#ref-ref336r">2012</a>; Das B. and Chakrabarty D. <a href="bibliography.html#ref-ref326b">2016</a>)</span>.</p>
</div>
<div id="newton-interpolation" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.3</span> Newton interpolation <a href="numericallinearalgebra.html#newton-interpolation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Newton interpolation uses the following equation with a <strong>divide difference</strong> component, <span class="math inline">\(f[x_j]\)</span>, and with newton basis polynomials <span class="citation">(Berrut JP and Trefethen L. N. <a href="bibliography.html#ref-ref336r">2012</a>)</span>.</p>
<p><span class="math display">\[\begin{align}
N(x) = \sum_{j=0}^k f[x_j] n_j(x) \label{eqn:eqnnumber5}
\end{align}\]</span></p>
<p>with the <strong>Newton polynomial formula</strong> as our <strong>basis function</strong>: </p>
<p><span class="math display">\[\begin{align}
n_j(x) 
= \prod_{i = 0}^{j-1} (x - x_i)
=  (x - x_0) (x - x_1) ... (x - x_{j - 1})
\end{align}\]</span></p>
<p><strong>Divide Difference</strong> expression:</p>
<p><span class="math display">\[
f[x_j] = [y_0, ..., y_j]
\]</span>
Suppose in the expression that j = 5th order, then we can reference Figure <a href="numericallinearalgebra.html#fig:dividedifference">3.19</a>. The table in the Figure <a href="numericallinearalgebra.html#fig:dividedifference">3.19</a> is a <strong>divide-difference</strong> table only up to the 5th order. Note that we are using a triangular scheme in this discussion.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dividedifference"></span>
<img src="divide_difference.png" alt="Divide Difference Table" width="95%" />
<p class="caption">
Figure 3.19: Divide Difference Table
</p>
</div>
<p>The following formula for <strong>divide-difference</strong> applies to the 4th order: </p>

<p><span class="math display">\[\begin{align}
\text{1st order difference} = f[x0, x1] {} &amp;= \frac{f(x1) - f(x0)}{x1 - x0} \\
\text{2nd order difference} = f[x0, x1,x2] &amp;= \frac{f[x1, x2] - f[x0, x1]}{x2 - x0}\\
\text{3rd order difference} = f[x0, x1, x2, x3] &amp;=
\frac{f[x1, x2, x3] - f[x0, x1, x2]}{x3 - x0} \\
\text{4th order difference} =  f[x0, x1, x2, x3, x4] &amp;=
\frac{f[x1, x2, x3, x4] - f[x0, x1, x2, x3]}{x4 - x0} 
\end{align}\]</span>
</p>
<p>To give an example, suppose we use <strong>Runge</strong> function for the <strong>divide-difference</strong>. Let us initialize the x-y value pair in R code:</p>

<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1">runge &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">25</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span>) }</a>
<a class="sourceLine" id="cb96-2" data-line-number="2">n =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb96-3" data-line-number="3">(<span class="dt">xi =</span> <span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>, <span class="dt">length.out =</span> n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)) <span class="co"># uniform interval</span></a></code></pre></div>
<pre><code>## [1] -1.0 -0.5  0.0  0.5  1.0</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1">(<span class="dt">yi =</span> <span class="kw">round</span>(<span class="kw">runge</span>(xi),<span class="dv">2</span>)) <span class="co">#  yi = f[xi]</span></a></code></pre></div>
<pre><code>## [1] 0.04 0.14 1.00 0.14 0.04</code></pre>

<p>Using the generated x-y pair (xi, f[xi]), we can then construct the triangular <strong>divide-difference</strong> table like so:</p>

<p><span class="math display">\[
 \left[
 \begin{array}{lr} xi \\ .\\ x_0 =&amp; -1.00 \\ x_1 =&amp; -0.50 \\ x_2 =&amp; 0.00 \\ x_3 =&amp; 0.50 \\ x_4 =&amp; 1.00 \end{array}
 \left|\begin{array}{r} yi = f(xi) \\ .\\ 0.04 \\ 0.14\\ 1.00 \\ 0.14 \\ 0.04 \end{array}\right.
 \left|\begin{array}{r} 1st\ order \\\Delta^1y_1  \\
        0.20  \\    
        1.72 \\
        -1.72  \\ 
        -0.20\\ .\end{array}\right.
 \left|\begin{array}{r} 2nd\ order \\\Delta^2y_2  \\
        1.52 \\    
        -3.44 \\
      1.52 \\ .
        \\ .\end{array}\right.
   \left|\begin{array}{r} 3rd\ order \\ \Delta^3y_3  \\
       -3.31  \\    
         3.31 \\ .
        \\  .\\ .\end{array}\right.
     \left|\begin{array}{r} 4th\ order \\\Delta^4y_4  \\
       3.31  \\ .   
          \\ .\\  .\\. \end{array}\right.
 \right]
\]</span></p>
<p>For the First Order:</p>
<p><span class="math display">\[\begin{align*}
f[x0, x1] &amp;= \frac{0.14-0.04}{-0.50 +1.00} = 0.20\\
f[x1, x2] &amp;= \frac{1.00-0.14}{-0.00 +0.50} = 1.72\\
f[x2, x3] &amp;= \frac{0.14-1.00}{0.50 +0.00} = -1.72\\
f[x3, x4] &amp;= \frac{0.04-0.14}{1.00 -0.50} = -0.20
\end{align*}\]</span></p>
<p>For the Second Order:</p>
<p><span class="math display">\[\begin{align*}
f[x0, x1, x2] &amp;=  \frac{1.72-0.20}{-0.00 +1.00} = 1.52 \\    
f[x1, x2, x3] &amp;=  \frac{-1.72-1.72}{0.50 +0.50} = -3.44 \\
f[x2, x3, x4] &amp;=  \frac{-0.20+1.72}{1.00 +0.00} = 1.52 
\end{align*}\]</span></p>
<p>For the Third Order:</p>
<p><span class="math display">\[\begin{align*}
f[x0, x1, x2, x3] &amp;=  \frac{-3.44-1.52}{0.50 +1.00} = -3.31  \\    
f[x1, x2, x3, x4] &amp;=  \frac{1.52+3.44}{1.00 +0.50} = 3.31 
\end{align*}\]</span></p>
<p>For the Fourth Order:</p>
<p><span class="math display">\[\begin{align*}
f[x0, x1, x2, x3, x4] &amp;= \frac{3.31+3.31}{1.00 +1.00} = 3.31 
\end{align*}\]</span></p>

<p>And to show that in R code:</p>

<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1">runge &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">25</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span>) }</a>
<a class="sourceLine" id="cb100-2" data-line-number="2">divide_difference &lt;-<span class="st"> </span><span class="cf">function</span>(x, y) {</a>
<a class="sourceLine" id="cb100-3" data-line-number="3">    n =<span class="st"> </span><span class="kw">length</span>(y) </a>
<a class="sourceLine" id="cb100-4" data-line-number="4">    o =<span class="st"> </span><span class="kw">matrix</span> (<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">^</span><span class="dv">2</span>), n)</a>
<a class="sourceLine" id="cb100-5" data-line-number="5">    o[,<span class="dv">1</span>] =<span class="st"> </span>y</a>
<a class="sourceLine" id="cb100-6" data-line-number="6">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb100-7" data-line-number="7">        <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n<span class="op">-</span>i)) {</a>
<a class="sourceLine" id="cb100-8" data-line-number="8">                o[j,i<span class="op">+</span><span class="dv">1</span>] =<span class="st">  </span>(o[j<span class="op">+</span><span class="dv">1</span>, i] <span class="op">-</span><span class="st"> </span>o[j, i]) <span class="op">/</span><span class="st"> </span>(x[j<span class="op">+</span>i] <span class="op">-</span><span class="st"> </span>x[j])</a>
<a class="sourceLine" id="cb100-9" data-line-number="9">        }</a>
<a class="sourceLine" id="cb100-10" data-line-number="10">    }</a>
<a class="sourceLine" id="cb100-11" data-line-number="11">    <span class="kw">colnames</span>(o) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;f(xi)&quot;</span>, <span class="st">&quot;1st order&quot;</span>, <span class="st">&quot;2nd order&quot;</span>, </a>
<a class="sourceLine" id="cb100-12" data-line-number="12">                              <span class="st">&quot;3rd order&quot;</span>, <span class="st">&quot;4th order&quot;</span>)</a>
<a class="sourceLine" id="cb100-13" data-line-number="13">    o</a>
<a class="sourceLine" id="cb100-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb100-15" data-line-number="15">n =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb100-16" data-line-number="16">xi =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>, <span class="dt">length.out =</span> n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="co"># uniform interval</span></a>
<a class="sourceLine" id="cb100-17" data-line-number="17">yi =<span class="st"> </span><span class="kw">round</span>(<span class="kw">runge</span>(xi),<span class="dv">2</span>) <span class="co">#  yi = f[xi]</span></a>
<a class="sourceLine" id="cb100-18" data-line-number="18"></a>
<a class="sourceLine" id="cb100-19" data-line-number="19"><span class="kw">round</span>(<span class="kw">divide_difference</span>(xi, yi),<span class="dv">2</span>)</a></code></pre></div>
<pre><code>##      f(xi) 1st order 2nd order 3rd order 4th order
## [1,]  0.04      0.20      1.52     -3.31      3.31
## [2,]  0.14      1.72     -3.44      3.31      0.00
## [3,]  1.00     -1.72      1.52      0.00      0.00
## [4,]  0.14     -0.20      0.00      0.00      0.00
## [5,]  0.04      0.00      0.00      0.00      0.00</code></pre>

<p>Therefore, the divide difference method yields:<br />
<span class="math display">\[
f[x_j] = f[x_0, ... x_4] = 3.31\ \ \ \sim \text{3.32, if 3-decimal precision}
\]</span></p>
<p>We can solve for N(x) from Equation (<span class="math inline">\(\ref{eqn:eqnnumber5}\)</span>) using R code. The expanded version is written as such:</p>
<p><span class="math display">\[\begin{align*}
N(x)  {}&amp;= f[x_0] + \Delta^1y_1(x-x_0) + \Delta^2y_2(x-x_0)(x-x_1) + ... \\
     &amp;+ \Delta^ky_k(x-x_0) ...(x - x_k) 
\end{align*}\]</span></p>
<p>which is also the same as:</p>
<p><span class="math display">\[\begin{align*}
N(x) {}&amp;= f[x_0] + f[x_0, x_1](x-x_0) + f[x_0, x_1, x_2](x-x_0)(x-x_1) + ... \\
    &amp;+ f[x_0, ... , x_k](x-x_0) ...(x - x_k)
\end{align*}\]</span></p>
<p>The R code and result of <strong>Newton</strong> interpolation is demonstrated in Figure <a href="numericallinearalgebra.html#fig:newtonpoly">3.20</a> using Chebyshev interval. Try to run the R code but using uniform interval. Compare the result against Figure <a href="numericalmethods.html#fig:runge">1.1</a>. Also, for other functions, try to use <span class="math inline">\(exp(x)\)</span> instead of <span class="math inline">\(runge(x)\)</span> for experiment.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:newtonpoly"></span>
<img src="embed0012.png" alt="Newton Interpolation (Chebyshev Interval)" width="70%" />
<p class="caption">
Figure 3.20: Newton Interpolation (Chebyshev Interval)
</p>
</div>

</div>
<div id="newton-forward-interpolation" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.4</span> Newton Forward interpolation <a href="numericallinearalgebra.html#newton-forward-interpolation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Newton Forward interpolation</strong> is another version of <strong>Newton interpolation</strong>. Below is the formula for the interpolation.</p>
<p><span class="math display">\[\begin{align}
N_{fw}(x) = f(x) + \Delta y_n u + \Delta^2y_n\frac{u(u-1)}{2!} +
  \Delta^3y_n\frac{u(u-1)(u-2)}{3!} + ...
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
u = \frac{(x - x_n)}{h}, \text{derived from } x = xn + uh
\]</span></p>
<p>The method also requires the <strong>divide difference</strong> table to solve for the polynomial. Similar to the <strong>Newton interpolation</strong>, once a dataset (e.g., Runge function dataset) is provided, it is run against the <strong>divide difference</strong> algorithm to get the <strong>differences</strong>, <span class="math inline">\(\Delta^n y_n\)</span>, which will be used to construct the polynomial needed to solve for <span class="math inline">\(N_{fw}(x)\)</span>.</p>
</div>
<div id="newton-backward-interpolation" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.5</span> Newton Backward interpolation <a href="numericallinearalgebra.html#newton-backward-interpolation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Newton Backward interpolation</strong> is identical to the <strong>Newton Forward interpolation</strong> except the basis uses <strong>addition</strong> instead of <strong>subtraction</strong>.</p>
<p><span class="math display">\[\begin{align}
N_{bw}(x) = f(x) + \nabla y_n u + \nabla^2y_n\frac{u(u+1)}{2!} +
  \nabla^3y_n\frac{u(u+1)(u+2)}{3!} + ...
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
u = \frac{(x - x_n)}{h}, \text{derived from } x = xn + uh
\]</span></p>
<p>Here, similar to <strong>Newton Forward interpolation</strong>, we also run against the <strong>divide difference</strong> algorithm to get the <strong>differences</strong>, <span class="math inline">\(\nabla^n y_n\)</span>, which will be used to construct the polynomial needed to solve for <span class="math inline">\(N_{bw}(x)\)</span>.</p>
</div>
<div id="interpolation-considerations" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.6</span> Interpolation Considerations<a href="numericallinearalgebra.html#interpolation-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Seeing that some interpolations tend to show instability, it is not a wonder to have many other numerical methods of interpolations today. Though this book does not cover them all in detail, there are other interpolations that are discussed in other literature; and it helps to be familiar with other interpolation methods and understand both the advantages and disadvantages. Just a few are listed below:</p>
<ul>
<li>Gauss Interpolation</li>
<li>Stirling Interpolation</li>
<li>Bessel Interpolation</li>
<li>Hermite Interpolation</li>
<li>Aitken Interpolation</li>
</ul>
<p>Also, instead of using other interpolation methods, one technique that can mitigate the oscillation is by avoiding <strong>Uniform intervals</strong>; instead, use <strong>Chebyshev intervals</strong> as reflected in the R code samples provided for <strong>Lagrange interpolation and Newton interpolation</strong>.</p>
<p>While this helps reduce oscillation, there are also other considerations to take when interpolating. For example, will the derived polynomial being used for interpolation have to be re-constructed for every new data point?</p>
</div>
<div id="lebesque-constant" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.7</span> Lebesque Constant <a href="numericallinearalgebra.html#lebesque-constant" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have decided to use a <strong>Chebyshev cubic polynomial</strong> of the <strong>first kind</strong>.</p>
<p><span class="math display">\[
T_3(x) = 4x^3 - 3x
\]</span></p>
<p>Suppose also that we can construct an interpolant while trying to approximate the <strong>Chebyshev polynomial</strong>.</p>
<p><span class="math display">\[
P(x) = 5x^3 - 4x
\]</span></p>
<p>Here is an R code to show the outcome. See Figure <a href="numericallinearalgebra.html#fig:lebesgue">3.21</a>.</p>

<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" data-line-number="1">chebyshev &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">4</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">3</span><span class="op">*</span>x  }</a>
<a class="sourceLine" id="cb102-2" data-line-number="2">interpolant &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">5</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">4</span><span class="op">*</span>x  }</a>
<a class="sourceLine" id="cb102-3" data-line-number="3"></a>
<a class="sourceLine" id="cb102-4" data-line-number="4">x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb102-5" data-line-number="5"></a>
<a class="sourceLine" id="cb102-6" data-line-number="6"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>)</a>
<a class="sourceLine" id="cb102-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb102-8" data-line-number="8"><span class="kw">curve</span>(<span class="kw">chebyshev</span>(x), <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>,  <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb102-9" data-line-number="9"><span class="kw">curve</span>(<span class="kw">interpolant</span>(x), <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>,  <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb102-10" data-line-number="10"><span class="kw">legend</span>(<span class="dv">0</span>, <span class="dv">2</span>, </a>
<a class="sourceLine" id="cb102-11" data-line-number="11">   <span class="kw">c</span>( <span class="st">&quot;chebyshev&quot;</span>, <span class="st">&quot;interpolant&quot;</span>),</a>
<a class="sourceLine" id="cb102-12" data-line-number="12"><span class="dt">fill=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>), </a>
<a class="sourceLine" id="cb102-13" data-line-number="13">     <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lebesgue"></span>
<img src="embed0013.png" alt="Lebesgue Constant" width="70%" />
<p class="caption">
Figure 3.21: Lebesgue Constant
</p>
</div>

<p>Notice that the difference between the two polynomial is in the coefficients. For example:</p>
<p><span class="math display">\[\begin{align*}
\text{ chebyshev polynomial } = (4, 3) \\
\text{ interpolant } = (5,4)
\end{align*}\]</span></p>
<p>Early in this chapter, we described the sensitivity of values and the condition number of a system in terms of the ratio of change between input and output. Considering those, let us then come back to put some intution into the relative <strong>goodness</strong> of an interpolant, computationally (or mathematically). If this is about <strong>linear regression</strong>, we can say that the <strong>Residual</strong> may be a good starting point to determine the <strong>goodness</strong> of a <strong>fit</strong>; similarly, there is a constant we can use to effectively measure the <strong>sensitivity or conditioning</strong> of an interpolant. This constant is the <strong>Lebesgue constants</strong>. We know that the <strong>conditioning</strong> of a system is measured using the following formula:</p>
<p><span class="math display">\[\begin{align}
\text{Condition Number}  = \frac{\text{Relative Change in Output}}{\text{Relative Change in Input}}
\end{align}\]</span></p>
<p>Using the formula, let us compute for the relative change of polynomial.</p>
<p><span class="math display">\[\begin{align}
\text{Relative Change of Polynomial} = \frac{P(x) - T_3(x)}{T_3(x)}
\end{align}\]</span></p>
<p>We can also compute for the relative change of the coefficients.</p>
<p>Let <strong>v</strong> be a vector (4,3) and <span class="math inline">\(\hat{v}\)</span> be a vactor of change (5,4). We get.</p>
<p><span class="math display">\[\begin{align}
\text{Relative Change of Coefficients} = \frac{\hat{v} - v}{v}
\end{align}\]</span></p>
<p>Here is the R code:</p>

<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" data-line-number="1">chebyshev &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">4</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">3</span><span class="op">*</span>x  }</a>
<a class="sourceLine" id="cb103-2" data-line-number="2">interpolant &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">5</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">4</span><span class="op">*</span>x  }</a>
<a class="sourceLine" id="cb103-3" data-line-number="3"></a>
<a class="sourceLine" id="cb103-4" data-line-number="4">relative_change &lt;-<span class="st"> </span><span class="cf">function</span>(a, b) { <span class="kw">mean</span>((a<span class="op">-</span>b)<span class="op">/</span>b)}</a>
<a class="sourceLine" id="cb103-5" data-line-number="5"></a>
<a class="sourceLine" id="cb103-6" data-line-number="6">x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb103-7" data-line-number="7"></a>
<a class="sourceLine" id="cb103-8" data-line-number="8"><span class="co"># Start from a polynomial with no change and </span></a>
<a class="sourceLine" id="cb103-9" data-line-number="9"><span class="co"># compute for the relative error.</span></a>
<a class="sourceLine" id="cb103-10" data-line-number="10">T_x =<span class="st">  </span>P_x =<span class="st"> </span><span class="kw">chebyshev</span>(x)</a>
<a class="sourceLine" id="cb103-11" data-line-number="11">(<span class="dt">relative_error =</span> <span class="kw">relative_change</span>(P_x, T_x))</a></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" data-line-number="1"><span class="co"># Now introduce  the interpolant function and </span></a>
<a class="sourceLine" id="cb105-2" data-line-number="2"><span class="co"># compute for the relative error.</span></a>
<a class="sourceLine" id="cb105-3" data-line-number="3">P_x =<span class="st">  </span><span class="kw">interpolant</span>(x)</a>
<a class="sourceLine" id="cb105-4" data-line-number="4">(<span class="dt">relative_error =</span> <span class="kw">relative_change</span>(P_x, T_x))</a></code></pre></div>
<pre><code>## [1] 0.2361586</code></pre>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb107-1" data-line-number="1"><span class="co"># Next compute for the relative error of a change in coefficients.</span></a>
<a class="sourceLine" id="cb107-2" data-line-number="2">v =<span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb107-3" data-line-number="3">v_hat =<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb107-4" data-line-number="4">(<span class="dt">relative_error =</span> <span class="kw">relative_change</span>(v_hat, v))</a></code></pre></div>
<pre><code>## [1] 0.2916667</code></pre>

<p>To now compute for the <strong>condition number</strong>, we use the following formula:</p>
<p><span class="math display">\[\begin{align}
\text{Condition Number} (\Lambda) = \frac{\text{Relative Change of Polynomial}}{\text{Relative Change of Coefficients}}
\end{align}\]</span></p>

<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" data-line-number="1"><span class="co"># Compute for the condition number</span></a>
<a class="sourceLine" id="cb109-2" data-line-number="2">(<span class="dt">condition_number =</span> <span class="kw">relative_change</span>(P_x, T_x) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb109-3" data-line-number="3"><span class="st">                    </span><span class="kw">relative_change</span>(v_hat, v) )</a></code></pre></div>
<pre><code>## [1] 0.8096866</code></pre>

<p>Here, <span class="math inline">\(\Lambda\)</span> is the <strong>Lebesgue constant</strong> used as a <strong>conditioning number</strong> and in our computation, the <strong>Lebesgue constant</strong> equals <strong>0.8096866</strong>.</p>
</div>
<div id="horners-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.8</span> Hornerâs method <a href="numericallinearalgebra.html#horners-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One way to evaluate an interpolant (its polynomial expression) is by using the <strong>Hornerâs method</strong>.</p>
<p>Let us use <strong>Chebyshev polynomial of degree 5 in the first kind</strong> to explain the method:</p>
<p><span class="math display">\[
P_5(x) = 16x^5 - 20x^3 + 5x
\]</span>
This <strong>Chebyshev polynomial</strong> expands into the following <strong>Hornerâs equation</strong>:</p>
<p><span class="math display">\[
H(x) = 0 + x(5 + x(0 + x(-20 + x(0 + x(16)))))
\]</span></p>
<p>Without loss of generality, the <strong>Hornerâs method</strong> is expressed as:</p>
<p><span class="math display">\[\begin{align}
p(x) = a_o + x(a_1 + x(a_2 + x (a_3 + ... + x(a_{n-1} + a_nx))))
\end{align}\]</span></p>
<p>for the following equation:</p>
<p><span class="math display">\[
p(x) = a_o + a_1x + a_2x^2 + a_3x^3 + ... + a_nx^n
\]</span></p>
<p>To show the method implemented in R code, let us evaluate <span class="math inline">\(P_5(X)\)</span> and <span class="math inline">\(H(x)\)</span>:</p>

<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" data-line-number="1">polynomial &lt;-<span class="st"> </span><span class="cf">function</span>(A,x) {</a>
<a class="sourceLine" id="cb111-2" data-line-number="2">  p =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb111-3" data-line-number="3">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(A)) {</a>
<a class="sourceLine" id="cb111-4" data-line-number="4">    p =<span class="st"> </span>p <span class="op">+</span><span class="st"> </span>A[i] <span class="op">*</span><span class="st"> </span>x<span class="op">^</span>(i<span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb111-5" data-line-number="5">  }</a>
<a class="sourceLine" id="cb111-6" data-line-number="6">  p</a>
<a class="sourceLine" id="cb111-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb111-8" data-line-number="8">horner_rule &lt;-<span class="st"> </span><span class="cf">function</span>(A,x) {</a>
<a class="sourceLine" id="cb111-9" data-line-number="9">  p =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb111-10" data-line-number="10">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">length</span>(A)<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb111-11" data-line-number="11">    p =<span class="st"> </span>A[i] <span class="op">+</span><span class="st">  </span>x <span class="op">*</span><span class="st"> </span>p </a>
<a class="sourceLine" id="cb111-12" data-line-number="12">  }</a>
<a class="sourceLine" id="cb111-13" data-line-number="13">  p </a>
<a class="sourceLine" id="cb111-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb111-15" data-line-number="15">A =<span class="st"> </span><span class="kw">c</span>(<span class="dv">16</span>, <span class="dv">0</span>, <span class="dv">-20</span>, <span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb111-16" data-line-number="16">x =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb111-17" data-line-number="17"><span class="co"># evaluate if polynomial result is identical to horner rule result</span></a>
<a class="sourceLine" id="cb111-18" data-line-number="18"><span class="kw">identical</span>( <span class="kw">polynomial</span>(A,x) , <span class="kw">horner_rule</span>(A,x) )</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>

<p>There are other methods to be familiar with when evaluating other forms of polynomials:</p>
<ul>
<li><strong>Cleanshaw Method</strong> - to evaluate Chebyshev polynomials.</li>
<li><strong>De Boorâs Method</strong> - to evaluate Spline polynomials.</li>
</ul>
<p>In the next section, we will begin to cover <strong>Splines</strong>.</p>
</div>
<div id="piecewise-polynomial-interpolation" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.9</span> Piecewise Polynomial Interpolation <a href="numericallinearalgebra.html#piecewise-polynomial-interpolation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, we have covered <strong>Lagrange interpolation</strong> and <strong>Newton interpolation</strong>. In our examples, we have used <strong>Rungeâs function</strong> to <strong>sample</strong> our dataset for interpolation. We can observe that the two interpolations, including the <strong>Monomial interpolation</strong>, do manifest oscillation behavior at the end of the curves when using <strong>uniform intervals</strong>, manifesting the <strong>Rungeâs phenomenon</strong> in Figure <a href="numericalmethods.html#fig:runge">1.1</a>.</p>
<p>Also, for interpolation, we have only introduced polynomials in 2nd, 3rd, 4th, or even 5th-degree terms - so-called the <strong>power-series polynomials</strong>. One point to emphasize in Figure <a href="numericallinearalgebra.html#fig:legendre">3.13</a> is that the rendered polynomials are rather <strong>regular</strong> curves. In some, if not most, situations, however, datasets tend to characterize <strong>complex</strong>, <strong>irregular</strong> patterns or curves. See Figure <a href="numericallinearalgebra.html#fig:irregularcurves">3.22</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:irregularcurves"></span>
<img src="irregular_curves.png" alt="Irregular Curves" width="90%" />
<p class="caption">
Figure 3.22: Irregular Curves
</p>
</div>
<p>In a previous discussion in <strong>linear equations</strong>, we deal with polynomials of 1st degree, e.g.</p>
<p><span class="math display">\[
y = mx + b
\]</span></p>
<p>Polynomials of higher degree however are non-linear and curvy and have the following equation:</p>
<p><span class="math display">\[\begin{align}
y = c_1x^0 + c_2x^1 + c_3x^2  + ... + c_{n}x^{n-1}
\end{align}\]</span></p>
<p>But even such polynomial equations may have limitations in terms of flexibility in handling <strong>irregular curves</strong> such as those in Figure <a href="numericallinearalgebra.html#fig:irregularcurves">3.22</a>. One way to handle such <strong>irregularity</strong> is not to take the curves in one fitting. Rather, cut the curves into pieces and deal with each piece individually - we call this method <strong>piecewise polynomial interpolation</strong>. The entire curve is called the <strong>spline</strong>. The <strong>slices</strong> of the <strong>spline</strong> are called <strong>spline segments or spline curves</strong>. The points where <strong>spline segments</strong> are connected to form two continuous curves and eventually make the whole <strong>spline</strong> continuous are called <strong>interior knots</strong> or <strong>just knots</strong>.  </p>
<p><strong>Natural Cubic Spline</strong> </p>
<p>A slice described as a 3rd degree polynomial - e.g. <span class="math inline">\(ax^3 + bx^2 + cx + d\)</span> - is called a <strong>cubic spline segment</strong>. A function that holds an expression for each of the <strong>cubic spline segment</strong> is a <strong>spline segment function</strong>. Because each <strong>segment</strong> describes a 3rd-degree polynomial, we also can say that the segments are <strong>cubic polynomials</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:splinesegments"></span>
<img src="splinesegments.png" alt="A spline" width="60%" />
<p class="caption">
Figure 3.23: A spline
</p>
</div>
<p>In Figure <a href="numericallinearalgebra.html#fig:splinesegments">3.23</a>, the <strong>spline</strong> runs through four knots, {<span class="math inline">\(t_0=1, t_1=2, t_2=3, t_n=4\)</span>} where n = 3, and sliced into three <strong>spline segments</strong>, S = {<span class="math inline">\(s_0, s_1, s_k\)</span>}, where k is the degree or order of the segment (in this case, k = n - 1), corresponding to three intervals or three <strong>spline functions</strong>, <span class="math inline">\(\phi_i(t)\)</span> = {<span class="math inline">\(\phi_0(t), ..., \phi_2(t)\)</span>}. Each <strong>segment</strong> has two points at a given abscissa <strong>t</strong> that can be expressed in terms of <strong>time</strong> - equivalently, positioned at the <strong>x</strong> location in the x-axis.</p>
<p><strong>The spline segments ( n = k + 1 ) :</strong></p>
<p><span class="math display">\[\begin{align}
s_0 {}&amp;= \phi_0(t) \leftarrow \text{1st interval}  \\
s_1 &amp;= \phi_1(t) \leftarrow \text{2nd interval} \\
s_2 &amp;= \phi_2(t) \leftarrow \text{3rd interval} 
\end{align}\]</span></p>
<p><span class="math display">\[
\phi_i(t)\ \ \ \ \ \ \text{where }i = 0\ ...\ n\ \ \ and\ n = \text{number of splines or intervals}
\]</span></p>
<p><strong>The knots ( n + 1):</strong></p>
<p><span class="math display">\[\begin{align}
\phi_0(t_0) {}&amp; \leftarrow \text{1st endpoint knot} \\
\phi_0(t_1) &amp; \leftarrow \text{1st interior knot} \rightarrow \text{connecting to } \phi_1(t_1) \\
\phi_1(t_2) &amp; \leftarrow \text{2nd interior knot} \rightarrow \text{connecting to } \phi_2(t_2)  \\
\phi_3(t_3) &amp; \leftarrow \text{2nd endpoint knot}
\end{align}\]</span></p>
<p><strong>Constraining a Natural Cubic Spline:</strong></p>
<p><strong>First, for equations describing the cubic polynomials at the knots:</strong></p>
<p>We know that the 1st spline segment <strong>s0</strong> starts at <strong>t0</strong> and ends at <strong>t1</strong>, the 2nd spline segment <strong>s1</strong> starts at <strong>t1</strong> and ends at <strong>t2</strong>, and so on. Therefore, we have six equations for all four points of the three cubic spline segments.</p>

<p><span class="math display">\[\begin{align}
\phi_0(t_0) {}&amp;= y0 = a_0t_0^3 + a_1t_0^2 + a_2t_0 + a_3  &amp; \phi_0(t_1) {}&amp;= y1 = a_0t_1^3 + a_1t_1^2 + a_2t_1 + a_3 \\ 
\phi_1(t_1) &amp;= y1 = b_0t_1^3 + b_1t_1^2 + b_2t_1 + b_3   &amp; \phi_1(t_2) &amp;= y2 =  b_0t_2^3 + b_1t_2^2 + b_2t_2 + b_3  \\ 
\phi_2(t_2) &amp;= y2 = c_0t_2^3 + c_1t_2^2 + c_2t_2 + c_3   &amp; \phi_2(t_3) &amp;= y3 =  c_0t_3^3 + c_1t_3^2 + c_2t_3 + c_3 
\end{align}\]</span>
</p>
<p>Looking closely at each of the equations, we have four unknowns, e.g. {<span class="math inline">\(a_0, a_1, a_2, a_3\)</span>}, for each of our <strong>spline functions</strong>. We have <strong>twelve unknowns</strong> (four unknowns times three segments).</p>
<p>The equations above represent rules (or constraints) that require us to ensure our <strong>segments</strong> go through the points. So that gives us <strong>2n</strong> constraints equivalent to six equations.</p>
<p><strong>Second, for equations describing the slopes of the three connecting knots:</strong></p>
<p>The fact that we see in Figure <a href="numericallinearalgebra.html#fig:splinesegments">3.23</a> a continuous curve means that the three spline segments have to be connected smoothly. To re-iterate, each knot between two segments has to be smoothly connected - or <strong>twice continuously differentiable</strong>. To show this smoothness in an equation, we need to show that the slope of the curve at the ending knot - that is located at abscissa <strong>t1</strong> - of the first spline <strong>s0</strong> is the same slope at the starting knot - that is still at abscissa <strong>t1</strong> - of the second spline <strong>s1</strong>. And because a slope can be computed as the <strong>1st derivative of the spline functions</strong> of <strong>s0</strong> and <strong>s1</strong> at a given point (<strong>t</strong>), we then can express the equality of those slopes this way:</p>
<p><span class="math display">\[\begin{align}
\phi_0^{&#39;}(t_1) = \phi_1^{&#39;}(t_1)
\end{align}\]</span></p>
<p>The equation is read as the slopes of s1 and s2 at the connecting ends (knots) are equal.</p>
<p>Let us then compute for the equations of the <strong>1st derivatives</strong> of the spline functions - <strong>Hermite cubic</strong> interpolation. Additionally, to get a solid smooth joint, let us also perform the <strong>2nd derivative</strong> - <strong>Cubic Spline</strong> interpolation. That gives us four additional equations by computing for the <strong>1st and 2nd derivatives</strong> of the <strong>spline functions</strong>.</p>
<p><span class="math display">\[\begin{align}
\phi_0^{&#39;}(t_1) = \phi_1^{&#39;}(t_1) {}&amp; &amp; &amp; \phi_0^{&#39;&#39;}(t_1) = \phi_1^{&#39;&#39;}(t_1)  \\
\phi_1^{&#39;}(t_2) = \phi_2^{&#39;}(t_2) {}&amp; &amp; &amp; \phi_1^{&#39;&#39;}(t_2) = \phi_2^{&#39;&#39;}(t_2)  
\end{align}\]</span></p>
<p>In other words, the equations above represent rules (or constraints) that require us to ensure each <strong>spline segment</strong> goes through the knots smoothly connecting through to the next <strong>spline segment</strong>. Given that we have <strong>n-1</strong> constraints for the 1st derivative and <strong>n-1</strong> constraints for the 2nd derivative, we, therefore, have <strong>2(n-1) constraints</strong> corresponding to four equations as above in our case.</p>
<p><strong>Third, for equations describing the slopes of the two endpoints:</strong></p>
<p>Finally, for <strong>natural cubic spline</strong>, we need two more equations computing for the <strong>2nd derivative</strong> at the beginning of the first spline <strong>s1</strong> and at the end of the last spline <strong>s4</strong>. Our constraint is to have the <strong>2nd derivatives</strong> result to zero.</p>
<p><span class="math display">\[\begin{align}
\phi_0^{&#39;&#39;}(t_0) = 0 {}&amp; &amp; &amp; \phi_2^{&#39;&#39;}(t_3) = 0
\end{align}\]</span></p>
<p>Note here that we only deal with <strong>Natural Cubic</strong> endpoints. We leave readers to investigate <strong>Clamped</strong> endpoints and <strong>Not-a-knot</strong> endpoints.</p>
<p>Overall, we have a system of twelve equations to solve: <strong>4n</strong> unknowns and <strong>4n</strong> equations.</p>
<p>We now can solve the unknowns using the <strong>matrix formula</strong>: <span class="math inline">\(Ax = b\)</span>.</p>

<p><span class="math display">\[
\left[
\begin{array}{rrrrrrrrrrrr}
t_0^3 &amp; t_0^2 &amp; t_0 &amp; 1 &amp; .  &amp; . &amp; . &amp; .  &amp; . &amp; . &amp; . &amp; . \\
t_1^3 &amp; t_1^2 &amp; t_1 &amp; 1 &amp; .  &amp; . &amp; . &amp; .  &amp; .  &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; t_1^3 &amp; t_1^2 &amp; t_1 &amp; 1 &amp; .  &amp; .  &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; t_2^3 &amp; t_2^2 &amp; t_2 &amp; 1 &amp; .  &amp; .  &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; .  &amp; .  &amp; t_2^3 &amp; t_2^2 &amp; t_2 &amp; 1 \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; .  &amp; .  &amp; t_3^3 &amp; t_3^2 &amp; t_3 &amp; 1 \\
3t_1^2 &amp; 2t_1 &amp; 1 &amp; . &amp; -3t_1^2 &amp; -2t_1 &amp; -1  &amp; . &amp; . &amp; . &amp; . &amp; . \\
6t_1 &amp; 2 &amp; . &amp; . &amp; -6t_1 &amp; -2 &amp; . &amp; .  &amp; .  &amp; .  &amp; .  &amp; . \\
. &amp; . &amp; . &amp; . &amp; 3t_2^2 &amp; 2t_2 &amp; 1 &amp; . &amp; -3t_2^2 &amp; -2t_2 &amp; -1  &amp; . \\
. &amp; . &amp; . &amp; . &amp; 6t_2 &amp; 2 &amp; . &amp; . &amp; -6t_2 &amp; -2 &amp; . &amp; . \\
6t_0 &amp; 2 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; 6t_3 &amp; 2 &amp; . &amp; .
\end{array}
\right]
\left[\begin{array}{c}
a_0 \\ a_1 \\ a_2 \\ a_3 \\ b_0 \\ b_1 \\ b_2 \\ b_3 \\ c_0 \\ c_1 \\ c_2 \\ c_3 
\end{array}\right]=
\left[\begin{array}{c}
y_0 \\ y_1 \\ y_1 \\ y_2 \\ y_2 \\ y_3 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 
\end{array}\right]
\]</span>
</p>
<p>For example, given the dataset, (1,2), (2,4),(3,1),(4,3.5), in Figure <a href="numericallinearalgebra.html#fig:splinesegments">3.23</a>, we then get the initial set of equations:</p>

<p><span class="math display">\[\begin{align}
\phi_0(t_0) {}&amp;= \phi_0(1) = y0 = 2 &amp; \phi_0(t_1) {}&amp;= \phi_0(2) = y1 = 4 \\ 
\phi_1(t_1) &amp;= \phi_1(2) = y1 = 4 &amp; \phi_1(t_2) {}&amp;= \phi_1(3) = y2 = 1 \\ 
\phi_2(t_2) &amp;= \phi_2(3) = y2 = 1 &amp; \phi_2(t_3) {}&amp;= \phi_2(4) = y3 = 3.5 
\end{align}\]</span>
</p>
<p>Solving for the matrix above, e.g. <span class="math inline">\(x = solve(A,y)\)</span> in R where x = {<span class="math inline">\(a_0, a_1, a_2\)</span>, <span class="math inline">\(a_3, b_0, b_1, b_2, b_3, c_0, c_1, c_2, c_3\)</span>}, we get the following:</p>
<p><span class="math display">\[\begin{align*}
a_0 {}&amp;= -1.7 &amp; b_0 {}&amp;= 3.5 &amp; c_0 {}&amp;= -1.8 \\
a_1 &amp;= +5.1 &amp; b_1 &amp;= -26.10 &amp; c_1 &amp;= 21.6 \\
a_2 &amp;= -1.4 &amp; b_2 &amp;= 61.0 &amp; c_2 &amp;= -82.1 \\
a_3 &amp;= +0.0 &amp; b_3 &amp;= -41.6 &amp; c_3 &amp;= 101.5 
\end{align*}\]</span></p>
<p>along with the corresponding <strong>cubic polynomials</strong>:</p>
<p><span class="math display">\[\begin{align*}
\phi_0(t) {}&amp;= -1.7t^3 + 5.1t^2 - 1.4t + 0 \\
\phi_1(t) &amp;= 3.5t^3 - 26.1t^2 + 61t - 41.6 \\
\phi_2(t) &amp;= -1.8t^3 + 21.6t^2 -82.1t + 101.5 
\end{align*}\]</span></p>
<p>Now let us use R code to show how we stitch the three cubic polynomials - the pieces. See Figure <a href="numericallinearalgebra.html#fig:rcubicspline">3.24</a>.</p>

<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1">phi0 &lt;-<span class="st"> </span><span class="cf">function</span>(t) {  <span class="fl">-1.7</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="fl">5.1</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="fl">-1.4</span><span class="op">*</span>t <span class="op">+</span><span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb113-2" data-line-number="2">phi1 &lt;-<span class="st"> </span><span class="cf">function</span>(t) {  <span class="fl">3.5</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="fl">26.1</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">61</span><span class="op">*</span>t <span class="op">-</span><span class="st"> </span><span class="fl">41.6</span> }</a>
<a class="sourceLine" id="cb113-3" data-line-number="3">phi2 &lt;-<span class="st"> </span><span class="cf">function</span>(t) {  <span class="fl">-1.8</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="fl">21.6</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="fl">-82.1</span><span class="op">*</span>t <span class="op">+</span><span class="st"> </span><span class="fl">101.5</span> }</a>
<a class="sourceLine" id="cb113-4" data-line-number="4">x1 =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">2</span>, <span class="dt">length.out=</span><span class="dv">50</span>)</a>
<a class="sourceLine" id="cb113-5" data-line-number="5">x2 =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">2</span>,<span class="dv">3</span>, <span class="dt">length.out=</span><span class="dv">50</span>)</a>
<a class="sourceLine" id="cb113-6" data-line-number="6">x3 =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">3</span>,<span class="dv">4</span>, <span class="dt">length.out=</span><span class="dv">50</span>)</a>
<a class="sourceLine" id="cb113-7" data-line-number="7">y1 =<span class="st"> </span><span class="kw">phi0</span>(x1)   <span class="co"># 1st piece</span></a>
<a class="sourceLine" id="cb113-8" data-line-number="8">y2 =<span class="st"> </span><span class="kw">phi1</span>(x2)   <span class="co"># 2nd piece</span></a>
<a class="sourceLine" id="cb113-9" data-line-number="9">y3 =<span class="st"> </span><span class="kw">phi2</span>(x3)   <span class="co"># 3rd piece</span></a>
<a class="sourceLine" id="cb113-10" data-line-number="10"><span class="co"># plot</span></a>
<a class="sourceLine" id="cb113-11" data-line-number="11"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">5</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">5</span>), </a>
<a class="sourceLine" id="cb113-12" data-line-number="12">     <span class="dt">xlab=</span><span class="st">&quot;t (knot vector)&quot;</span>, <span class="dt">ylab=</span><span class="kw">expression</span>(<span class="kw">paste</span>(coeffs[i])))</a>
<a class="sourceLine" id="cb113-13" data-line-number="13"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb113-14" data-line-number="14"><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb113-15" data-line-number="15"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb113-16" data-line-number="16"><span class="kw">lines</span>(x1, y1, <span class="dt">col=</span><span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)  </a>
<a class="sourceLine" id="cb113-17" data-line-number="17"><span class="kw">lines</span>(x2, y2, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)       </a>
<a class="sourceLine" id="cb113-18" data-line-number="18"><span class="kw">lines</span>(x3, y3, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)  </a>
<a class="sourceLine" id="cb113-19" data-line-number="19"><span class="kw">points</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>), <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="fl">3.5</span>), <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;darkblue&quot;</span>)</a>
<a class="sourceLine" id="cb113-20" data-line-number="20"><span class="kw">text</span>( <span class="fl">1.2</span>, <span class="fl">3.5</span>, <span class="kw">expression</span>(phi[<span class="dv">0</span>](t)) )</a>
<a class="sourceLine" id="cb113-21" data-line-number="21"><span class="kw">text</span>( <span class="fl">2.2</span>, <span class="fl">2.5</span>, <span class="kw">expression</span>(phi[<span class="dv">1</span>](t)) )</a>
<a class="sourceLine" id="cb113-22" data-line-number="22"><span class="kw">text</span>( <span class="fl">3.5</span>, <span class="fl">2.5</span>, <span class="kw">expression</span>(phi[<span class="dv">2</span>](t)) )</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rcubicspline"></span>
<img src="embed0014.png" alt="Natural Cubic Spline in R" width="80%" />
<p class="caption">
Figure 3.24: Natural Cubic Spline in R
</p>
</div>

<p>Next, let us discuss B-spline interpolation as a method to deal with a higher degree of polynomials.</p>
</div>
<div id="b-spline-interpolation" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.10</span> B-Spline interpolation <a href="numericallinearalgebra.html#b-spline-interpolation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>B-Spline Curves:</strong></p>
<p><strong>B-Spline</strong> is a linear combination of coefficients and basis functions that describe a curve. Like a <strong>linear combination</strong> such as below:</p>
<p><span class="math display">\[\begin{align}
f(x) =  c_0x_0 + c_1x_1 + c_2x_2  + ... + c_nx_n
\end{align}\]</span></p>
<p>we take that equation and convert the <strong>x</strong> variables into <strong>basis functions</strong>:</p>
<p><span class="math display">\[\begin{align}
f(t) = c_0\phi_0(t) + c_1\phi_1(t)  + c_2\phi_2(t)   + ... + c_n\phi_n(t) 
\end{align}\]</span></p>
<p>We then have a general formula for B-spline:</p>
<p><span class="math display">\[\begin{align}
B(t) = \sum_{i=0}^n c_i \phi_{i,k}(t)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align*}
\text{where }i {}&amp;= \{0\ ...\ n\} \\
\\
n + 1 &amp;= \text{number of spline segments, also number of control points} \\
\\
k &amp;= \text{derivative order or degree of basis functions}, \\
&amp;\ \ \ \ \text{e.g. 1 - linear, 2 - quadratic, 3 - cubic; with index=0} \\
\end{align*}\]</span></p>
<p>Here, we have a set of corresponding coefficients (or <strong>control points</strong>):</p>
<p>where n + 1 = number of control points (coefficients)</p>
<p><span class="math display">\[
c_i = \{c_0, c_1, ..., c_n\}.
\]</span></p>
<p>We also have a set of <strong>knots</strong> called <strong>knot vector</strong>, T = {<span class="math inline">\(t_0, t_1, t_3, ..., t_{n + k + 1}\)</span>}, with the <strong>Cox-de Boor recursion</strong> formula, <span class="math inline">\(\phi_{i,k}(t)\)</span>, as our basis function <span class="citation">(Carl De Boor <a href="bibliography.html#ref-ref362c">2002</a>)</span>: </p>
<p><strong>If indices start with 1</strong>:</p>
<p><span class="math display">\[\begin{align}
\phi_{i,j}(t) {}&amp;= 
\left(\frac{t - t_i}{t_{i+j-1} - t_i}\right) \phi_{i,j-1}(t) \ +\  
\left(\frac{t_{i+j} - t}{t_{i+j} - t_{i+1}}\right) \phi_{i+1,j-1}(t) \label{eqn:eqnnumber300}\\
\nonumber \\
\phi_{i,1}(t) &amp;= \begin{cases} 1 &amp;  if\ t_i \leq t &lt; t_{i+1}  \\ 0 &amp; otherwise \end{cases}
\leftarrow\ \ \ \ \text{indicator function} \label{eqn:eqnnumber702}
\end{align}\]</span></p>
<p><strong>If indices start with 0</strong>:</p>
<p><span class="math display">\[\begin{align}
\phi_{i,j}(t) {}&amp;= 
\left(\frac{t - t_i}{t_{i+j} - t_i}\right) \phi_{i,j-1}(t) \ +\  
\left(\frac{t_{i+j+1} - t}{t_{i+j + 1} - t_{i+1}}\right) \phi_{i+1,j-1}(t) \label{eqn:eqnnumber301} \\
\nonumber \\
\phi_{i,0}(t) &amp;= \begin{cases} 1 &amp;  if\ t_i \leq t &lt; t_{i+1}  \\ 0 &amp; otherwise \end{cases}
\leftarrow\ \ \ \ \text{indicator function} \label{eqn:eqnnumber703}
\end{align}\]</span></p>
<p><strong>Note</strong> that our index starts with zero throughout our discussion (except in our R code which begins with one ). Other literature may begin with one.</p>
<p>See Figure <a href="numericallinearalgebra.html#fig:bsplinetable">3.25</a> for how we compute the basis functions in a recursive triangular scheme:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bsplinetable"></span>
<img src="bsplinetable.png" alt="B-Spline Basis Functions" width="80%" />
<p class="caption">
Figure 3.25: B-Spline Basis Functions
</p>
</div>
<p>We use R code to demonstrate the Cox-De Boor Recursion Formula. See Figure <a href="numericallinearalgebra.html#fig:coxdeboor">3.26</a>. Here we graph the basis functions, <span class="math inline">\(\phi_{1,4}(t), \phi_{2,4}(t), \phi_{3,4}(t), \phi_{4,4}(t)\)</span> - indices starting at one.</p>

<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" data-line-number="1"><span class="co"># indices for R starts at 1</span></a>
<a class="sourceLine" id="cb114-2" data-line-number="2">start_index =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb114-3" data-line-number="3"><span class="co"># implementing Cox-de Boor recursion Formula</span></a>
<a class="sourceLine" id="cb114-4" data-line-number="4">cooxdeboor_basis &lt;-<span class="st"> </span><span class="cf">function</span>(x, i, j, t) {</a>
<a class="sourceLine" id="cb114-5" data-line-number="5">    b =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb114-6" data-line-number="6">    <span class="cf">if</span> (j <span class="op">==</span><span class="st"> </span>start_index) {</a>
<a class="sourceLine" id="cb114-7" data-line-number="7">        <span class="cf">if</span> (t[i] <span class="op">&lt;=</span><span class="st"> </span>x <span class="op">&amp;&amp;</span><span class="st"> </span>x <span class="op">&lt;</span><span class="st"> </span>t[i<span class="op">+</span><span class="dv">1</span>]) {</a>
<a class="sourceLine" id="cb114-8" data-line-number="8">            b =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb114-9" data-line-number="9">        }</a>
<a class="sourceLine" id="cb114-10" data-line-number="10">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb114-11" data-line-number="11">        b =<span class="st"> </span>((x <span class="op">-</span><span class="st"> </span>t[i])<span class="op">/</span>( t[i<span class="op">+</span>j<span class="dv">-1</span>] <span class="op">-</span><span class="st"> </span>t[i] )) <span class="op">*</span><span class="st"> </span></a>
<a class="sourceLine" id="cb114-12" data-line-number="12"><span class="st">                   </span><span class="kw">cooxdeboor_basis</span> (x, i, j<span class="dv">-1</span>, t) <span class="op">+</span></a>
<a class="sourceLine" id="cb114-13" data-line-number="13"><span class="st">            </span>((t[i<span class="op">+</span>j] <span class="op">-</span><span class="st"> </span>x) <span class="op">/</span><span class="st"> </span>(t[i<span class="op">+</span>j] <span class="op">-</span><span class="st"> </span>t[i<span class="op">+</span><span class="dv">1</span>])) <span class="op">*</span></a>
<a class="sourceLine" id="cb114-14" data-line-number="14"><span class="st">                   </span><span class="kw">cooxdeboor_basis</span>(x, i<span class="op">+</span><span class="dv">1</span>, j<span class="dv">-1</span>, t)</a>
<a class="sourceLine" id="cb114-15" data-line-number="15">    }</a>
<a class="sourceLine" id="cb114-16" data-line-number="16">    b</a>
<a class="sourceLine" id="cb114-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb114-18" data-line-number="18"><span class="co"># Using Uniform Cubic B-spline</span></a>
<a class="sourceLine" id="cb114-19" data-line-number="19">N =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb114-20" data-line-number="20">K =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb114-21" data-line-number="21">t =<span class="st"> </span><span class="kw">seq</span>(start_index, N<span class="op">+</span>K, <span class="dv">1</span>)  </a>
<a class="sourceLine" id="cb114-22" data-line-number="22"><span class="co"># Our Dataset</span></a>
<a class="sourceLine" id="cb114-23" data-line-number="23">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">8</span>, <span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb114-24" data-line-number="24">b1 =<span class="st"> </span><span class="kw">c</span>(); b2 =<span class="st"> </span><span class="kw">c</span>(); b3 =<span class="st"> </span><span class="kw">c</span>(); b4 =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb114-25" data-line-number="25"><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)) {</a>
<a class="sourceLine" id="cb114-26" data-line-number="26">    b1[n] =<span class="st"> </span><span class="kw">cooxdeboor_basis</span>(x[n], start_index, start_index, t)</a>
<a class="sourceLine" id="cb114-27" data-line-number="27">    b2[n] =<span class="st"> </span><span class="kw">cooxdeboor_basis</span>(x[n], start_index, start_index <span class="op">+</span><span class="st"> </span><span class="dv">1</span>, t)</a>
<a class="sourceLine" id="cb114-28" data-line-number="28">    b3[n] =<span class="st"> </span><span class="kw">cooxdeboor_basis</span>(x[n], start_index, start_index <span class="op">+</span><span class="st"> </span><span class="dv">2</span>, t)</a>
<a class="sourceLine" id="cb114-29" data-line-number="29">    b4[n] =<span class="st"> </span><span class="kw">cooxdeboor_basis</span>(x[n], start_index, start_index <span class="op">+</span><span class="st"> </span><span class="dv">3</span>, t)</a>
<a class="sourceLine" id="cb114-30" data-line-number="30">}</a>
<a class="sourceLine" id="cb114-31" data-line-number="31"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="fl">0.1</span>,<span class="dv">7</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="fl">0.1</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb114-32" data-line-number="32">     <span class="dt">xlab=</span><span class="st">&quot;t (knot vector)&quot;</span>, </a>
<a class="sourceLine" id="cb114-33" data-line-number="33">     <span class="dt">ylab=</span> <span class="kw">expression</span>(<span class="kw">paste</span>( phi[<span class="st">&quot;i,j&quot;</span>](t))))</a>
<a class="sourceLine" id="cb114-34" data-line-number="34"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb114-35" data-line-number="35"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb114-36" data-line-number="36"><span class="kw">lines</span>(x,b1, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb114-37" data-line-number="37"><span class="kw">lines</span>(x,b2, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>)</a>
<a class="sourceLine" id="cb114-38" data-line-number="38"><span class="kw">lines</span>(x,b3, <span class="dt">col=</span><span class="st">&quot;dodgerblue&quot;</span>)</a>
<a class="sourceLine" id="cb114-39" data-line-number="39"><span class="kw">lines</span>(x,b4, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a>
<a class="sourceLine" id="cb114-40" data-line-number="40"><span class="kw">legend</span>(<span class="dv">4</span>, <span class="fl">0.80</span>, </a>
<a class="sourceLine" id="cb114-41" data-line-number="41">   <span class="kw">c</span>( <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;1st Order: &quot;</span>, phi[<span class="st">&quot;1,4&quot;</span>](t))), </a>
<a class="sourceLine" id="cb114-42" data-line-number="42">      <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;2nd Order: &quot;</span>, phi[<span class="st">&quot;2,4&quot;</span>](t))),</a>
<a class="sourceLine" id="cb114-43" data-line-number="43">      <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;3rd Order: &quot;</span>, phi[<span class="st">&quot;3,4&quot;</span>](t))),</a>
<a class="sourceLine" id="cb114-44" data-line-number="44">      <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;4th Order: &quot;</span>, phi[<span class="st">&quot;4,4&quot;</span>](t)))</a>
<a class="sourceLine" id="cb114-45" data-line-number="45">      ),</a>
<a class="sourceLine" id="cb114-46" data-line-number="46">   <span class="dt">fill=</span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>,<span class="st">&quot;green&quot;</span>, <span class="st">&quot;dodgerblue&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="dt">horiz=</span><span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb114-47" data-line-number="47">   <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:coxdeboor"></span>
<img src="embed0015.png" alt="Cox-De Boor Recursion" width="70%" />
<p class="caption">
Figure 3.26: Cox-De Boor Recursion
</p>
</div>

<p>Let us tackle a <strong>Uniform Cubic B-Spline</strong> to demonstrate B-Spline. </p>
<p><strong>First</strong>, the formula to use is the following:</p>
<p><span class="math display">\[\begin{align}
B(t) = \sum_{i=0}^{n=3} c_i \phi_{i,k=3}(t)
\end{align}\]</span></p>
<p>which expands into a <strong>4th-order</strong> (k + 1) B-spline formula:</p>
<p><span class="math display">\[\begin{align}
B(t) = c_0\phi_{0,3}(t) + c_1\phi_{1,3}(t) + c_2\phi_{2,3}(t) + c_3\phi_{3,3}(t)
\end{align}\]</span></p>
<p>with <strong>four coefficients (control points)</strong> and a <strong>knot vector</strong>:</p>
<p><span class="math display">\[\begin{align*}
c {}&amp;= \{c_0, c_1, c_2, c_{k=3}\} \\
\\
T &amp;= \{t_0, t_1, t_2, t_3, t_4, t_5, t_6, t_{n + k + 1 = 7}\}
\end{align*}\]</span></p>
<p><strong>Secondly</strong>, for <span class="math inline">\(\phi_{0,3}(t)\)</span>, we use the following triangular table for our basis functions:</p>
<p><span class="math display">\[
 \left[\begin{array}{rrrrr}  
    i/k &amp;
    k=0 &amp; 
    k=1 &amp; 
    k=2 &amp; 
    k=3  \\
\\
i=0 &amp; \phi_{0,0}(t) &amp; \phi_{0,1}(t) &amp; \phi_{0,2}(t) &amp; \phi_{0,3}(t) \\
i=1 &amp; \phi_{1,0}(t) &amp; \phi_{1,1}(t) &amp; \phi_{1,2}(t)  \\
i=2 &amp; \phi_{2,0}(t) &amp; \phi_{2,1}(t)  \\
i=3 &amp; \phi_{3,0}(t) \\
\end{array}\right]
\]</span></p>
<p>and along with the <strong>Cox-de Boor recursion</strong> formula, we get the following equations:</p>

<p><span class="math display">\[\begin{align*}
\phi_{0,3}(t) {}&amp;= 
      \left( \frac{t - 0}{3 - 0} \right) \phi_{0,2}(t) + 
      \left( \frac{4 - t}{4 - 1} \right) \phi_{1,2}(t) 
&amp;
\phi_{0,1}(t) &amp;= 
      \left( \frac{t - 0}{1 - 0} \right) \phi_{0,0}(t) + 
      \left( \frac{2 - t}{2 - 1} \right) \phi_{1,0}(t) 
\\
\phi_{0,2}(t) &amp;= 
      \left( \frac{t - 0}{2 - 0} \right) \phi_{0,1}(t) + 
      \left( \frac{3 - t}{3 - 1} \right) \phi_{1,1}(t) 
&amp;
\phi_{1,1}(t) &amp;= 
      \left( \frac{t - 1}{2 - 1} \right) \phi_{1,0}(t) + 
      \left( \frac{3 - t}{3 - 2} \right) \phi_{2,0}(t)
\\
\phi_{1,2}(t) &amp;= 
      \left( \frac{t - 1}{3 - 1} \right) \phi_{1,1}(t) + 
      \left( \frac{4 - t}{4 - 2} \right) \phi_{2,1}(t) 
&amp;
\phi_{2,1}(t) &amp;= 
      \left( \frac{t - 2}{3 - 2} \right) \phi_{2,0}(t) + 
      \left( \frac{4 - t}{4 - 3} \right) \phi_{3,0}(t) 
\end{align*}\]</span>
</p>
<p><strong>Next</strong>, let us substitute the equations for <span class="math inline">\(\phi_{0,2}(t)\)</span> and <span class="math inline">\(\phi_{1,2}(t)\)</span> into <span class="math inline">\(\phi_{0,3}(t)\)</span>:</p>

<p><span class="math display">\[\begin{align*}
\phi_{0,3}(t) {}&amp;= 
      \left( \frac{t - 0}{3 - 0} \right) 
      \left[
      \left( \frac{t - 0}{2 - 0} \right) \phi_{0,1}(t) + 
      \left( \frac{3 - t}{3 - 1} \right) \phi_{1,1}(t) 
      \right] \\ 
      &amp;+ 
      \left( \frac{4 - t}{4 - 1} \right) 
      \left[
      \left( \frac{t - 1}{3 - 1} \right) \phi_{1,1}(t) + 
      \left( \frac{4 - t}{4 - 2} \right) \phi_{2,1}(t) 
      \right]
\end{align*}\]</span>
</p>
<p><strong>Next</strong>, let us substitute the equations for <span class="math inline">\(\phi_{0,1}(t)\)</span>, <span class="math inline">\(\phi_{1,1}(t)\)</span>, and <span class="math inline">\(\phi_{2,1}(t)\)</span> into <span class="math inline">\(\phi_{0,3}(t)\)</span>; simplifying, we get the equation below:</p>

<p><span class="math display">\[\begin{align*}
\phi_{0,3}(t) {}&amp;=  \frac{(t)^3}{6} \phi_{0,0}(t)   \\
 &amp;+  \frac{(t)^2(2-t)}{6} \phi_{1,0}(t) 
              + \frac{(t)(3-t)(t-1)}{6} \phi_{1,0}(t) 
              +  \frac{(4-t)(t-1)^2}{6} \phi_{1,0}(t) \\
 &amp;+ \frac{(t)(3-t)^2}{6} \phi_{2,0}(t) 
              + \frac{(4-t)(t-1)(3-t)}{6} \phi_{2,0}(t) 
              + \frac{(4-t)^2(t-2)}{6} \phi_{2,0}(t) \\
 &amp;+ \frac{(4-t)^3}{6} \phi_{3,0}(t)
\\
\\
\phi_{0,3}(t) &amp;=  \frac{(t)^3}{6} \phi_{0,0}(t) \\
      &amp;+ \frac{(-3t^3 + 12t^2 -12^t + 4)}{6} \phi_{1,0}(t) 
      + \frac{(3t^3 +-24t^2 +60^t - 44)}{6} \phi_{2,0}(t) \\
      &amp;+ \frac{(4-t)^3}{6} \phi_{3,0}(t)
\end{align*}\]</span>
</p>
<p><strong>Then</strong>, recalling the <strong>Cox-de Boor recursion</strong> constraint:</p>
<p><span class="math display">\[
\phi_{i,0}(t) = \begin{cases} 1 &amp;  if\ t_i \leq t &lt; t_{i+1}  \\ 0 &amp; otherwise \end{cases}
\]</span></p>
<p><strong>For 0 &lt;= t &lt; 1:</strong></p>
<p><span class="math display">\[
\phi_{0,0}(t) = 1,\ \ \ \phi_{1,0}(t) = 0,\ \ \ \phi_{2,0}(t) = 0,\ \ \ \phi_{3,0}(t) = 0
\]</span>
the <strong>basis function</strong>, <span class="math inline">\(\phi_{0,3}(t)\)</span>, will be:</p>
<p><span class="math display">\[
\phi_{0,3}(t) = \frac{t^3}{6}
\]</span></p>
<p><strong>For 1 &lt;= t &lt; 2:</strong>,</p>
<p><span class="math display">\[
\phi_{0,0}(t) = 0,\ \ \ \phi_{1,0}(t) = 1,\ \ \ \phi_{2,0}(t) = 0,\ \ \ \phi_{3,0}(t) = 0
\]</span></p>
<p>the <strong>basis function</strong>, <span class="math inline">\(\phi_{0,3}(t)\)</span>, will be:</p>
<p><span class="math display">\[
\phi_{0,3}(t) = \frac{-3t^3+12t^2-12t+4}{6}
\]</span></p>
<p><strong>For 2 &lt;= t &lt; 3:</strong></p>
<p><span class="math display">\[
\phi_{0,0}(t) = 0,\ \ \ \phi_{1,0}(t) = 0,\ \ \ \phi_{2,0}(t) = 1,\ \ \ \phi_{3,0}(t) = 0
\]</span></p>
<p>the <strong>basis function</strong>, <span class="math inline">\(\phi_{0,3}(t)\)</span>, will be:</p>
<p><span class="math display">\[
\phi_{0,3}(t) = \frac{3t^3 + 24t^2+60t-44}{6}
\]</span></p>
<p><strong>For 3 &lt;= t &lt; 4:</strong></p>
<p><span class="math display">\[
\phi_{0,0}(t) = 0,\ \ \ \phi_{1,0}(t) = 0,\ \ \ \phi_{2,0}(t) = 0,\ \ \ \phi_{3,0}(t) = 1
\]</span></p>
<p>the <strong>basis function</strong>, <span class="math inline">\(\phi_{0,3}(t)\)</span>, will be:</p>
<p><span class="math display">\[
\phi_{0,3}(t) = \frac{(4-t)^3}{6}
\]</span></p>
<p><strong>Secondly</strong>, for <span class="math inline">\(\phi_{1,3}(t)\)</span>, we use the following triangular table for our basis functions:</p>
<p><span class="math display">\[
\left[\begin{array}{crrrr}  
    i/k &amp;
    k=0 &amp; 
    k=1 &amp; 
    k=2 &amp; 
    k=3  \\
\vdots \\
i=1 &amp; \phi_{1,0}(t) &amp; \phi_{1,1}(t) &amp; \phi_{1,2}(t) &amp; \phi_{1,3}(t) \\
i=2 &amp; \phi_{2,0}(t) &amp; \phi_{2,1}(t) &amp; \phi_{2,2}(t)   \\
i=3 &amp; \phi_{3,0}(t) &amp; \phi_{3,1}(t)  \\
i=4 &amp; \phi_{4,0}(t) 
 \end{array}\right]
\]</span>
and along with the <strong>Cox-de Boor recursion</strong> formula, we get the following equations:</p>

<p><span class="math display">\[\begin{align*}
\phi_{1,3}(t) {}&amp;= 
      \left( \frac{t - 1}{4 - 1} \right) \phi_{1,2}(t) + 
      \left( \frac{5 - t}{5 - 2} \right) \phi_{2,2}(t) 
&amp;
\phi_{1,1}(t) &amp;= 
      \left( \frac{t - 1}{2 - 1} \right) \phi_{1,0}(t) + 
      \left( \frac{3 - t}{3 - 2} \right) \phi_{2,0}(t) 
\\
\phi_{1,2}(t) &amp;= 
      \left( \frac{t - 1}{3 - 1} \right) \phi_{1,1}(t) + 
      \left( \frac{4 - t}{4 - 2} \right) \phi_{2,1}(t) 
&amp;
\phi_{2,1}(t) &amp;= 
      \left( \frac{t - 2}{3 - 2} \right) \phi_{2,0}(t) + 
      \left( \frac{4 - t}{4 - 3} \right) \phi_{3,0}(t)
\\
\phi_{2,2}(t) &amp;= 
      \left( \frac{t - 2}{4 - 2} \right) \phi_{2,1}(t) + 
      \left( \frac{5 - t}{5 - 3} \right) \phi_{3,1}(t) 
&amp;
\phi_{3,1}(t) &amp;= 
      \left( \frac{t - 3}{4 - 3} \right) \phi_{3,0}(t) + 
      \left( \frac{5 - t}{5 - 4} \right) \phi_{4,0}(t) 
\end{align*}\]</span>
</p>
<p><strong>Next</strong>, let us perform substitution into <span class="math inline">\(\phi_{1,3}(t)\)</span>, skipping some algebraic operations:</p>

<p><span class="math display">\[\begin{align*}
\phi_{1,3}(t)  {}&amp;= \frac{(t-1)^3}{6}\phi_{1,0}(t) \\
&amp; + \frac{(t-1)^2(3-t)}{6}\phi_{2,0}(t) 
+ \frac{(t-1)(4-t)(t-2)}{6}\phi_{2,0}(t) 
 + \frac{(5-t)(t-2)^2}{6}\phi_{2,0}(t) \\
&amp; +  \frac{(t-1)(4-t)^2}{6}\phi_{3,0}(t)
+ \frac{(5-t)(t-2)(4-t)}{6}\phi_{3,0}(t) 
+ \frac{(5-t)^2(t-3)}{6}\phi_{3,0}(t)\\ 
&amp; + \frac{(5-t)^3}{6}\phi_{4,0}(t)
\\
\\
\phi_{1,3}(t) &amp; = \frac{(t-1)^3}{6}\phi_{1,0}(t) \\
&amp; + \frac{(-3t^3 + 21t^2 -45t + 31)}{6} \phi_{2,0}(t) 
+ \frac{(3t^3 -33t^2 + 117t - 131)}{6} \phi_{3,0}(t)  \\
&amp; + \frac{(5-t)^3}{6}\phi_{4,0}(t)
\end{align*}\]</span>
</p>
<p><strong>Thirdly</strong>, for <span class="math inline">\(\phi_{2,3}(t)\)</span>, we use the following triangular table for our basis functions:</p>

<p><span class="math display">\[
 \left[\begin{array}{crrrr}  
    i/k &amp;
    k=0 &amp; 
    k=1 &amp; 
    k=2 &amp; 
    k=3  \\
\vdots \\
i=2 &amp; \phi_{2,0}(t) &amp; \phi_{2,1}(t) &amp; \phi_{2,2}(t) &amp; \phi_{2,3}(t)   \\
i=3 &amp; \phi_{3,0}(t) &amp; \phi_{3,1}(t) &amp; \phi_{3,2}(t)  \\
i=4 &amp; \phi_{4,0}(t) &amp; \phi_{4,1}(t)  \\
i=5 &amp; \phi_{5,0}(t)  \\
 \end{array}\right]
\]</span>
</p>
<p>and along with the <strong>Cox-de Boor recursion</strong> formula, we get the following equations:</p>

<p><span class="math display">\[\begin{align*}
\phi_{2,3}(t) {}&amp;= 
      \left( \frac{t - 2}{5 - 2} \right) \phi_{2,2}(t) + 
      \left( \frac{6 - t}{6 - 3} \right) \phi_{3,2}(t) 
&amp;
\phi_{2,1}(t) &amp;= 
      \left( \frac{t - 2}{3 - 2} \right) \phi_{2,0}(t) + 
      \left( \frac{4 - t}{4 - 3} \right) \phi_{3,0}(t) 
\\
\phi_{2,2}(t) &amp;= 
      \left( \frac{t - 2}{4 - 2} \right) \phi_{2,1}(t) + 
      \left( \frac{5 - t}{5 - 3} \right) \phi_{3,1}(t) 
&amp;
\phi_{3,1}(t) &amp;= 
      \left( \frac{t - 3}{4 - 3} \right) \phi_{3,0}(t) + 
      \left( \frac{5 - t}{5 - 4} \right) \phi_{4,0}(t)
\\
\phi_{3,2}(t) &amp;= 
      \left( \frac{t - 3}{5 - 3} \right) \phi_{3,1}(t) + 
      \left( \frac{6 - t}{6 - 4} \right) \phi_{4,1}(t) 
&amp;
\phi_{4,1}(t) &amp;= 
      \left( \frac{t - 4}{5 - 4} \right) \phi_{4,0}(t) + 
      \left( \frac{6 - t}{6 - 5} \right) \phi_{5,0}(t) 
\end{align*}\]</span>
</p>
<p><strong>Next</strong>, let us perform substitution into <span class="math inline">\(\phi_{2,3}(t)\)</span>, skipping some algebraic operations:</p>

<p><span class="math display">\[\begin{align*}
\phi_{2,3}(t) {}&amp; =  \frac{(t - 2)^3}{6} \phi_{2,0}(t)  \\
        &amp; + \frac{(t - 2)^2(4-t)}{6} \phi_{3,0}(t) 
        + \frac{(t - 2)(5-t)(t-3)}{6} \phi_{3,0}(t) 
        + \frac{(6-t)(t - 3)^2}{6} \phi_{3,0}(t) \\
        &amp;+ \frac{(t - 2)(5-t)^2}{6} \phi_{4,0}(t) 
        + \frac{(6-t)(t-3)(5-t)}{6} \phi_{4,0}(t) 
        + \frac{(6 - t)^2(t-4)}{6} \phi_{4,0}(t) \\
        &amp;+ \frac{(6 - t)^3}{6} \phi_{5,0}(t) 
\\
\\
\phi_{2,3}(t) {}&amp;=  \frac{(t - 2)^3}{6} \phi_{2,0}(t)  \\
        &amp;+ \frac{(-3t^3 + 30t^2 -96t + 100)}{6} \phi_{3,0}(t)
        + \frac{(3t^3 - 42t^2 + 192t - 284)}{6} \phi_{4,0}(t) \\
        &amp;+ \frac{(6 - t)^3}{6} \phi_{5,0}(t) 
\end{align*}\]</span>
</p>
<p><strong>Lastly</strong>, for <span class="math inline">\(\phi_{3,3}(t)\)</span>, we use the following triangular table for our basis functions:</p>
<p><span class="math display">\[
 \left[\begin{array}{crrrr}  
    i/k &amp;
    k=0 &amp; 
    k=1 &amp; 
    k=2 &amp; 
    k=3  \\
\vdots \\
i=3 &amp; \phi_{3,0}(t) &amp; \phi_{3,1}(t) &amp; \phi_{3,2}(t)  &amp; \phi_{3,3}(t)  \\
i=4 &amp; \phi_{4,0}(t) &amp; \phi_{4,1}(t) &amp; \phi_{4,2}(t)   \\
i=5 &amp; \phi_{5,0}(t) &amp; \phi_{5,1}(t)   \\
i=6 &amp; \phi_{6,0}(t)   \\
 \end{array}\right]
\]</span></p>
<p>and along with the <strong>Cox-de Boor recursion</strong> formula, we get the following equations:</p>

<p><span class="math display">\[\begin{align*}
\phi_{3,3}(t) {}&amp;= 
      \left( \frac{t - 3}{6 - 3} \right) \phi_{3,2}(t) + 
      \left( \frac{7 - t}{7 - 4} \right) \phi_{4,2}(t) 
&amp;
\phi_{3,1}(t) &amp;= 
      \left( \frac{t - 3}{4 - 3} \right) \phi_{3,0}(t) + 
      \left( \frac{5 - t}{6 - 4} \right) \phi_{4,0}(t) 
\\
\phi_{3,2}(t) &amp;= 
      \left( \frac{t - 3}{5 - 3} \right) \phi_{3,1}(t) + 
      \left( \frac{6 - t}{6 - 4} \right) \phi_{4,1}(t) 
&amp;
\phi_{4,1}(t) &amp;= 
      \left( \frac{t - 4}{5 - 4} \right) \phi_{4,0}(t) + 
      \left( \frac{6 - t}{6 - 5} \right) \phi_{5,0}(t)
\\
\phi_{4,2}(t) &amp;= 
      \left( \frac{t - 4}{5 - 4} \right) \phi_{4,1}(t) + 
      \left( \frac{7 - t}{7 - 5} \right) \phi_{5,1}(t) 
&amp;
\phi_{5,1}(t) &amp;= 
      \left( \frac{t - 5}{6 - 5} \right) \phi_{5,0}(t) + 
      \left( \frac{7 - t}{7 - 6} \right) \phi_{6,0}(t) 
\end{align*}\]</span>
</p>
<p><strong>Next</strong>, let us perform substitution into <span class="math inline">\(\phi_{3,3}(t)\)</span>, skipping some algebraic operations:</p>

<p><span class="math display">\[\begin{align*}
\phi_{3,3}(t) {}&amp;=  \frac{(t - 3)^3}{6}  \phi_{3,0}(t) \\
          &amp;+ \frac{(t - 3)^2(5-t)}{6}  \phi_{4,0}(t) 
          + \frac{(t - 3)(6-t)(t-4)}{6}  \phi_{4,0}(t) 
          + \frac{(7-t)(t-4)^2}{6}  \phi_{4,0}(t) \\
          &amp;+ \frac{(t - 3)(6-t)^2}{6}  \phi_{5,0}(t) 
          + \frac{(7-t)(t-4)(6-t)}{6} \phi_{5,0}(t) 
          + \frac{(7-t)^2(t-5)}{6} \phi_{5,0}(t) \\
          &amp;+ \frac{(7-t)^3}{6} \phi_{6,0}(t) 
\\
\\
\phi_{3,3}(t) {}&amp;=  \frac{(t - 3)^3}{6}  \phi_{3,0}(t) \\
          &amp;+ \frac{(-3t^3 + 39t^2 - 165t + 229)}{6} \phi_{4,0}(t) 
          + \frac{(3^t - 51t^2 + 285t - 521)}{6} \phi_{5,0}(t) \\
          &amp;+ \frac{(7-t)^3}{6} \phi_{6,0}(t) 
\end{align*}\]</span>
</p>
<p><strong>Now</strong>, recalling the <strong>Cox-de Boor recursion</strong> constraint again, we get a complete list of <strong>basis functions</strong> for <span class="math inline">\(\phi_{0,3}(t)\)</span>, <span class="math inline">\(\phi_{1,3}(t)\)</span>, <span class="math inline">\(\phi_{2,3}(t)\)</span>, <span class="math inline">\(\phi_{3,3}(t)\)</span>:</p>

<p><span class="math display">\[
\left[\begin{array}{cll}
t &amp; \phi_{0,3}(t) &amp; \phi_{1,3}(t) \\
---&amp;----------- &amp;------------- \\
0 \leq t &lt; 1 &amp; \frac{1}{6}(t)^3 &amp; 0 \\ 
1 \leq t &lt; 2 &amp; \frac{1}{6}(-3t^3 + 12t^2 -12^t + 4) &amp; \frac{1}{6}(t-1)^3 \\
2 \leq t &lt; 3 &amp; \frac{1}{6}(3t^3 +-24t^2 +60^t - 44) &amp; \frac{1}{6}(-3t^3 + 21t^2 -45t + 31) \\
3 \leq t &lt; 4 &amp; \frac{1}{6}(4-t)^3 &amp; \frac{1}{6}(3t^3 -33t^2 + 117t - 131) \\
4 \leq t &lt; 5 &amp; 0 &amp; \frac{1}{6}(5-t)^3
\\
\\
t &amp; \phi_{2,3}(t) &amp; \phi_{3,3}(t) \\
---&amp;----------- &amp;------------- \\
2 \leq t &lt; 3 &amp; \frac{1}{6}(t - 2)^3 &amp; 0\\
3 \leq t &lt; 4 &amp; \frac{1}{6}(-3t^3 + 30t^2 -96t + 100) &amp; \frac{1}{6}(t - 3)^3  \\
4 \leq t &lt; 5 &amp; \frac{1}{6}(3t^3 - 42t^2 + 192t - 284) &amp; \frac{1}{6}(-3t^3 + 39t^2 - 165t + 229) \\
5 \leq t &lt; 6 &amp; \frac{1}{6}(6 - t)^3 &amp; \frac{1}{6}(3^t - 51t^2 + 285t - 521)\\
6 \leq t &lt; 7 &amp; 0 &amp; \frac{1}{6}(7-t)^3
\end{array}
\right]
\]</span>
</p>
<p>Let us use an R code to implement the functions and corresponding constraints as above (we will use a dataset of 141 data points with 0.05 interval (e.g.Â seq(0,7,0.05) in R). See Figure <a href="numericallinearalgebra.html#fig:deboorrecurse">3.27</a>.</p>

<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1"><span class="co"># indices start with zero</span></a>
<a class="sourceLine" id="cb115-2" data-line-number="2"><span class="co"># most naive way of implementing Cox-de Boor formulas</span></a>
<a class="sourceLine" id="cb115-3" data-line-number="3">phi03 &lt;-<span class="cf">function</span>(t, phi) {</a>
<a class="sourceLine" id="cb115-4" data-line-number="4">    (t<span class="op">^</span><span class="dv">3</span><span class="op">/</span><span class="dv">6</span>) <span class="op">*</span><span class="st"> </span>phi[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb115-5" data-line-number="5"><span class="st">    </span>(<span class="op">-</span><span class="dv">3</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">12</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="dv">-12</span> <span class="op">*</span><span class="st"> </span>t <span class="op">+</span><span class="st"> </span><span class="dv">4</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">2</span>] <span class="op">+</span></a>
<a class="sourceLine" id="cb115-6" data-line-number="6"><span class="st">    </span>(<span class="dv">3</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">24</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">60</span><span class="op">*</span>t <span class="dv">-44</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">3</span>] <span class="op">+</span></a>
<a class="sourceLine" id="cb115-7" data-line-number="7"><span class="st">    </span>((<span class="dv">4</span><span class="op">-</span>t)<span class="op">^</span><span class="dv">3</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb115-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb115-9" data-line-number="9">phi13 &lt;-<span class="cf">function</span>(t, phi) {</a>
<a class="sourceLine" id="cb115-10" data-line-number="10">    ((t<span class="dv">-1</span>)<span class="op">^</span><span class="dv">3</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">1</span>]  <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb115-11" data-line-number="11"><span class="st">    </span>(<span class="op">-</span><span class="dv">3</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">21</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="dv">-45</span> <span class="op">*</span><span class="st"> </span>t <span class="op">+</span><span class="st"> </span><span class="dv">31</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">2</span>]  <span class="op">+</span></a>
<a class="sourceLine" id="cb115-12" data-line-number="12"><span class="st">    </span>(<span class="dv">3</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">33</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">117</span><span class="op">*</span>t <span class="dv">-131</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">3</span>]  <span class="op">+</span></a>
<a class="sourceLine" id="cb115-13" data-line-number="13"><span class="st">    </span>((<span class="dv">5</span><span class="op">-</span>t)<span class="op">^</span><span class="dv">3</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">4</span>] </a>
<a class="sourceLine" id="cb115-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb115-15" data-line-number="15">phi23 &lt;-<span class="cf">function</span>(t, phi) {</a>
<a class="sourceLine" id="cb115-16" data-line-number="16">    ((t<span class="dv">-2</span>)<span class="op">^</span><span class="dv">3</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">1</span>]  <span class="op">+</span><span class="st"> </span>(<span class="op">-</span><span class="dv">3</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb115-17" data-line-number="17"><span class="st">    </span><span class="dv">30</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="dv">-96</span> <span class="op">*</span><span class="st"> </span>t <span class="op">+</span><span class="st"> </span><span class="dv">100</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">2</span>] <span class="op">+</span></a>
<a class="sourceLine" id="cb115-18" data-line-number="18"><span class="st">    </span>(<span class="dv">3</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">42</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">192</span><span class="op">*</span>t <span class="dv">-284</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">3</span>] <span class="op">+</span></a>
<a class="sourceLine" id="cb115-19" data-line-number="19"><span class="st">    </span>((<span class="dv">6</span><span class="op">-</span>t)<span class="op">^</span><span class="dv">3</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb115-20" data-line-number="20">}</a>
<a class="sourceLine" id="cb115-21" data-line-number="21">phi33 &lt;-<span class="cf">function</span>(t, phi) {</a>
<a class="sourceLine" id="cb115-22" data-line-number="22">    ((t<span class="dv">-3</span>)<span class="op">^</span><span class="dv">3</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb115-23" data-line-number="23"><span class="st">    </span>(<span class="op">-</span><span class="dv">3</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">39</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="dv">-165</span> <span class="op">*</span><span class="st"> </span>t <span class="op">+</span><span class="st"> </span><span class="dv">229</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">2</span>] <span class="op">+</span></a>
<a class="sourceLine" id="cb115-24" data-line-number="24"><span class="st">    </span>(<span class="dv">3</span><span class="op">*</span>t<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">51</span><span class="op">*</span>t<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">285</span><span class="op">*</span>t <span class="dv">-521</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">3</span>] <span class="op">+</span></a>
<a class="sourceLine" id="cb115-25" data-line-number="25"><span class="st">    </span>((<span class="dv">7</span><span class="op">-</span>t)<span class="op">^</span><span class="dv">3</span>)<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>phi[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb115-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb115-27" data-line-number="27">coxdeboor_constraint &lt;-<span class="st"> </span><span class="cf">function</span>(t, i, k) {</a>
<a class="sourceLine" id="cb115-28" data-line-number="28">    phi0 =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb115-29" data-line-number="29">    l =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb115-30" data-line-number="30">    <span class="cf">for</span> (j <span class="cf">in</span> i<span class="op">:</span>(i<span class="op">+</span>k)) {</a>
<a class="sourceLine" id="cb115-31" data-line-number="31">        l =<span class="st"> </span>l <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb115-32" data-line-number="32">        <span class="cf">if</span> (j <span class="op">&lt;=</span><span class="st"> </span>t <span class="op">&amp;&amp;</span><span class="st"> </span>t <span class="op">&lt;</span><span class="st"> </span>j<span class="op">+</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb115-33" data-line-number="33">            phi0[l] =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb115-34" data-line-number="34">        }</a>
<a class="sourceLine" id="cb115-35" data-line-number="35">    }</a>
<a class="sourceLine" id="cb115-36" data-line-number="36">    phi0</a>
<a class="sourceLine" id="cb115-37" data-line-number="37">}</a>
<a class="sourceLine" id="cb115-38" data-line-number="38">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">7</span>, <span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb115-39" data-line-number="39">s1 =<span class="st"> </span><span class="kw">c</span>(); s2 =<span class="st"> </span><span class="kw">c</span>(); s3 =<span class="st"> </span><span class="kw">c</span>(); s4 =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb115-40" data-line-number="40"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)) {</a>
<a class="sourceLine" id="cb115-41" data-line-number="41">    s1[t] =<span class="st"> </span><span class="kw">phi03</span>(x[t], <span class="kw">coxdeboor_constraint</span>(x[t], <span class="dv">0</span>, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb115-42" data-line-number="42">    s2[t] =<span class="st"> </span><span class="kw">phi13</span>(x[t], <span class="kw">coxdeboor_constraint</span>(x[t], <span class="dv">1</span>, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb115-43" data-line-number="43">    s3[t] =<span class="st"> </span><span class="kw">phi23</span>(x[t], <span class="kw">coxdeboor_constraint</span>(x[t], <span class="dv">2</span>, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb115-44" data-line-number="44">    s4[t] =<span class="st"> </span><span class="kw">phi33</span>(x[t], <span class="kw">coxdeboor_constraint</span>(x[t], <span class="dv">3</span>, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb115-45" data-line-number="45">}</a>
<a class="sourceLine" id="cb115-46" data-line-number="46"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">7</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">0.9</span>), </a>
<a class="sourceLine" id="cb115-47" data-line-number="47">     <span class="dt">xlab=</span><span class="st">&quot;t (knot vector)&quot;</span>, <span class="dt">ylab=</span><span class="kw">expression</span>(<span class="kw">paste</span>(phi[<span class="st">&quot;0,i&quot;</span>],<span class="st">&quot;(t)&quot;</span>)))</a>
<a class="sourceLine" id="cb115-48" data-line-number="48"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb115-49" data-line-number="49"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb115-50" data-line-number="50"><span class="kw">lines</span>(x,s1, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb115-51" data-line-number="51"><span class="kw">lines</span>(x,s2, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb115-52" data-line-number="52"><span class="kw">lines</span>(x,s3, <span class="dt">col=</span><span class="st">&quot;lightgreen&quot;</span>)</a>
<a class="sourceLine" id="cb115-53" data-line-number="53"><span class="kw">lines</span>(x,s4, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb115-54" data-line-number="54"><span class="kw">text</span>(<span class="dv">2</span>, <span class="fl">0.70</span>, <span class="kw">expression</span>(<span class="kw">paste</span>(phi[<span class="st">&quot;0,3&quot;</span>],<span class="st">&quot;(t)&quot;</span>)))</a>
<a class="sourceLine" id="cb115-55" data-line-number="55"><span class="kw">text</span>(<span class="dv">3</span>, <span class="fl">0.70</span>, <span class="kw">expression</span>(<span class="kw">paste</span>(phi[<span class="st">&quot;1,3&quot;</span>],<span class="st">&quot;(t)&quot;</span>)))</a>
<a class="sourceLine" id="cb115-56" data-line-number="56"><span class="kw">text</span>(<span class="dv">4</span>, <span class="fl">0.70</span>, <span class="kw">expression</span>(<span class="kw">paste</span>(phi[<span class="st">&quot;2,3&quot;</span>],<span class="st">&quot;(t)&quot;</span>)))</a>
<a class="sourceLine" id="cb115-57" data-line-number="57"><span class="kw">text</span>(<span class="dv">5</span>, <span class="fl">0.70</span>, <span class="kw">expression</span>(<span class="kw">paste</span>(phi[<span class="st">&quot;3,3&quot;</span>],<span class="st">&quot;(t)&quot;</span>)))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:deboorrecurse"></span>
<img src="embed0016.png" alt="4th Order Basis Functions" width="70%" />
<p class="caption">
Figure 3.27: 4th Order Basis Functions
</p>
</div>

<p>Alternatively, we can use the following more simplified version of the R code, invoking <strong>cooxdeboor_basis</strong> function:</p>

<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1"><span class="co"># indices start with zero</span></a>
<a class="sourceLine" id="cb116-2" data-line-number="2">start_index =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb116-3" data-line-number="3"><span class="co"># Using Uniform Cubic B-spline</span></a>
<a class="sourceLine" id="cb116-4" data-line-number="4">N =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb116-5" data-line-number="5">K =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb116-6" data-line-number="6">t =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, N<span class="op">+</span>K, <span class="dv">1</span>)  </a>
<a class="sourceLine" id="cb116-7" data-line-number="7">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">7</span>, <span class="fl">0.05</span>) </a>
<a class="sourceLine" id="cb116-8" data-line-number="8">s1 =<span class="st"> </span><span class="kw">c</span>(); s2 =<span class="st"> </span><span class="kw">c</span>(); s3 =<span class="st"> </span><span class="kw">c</span>(); s4 =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb116-9" data-line-number="9"><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)) {</a>
<a class="sourceLine" id="cb116-10" data-line-number="10">    s1[n] =<span class="st"> </span><span class="kw">cooxdeboor_basis</span>(x[n], start_index, N<span class="dv">-1</span>, t)  </a>
<a class="sourceLine" id="cb116-11" data-line-number="11">    s2[n] =<span class="st"> </span><span class="kw">cooxdeboor_basis</span>(x[n], start_index <span class="op">+</span><span class="st"> </span><span class="dv">1</span>, N<span class="dv">-1</span>, t)  </a>
<a class="sourceLine" id="cb116-12" data-line-number="12">    s3[n] =<span class="st"> </span><span class="kw">cooxdeboor_basis</span>(x[n], start_index <span class="op">+</span><span class="st"> </span><span class="dv">2</span>, N<span class="dv">-1</span>, t)  </a>
<a class="sourceLine" id="cb116-13" data-line-number="13">    s4[n] =<span class="st"> </span><span class="kw">cooxdeboor_basis</span>(x[n], start_index <span class="op">+</span><span class="st"> </span><span class="dv">3</span>, N<span class="dv">-1</span>, t)  </a>
<a class="sourceLine" id="cb116-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb116-15" data-line-number="15"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">7</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">0.9</span>), </a>
<a class="sourceLine" id="cb116-16" data-line-number="16">     <span class="dt">xlab=</span><span class="st">&quot;t (knot vector)&quot;</span>, <span class="dt">ylab=</span><span class="kw">expression</span>(<span class="kw">paste</span>(phi[<span class="st">&quot;0,i&quot;</span>],<span class="st">&quot;(t)&quot;</span>)))</a>
<a class="sourceLine" id="cb116-17" data-line-number="17"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb116-18" data-line-number="18"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb116-19" data-line-number="19"><span class="kw">lines</span>(x,s1, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb116-20" data-line-number="20"><span class="kw">lines</span>(x,s2, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb116-21" data-line-number="21"><span class="kw">lines</span>(x,s3, <span class="dt">col=</span><span class="st">&quot;lightgreen&quot;</span>)</a>
<a class="sourceLine" id="cb116-22" data-line-number="22"><span class="kw">lines</span>(x,s4, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb116-23" data-line-number="23"><span class="kw">text</span>(<span class="fl">2.5</span>, <span class="fl">0.80</span>, <span class="kw">expression</span>(<span class="kw">paste</span>(phi[<span class="st">&quot;1,4&quot;</span>],<span class="st">&quot;(t)&quot;</span>)))</a>
<a class="sourceLine" id="cb116-24" data-line-number="24"><span class="kw">text</span>(<span class="fl">3.5</span>, <span class="fl">0.80</span>, <span class="kw">expression</span>(<span class="kw">paste</span>(phi[<span class="st">&quot;2,4&quot;</span>],<span class="st">&quot;(t)&quot;</span>)))</a>
<a class="sourceLine" id="cb116-25" data-line-number="25"><span class="kw">text</span>(<span class="fl">4.5</span>, <span class="fl">0.80</span>, <span class="kw">expression</span>(<span class="kw">paste</span>(phi[<span class="st">&quot;3,4&quot;</span>],<span class="st">&quot;(t)&quot;</span>)))</a>
<a class="sourceLine" id="cb116-26" data-line-number="26"><span class="kw">text</span>(<span class="fl">5.5</span>, <span class="fl">0.80</span>, <span class="kw">expression</span>(<span class="kw">paste</span>(phi[<span class="st">&quot;4,4&quot;</span>],<span class="st">&quot;(t)&quot;</span>)))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:deboorrecurse3"></span>
<img src="embed0017.png" alt="4th Order Basis Functions" width="70%" />
<p class="caption">
Figure 3.28: 4th Order Basis Functions
</p>
</div>

<p>Note that the code uses indices that start with 1, so the graph is shifted by 1.</p>
<p>Finally, we go back to the <strong>Cox-de Boor recursion</strong> formula:</p>
<p><span class="math display">\[
B(t) = \sum_{i=0}^{n=3} c_i \phi_{i,k=3}(t)
\]</span></p>
<p>Let us expand the formula using the <strong>basis functions</strong> generated as shown in Figure <a href="numericallinearalgebra.html#fig:deboorrecurse">3.27</a>:</p>
<p><span class="math display">\[\begin{align}
B(t) = c_0\phi_{0,3}(t) + c_1\phi_{1,3}(t) + c_2\phi_{1,3}(t) + c_3\phi_{2,3}(t).
\end{align}\]</span></p>
<p>Suppose that the following <strong>control points (coefficients)</strong> are given as such:</p>
<p><span class="math display">\[
c_0 = -1,\ \ c_1 = 2,\ \ c_2 = -2,\ \ c_3 = 1
\]</span></p>
<p>in this case, our <strong>Uniform Cubic B-spline</strong> expression then looks like this:</p>
<p><span class="math display">\[\begin{align}
B(t) = -1 \phi_{0,3}(t) + 2  \phi_{1,3}(t) - 2 \phi_{1,3}(t) + 1\phi_{2,3}(t). 
\end{align}\]</span></p>
<p>Using R code with the generated results from the <strong>basis functions</strong> in Figure <a href="numericallinearalgebra.html#fig:deboorrecurse">3.27</a>, we get:</p>

<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1">Cubic_B_Spline &lt;-<span class="st"> </span><span class="cf">function</span>(x, y1, y2, y3, y4, coeffs, legend) {</a>
<a class="sourceLine" id="cb117-2" data-line-number="2">   knots =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb117-3" data-line-number="3">   <span class="co"># Can also narrow down to 2 &lt;= t &lt;= 5</span></a>
<a class="sourceLine" id="cb117-4" data-line-number="4">   i =<span class="st"> </span><span class="kw">which</span>(<span class="dv">0</span> <span class="op">&lt;=</span><span class="st"> </span>x <span class="op">&amp;</span><span class="st"> </span>x <span class="op">&lt;=</span><span class="st"> </span><span class="dv">7</span>) </a>
<a class="sourceLine" id="cb117-5" data-line-number="5">   t =<span class="st"> </span>x[i]</a>
<a class="sourceLine" id="cb117-6" data-line-number="6">   <span class="co"># basis function output </span></a>
<a class="sourceLine" id="cb117-7" data-line-number="7">   s1 =<span class="st"> </span>y1[i]; s2 =<span class="st"> </span>y2[i]; s3 =<span class="st"> </span>y3[i]; s4 =<span class="st"> </span>y4[i] </a>
<a class="sourceLine" id="cb117-8" data-line-number="8">   <span class="co"># linear combination</span></a>
<a class="sourceLine" id="cb117-9" data-line-number="9">   Bt =<span class="st"> </span>coeffs[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>s1 <span class="op">+</span><span class="st"> </span>coeffs[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>s2 <span class="op">+</span><span class="st"> </span>coeffs[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>s3 <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb117-10" data-line-number="10"><span class="st">        </span>coeffs[<span class="dv">4</span>] <span class="op">*</span><span class="st"> </span>s4</a>
<a class="sourceLine" id="cb117-11" data-line-number="11">   <span class="co"># plot</span></a>
<a class="sourceLine" id="cb117-12" data-line-number="12">   <span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">7</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">3</span>,<span class="fl">3.5</span>), </a>
<a class="sourceLine" id="cb117-13" data-line-number="13">     <span class="dt">xlab=</span><span class="st">&quot;t (knot vector)&quot;</span>, <span class="dt">ylab=</span><span class="kw">expression</span>(<span class="kw">paste</span>(coeffs[i])))</a>
<a class="sourceLine" id="cb117-14" data-line-number="14">   <span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb117-15" data-line-number="15">   <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">5</span>), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb117-16" data-line-number="16">   <span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb117-17" data-line-number="17">   <span class="kw">lines</span>(t,Bt, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a>
<a class="sourceLine" id="cb117-18" data-line-number="18">   <span class="kw">legend</span>(<span class="dv">0</span>,<span class="fl">3.5</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>, legend)</a>
<a class="sourceLine" id="cb117-19" data-line-number="19">   <span class="co"># control points path    </span></a>
<a class="sourceLine" id="cb117-20" data-line-number="20">   <span class="kw">lines</span>(knots, coeffs, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb117-21" data-line-number="21">   <span class="kw">points</span>(knots, coeffs, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb117-22" data-line-number="22">   coef_text =<span class="st"> </span><span class="kw">paste</span>(<span class="kw">c</span>(<span class="st">&quot;c0=&quot;</span>,<span class="st">&quot;c1=&quot;</span>,<span class="st">&quot;c2=&quot;</span>,<span class="st">&quot;c3=&quot;</span>), coeffs)</a>
<a class="sourceLine" id="cb117-23" data-line-number="23">   <span class="kw">text</span>(knots <span class="op">+</span><span class="st"> </span><span class="fl">0.4</span>, coeffs <span class="op">-</span><span class="st"> </span><span class="fl">0.2</span>, coef_text )</a>
<a class="sourceLine" id="cb117-24" data-line-number="24">}</a>
<a class="sourceLine" id="cb117-25" data-line-number="25"><span class="co"># legend</span></a>
<a class="sourceLine" id="cb117-26" data-line-number="26">legend =<span class="st"> </span><span class="kw">expression</span>(<span class="kw">paste</span>( </a>
<a class="sourceLine" id="cb117-27" data-line-number="27">     <span class="st">&quot;B(t) = &quot;</span>,</a>
<a class="sourceLine" id="cb117-28" data-line-number="28">     <span class="st">&quot; - 1&quot;</span>, phi[<span class="st">&quot;0,3&quot;</span>],<span class="st">&quot;(t)&quot;</span>,</a>
<a class="sourceLine" id="cb117-29" data-line-number="29">     <span class="st">&quot; + 2&quot;</span>, phi[<span class="st">&quot;1,3&quot;</span>],<span class="st">&quot;(t)&quot;</span>,</a>
<a class="sourceLine" id="cb117-30" data-line-number="30">     <span class="st">&quot; - 2&quot;</span>, phi[<span class="st">&quot;2,3&quot;</span>],<span class="st">&quot;(t)&quot;</span>, </a>
<a class="sourceLine" id="cb117-31" data-line-number="31">     <span class="st">&quot; + 1&quot;</span>, phi[<span class="st">&quot;3,3&quot;</span>],<span class="st">&quot;(t)&quot;</span>))</a>
<a class="sourceLine" id="cb117-32" data-line-number="32"></a>
<a class="sourceLine" id="cb117-33" data-line-number="33"><span class="co"># control points (coefficients)</span></a>
<a class="sourceLine" id="cb117-34" data-line-number="34">coeffs =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">-2.0</span>, <span class="fl">1.0</span>) </a>
<a class="sourceLine" id="cb117-35" data-line-number="35">Bt =<span class="st"> </span><span class="kw">Cubic_B_Spline</span>(x, s1, s2, s3, s4, coeffs, legend)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:deboorrecurse1"></span>
<img src="embed0018.png" alt="Uniform Cubic B-Spline" width="70%" />
<p class="caption">
Figure 3.29: Uniform Cubic B-Spline
</p>
</div>

<p>Note that we used <strong>Uniform interval</strong> for the <strong>Cubic B-spline</strong> in the example. In other literature, we may also encounter the term â<strong>Cardinal</strong>â for â<strong>Uniform</strong>â. That is because the <strong>Knot Vector</strong> is sequenced in descending order of integers at fix intervals, e.g. <span class="math inline">\(T_i = \{0, 1, 2, ... N + K + 1\}\)</span>. </p>
<p>The same <strong>Cox-de Boor recursion</strong> formula also applies to <strong>Non-Uniform B-splines</strong> if choosing <strong>Non-Cardinal</strong> intervals.</p>
<p>Also, we showed a <strong>Uniform Cubic B-spline</strong> with N=3 and K=3 expressed in the <strong>4th order</strong>:</p>
<p><span class="math display">\[
B(t) = c_0\phi_{0,3}(t) + c_0\phi_{1,3}(t) + c_0\phi_{2,3}(t) + c_0\phi_{3,3}(t)
\]</span></p>
<p>As another example, a <strong>Uniform Quartic B-Spline</strong> with N=6 and K=4 parameters is expressed as such:</p>
<p><span class="math display">\[\begin{align}
B(t) = \sum_{i=0}^{n=6} c_i \phi_{i,k=4}(t)
\end{align}\]</span></p>
<p>which expands into a <strong>5th-order</strong> (k+1) B-spline formula:</p>
<p><span class="math display">\[
B(t) = c_0\phi_{0,4}(t) + c_1\phi_{1,4}(t) + c_2\phi_{2,4}(t) + c_3\phi_{3,4}(t) + c_4\phi_{4,4}(t) + c_5\phi_{5,4}(t) + c_6\phi_{6,4}(t) 
\]</span></p>
<p>with <strong>seven coefficients (control points)</strong> and a <strong>knot vector</strong>:</p>
<p><span class="math display">\[\begin{align*}
c_{i} {}&amp;= \{c_0, c_1, c_2, c_3, c_4, c_5, c_{k=6}\} \\
\\
T &amp;= \{t_0, t_1, t_2, t_3, t_4, t_5, t_6, t_7, t_8, t_9, t_{n+k+1 = 10}\}
\end{align*}\]</span></p>
<p>For this, it is best to use computer software libraries in a language of choice (e.g., R, python, java, etc.)</p>
<p><strong>BÃ©zier Curve:</strong> </p>
<p>A <strong>BÃ©zier</strong> curve is a special <strong>B-spline</strong> curve with restriction. The number of control points (N) matches the degree of the polynomial (K). For example:</p>
<p><span class="math display">\[\begin{align}
B(t) = \sum_{i=0}^{n=3} c_i \phi_{i, k=3}(t)
\end{align}\]</span></p>
<p>with the <strong>Bernstein polynomial</strong> formula as our <strong>basis function</strong>: </p>
<p><span class="math display">\[\begin{align}
\phi_{i,k}(t) = \binom{n}{k} t^i(1 - t)^{n-i}
\end{align}\]</span></p>
<p>In Figure <a href="numericallinearalgebra.html#fig:deboorrecurse1">3.29</a>, there are four coefficients in the B-spline of order four. Adjusting any one of the coefficients does not greatly affect the neighboring curves - this makes the coefficients <strong>a local control point</strong>. Let us change the coefficient of the 2nd-order (2nd term) from +2 to 0 and observe the outcome in Figure <a href="numericallinearalgebra.html#fig:deboorrecurse2">3.30</a>, comparing that with Figure <a href="numericallinearalgebra.html#fig:deboorrecurse1">3.29</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:deboorrecurse2"></span>
<img src="embed0019.png" alt="B-Spline (Local Control)" width="70%" />
<p class="caption">
Figure 3.30: B-Spline (Local Control)
</p>
</div>

<p>On the other hand, <strong>BÃ©zier</strong> curves are controlled by <strong>global control points</strong>, making those control points somehow restricted.</p>
<p>This book does not cover <strong>BÃ©zier</strong> curves in detail, given that we have covered <strong>B-spline</strong> curves as a generalization of <strong>BÃ©zier</strong> curves. However, for reference, it helps to study <strong>De Casteljauâs algorithm</strong> and <strong>Bernstein polynomial</strong> <span class="citation">(Prautzsch H. et al. <a href="bibliography.html#ref-ref356h">2002</a>)</span>.</p>
</div>
<div id="bspline" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.11</span> B-Spline Regression<a href="numericallinearalgebra.html#bspline" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Based on our discussions about B-splines, we now recognize four knobs we use to influence the curvature of B-splines when interpolating through all the data points. </p>
<ul>
<li>the coefficients</li>
<li>the number of knots</li>
<li>the placements of the knots</li>
<li>the basis function</li>
</ul>
<p>In <strong>B-Spline Regression</strong>, we deal with approximation. Here we perform curve fitting. We also need to consider how to tune the knobs above to fit well. As for the number of knots, a <strong>large number</strong> of knots tend to <strong>overfit</strong> the curve, and a <strong>small number</strong> of knots tend to <strong>underfit</strong> it.</p>
<p>Now, recall <strong>residual sum of squares(RSS)</strong> in <strong>linear regression</strong>. Similarly, our method is to look for RSS amongst a system of b-spline equations, then <strong>minimize the objective</strong> - that is, determine the minimum RSS to find the best fit for a curve.</p>
<p>For a <strong>uniform cubic b-spline</strong>, we assume our spline to be approximate by considering some <strong>noise</strong>, <span class="math inline">\(\epsilon\)</span>. For example:</p>
<p><span class="math display">\[\begin{align}
B(t) {}&amp;=c_0\phi_{0,3}(t) + c_1\phi_{1,3}(t) + c_2\phi_{2,3}(t) + c_3\phi_{3,3}(t) \\
\nonumber \\
\hat{B}(t) &amp;= B(t) + \epsilon
\end{align}\]</span></p>
<p>Therefore, our <strong>residual</strong> looks like so:</p>
<p><span class="math display">\[
\epsilon = \hat{B}(t) - B(t) 
\]</span></p>
<p>and our objective function to minimize becomes:</p>
<p><span class="math display">\[\begin{align}
RSS = \sum_{i=1}^k |\hat{B}(t) - B(t)|^2 = \sum_{i=1}^k |\epsilon |^2
\end{align}\]</span></p>
<p>Another way to look at <strong>residual</strong> is the delta change between the actual value of coefficients and the approximate value of coefficients (in this case, excluding noise, <span class="math inline">\(\epsilon\)</span>).</p>
<p>where:</p>
<p><span class="math display">\[\begin{align*}
\hat{B}(t) {}&amp;= \hat{c_0}\phi_{0,3}(t) + \hat{c_1}\phi_{1,3}(t) + \hat{c_2}\phi_{2,3}(t) + \hat{c_3}\phi_{3,3}(t)  
\\
\\
\hat{c} &amp;= \{ \hat{c_0}, \hat{c_1}, \hat{c_2}, ..., \hat{c_n} \} \leftarrow\ \text{find optimal coefficients}
\end{align*}\]</span></p>
<p>Recall the formula below (See Equation <span class="math inline">\(\ref{eqn:eqnnumber3}\)</span>):</p>
<p><span class="math display">\[
\hat{x} = (A^T \cdotp A)^{-1} \cdotp A^T \cdotp y
\]</span></p>
<p>where, this time, A is <strong>a matrix of basis functions</strong> and <span class="math inline">\(\hat{x}\)</span> is a vector of <strong>optimal coefficients</strong>, both form part in the below equation:</p>
<p><span class="math display">\[\begin{align}
\hat{c} = (\phi{i,j}(t)^T \cdotp \phi{i,j}(t))^{-1} \cdotp \phi{i,j}(t)^T \cdotp S \rightarrow\ \ \ \  
\hat{c} = (B^T \cdotp B)^{-1} \cdotp B^T \cdotp S
\end{align}\]</span></p>
<p>In solving for the system of equations using <strong>matrix computation</strong> for <span class="math inline">\(\hat{c}\)</span>, we end up with the optimal spline for the best fit:</p>
<p><span class="math display">\[\begin{align}
\hat{S}(t) = \sum_{i=0}^k \hat{c_i} \phi_{i,k}(t)
\end{align}\]</span></p>
<p><strong>Interpretability of coefficients</strong></p>
<p>Recall that <strong>B-spline</strong> is a curve function consisting of a linear combination of coefficients and polynomials (basis functions). The critical point for a âB-splineâ is the coefficientsâ context. For these control points to be <strong>interpretable</strong>, it matters most to know that the coefficients can be treated as flexible control points. For example, being scalar to basis functions may not be enough. Some simple <strong>weight</strong> against the basis functions may be required to serve as <strong>regularizers</strong> that enforce penalty or reward. Either way, they add extra <strong>weight</strong> to the coefficients and add some nudge of constraint against the basis functions.</p>
<p>Here, suppose we replace the <strong>coefficients</strong> with the <strong>weight</strong> function so that the formula below:</p>
<p><span class="math display">\[\begin{align}
B(t) = \sum_{i=0}^{n} c_i \phi_{i,k}(t)
\end{align}\]</span></p>
<p>is described into the following more flexible form:</p>
<p><span class="math display">\[\begin{align}
B(t) = \sum_{i=0}^{n} \omega_{i}(c_i) \phi_{i,k}(t)
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\omega_{i}(c_i)\ \text{is a weight function of any kind}
\]</span></p>
<p>From here, we can describe the <strong>weight</strong> functions in ways more contextual (or more interpretable) for the coefficients, and thus the implementation then varies. For example, when dealing with <strong>weights</strong>, we can impose a <strong>brute force</strong> approach or <strong>trial and error</strong> approach to <strong>approximate the weight</strong> and then estimate the <strong>Least-Square</strong> between the actual values and approximate values of the weights, both of which may be costly. Another approach is to <strong>approximate the likelihood</strong> of the weights by <strong>random sampling</strong>. This is where we talk about <strong>Bayesian methods</strong>, which we cover in Volume II of this book.</p>
</div>
<div id="p-spline-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.7.12</span> P-Spline Regression <a href="numericallinearalgebra.html#p-spline-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>P-spline regression</strong> is a <strong>âPenalized B-spline regressionâ</strong>. Here, <strong>penalty</strong> is a way to smoothen the roughness of spline curves.</p>
<p><strong>Smoothness of splines</strong></p>
<p>If we look carefully at our <strong>optimal coefficients</strong> in previous section, we have the following (See Equation <span class="math inline">\(\ref{eqn:eqnnumber3}\)</span>):</p>
<p><span class="math display">\[\begin{align}
\hat{c} = (B^T \cdotp B)^{-1} \cdotp B^T \cdotp S
\end{align}\]</span></p>
<p>One way to optimize the smoothness of curves is to add penalty, <span class="math inline">\(\lambda\)</span>, into the equation as such:</p>
<p><span class="math display">\[\begin{align}
\hat{c}_{penalized} = (B^T \cdotp B + \lambda D_k^TD_k)^{-1} \cdotp B^T \cdotp S
\end{align}\]</span></p>
<p>where <span class="math inline">\(D\)</span> is the matrix representation of the difference, <span class="math inline">\(\Delta_k\)</span>.</p>
<p>Here, we mention two ways to add a penalty, <span class="math inline">\(\lambda\)</span>, to the objective function in terms of <strong>residuals</strong>:</p>
<p>The <strong>Eilers-Marx</strong> method (Eilers and Marx - 1996): </p>
<p><span class="math display">\[\begin{align}
RSS_{penalty} = \sum_{i=1}^m |\hat{B}(t) - B(t)|^2 + \lambda \sum_{j=k+1}^m \left( \Delta^k c_j \right)^2 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\lambda &gt; 0\)</span> and <span class="math inline">\(\Delta^kc_i\)</span> is backward difference for <span class="math inline">\(\Delta{ci} = c_i - c_{i-1}\)</span>.</p>
<p>The <strong>Wand-Ormerod</strong> method (Wand and Ormerod - 2008): </p>
<p><span class="math display">\[\begin{align}
RSS_{penalty} = \sum_{i=1}^m |\hat{B}(t) - B(t)|^2 + \lambda \int_{min\ t}^{ma\ t} \left( \hat{B}&#39;&#39;(t;k) \right)^2 dx
\end{align}\]</span></p>
<p>where k in <span class="math inline">\(\hat{B}&#39;&#39;(t;k)\)</span> is the k-th order of the B-spline.</p>
<p>We leave readers to investigate more on those penalty approaches.</p>
</div>
</div>
<div id="polynomialsmoothing" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.8</span> Approximating Polynomial Functions by Smoothing<a href="numericallinearalgebra.html#polynomialsmoothing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have partially covered polynomial smoothing under the <strong>P-spline regression</strong> section. In this section, we explain polynomial smoothing methods. The main focus of our discussion is around LOWESS and LOESS; however, it helps to have a prior understanding of concepts that complement them. </p>
<div id="bin-smoothing" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.8.1</span> Bin Smoothing <a href="numericallinearalgebra.html#bin-smoothing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to <strong>piecewise techniques</strong> used in <strong>B-spline interpolation</strong>, the idea of <strong>bin smoothing</strong> is to discretize data into <strong>buckets</strong> called <strong>bins</strong>. There are different methods to perform <strong>Bin Smoothing</strong>.</p>
<p><strong>First</strong>, let us start with the following polynomial data:</p>

<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb118-2" data-line-number="2">sample_size =<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb118-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">30</span> </a>
<a class="sourceLine" id="cb118-4" data-line-number="4">sample.poly =<span class="st"> </span><span class="kw">poly</span>(<span class="dv">1</span><span class="op">:</span>sample_size, <span class="dt">degree=</span><span class="dv">3</span>, <span class="dt">simple=</span><span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb118-5" data-line-number="5">data =<span class="st"> </span>sample.poly[,<span class="dv">3</span>] <span class="op">+</span><span class="st"> </span>e <span class="co"># add Gauss. residual using 3rd degree poly</span></a>
<a class="sourceLine" id="cb118-6" data-line-number="6"><span class="kw">names</span>(data ) =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, sample_size)</a>
<a class="sourceLine" id="cb118-7" data-line-number="7"><span class="kw">summary</span>(data)</a></code></pre></div>
<pre><code>##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -0.2326270 -0.0663426  0.0068862 -0.0001126  0.0624455  0.2035865</code></pre>

<p><strong>Second</strong>, choose a window size.</p>
<p>The following formulas are available to determine the window size:</p>
<p><span class="math display">\[\begin{align*}
W_{size}{}&amp;= 1 + 3.322 log N\ \ &amp; \text{Sturge&#39;s Rule} \\
W_{size} &amp;= 2(IQR)n^{-1/3}\ \ &amp; \text{Freedman-Diaconis&#39; Rule} \\
W_{size} &amp;= 3.49\sigma n^{-1/3}\ \ &amp; \text{Scott&#39;s Rule} \\
W_{size} &amp;= (\text{sample size})^{1/3} \times 2\ \ &amp; \text{Rice&#39;s Rule} 
\end{align*}\]</span>
 
 </p>
<p>Here, we use <strong>Riceâs rule</strong>.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1">(<span class="dt">window_size =</span> sample_size<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)<span class="op">*</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 11.69607</code></pre>
<p><strong>Third</strong>, choose a partitioning method.</p>
<p><strong>Partitioning methods</strong>:</p>
<ul>
<li>By equal <strong>width (bandwidth)</strong> - note here that <strong>width</strong> is an interval and also can be called <strong>window size</strong> or <strong>span</strong>. This works best for ordered (sorted data); thus, it may not be ideal for the smoothing we need. Nonetheless, here is an example naive implementation of binning by width.</li>
</ul>

<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1">sample.data =<span class="kw">c</span> (<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">12</span>,<span class="dv">18</span>,<span class="dv">20</span>,<span class="dv">23</span>,<span class="dv">50</span>,<span class="dv">61</span>,<span class="dv">70</span>)</a>
<a class="sourceLine" id="cb122-2" data-line-number="2">bin_count =<span class="st"> </span><span class="kw">floor</span>(<span class="kw">length</span>(sample.data)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)<span class="op">*</span><span class="dv">2</span>) <span class="co"># Rice&#39;s rule</span></a>
<a class="sourceLine" id="cb122-3" data-line-number="3">smallest =<span class="st"> </span><span class="kw">min</span>(sample.data); largest =<span class="st"> </span><span class="kw">max</span>(sample.data)</a>
<a class="sourceLine" id="cb122-4" data-line-number="4">interval =<span class="st">  </span><span class="kw">ceiling</span>((largest <span class="op">-</span><span class="st"> </span>smallest) <span class="op">/</span><span class="st"> </span>bin_count)</a>
<a class="sourceLine" id="cb122-5" data-line-number="5">ordered.data =<span class="st"> </span><span class="kw">sort</span>(sample.data, <span class="dt">decreasing=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb122-6" data-line-number="6">start =<span class="st"> </span>smallest</a>
<a class="sourceLine" id="cb122-7" data-line-number="7">bins =<span class="st"> </span>ordered.data</a>
<a class="sourceLine" id="cb122-8" data-line-number="8">intervals =<span class="st"> </span>ordered.data</a>
<a class="sourceLine" id="cb122-9" data-line-number="9"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>bin_count) {</a>
<a class="sourceLine" id="cb122-10" data-line-number="10">   end =<span class="st"> </span>start <span class="op">+</span><span class="st"> </span>interval</a>
<a class="sourceLine" id="cb122-11" data-line-number="11">   idx =<span class="st"> </span><span class="kw">which</span>(ordered.data <span class="op">&gt;=</span><span class="st"> </span>start  <span class="op">&amp;</span><span class="st"> </span>ordered.data <span class="op">&lt;=</span><span class="st"> </span>end)</a>
<a class="sourceLine" id="cb122-12" data-line-number="12">   intervals[idx] =<span class="st">   </span><span class="kw">paste0</span>(start,<span class="st">&quot;-&quot;</span>,end)</a>
<a class="sourceLine" id="cb122-13" data-line-number="13">   bins[idx] =<span class="st"> </span>i</a>
<a class="sourceLine" id="cb122-14" data-line-number="14">   start =<span class="st"> </span>end <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb122-15" data-line-number="15">}</a>
<a class="sourceLine" id="cb122-16" data-line-number="16">binned.data =<span class="st"> </span><span class="kw">rbind</span>(intervals , ordered.data)</a>
<a class="sourceLine" id="cb122-17" data-line-number="17">binned.data =<span class="st"> </span><span class="kw">rbind</span>(binned.data , bins)</a>
<a class="sourceLine" id="cb122-18" data-line-number="18">binned.data[,<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>]</a></code></pre></div>
<pre><code>##              [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]    [,8]   
## intervals    &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;22-39&quot; &quot;40-57&quot;
## ordered.data &quot;4&quot;    &quot;8&quot;    &quot;9&quot;    &quot;12&quot;   &quot;18&quot;   &quot;20&quot;   &quot;23&quot;    &quot;50&quot;   
## bins         &quot;1&quot;    &quot;1&quot;    &quot;1&quot;    &quot;1&quot;    &quot;1&quot;    &quot;1&quot;    &quot;2&quot;     &quot;3&quot;</code></pre>

<ul>
<li>By equal <strong>depth (frequency)</strong> - note that each window contains an equal number of data points. The idea is to slice the dataset into an equal number of data points. Let us use this partition for our illustration.</li>
</ul>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1">bin_count =<span class="st"> </span><span class="kw">ceiling</span>(<span class="kw">length</span>(data) <span class="op">/</span><span class="st"> </span>window_size)</a>
<a class="sourceLine" id="cb124-2" data-line-number="2">bins =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>bin_count, <span class="kw">rep</span>(window_size, bin_count))[<span class="dv">1</span><span class="op">:</span>sample_size]</a>
<a class="sourceLine" id="cb124-3" data-line-number="3">binned.data =<span class="st"> </span><span class="kw">rbind</span>(data, bins)</a>
<a class="sourceLine" id="cb124-4" data-line-number="4">binned.data[,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</a></code></pre></div>
<pre><code>##               1          2          3          4         5
## data -0.1689877 -0.1605539 -0.1965348 -0.1872164 -0.232627
## bins  1.0000000  1.0000000  1.0000000  1.0000000  1.000000</code></pre>
<p><strong>Fourth</strong>, choose a smoothing method.</p>
<p>Before that, let us add three rows to accommodate the three smoothing methods.</p>

<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" data-line-number="1">mean =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,sample_size)</a>
<a class="sourceLine" id="cb126-2" data-line-number="2">median =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,sample_size)</a>
<a class="sourceLine" id="cb126-3" data-line-number="3">boundary =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,sample_size)</a>
<a class="sourceLine" id="cb126-4" data-line-number="4">smooth.data =<span class="st"> </span><span class="kw">rbind</span>(binned.data, mean )</a>
<a class="sourceLine" id="cb126-5" data-line-number="5">smooth.data =<span class="st"> </span><span class="kw">rbind</span>(smooth.data, median )</a>
<a class="sourceLine" id="cb126-6" data-line-number="6">smooth.data =<span class="st"> </span><span class="kw">rbind</span>(smooth.data, boundary )</a>
<a class="sourceLine" id="cb126-7" data-line-number="7">smooth.data[,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</a></code></pre></div>
<pre><code>##                   1          2          3          4         5
## data     -0.1689877 -0.1605539 -0.1965348 -0.1872164 -0.232627
## bins      1.0000000  1.0000000  1.0000000  1.0000000  1.000000
## mean      0.0000000  0.0000000  0.0000000  0.0000000  0.000000
## median    0.0000000  0.0000000  0.0000000  0.0000000  0.000000
## boundary  0.0000000  0.0000000  0.0000000  0.0000000  0.000000</code></pre>

<p><strong>Smoothing methods</strong>:</p>
<ul>
<li>By median, mean - values in a bin are replaced with the mean (or median) of the bin.</li>
</ul>

<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" data-line-number="1">bin.means  =<span class="st"> </span><span class="kw">aggregate</span>(binned.data[<span class="dv">1</span>,], <span class="dt">by=</span><span class="kw">list</span>(binned.data[<span class="dv">2</span>,]), mean)</a>
<a class="sourceLine" id="cb128-2" data-line-number="2">bin.median =<span class="st"> </span><span class="kw">aggregate</span>(binned.data[<span class="dv">1</span>,], <span class="dt">by=</span><span class="kw">list</span>(binned.data[<span class="dv">2</span>,]), </a>
<a class="sourceLine" id="cb128-3" data-line-number="3">                       median)</a></code></pre></div>

<ul>
<li>By boundary - observations in a bin are replaced with the min or max value of the bin, depending on which boundary is closest.</li>
</ul>

<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb129-1" data-line-number="1">boundary &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb129-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb129-3" data-line-number="3">  a =<span class="st"> </span><span class="kw">min</span>(x);  b =<span class="st"> </span><span class="kw">max</span>(x)</a>
<a class="sourceLine" id="cb129-4" data-line-number="4">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb129-5" data-line-number="5">    dist_a =<span class="st"> </span><span class="kw">abs</span>(a <span class="op">-</span><span class="st"> </span>x[i])</a>
<a class="sourceLine" id="cb129-6" data-line-number="6">    dist_b =<span class="st"> </span><span class="kw">abs</span>(b <span class="op">-</span><span class="st"> </span>x[i])</a>
<a class="sourceLine" id="cb129-7" data-line-number="7">    <span class="cf">if</span> (dist_a <span class="op">&lt;</span><span class="st"> </span>dist_b ) { x[i] =<span class="st"> </span>a } <span class="cf">else</span> {x[i] =<span class="st"> </span>b}</a>
<a class="sourceLine" id="cb129-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb129-9" data-line-number="9">  x</a>
<a class="sourceLine" id="cb129-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb129-11" data-line-number="11">bin.boundary =<span class="st"> </span><span class="kw">aggregate</span>(binned.data[<span class="dv">1</span>,], <span class="dt">by=</span><span class="kw">list</span>(binned.data[<span class="dv">2</span>,]), </a>
<a class="sourceLine" id="cb129-12" data-line-number="12">                         boundary)</a></code></pre></div>

<p><strong>Fifth</strong>, populate bins with new values.</p>

<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" data-line-number="1"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>bin_count) {</a>
<a class="sourceLine" id="cb130-2" data-line-number="2">  idx =<span class="st"> </span><span class="kw">which</span>(smooth.data[<span class="dv">2</span>,] <span class="op">==</span><span class="st"> </span>i)</a>
<a class="sourceLine" id="cb130-3" data-line-number="3">  smooth.data[<span class="dv">3</span>, idx] =<span class="st"> </span>bin.means[i,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb130-4" data-line-number="4">  smooth.data[<span class="dv">4</span>, idx] =<span class="st"> </span>bin.median[i,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb130-5" data-line-number="5">  smooth.data[<span class="dv">5</span>, idx] =<span class="st"> </span>bin.boundary[i,<span class="dv">2</span>][[<span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb130-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb130-7" data-line-number="7">smooth.data[,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</a></code></pre></div>
<pre><code>##                   1          2          3          4          5
## data     -0.1689877 -0.1605539 -0.1965348 -0.1872164 -0.2326270
## bins      1.0000000  1.0000000  1.0000000  1.0000000  1.0000000
## mean     -0.1366121 -0.1366121 -0.1366121 -0.1366121 -0.1366121
## median   -0.1182712 -0.1182712 -0.1182712 -0.1182712 -0.1182712
## boundary -0.2326270 -0.2326270 -0.2326270 -0.2326270 -0.2326270</code></pre>

<p><strong>Sixth</strong>, create a separate curve line for the groups (use the average).</p>

<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" data-line-number="1">group.means =<span class="st"> </span><span class="kw">length</span>(bin.means[,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb132-2" data-line-number="2">curve.fit =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb132-3" data-line-number="3">location =<span class="st"> </span><span class="dv">0</span>; step =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb132-4" data-line-number="4"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(group.means)) {</a>
<a class="sourceLine" id="cb132-5" data-line-number="5">   idx =<span class="st"> </span><span class="kw">which</span>(smooth.data[<span class="dv">2</span>,] <span class="op">==</span><span class="st"> </span>i)</a>
<a class="sourceLine" id="cb132-6" data-line-number="6">   bin  =<span class="st"> </span>smooth.data[<span class="dv">3</span>, idx]</a>
<a class="sourceLine" id="cb132-7" data-line-number="7">   center =<span class="st"> </span><span class="kw">round</span>( <span class="kw">length</span>(bin) <span class="op">/</span><span class="st"> </span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb132-8" data-line-number="8">   location =<span class="st"> </span>step <span class="op">+</span><span class="st"> </span>center</a>
<a class="sourceLine" id="cb132-9" data-line-number="9">   step =<span class="st"> </span>step <span class="op">+</span><span class="st"> </span><span class="kw">length</span>(bin)</a>
<a class="sourceLine" id="cb132-10" data-line-number="10">   curve.fit =<span class="st"> </span><span class="kw">cbind</span>(curve.fit, location)</a>
<a class="sourceLine" id="cb132-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb132-12" data-line-number="12">smooth.fit =<span class="st"> </span><span class="kw">as.numeric</span>(curve.fit)</a></code></pre></div>

<p><strong>Finally</strong>, let us plot. See <a href="numericallinearalgebra.html#fig:binsmoothing">3.31</a>.</p>

<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">1</span>,sample_size), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="fl">0.3</span>,<span class="fl">0.2</span>), </a>
<a class="sourceLine" id="cb133-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>,</a>
<a class="sourceLine" id="cb133-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Bin Smoothing&quot;</span>)</a>
<a class="sourceLine" id="cb133-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb133-5" data-line-number="5"><span class="kw">points</span>(data, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>)</a>
<a class="sourceLine" id="cb133-6" data-line-number="6"><span class="co">#lines(smooth.data[4,], col=&quot;magenta&quot;, pch=16) # median</span></a>
<a class="sourceLine" id="cb133-7" data-line-number="7"><span class="kw">lines</span>(smooth.data[<span class="dv">5</span>,], <span class="dt">col=</span><span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>) <span class="co"># boundary</span></a>
<a class="sourceLine" id="cb133-8" data-line-number="8"><span class="kw">lines</span>(smooth.data[<span class="dv">3</span>,], <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">lwd=</span><span class="dv">2</span>) <span class="co"># mean</span></a>
<a class="sourceLine" id="cb133-9" data-line-number="9"><span class="kw">points</span>(smooth.fit, bin.means[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb133-10" data-line-number="10"><span class="kw">lines</span>(smooth.fit, bin.means[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb133-11" data-line-number="11"><span class="kw">legend</span>(<span class="dv">40</span>, <span class="fl">-0.1</span>, </a>
<a class="sourceLine" id="cb133-12" data-line-number="12">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;boundary&quot;</span>, <span class="st">&quot;mean&quot;</span>,  <span class="st">&quot;smooth.fit&quot;</span>),</a>
<a class="sourceLine" id="cb133-13" data-line-number="13">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;dodgerblue&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>),</a>
<a class="sourceLine" id="cb133-14" data-line-number="14">           , <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:binsmoothing"></span>
<img src="embed0020.png" alt="Bin Smoothing" width="70%" />
<p class="caption">
Figure 3.31: Bin Smoothing
</p>
</div>

<p>As an extension to our previous discussion around <strong>polynomial regression</strong> and our recent discussion around <strong>bin smoothing</strong>, here, we introduce two <strong>smoothing techniques</strong>: <strong>LOWESS</strong> and <strong>LOESS</strong>, which use <strong>local weighted regression</strong> techniques <span class="citation">(Cleveland, W. S. et al. <a href="bibliography.html#ref-ref373c">1988</a>)</span>.</p>
<p><strong>LOWESS</strong> and <strong>LOESS</strong> are polynomial smoothers that use locally weighted (linear) functions to fit a <strong>smooth</strong> model to each bin. <strong>LOWESS</strong> stands for <strong>locally weighted scatterplot smoothing</strong> and <strong>LOESS</strong> stands for <strong>locally estimated scatterplot smoothing</strong>.  </p>
<p><strong>LOWESS</strong> uses the following two weighing functions:</p>
<ul>
<li>A <strong>tricube weight function</strong> for width (distance amongst neighboring points) <span class="citation">(Cleveland, W. S. et al. <a href="bibliography.html#ref-ref373c">1988</a>)</span> (See also <strong>Kernel Smoothing</strong> for the tricube kernel):</li>
</ul>
<p><span class="math display">\[\begin{align}
W_{(width)}(w) = \begin{cases}
\left[1 - |w|^3 \right]^3 &amp; |w| &lt; 1\\
0 &amp; |w| \ge 1
\end{cases}\ \ \ \ \ \ \ \ \
where\ \  
w = w_k(x_i) = \frac{x_i - x_k}{d_k},\ \ \ \label{eqn:eqnnumber704}
\end{align}\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{d_k}\)</span> is kth nearest neighbor (kth smallest distance) derived from a list of <strong>x</strong> distances arranged in ascending order, e.g. <span class="math inline">\(d_i = \{ |x_i - x_1|, |x_i - x_2|,|x_i - x_3|,...,|x_i - x_n| \}_{sort}\)</span>.</p>
<p><span class="math display">\[\begin{align}
d_k = knn(d_i^{(sort)}, kth),\ \ \ \leftarrow\ \ \ \ \ kth = f\times n
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>n</strong> is the number of data points</li>
<li><strong>f</strong> is a proportion of <strong>n</strong>.</li>
</ul>
<p>Note that by setting the weight to zero when <span class="math inline">\(|w| \ge 1\)</span>, it eliminates the influence of distant neighbors.</p>
<ul>
<li>An optional <strong>bisquare weight function</strong> for depth (to control the influence of outliers) - this is also called <strong>robust weighing</strong>:</li>
</ul>
<p><span class="math display">\[\begin{align}
W_{(depth)}(w) = \begin{cases}
\left[1 - |w|^2 \right]^2 &amp; |w| &lt; 1\\
0 &amp; |w| \ge 1
\end{cases} \label{eqn:eqnnumber705}
\end{align}\]</span></p>
<p><span class="math display">\[
where\ \ 
w = w_k(x_i) = \frac{e_k}{6 \times median(|e_1|, ...,|e_n|)}
\]</span></p>
<p>The <strong>local weight</strong> becomes:</p>
<p><span class="math display">\[\begin{align}
W^{local}_k = W_{(width)}(w_k) W_{(depth)}(w_k) 
\end{align}\]</span></p>
<p>Note that by setting the weight to zero when <span class="math inline">\(|w| \ge 1\)</span>, it eliminates the influence of outliers.</p>
<p>Now, recall our discussion around <strong>OLS</strong> in the <strong>polynomial regression</strong> section.</p>
<p><span class="math display">\[\begin{align}
RSS(\beta) &amp;= \sum_{k=1}^n | y_k - \hat{y}_k |^2  = \sum_{k=1}^n |\epsilon_k|^2 \\
\text{1st degree or multivariate} &amp;= \sum_{k=1}^n  \left| y_k - \left(\beta_0 + \sum_{j=1}^m \beta_j x_{j,k} \right) \right| ^2 \\
\text{higher degree} &amp;= \sum_{k=1}^n \left| y_k - \left(\beta_0 + \sum_{j=1}^m \beta_j x_k^j \right) \right| ^2
\end{align}\]</span></p>
<p>where <strong>RSS</strong> is residual sum square.  </p>
<p>Here, we inject the <strong>local weight</strong> into <strong>RSS</strong>. The equation becomes:</p>
<p><span class="math display">\[\begin{align}
WSS(\beta, w_k)  &amp;= \sum_{i=1}^n w_k | y_k - \hat{y}_k |^2  = \sum_{k=1}^n w_k |\epsilon_k|^2 \\
\text{1st degree or multivariate} &amp;= \sum_{k=1}^n w_k \left| y_k - \left(\beta_0 + \sum_{j=1}^m \beta_j x_{j,k} \right) \right| ^2 \\
\text{higher degree} &amp;= \sum_{k=1}^n w_k \left| y_k - \left(\beta_0 + \sum_{j=1}^m \beta_j x_k^j \right) \right| ^2
\end{align}\]</span></p>
<p>where <strong>WSS</strong> is weighted residual sum square and <span class="math inline">\(\mathbf{w_k}\)</span> is our local weight, <span class="math inline">\(W^{(local)}_k\)</span>.  </p>
<p>By performing <strong>weighted least squares</strong>, we generate a vector of <strong>coefficients</strong> that are weighted - we call them <strong>beta hats</strong>, <span class="math inline">\(\hat{\beta}\)</span>:</p>
<p>For first degree equations, we derive the coefficients (beta hats) using the following formulation:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta}_1 
 =  \frac{n\sum_{k=1}^n{(x_ky_k)} - \sum_{k=1}^n{x_k}\sum_{k=1}^n{y_k}}{n\sum_{k=1}^n{(x_k^2)} - (\sum_{k=1}^n{x_k})^2} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
\end{align}\]</span></p>
<p>We then modify the <span class="math inline">\(\beta\)</span> formulae to inject the weighted function:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta}_1 =  \frac{\sum_{k=1}^n (w_k  x_k y_k) - \bar{x} \cdot \bar{y}\sum_{k=1}^n w_k}
{\sum_{k=1} (w_k x_k^2) - \bar{x}^2 \cdot \sum_{k=1}^n w_k}
 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
\bar{x} = \frac{\sum_{k=1}^n(w_k x_k)}{\sum_{k=1}^n w_k}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\bar{y} = \frac{\sum_{k=1}^n(w_k y_k)}{\sum_{k=1}^n w_k}
\end{align}\]</span></p>
<p>For multivariate equations and non-linear (e.g., quadratic) equations, recall the use of a Vandermonde matrix - for our matrix equation - under the <strong>polynomial regression</strong> section.</p>
<p><span class="math display">\[\begin{align}
\hat{\beta} \approx (A^T \cdot A)^{-1} \cdot A^T \cdot y\ \ \ \ \leftarrow\ \ \ \ \ \ y = \hat{\beta}^TA
\end{align}\]</span></p>
<p>We modify the equation to include the weighted function:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta} \approx (A^T \cdot W \cdot A)^{-1} \cdot A^T \cdot  W  \cdot y
\end{align}\]</span></p>
<p>Finally, we can perform local weighted linear regression to our data points.</p>
<p>For <strong>LOWESS</strong>, we use a simple linear equation for regression: <span class="math inline">\(y_k = \beta_0 + \beta_1 x_k\)</span>.</p>
<p><span class="math display">\[\begin{align}
WSS(\beta, W_k^{(local)}) = \sum_{k=1}^n W_k^{local}(y_k - (\beta_0 + \beta_1 x_k))
\end{align}\]</span></p>
<p>For <strong>LOESS</strong>, we also can use a quadratic (parabolic) equation for regression: <span class="math inline">\(y_k = \beta_0 + \beta_1 x_k + \beta_2 x_k^2\)</span>.</p>
<p><span class="math display">\[\begin{align}
WSS(\beta, W_k^{(local)}) = \sum_{k=1}^n W_k^{local}(y_k - (\beta_0 + \beta_1 x_k + \beta_2 x_k^2))
\end{align}\]</span></p>
<p>Note that <strong>LOESS</strong> is a generalized polynomial version of <strong>LOWESS</strong>. For that reason, we use the normal matrix equation.</p>
<p>Let us illustrate:</p>
<p><strong>First</strong>, let us create a distance table using the built-in R function called <strong>dist()</strong>:</p>

<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb134-2" data-line-number="2">sample_size =<span class="st"> </span>n =<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb134-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">30</span> </a>
<a class="sourceLine" id="cb134-4" data-line-number="4">sample.poly =<span class="st"> </span><span class="kw">poly</span>(<span class="dv">1</span><span class="op">:</span>sample_size, <span class="dt">degree=</span><span class="dv">3</span>, <span class="dt">simple=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb134-5" data-line-number="5">y =<span class="st"> </span>sample.poly[,<span class="dv">3</span>] <span class="op">+</span><span class="st"> </span>e <span class="co"># add Gausian residual using 3rd degree poly</span></a>
<a class="sourceLine" id="cb134-6" data-line-number="6">x =<span class="st"> </span><span class="kw">sort</span>( <span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">80</span>), <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb134-7" data-line-number="7">ymin =<span class="st"> </span><span class="kw">min</span>(y); ymax =<span class="st"> </span><span class="kw">max</span>(y); xmax =<span class="st"> </span><span class="kw">max</span>(x)</a>
<a class="sourceLine" id="cb134-8" data-line-number="8">D =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">abs</span>( <span class="kw">dist</span>(x, <span class="dt">upper=</span><span class="ot">TRUE</span>)))</a>
<a class="sourceLine" id="cb134-9" data-line-number="9"><span class="kw">colnames</span>(D) =<span class="st"> </span><span class="kw">paste0</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)), <span class="st">&quot;k&quot;</span>)</a>
<a class="sourceLine" id="cb134-10" data-line-number="10"><span class="kw">rownames</span>(D) =<span class="st"> </span><span class="kw">paste0</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)), <span class="st">&quot;i&quot;</span>)</a>
<a class="sourceLine" id="cb134-11" data-line-number="11">D[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]  <span class="co"># limit display to 10</span></a></code></pre></div>
<pre><code>##     1k 2k 3k 4k 5k 6k 7k 8k 9k 10k
## 1i   0  1  2  4  5  9 11 13 16  17
## 2i   1  0  1  3  4  8 10 12 15  16
## 3i   2  1  0  2  3  7  9 11 14  15
## 4i   4  3  2  0  1  5  7  9 12  13
## 5i   5  4  3  1  0  4  6  8 11  12
## 6i   9  8  7  5  4  0  2  4  7   8
## 7i  11 10  9  7  6  2  0  2  5   6
## 8i  13 12 11  9  8  4  2  0  3   4
## 9i  16 15 14 12 11  7  5  3  0   1
## 10i 17 16 15 13 12  8  6  4  1   0</code></pre>

<p><strong>Second</strong>, now use a fraction number for our range of nearest neighbors:</p>

<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" data-line-number="1">f =<span class="st"> </span><span class="fl">0.40</span></a>
<a class="sourceLine" id="cb136-2" data-line-number="2">n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb136-3" data-line-number="3"><span class="co"># kth nearest neighbor (for our kth smallest distance)</span></a>
<a class="sourceLine" id="cb136-4" data-line-number="4">(<span class="dt">kth =</span> <span class="kw">round</span>( f <span class="op">*</span><span class="st"> </span>n )) </a></code></pre></div>
<pre><code>## [1] 20</code></pre>

<p><strong>Third</strong>, let us compute for <strong>kth</strong> nearest distance:</p>

<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" data-line-number="1">knn &lt;-<span class="st"> </span><span class="cf">function</span>(D, kth) {</a>
<a class="sourceLine" id="cb138-2" data-line-number="2">  dk  =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb138-3" data-line-number="3">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb138-4" data-line-number="4">    sorted_dist =<span class="st"> </span><span class="kw">sort</span>(D[i,], <span class="dt">decreasing=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb138-5" data-line-number="5">    <span class="co">#  kth nearest neighbor (kth smallest distance)</span></a>
<a class="sourceLine" id="cb138-6" data-line-number="6">    dk [i] =<span class="st"> </span>sorted_dist[kth] </a>
<a class="sourceLine" id="cb138-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb138-8" data-line-number="8">  <span class="kw">names</span>(dk) =<span class="st"> </span><span class="kw">paste0</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)), <span class="st">&quot;i&quot;</span>)</a>
<a class="sourceLine" id="cb138-9" data-line-number="9">  dk</a>
<a class="sourceLine" id="cb138-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb138-11" data-line-number="11">(<span class="dt">dk =</span> <span class="kw">knn</span>(D,kth))[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]  <span class="co"># limit display to 10</span></a></code></pre></div>
<pre><code>##  1i  2i  3i  4i  5i  6i  7i  8i  9i 10i 
##  31  30  29  27  26  22  20  18  16  16</code></pre>

<p><strong>Fourth</strong>, compute for the weight (for width):</p>

<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb140-1" data-line-number="1">weight_width &lt;-<span class="st"> </span><span class="cf">function</span>(D, dk) {</a>
<a class="sourceLine" id="cb140-2" data-line-number="2">  w =<span class="st">  </span><span class="kw">abs</span>(D)  <span class="op">/</span><span class="st"> </span>dk</a>
<a class="sourceLine" id="cb140-3" data-line-number="3">  <span class="co"># by assigning 1, this is equivalent to 0 for ( 1 - W^3)^3 if W &gt;=1</span></a>
<a class="sourceLine" id="cb140-4" data-line-number="4">  w[w <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span> </a>
<a class="sourceLine" id="cb140-5" data-line-number="5">  <span class="co"># relaxed version of tricube</span></a>
<a class="sourceLine" id="cb140-6" data-line-number="6">  <span class="co"># without the 70/81 scale</span></a>
<a class="sourceLine" id="cb140-7" data-line-number="7">  (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>w<span class="op">^</span><span class="dv">3</span>)<span class="op">^</span><span class="dv">3</span> <span class="co"># tricube smoother</span></a>
<a class="sourceLine" id="cb140-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb140-9" data-line-number="9"><span class="co"># limit display to 10x10</span></a>
<a class="sourceLine" id="cb140-10" data-line-number="10">(<span class="dt">W =</span> <span class="kw">round</span>( <span class="kw">weight_width</span>(D, dk), <span class="dv">3</span>))[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]  </a></code></pre></div>
<pre><code>##        1k    2k    3k    4k    5k    6k    7k    8k    9k   10k
## 1i  1.000 1.000 0.999 0.994 0.987 0.928 0.872 0.795 0.642 0.582
## 2i  1.000 1.000 1.000 0.997 0.993 0.944 0.893 0.820 0.670 0.610
## 3i  0.999 1.000 1.000 0.999 0.997 0.958 0.913 0.845 0.699 0.640
## 4i  0.990 0.996 0.999 1.000 1.000 0.981 0.949 0.893 0.759 0.701
## 5i  0.979 0.989 0.995 1.000 1.000 0.989 0.964 0.915 0.790 0.733
## 6i  0.808 0.863 0.906 0.965 0.982 1.000 0.998 0.982 0.906 0.863
## 7i  0.579 0.670 0.751 0.877 0.921 0.997 1.000 0.997 0.954 0.921
## 8i  0.242 0.348 0.460 0.670 0.759 0.967 0.996 1.000 0.986 0.967
## 9i  0.000 0.005 0.036 0.193 0.308 0.769 0.911 0.980 1.000 0.999
## 10i 0.000 0.000 0.005 0.100 0.193 0.670 0.850 0.954 0.999 1.000</code></pre>

<p><strong>Fifth</strong>, perform local regression using the computed weights for width (close neighbors). Generate our fitted <strong>y</strong>:</p>

<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb142-1" data-line-number="1">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x), n, <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb142-2" data-line-number="2">B =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb142-3" data-line-number="3">y.hat =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb142-4" data-line-number="4">residual =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb142-5" data-line-number="5"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb142-6" data-line-number="6">  Wk =<span class="st"> </span><span class="kw">as.numeric</span>( W[,k] )  </a>
<a class="sourceLine" id="cb142-7" data-line-number="7">  Wk =<span class="st"> </span><span class="kw">diag</span>(Wk)</a>
<a class="sourceLine" id="cb142-8" data-line-number="8">  <span class="co"># least square</span></a>
<a class="sourceLine" id="cb142-9" data-line-number="9">  B[[k]] =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>Wk <span class="op">%*%</span><span class="st"> </span>A) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>Wk <span class="op">%*%</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb142-10" data-line-number="10">  beta =<span class="st"> </span>B[[k]]</a>
<a class="sourceLine" id="cb142-11" data-line-number="11">  y.fit =<span class="st"> </span>beta[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x[k] <span class="co"># linear fit</span></a>
<a class="sourceLine" id="cb142-12" data-line-number="12">  y.hat =<span class="st"> </span><span class="kw">c</span>(y.hat, y.fit)</a>
<a class="sourceLine" id="cb142-13" data-line-number="13">  residual =<span class="st"> </span><span class="kw">c</span>(residual, <span class="kw">abs</span>(y[k] <span class="op">-</span><span class="st"> </span>y.fit))</a>
<a class="sourceLine" id="cb142-14" data-line-number="14">}</a></code></pre></div>

<p><strong>Sixth</strong>, let us work on the <strong>robust weight</strong> for outliers by computing for the residuals, <strong>e</strong>:</p>

<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb143-1" data-line-number="1">weight_depth &lt;-<span class="st"> </span><span class="cf">function</span>(residual) {</a>
<a class="sourceLine" id="cb143-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(residual)</a>
<a class="sourceLine" id="cb143-3" data-line-number="3">  s =<span class="st"> </span><span class="kw">median</span>(residual)</a>
<a class="sourceLine" id="cb143-4" data-line-number="4">  w =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb143-5" data-line-number="5">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb143-6" data-line-number="6">    e_k =<span class="st"> </span>residual[k]</a>
<a class="sourceLine" id="cb143-7" data-line-number="7">    w =<span class="st"> </span><span class="kw">c</span>(w, e_k <span class="op">/</span><span class="st"> </span>(<span class="dv">6</span><span class="op">*</span>s) )</a>
<a class="sourceLine" id="cb143-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb143-9" data-line-number="9">  <span class="co"># by assigning 1, this is equivalent to 0 for ( 1 - W^2)^2 if W &gt;=1</span></a>
<a class="sourceLine" id="cb143-10" data-line-number="10">  w[ w <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb143-11" data-line-number="11">  (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>w<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span> <span class="co"># bisquare smoother</span></a>
<a class="sourceLine" id="cb143-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb143-13" data-line-number="13">w.depth =<span class="st"> </span><span class="kw">weight_depth</span>(residual)</a>
<a class="sourceLine" id="cb143-14" data-line-number="14"><span class="kw">round</span>(w.depth,<span class="dv">4</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>] <span class="co"># limit display to 8</span></a></code></pre></div>
<pre><code>## [1] 0.9516 0.9835 0.9932 0.9891 0.9911 0.8109 0.7614 0.9652</code></pre>

<p><strong>Finally</strong>, perform smoothing using both width and depth weights:</p>

<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" data-line-number="1">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x), n, <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb145-2" data-line-number="2">B=<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb145-3" data-line-number="3">y.hat2 =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb145-4" data-line-number="4"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb145-5" data-line-number="5">  w.width =<span class="st"> </span><span class="kw">as.numeric</span>( W[,k] )  </a>
<a class="sourceLine" id="cb145-6" data-line-number="6">  w =<span class="st"> </span><span class="kw">diag</span>(w.width <span class="op">*</span><span class="st"> </span>w.depth[k])</a>
<a class="sourceLine" id="cb145-7" data-line-number="7">  <span class="cf">if</span> ( <span class="kw">length</span>( <span class="kw">which</span>(w <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>) ) <span class="op">==</span><span class="st"> </span><span class="dv">0</span> ) { w =<span class="st"> </span><span class="kw">diag</span>(w.width) }</a>
<a class="sourceLine" id="cb145-8" data-line-number="8">  B[[k]] =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>w <span class="op">%*%</span><span class="st"> </span>A) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>w <span class="op">%*%</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb145-9" data-line-number="9">  beta =<span class="st"> </span>B[[k]]</a>
<a class="sourceLine" id="cb145-10" data-line-number="10">  y.fit =<span class="st"> </span>beta[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x[k]</a>
<a class="sourceLine" id="cb145-11" data-line-number="11">  y.hat2 =<span class="st"> </span><span class="kw">c</span>(y.hat2, y.fit)</a>
<a class="sourceLine" id="cb145-12" data-line-number="12">}</a></code></pre></div>

<p>Here is a naive implementation of <strong>scatterplot smoothing</strong> for <strong>LOESS</strong> and <strong>LOWESS</strong> in R (emphasizing <strong>LOWESS</strong>). Also, note that the <strong>robustness</strong> of fit can be processed iteratively against the fit itself, which we skip implementing here:</p>

<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb146-1" data-line-number="1">scatterplot_smoothing &lt;-<span class="st"> </span><span class="cf">function</span>(x,y, <span class="dt">f=</span><span class="fl">0.40</span>, <span class="dt">span=</span><span class="fl">0.75</span>, </a>
<a class="sourceLine" id="cb146-2" data-line-number="2">                                  <span class="dt">smooth=</span><span class="st">&quot;lowess&quot;</span>, <span class="dt">degree=</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb146-3" data-line-number="3">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb146-4" data-line-number="4">    weight_width &lt;-<span class="st"> </span><span class="cf">function</span>(d, h) {</a>
<a class="sourceLine" id="cb146-5" data-line-number="5">      w =<span class="st">   </span>d <span class="op">/</span><span class="st"> </span>h</a>
<a class="sourceLine" id="cb146-6" data-line-number="6">      w[w <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span> </a>
<a class="sourceLine" id="cb146-7" data-line-number="7">      <span class="co"># relaxed version of tricube</span></a>
<a class="sourceLine" id="cb146-8" data-line-number="8">      <span class="co"># without the 70/81 scale</span></a>
<a class="sourceLine" id="cb146-9" data-line-number="9">      (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>w<span class="op">^</span><span class="dv">3</span>)<span class="op">^</span><span class="dv">3</span> <span class="co"># tricube smoother</span></a>
<a class="sourceLine" id="cb146-10" data-line-number="10">    }</a>
<a class="sourceLine" id="cb146-11" data-line-number="11">    weight_depth &lt;-<span class="st"> </span><span class="cf">function</span>(e) {</a>
<a class="sourceLine" id="cb146-12" data-line-number="12">      s =<span class="st"> </span><span class="kw">median</span>(e)</a>
<a class="sourceLine" id="cb146-13" data-line-number="13">      w =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb146-14" data-line-number="14">      <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb146-15" data-line-number="15">        e_k =<span class="st"> </span>e[k]</a>
<a class="sourceLine" id="cb146-16" data-line-number="16">        w =<span class="st"> </span><span class="kw">c</span>(w, e_k <span class="op">/</span><span class="st"> </span>(<span class="dv">6</span><span class="op">*</span>s) )</a>
<a class="sourceLine" id="cb146-17" data-line-number="17">      }</a>
<a class="sourceLine" id="cb146-18" data-line-number="18">      w[ w <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb146-19" data-line-number="19">      (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>w<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span> <span class="co"># bisquare smoother</span></a>
<a class="sourceLine" id="cb146-20" data-line-number="20">    }</a>
<a class="sourceLine" id="cb146-21" data-line-number="21">    knn &lt;-<span class="st"> </span><span class="cf">function</span>(d_i, kth) {</a>
<a class="sourceLine" id="cb146-22" data-line-number="22">        <span class="kw">sort</span>(d_i, <span class="dt">decreasing=</span><span class="ot">FALSE</span>)[kth] </a>
<a class="sourceLine" id="cb146-23" data-line-number="23">    }   </a>
<a class="sourceLine" id="cb146-24" data-line-number="24">    coeffs &lt;-<span class="st"> </span><span class="cf">function</span>(A, y, <span class="dt">W =</span> <span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb146-25" data-line-number="25">        n =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb146-26" data-line-number="26">        <span class="cf">if</span> (<span class="kw">length</span>(W) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { W =<span class="st"> </span><span class="kw">diag</span>(W,n) } <span class="cf">else</span> { W =<span class="st"> </span><span class="kw">diag</span>(W) }</a>
<a class="sourceLine" id="cb146-27" data-line-number="27">        <span class="kw">solve</span>(<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>W <span class="op">%*%</span><span class="st"> </span>A) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>W <span class="op">%*%</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb146-28" data-line-number="28">    }</a>
<a class="sourceLine" id="cb146-29" data-line-number="29">    fit.lowess &lt;-<span class="st"> </span><span class="cf">function</span>(B, x, <span class="dt">degree=</span><span class="dv">1</span>) {  B[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>B[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x<span class="op">^</span>degree }</a>
<a class="sourceLine" id="cb146-30" data-line-number="30">    fit.loess &lt;-<span class="st"> </span><span class="cf">function</span>(B, x, <span class="dt">degree=</span><span class="dv">2</span>)  {  </a>
<a class="sourceLine" id="cb146-31" data-line-number="31">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb146-32" data-line-number="32">            B[<span class="dv">1</span>] </a>
<a class="sourceLine" id="cb146-33" data-line-number="33">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb146-34" data-line-number="34">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb146-35" data-line-number="35">            B[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>B[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x </a>
<a class="sourceLine" id="cb146-36" data-line-number="36">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb146-37" data-line-number="37">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) {</a>
<a class="sourceLine" id="cb146-38" data-line-number="38">            B[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>B[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>B[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> </a>
<a class="sourceLine" id="cb146-39" data-line-number="39">        }</a>
<a class="sourceLine" id="cb146-40" data-line-number="40">    } </a>
<a class="sourceLine" id="cb146-41" data-line-number="41">    fit &lt;-<span class="st"> </span><span class="cf">function</span>(A, x, y, kth, fit.smooth, <span class="dt">degree =</span> <span class="dv">2</span>) {</a>
<a class="sourceLine" id="cb146-42" data-line-number="42">        <span class="co"># Generate distance of data points</span></a>
<a class="sourceLine" id="cb146-43" data-line-number="43">        D =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">abs</span>( <span class="kw">dist</span>(x, <span class="dt">upper=</span><span class="ot">TRUE</span>)))</a>
<a class="sourceLine" id="cb146-44" data-line-number="44">        y.hat =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb146-45" data-line-number="45">        y.res =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb146-46" data-line-number="46">        y.width =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb146-47" data-line-number="47">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb146-48" data-line-number="48">            <span class="co"># kth nearest neighbor (kth smallest distance)</span></a>
<a class="sourceLine" id="cb146-49" data-line-number="49">            dk =<span class="st"> </span><span class="kw">knn</span>(D[k,], kth)</a>
<a class="sourceLine" id="cb146-50" data-line-number="50">            w =<span class="st"> </span><span class="kw">weight_width</span>(D[,k], dk)</a>
<a class="sourceLine" id="cb146-51" data-line-number="51">            B =<span class="st"> </span><span class="kw">coeffs</span>(A, y, w)</a>
<a class="sourceLine" id="cb146-52" data-line-number="52">            y.fit =<span class="st"> </span><span class="kw">fit.smooth</span>(B, x[k], degree)  </a>
<a class="sourceLine" id="cb146-53" data-line-number="53">            y.hat =<span class="st"> </span><span class="kw">c</span>(y.hat, y.fit)</a>
<a class="sourceLine" id="cb146-54" data-line-number="54">            y.res =<span class="st"> </span><span class="kw">c</span>(y.res, <span class="kw">abs</span>(y[k] <span class="op">-</span><span class="st"> </span>y.hat))</a>
<a class="sourceLine" id="cb146-55" data-line-number="55">            y.width[[k]] =<span class="st"> </span>w</a>
<a class="sourceLine" id="cb146-56" data-line-number="56">        }</a>
<a class="sourceLine" id="cb146-57" data-line-number="57">        y.hat =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb146-58" data-line-number="58">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb146-59" data-line-number="59">            w.width =<span class="st"> </span>y.width[[k]]</a>
<a class="sourceLine" id="cb146-60" data-line-number="60">            w.depth =<span class="st"> </span><span class="kw">weight_depth</span>(y.res)</a>
<a class="sourceLine" id="cb146-61" data-line-number="61">            w =<span class="st"> </span>w.width <span class="op">*</span><span class="st"> </span>w.depth</a>
<a class="sourceLine" id="cb146-62" data-line-number="62">            <span class="cf">if</span> (<span class="kw">length</span>(<span class="kw">which</span>(w <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>)) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) { w =<span class="st"> </span>w.width }</a>
<a class="sourceLine" id="cb146-63" data-line-number="63">            B =<span class="st"> </span><span class="kw">coeffs</span>(A, y, w)</a>
<a class="sourceLine" id="cb146-64" data-line-number="64">            y.fit =<span class="st"> </span><span class="kw">fit.smooth</span>(B, x[k], degree)  </a>
<a class="sourceLine" id="cb146-65" data-line-number="65">            y.hat =<span class="st"> </span><span class="kw">c</span>(y.hat, y.fit)</a>
<a class="sourceLine" id="cb146-66" data-line-number="66">        }</a>
<a class="sourceLine" id="cb146-67" data-line-number="67">        <span class="kw">list</span>(<span class="st">&quot;x&quot;</span>=x, <span class="st">&quot;y&quot;</span>=y.hat)</a>
<a class="sourceLine" id="cb146-68" data-line-number="68">    } </a>
<a class="sourceLine" id="cb146-69" data-line-number="69">    lowess &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, f) {</a>
<a class="sourceLine" id="cb146-70" data-line-number="70">        A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x), n, <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb146-71" data-line-number="71">        <span class="co"># kth nearest neighbor</span></a>
<a class="sourceLine" id="cb146-72" data-line-number="72">        kth =<span class="st"> </span><span class="kw">round</span>(f <span class="op">*</span><span class="st"> </span>n)</a>
<a class="sourceLine" id="cb146-73" data-line-number="73">        <span class="kw">fit</span>(A, x, y, kth, fit.lowess, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb146-74" data-line-number="74">    }</a>
<a class="sourceLine" id="cb146-75" data-line-number="75">    loess &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, span, degree) {</a>
<a class="sourceLine" id="cb146-76" data-line-number="76">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb146-77" data-line-number="77">            A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n)), n, <span class="dv">1</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb146-78" data-line-number="78">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb146-79" data-line-number="79">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb146-80" data-line-number="80">             A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x), n, <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb146-81" data-line-number="81">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb146-82" data-line-number="82">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) {</a>
<a class="sourceLine" id="cb146-83" data-line-number="83">             A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x, x<span class="op">^</span><span class="dv">2</span>), n, <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb146-84" data-line-number="84">        }</a>
<a class="sourceLine" id="cb146-85" data-line-number="85">        <span class="co"># kth nearest neighbor</span></a>
<a class="sourceLine" id="cb146-86" data-line-number="86">        kth =<span class="st"> </span><span class="kw">round</span>(span <span class="op">*</span><span class="st"> </span>n)</a>
<a class="sourceLine" id="cb146-87" data-line-number="87">        <span class="kw">fit</span>(A, x, y, kth, fit.loess, degree)</a>
<a class="sourceLine" id="cb146-88" data-line-number="88">    }</a>
<a class="sourceLine" id="cb146-89" data-line-number="89">    <span class="cf">if</span> (smooth <span class="op">==</span><span class="st"> &quot;lowess&quot;</span>) {</a>
<a class="sourceLine" id="cb146-90" data-line-number="90">        <span class="kw">lowess</span>(x, y, f)</a>
<a class="sourceLine" id="cb146-91" data-line-number="91">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb146-92" data-line-number="92">    <span class="cf">if</span> (smooth <span class="op">==</span><span class="st"> &quot;loess&quot;</span>) {</a>
<a class="sourceLine" id="cb146-93" data-line-number="93">        <span class="kw">loess</span>(x, y, span, degree)</a>
<a class="sourceLine" id="cb146-94" data-line-number="94">    }</a>
<a class="sourceLine" id="cb146-95" data-line-number="95">}</a></code></pre></div>

<p>Let us plot the outcome of <strong>scatterplot smoothing</strong> function:</p>

<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb147-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb147-2" data-line-number="2">sample_size =<span class="st"> </span>n =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb147-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">30</span> </a>
<a class="sourceLine" id="cb147-4" data-line-number="4">sample.poly =<span class="st"> </span><span class="kw">poly</span>(<span class="dv">1</span><span class="op">:</span>n, <span class="dt">degree=</span><span class="dv">3</span>, <span class="dt">simple=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb147-5" data-line-number="5">y =<span class="st"> </span>sample.poly[,<span class="dv">3</span>] <span class="op">+</span><span class="st"> </span>e <span class="co"># add Gausian residual using 3rd degree poly</span></a>
<a class="sourceLine" id="cb147-6" data-line-number="6">x =<span class="st"> </span><span class="kw">sort</span>( <span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">size=</span>n, <span class="dt">replace=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb147-7" data-line-number="7">ymin =<span class="st"> </span><span class="kw">min</span>(y); ymax =<span class="st"> </span><span class="kw">max</span>(y); xmin =<span class="st"> </span><span class="kw">min</span>(x); xmax =<span class="st"> </span><span class="kw">max</span>(x)</a>
<a class="sourceLine" id="cb147-8" data-line-number="8">D =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">abs</span>( <span class="kw">dist</span>(x, <span class="dt">upper=</span><span class="ot">TRUE</span>)))</a>
<a class="sourceLine" id="cb147-9" data-line-number="9">our.lowess.fit =<span class="st"> </span><span class="kw">scatterplot_smoothing</span>(x, y, <span class="dt">f=</span><span class="fl">0.40</span>, <span class="dt">smooth=</span><span class="st">&quot;lowess&quot;</span>)</a>
<a class="sourceLine" id="cb147-10" data-line-number="10">our.loess.fit =<span class="st"> </span><span class="kw">scatterplot_smoothing</span>(x, y, <span class="dt">span=</span><span class="fl">0.40</span>, <span class="dt">smooth=</span><span class="st">&quot;loess&quot;</span>, </a>
<a class="sourceLine" id="cb147-11" data-line-number="11">                                      <span class="dt">degree=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb147-12" data-line-number="12">lowess.fit =<span class="st"> </span><span class="kw">lowess</span>(x, y, <span class="dt">f=</span><span class="fl">0.40</span> )</a>
<a class="sourceLine" id="cb147-13" data-line-number="13">loess.model =<span class="st"> </span><span class="kw">loess</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">span=</span><span class="fl">0.75</span>, <span class="dt">degree=</span><span class="dv">2</span>, <span class="dt">family=</span><span class="kw">c</span>(<span class="st">&quot;gaussian&quot;</span>))</a>
<a class="sourceLine" id="cb147-14" data-line-number="14">loess.fit =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(loess.model)</a>
<a class="sourceLine" id="cb147-15" data-line-number="15"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim =</span> <span class="kw">range</span>(xmin, xmax), <span class="dt">ylim=</span><span class="kw">range</span>(ymin,ymax ),</a>
<a class="sourceLine" id="cb147-16" data-line-number="16">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>,</a>
<a class="sourceLine" id="cb147-17" data-line-number="17">     <span class="dt">main=</span><span class="st">&quot;Scatterplot Smoothing (Local Linear Regression)&quot;</span>)</a>
<a class="sourceLine" id="cb147-18" data-line-number="18"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb147-19" data-line-number="19"><span class="kw">points</span>(x, y, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb147-20" data-line-number="20"><span class="kw">lines</span>(our.lowess.fit, <span class="dt">col=</span><span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb147-21" data-line-number="21"><span class="kw">lines</span>(our.loess.fit, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb147-22" data-line-number="22"><span class="kw">lines</span>(lowess.fit, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb147-23" data-line-number="23"><span class="kw">lines</span>(loess.fit, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lowess"></span>
<img src="embed0021.png" alt="Scatterplot Smoothing" width="80%" />
<p class="caption">
Figure 3.32: Scatterplot Smoothing
</p>
</div>

</div>
<div id="kernel-smoothing" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.8.2</span> Kernel Smoothing <a href="numericallinearalgebra.html#kernel-smoothing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Kernel Smoothing</strong>, also considered as <strong>Kernel Regression</strong>, is another <strong>smoothing or regression technique</strong> similar to <strong>LOESS</strong> and <strong>LOWESS</strong> smoothers.</p>
<p>There are a few <strong>kernel functions</strong> that can be used for <strong>Kernel regression (estimation)</strong> (See Table <a href="numericallinearalgebra.html#tab:kernfunction">3.1</a>) <span class="citation">(JoÅe E. ChacÃ³n J E. et al <a href="bibliography.html#ref-ref389j">2018</a>; Cheruiyot L. R. et al <a href="bibliography.html#ref-ref381l">2020</a>)</span>:        </p>

<table>
<caption><span id="tab:kernfunction">Table 3.1: </span>Kernel Functions</caption>
<colgroup>
<col width="9%" />
<col width="2%" />
<col width="45%" />
<col width="16%" />
<col width="17%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Kernel</th>
<th align="left">â</th>
<th align="left">Formula: <span class="math inline">\(I(x) = 1_{( \mid x \mid \le 1)}\)</span></th>
<th align="left"><span class="math inline">\(\mu_2(K)\)</span></th>
<th align="left">R(K)</th>
<th align="left">Efficiency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Epanechnikov</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{3}{4}(1 - x^2)\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{5}\)</span></td>
<td align="left"><span class="math inline">\(\frac{3}{5}\)</span></td>
<td align="left">1.0000</td>
</tr>
<tr class="even">
<td align="left">Cosine</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{\pi}{4}cos\left(\frac{\pi}{2}x\right)\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(1 - \frac{8}{\pi^2}\)</span></td>
<td align="left"><span class="math inline">\(\frac{\pi^2}{16}\)</span></td>
<td align="left">0.9995</td>
</tr>
<tr class="odd">
<td align="left">Tricube</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{70}{81}(1 - \mid x \mid^3)^3\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{35}{243}\)</span></td>
<td align="left"><span class="math inline">\(\frac{175}{247}\)</span></td>
<td align="left">0.9979</td>
</tr>
<tr class="even">
<td align="left">Quartic</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{15}{16}(1 - x^2)^2\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{7}\)</span></td>
<td align="left"><span class="math inline">\(\frac{5}{7}\)</span></td>
<td align="left">0.9939</td>
</tr>
<tr class="odd">
<td align="left">Triweight</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{35}{32}(1 - x^2)^3\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{9}\)</span></td>
<td align="left"><span class="math inline">\(\frac{350}{429}\)</span></td>
<td align="left">0.9867</td>
</tr>
<tr class="even">
<td align="left">Triangular</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = (1 - \mid x \mid)\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{6}\)</span></td>
<td align="left"><span class="math inline">\(\frac{2}{3}\)</span></td>
<td align="left">0.9859</td>
</tr>
<tr class="odd">
<td align="left">Gaussian</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{2\sqrt{\pi}}\)</span></td>
<td align="left">0.9512</td>
</tr>
<tr class="even">
<td align="left">Uniform</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{1}{2}\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{3}\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{2}\)</span></td>
<td align="left">0.9295</td>
</tr>
<tr class="odd">
<td align="left">Logistic</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = (e^x +_ 2 + e^{-x})^{-1}\)</span></td>
<td align="left"><span class="math inline">\(\frac{\pi^2}{3}\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{6}\)</span></td>
<td align="left">0.8876</td>
</tr>
</tbody>
</table>

<p>As shown in the table, the performance of the <strong>kernel functions</strong> in terms of efficiency is calculated based on the following equation and comparably measured against <strong>Epanechnikov Kernel</strong>, which effectively gets 100% efficiency:</p>
<p><span class="math display">\[\begin{align}
Efficiency = \sqrt{\mu_2(K)}\cdot R(K)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
where \ \ \ \
\mu_2(K) = \int x^2K(x)dx, \ \ \ \ \ R(K) = \int K^2(x) dx
\end{align}\]</span></p>
<p>For example, the efficiency of <strong>Epanechnikov Kernel</strong> is computed like so (let A = <span class="math inline">\(\mu_2(K)\)</span> and B = <span class="math inline">\(R(K)\)</span>):</p>
<p><span class="math display">\[
A = \mu_2(K) = \frac{1}{5},\ \ \ \ \ \ 
B = R(K) = \frac{3}{5}\ \ \rightarrow\ \ \ \ \ \ \ \ 
Efficiency = \sqrt{\frac{1}{5}} \cdot \frac{3}{5} = 0.2683282.
\]</span></p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb148-1" data-line-number="1">epanechnikov_kernel &lt;-<span class="cf">function</span>(x) <span class="dv">3</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>x<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb148-2" data-line-number="2">f1 &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">epanechnikov_kernel</span>(x)</a>
<a class="sourceLine" id="cb148-3" data-line-number="3">f2 &lt;-<span class="st"> </span><span class="cf">function</span>(x) (<span class="kw">epanechnikov_kernel</span>(x))<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb148-4" data-line-number="4">A =<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> f1, <span class="dt">lower =</span> <span class="dv">-1</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value</a>
<a class="sourceLine" id="cb148-5" data-line-number="5">B =<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> f2, <span class="dt">lower =</span> <span class="dv">-1</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value</a>
<a class="sourceLine" id="cb148-6" data-line-number="6">E_epanechnikov =<span class="st"> </span><span class="kw">sqrt</span>(A) <span class="op">*</span><span class="st"> </span>B</a>
<a class="sourceLine" id="cb148-7" data-line-number="7"><span class="kw">c</span>(<span class="st">&quot;A&quot;</span> =<span class="st"> </span>A, <span class="st">&quot;B&quot;</span> =<span class="st"> </span>B, <span class="st">&quot;Efficiency&quot;</span> =<span class="st"> </span>E_epanechnikov)</a></code></pre></div>
<pre><code>##          A          B Efficiency 
##  0.2000000  0.6000000  0.2683282</code></pre>
<p>The efficiency of <strong>Gaussian Kernel</strong> is computed like so:</p>
<p><span class="math display">\[
A = \mu_2(K) = 1,\ \ \ \ \ \ 
B = R(K) = \frac{1}{2\sqrt{\pi}}\ \ \rightarrow\ \ \ \ \ \
Efficiency = \sqrt{1} \cdot \frac{1}{2\sqrt{\pi}} = 0.2820948.
\]</span></p>

<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb150-1" data-line-number="1">gaussian_kernel &lt;-<span class="cf">function</span>(x)   <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi)) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>( <span class="dv">-1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span>)  </a>
<a class="sourceLine" id="cb150-2" data-line-number="2">f1 &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">gaussian_kernel</span>(x)</a>
<a class="sourceLine" id="cb150-3" data-line-number="3">f2 &lt;-<span class="st"> </span><span class="cf">function</span>(x) (<span class="kw">gaussian_kernel</span>(x))<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb150-4" data-line-number="4">A =<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> f1, <span class="dt">lower =</span> <span class="op">-</span><span class="ot">Inf</span>, <span class="dt">upper =</span> <span class="ot">Inf</span>)<span class="op">$</span>value</a>
<a class="sourceLine" id="cb150-5" data-line-number="5">B =<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> f2, <span class="dt">lower =</span> <span class="op">-</span><span class="ot">Inf</span>, <span class="dt">upper =</span> <span class="ot">Inf</span>)<span class="op">$</span>value</a>
<a class="sourceLine" id="cb150-6" data-line-number="6">E_gaussian =<span class="st"> </span><span class="kw">sqrt</span>(A) <span class="op">*</span><span class="st"> </span>B</a>
<a class="sourceLine" id="cb150-7" data-line-number="7"><span class="kw">c</span>(<span class="st">&quot;A&quot;</span> =<span class="st"> </span>A, <span class="st">&quot;B&quot;</span> =<span class="st"> </span>B, <span class="st">&quot;Efficiency&quot;</span> =<span class="st"> </span>E_gaussian)</a></code></pre></div>
<pre><code>##          A          B Efficiency 
##  1.0000000  0.2820948  0.2820948</code></pre>

<p>We then compare each efficiency against the calculated efficiency of <strong>Epanechnikov efficiency</strong>:</p>
<p><span class="math display">\[\begin{align*}
E_{epanechnikov} {}&amp;= 0.2683282 / 0.2683282 = 1.0000 \\
E_{gaussian} &amp;= 0.2683282 / 0.2820948 = 0.9511987 = 0.9512
\end{align*}\]</span></p>

<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb152-1" data-line-number="1">E.epanechnikov =<span class="st"> </span>E_epanechnikov <span class="op">/</span><span class="st"> </span>E_epanechnikov</a>
<a class="sourceLine" id="cb152-2" data-line-number="2">E.gaussian =<span class="st">  </span>E_epanechnikov <span class="op">/</span><span class="st"> </span>E_gaussian</a>
<a class="sourceLine" id="cb152-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;Efficiency (epanechnikov)&quot;</span> =<span class="st"> </span><span class="kw">round</span>(E.epanechnikov, <span class="dv">4</span>), </a>
<a class="sourceLine" id="cb152-4" data-line-number="4">   <span class="st">&quot;Efficiency (gaussian)&quot;</span> =<span class="st"> </span><span class="kw">round</span>(E.gaussian, <span class="dv">4</span>))</a></code></pre></div>
<pre><code>## Efficiency (epanechnikov)     Efficiency (gaussian) 
##                    1.0000                    0.9512</code></pre>

<p>Any function can be used as a kernel as long as the following properties are satisfied <span class="citation">(Zucchini W. <a href="bibliography.html#ref-ref116z">2003</a>)</span>:</p>
<p><span class="math display">\[
\int K(x)dx = 1,\ \ \ \ \ \ \ \ \ \
\int xK(x)dx = 0,\ \ \ \ \ \ \ \ \ \
\mu_2(K) := \int x^2K(x)dx &lt; \infty
\]</span></p>
<p>We can see the different functions plotted in Figure <a href="numericallinearalgebra.html#fig:kfunctions">3.33</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kfunctions"></span>
<img src="embed0022.png" alt="Kernel Functions" width="90%" />
<p class="caption">
Figure 3.33: Kernel Functions
</p>
</div>

<p>Now, to illustrate how <strong>Kernel Smoothing works</strong>, we start with a <strong>simple linear equation</strong>:</p>
<p><span class="math display">\[\begin{align}
y_i = \beta_0 + \beta_1 x_i + \epsilon
\end{align}\]</span></p>
<p>We focus on a <strong>linear equation</strong> with the goal of estimating the following:</p>
<p><span class="math display">\[\begin{align}
\hat{m}(x_i) \approx \beta_0 + \beta_1 x_i
\end{align}\]</span></p>
<p>so that we end up with the following linear equation:</p>
<p><span class="math display">\[\begin{align}
\hat{y}_i = \hat{m}(x_i) + \epsilon
\end{align}\]</span></p>
<p>The function <span class="math inline">\(\hat{m}(x)\)</span> is derived like so:</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(Y) \rightarrow \mathbb{E}(Y|X = x) {}&amp;= \int y \cdot f(y|x) dy \\
&amp;= \int y \cdot \frac{f(x,y)}{f(x)} dy\\
&amp;= \hat{m}(x).
\end{align}\]</span></p>
<p>From there, we obtain an equation for <span class="math inline">\(\hat{m}(x)\)</span>. For (<strong>NWKE, PCKE, GMKE</strong>), we have:</p>
<p><span class="math display">\[\begin{align}
\hat{m}(x) = \sum_{i=1}^n \omega_i(x)y_i
\end{align}\]</span></p>
<p>For <strong>KDE</strong>, we have:</p>
<p><span class="math display">\[\begin{align}
\hat{m}(x) = \sum_{i=1}^n \omega_i(x)
\end{align}\]</span></p>
<p>We then perform <strong>non-parametric</strong> estimation using a <strong>Kernel Estimator</strong>, <span class="math inline">\(\omega_i(x)\)</span>. There are choices for <span class="math inline">\(\omega_i(x)\)</span>:</p>
<ul>
<li><strong>Nadaraya-Watson Kernel Estimator (NWKE)</strong>:  </li>
</ul>
<p><span class="math display">\[\begin{align}
\omega_i(x) {}&amp;= \frac{K \left(\frac{x - x_i}{h}\right)} 
{\sum_{j=1}^n K \left(\frac{x - x_j}{h}\right)}\ \ \ \ \ \ \ \ or \ \ \ \ \ \ \  \\
\omega_i(x) &amp;= \frac{K \left(\frac{\|x - x_i\|_p}{h}\right)}
{\sum_{j=1}^n K \left(\frac{\|x - x_j\|_p}{h}\right)}\ \ \ \text{for multi-dimension}
\end{align}\]</span></p>
<ul>
<li><strong>Priestley-Chao Kernel Estimator (PCKE)</strong>:  </li>
</ul>
<p><span class="math display">\[\begin{align}
\omega_i(x) {}&amp;= \frac{\psi}{h} K\left(\frac{x - x_{(i+1)}}{h}\right)
\ \ where\  \psi = \frac{(b - a)}{n}\ \ \leftarrow\ \ \ \text{equally spaced}\\
\omega_i(x) {}&amp;= \frac{1}{h}(x_{(i+1)} - x_i) K\left(\frac{x - x_{(i+1)}}{h}\right)
\ \ \leftarrow\ \ \ \text{not equally spaced}
\end{align}\]</span></p>
<ul>
<li><strong>Gasser-Muller Kernel Estimator (GMKE)</strong>:  </li>
</ul>
<p><span class="math display">\[\begin{align}
\omega_i(x) = \frac{1}{h}\left|\int_{s_{i-1}}^{s_i} K \left(\frac{ x - t}{h}\right)dt \right|\ \ \ \ \ \leftarrow \ 
where\ \ s_i = \frac{x_i + x_{(i+1)}}{2}
\end{align}\]</span></p>
<ul>
<li><strong>Parzen-Rosenblatt window - Kernel Density estimator (KDE)</strong>:   </li>
</ul>
<p><span class="math display">\[\begin{align}
\omega_i(x) {}&amp;= \frac{1}{n}\sum_{j=1}^n \frac{1}{h}K\left(\frac{x - x_i}{h}\right)
\ \ \ \ \ \ \ \ or \ \ \ \ \ \ \ \\
\omega_i(x) &amp;= \frac{1}{n}\sum_{j=1}^n \frac{1}{h^d}K\left(\frac{\|x - x_i\|_p}{h}\right)
\ \ \ \text{for multi-dimension}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>d</strong> - d-dimensions</li>
<li><strong>p</strong> - p-norm, e.g.Â 2 for euclidean</li>
</ul>
<p>The <strong>kernel estimators</strong> rely on:</p>
<ul>
<li><strong>K</strong> - a choice of kernel as above (e.g.Â Gaussian, Box, Epanechnikov).</li>
<li><strong>h</strong> - the bandwidth.</li>
</ul>
<p>Let us now illustrate <strong>kernel smoothing</strong> by choosing one combination to implement in R. For our <strong>kernel function</strong>, let us choose <strong>Gaussian Kernel</strong>. For our <strong>kernel estimator</strong>, let us choose <strong>Nadaraya-Watson (NWKE)</strong>.</p>
<p>Here is a naive implementation of our <strong>kernel regression (smoothing)</strong> in R code using the chosen combination:</p>

<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb154-1" data-line-number="1">K &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">kernel =</span> <span class="st">&quot;normal&quot;</span>) {</a>
<a class="sourceLine" id="cb154-2" data-line-number="2">  I &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb154-3" data-line-number="3">    one =<span class="st"> </span><span class="kw">which</span>(x<span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>); zero =<span class="st"> </span><span class="kw">which</span>(x<span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb154-4" data-line-number="4">    x[one] =<span class="st"> </span><span class="dv">1</span>; x[zero] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb154-5" data-line-number="5">    x</a>
<a class="sourceLine" id="cb154-6" data-line-number="6">  }</a>
<a class="sourceLine" id="cb154-7" data-line-number="7">  <span class="cf">switch</span> (kernel,</a>
<a class="sourceLine" id="cb154-8" data-line-number="8">          <span class="st">&quot;epanechnikov&quot;</span> =<span class="st"> </span><span class="dv">3</span><span class="op">/</span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x),</a>
<a class="sourceLine" id="cb154-9" data-line-number="9">          <span class="st">&quot;normal&quot;</span>       =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x <span class="op">^</span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb154-10" data-line-number="10">          <span class="st">&quot;tricube&quot;</span>      =<span class="st"> </span><span class="dv">70</span><span class="op">/</span><span class="dv">81</span> <span class="op">*</span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">abs</span>(x)<span class="op">^</span><span class="dv">3</span>)<span class="op">^</span><span class="dv">3</span>   <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x),</a>
<a class="sourceLine" id="cb154-11" data-line-number="11">          <span class="st">&quot;rectangular&quot;</span>  =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x),</a>
<a class="sourceLine" id="cb154-12" data-line-number="12">          <span class="st">&quot;triangular&quot;</span>   =<span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">abs</span>(x))  <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x),</a>
<a class="sourceLine" id="cb154-13" data-line-number="13">          <span class="st">&quot;quartic&quot;</span>      =<span class="st"> </span><span class="dv">15</span><span class="op">/</span><span class="dv">16</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span>  <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x)</a>
<a class="sourceLine" id="cb154-14" data-line-number="14">          )</a>
<a class="sourceLine" id="cb154-15" data-line-number="15">} </a>
<a class="sourceLine" id="cb154-16" data-line-number="16">estimator &lt;-<span class="st"> </span><span class="cf">function</span>(X, x, y, h, <span class="dt">etype =</span> <span class="st">&quot;nwke&quot;</span>, <span class="dt">kernel =</span> <span class="st">&quot;normal&quot;</span>) {</a>
<a class="sourceLine" id="cb154-17" data-line-number="17">  nwke &lt;-<span class="st"> </span><span class="cf">function</span>(X, x, y, h, kernel) {</a>
<a class="sourceLine" id="cb154-18" data-line-number="18">      h =<span class="st"> </span><span class="fl">0.25</span> <span class="op">*</span><span class="st"> </span>h <span class="co"># see ksmooth for adjustment of bandwidth</span></a>
<a class="sourceLine" id="cb154-19" data-line-number="19">      n =<span class="st"> </span><span class="kw">length</span>(x); m =<span class="st"> </span><span class="kw">length</span>(X)</a>
<a class="sourceLine" id="cb154-20" data-line-number="20">      mx =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb154-21" data-line-number="21">      <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb154-22" data-line-number="22">          w =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb154-23" data-line-number="23">          <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb154-24" data-line-number="24">              numer =<span class="st"> </span><span class="kw">K</span> ( ( X[k] <span class="op">-</span><span class="st"> </span>x[i] ) <span class="op">/</span><span class="st"> </span>h , kernel )</a>
<a class="sourceLine" id="cb154-25" data-line-number="25">              denom =<span class="st"> </span><span class="kw">sum</span> ( <span class="kw">K</span> ( ( X[k] <span class="op">-</span><span class="st"> </span>x ) <span class="op">/</span><span class="st"> </span>h , kernel ) )</a>
<a class="sourceLine" id="cb154-26" data-line-number="26">              w =<span class="st"> </span><span class="kw">c</span>(w, numer <span class="op">/</span><span class="st"> </span>denom   )</a>
<a class="sourceLine" id="cb154-27" data-line-number="27">          }</a>
<a class="sourceLine" id="cb154-28" data-line-number="28">          mx =<span class="st"> </span><span class="kw">c</span>(mx, <span class="kw">sum</span>( w <span class="op">*</span><span class="st"> </span>y ) )</a>
<a class="sourceLine" id="cb154-29" data-line-number="29">      }</a>
<a class="sourceLine" id="cb154-30" data-line-number="30">      mx  </a>
<a class="sourceLine" id="cb154-31" data-line-number="31">  }</a>
<a class="sourceLine" id="cb154-32" data-line-number="32">  <span class="cf">if</span> (etype <span class="op">==</span><span class="st"> &quot;nwke&quot;</span>) {</a>
<a class="sourceLine" id="cb154-33" data-line-number="33">    <span class="kw">nwke</span>(X, x, y, h, kernel)</a>
<a class="sourceLine" id="cb154-34" data-line-number="34">  } </a>
<a class="sourceLine" id="cb154-35" data-line-number="35">}</a>
<a class="sourceLine" id="cb154-36" data-line-number="36">kernel_smoothing &lt;-<span class="st"> </span><span class="cf">function</span>( x, y, <span class="dt">kernel=</span><span class="st">&quot;normal&quot;</span>, <span class="dt">h =</span> <span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb154-37" data-line-number="37">  m &lt;-<span class="st"> </span>estimator</a>
<a class="sourceLine" id="cb154-38" data-line-number="38">  xmin =<span class="st"> </span><span class="kw">min</span>(x); xmax =<span class="st"> </span><span class="kw">max</span>(x)</a>
<a class="sourceLine" id="cb154-39" data-line-number="39">  X =<span class="st"> </span><span class="kw">seq</span>(xmin, xmax, <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb154-40" data-line-number="40">  Y =<span class="st"> </span><span class="kw">m</span>(X, x, y,  h, <span class="dt">etype=</span><span class="st">&quot;nwke&quot;</span>, kernel) </a>
<a class="sourceLine" id="cb154-41" data-line-number="41">  <span class="kw">list</span>(<span class="st">&quot;x&quot;</span>=X, <span class="st">&quot;y&quot;</span>=Y)</a>
<a class="sourceLine" id="cb154-42" data-line-number="42">}</a></code></pre></div>

<p>Let us plot and introduce a built-in R function called <strong>ksmooth()</strong> (see Figure <a href="numericallinearalgebra.html#fig:ksmooth">3.34</a>).</p>

<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb155-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb155-2" data-line-number="2">sample_size =<span class="st"> </span>n =<span class="st"> </span><span class="dv">100</span> </a>
<a class="sourceLine" id="cb155-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">30</span> </a>
<a class="sourceLine" id="cb155-4" data-line-number="4">sample.poly =<span class="st"> </span><span class="kw">poly</span>(<span class="dv">1</span><span class="op">:</span>n, <span class="dt">degree=</span><span class="dv">4</span>, <span class="dt">simple=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb155-5" data-line-number="5">y =<span class="st"> </span>sample.poly[,<span class="dv">4</span>] <span class="op">+</span><span class="st"> </span>e <span class="co"># add Gaussian residual using 3rd degree poly</span></a>
<a class="sourceLine" id="cb155-6" data-line-number="6">x =<span class="st"> </span><span class="kw">sort</span>( <span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">size=</span>n, <span class="dt">replace=</span><span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb155-7" data-line-number="7">ymin =<span class="st"> </span><span class="kw">min</span>(y); ymax =<span class="st"> </span><span class="kw">max</span>(y); xmin =<span class="st"> </span><span class="kw">min</span>(x); xmax =<span class="st"> </span><span class="kw">max</span>(x)</a>
<a class="sourceLine" id="cb155-8" data-line-number="8"></a>
<a class="sourceLine" id="cb155-9" data-line-number="9"><span class="co"># Run our own NW implementation</span></a>
<a class="sourceLine" id="cb155-10" data-line-number="10">nwke.fit =<span class="st"> </span><span class="kw">kernel_smoothing</span>(x, y,  <span class="dt">kernel=</span><span class="st">&quot;normal&quot;</span>, <span class="dt">h =</span> <span class="dv">7</span> )</a>
<a class="sourceLine" id="cb155-11" data-line-number="11"><span class="co"># Use ksmooth</span></a>
<a class="sourceLine" id="cb155-12" data-line-number="12">ksmooth.fit =<span class="st"> </span><span class="kw">ksmooth</span>(x, y, <span class="dt">kernel =</span> <span class="kw">c</span>( <span class="st">&quot;normal&quot;</span>), <span class="dt">bandwidth =</span> <span class="dv">5</span> )</a>
<a class="sourceLine" id="cb155-13" data-line-number="13"></a>
<a class="sourceLine" id="cb155-14" data-line-number="14"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim =</span> <span class="kw">range</span>(xmin, xmax), <span class="dt">ylim=</span><span class="kw">range</span>(ymin,ymax ),</a>
<a class="sourceLine" id="cb155-15" data-line-number="15">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>,</a>
<a class="sourceLine" id="cb155-16" data-line-number="16">     <span class="dt">main=</span><span class="st">&quot;Kernel Smoothing&quot;</span>)</a>
<a class="sourceLine" id="cb155-17" data-line-number="17"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb155-18" data-line-number="18"><span class="kw">points</span>(x, y, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb155-19" data-line-number="19"><span class="kw">lines</span>(ksmooth.fit, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb155-20" data-line-number="20"><span class="kw">lines</span>(nwke.fit, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb155-21" data-line-number="21"><span class="kw">legend</span>(<span class="dv">3</span>, ymax , </a>
<a class="sourceLine" id="cb155-22" data-line-number="22">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;our nwke(h=7)&quot;</span>, </a>
<a class="sourceLine" id="cb155-23" data-line-number="23">              <span class="st">&quot;built-in ksmooth(bandwidth=5)&quot;</span>),</a>
<a class="sourceLine" id="cb155-24" data-line-number="24">    <span class="dt">col=</span><span class="kw">c</span>( <span class="st">&quot;brown&quot;</span>, <span class="st">&quot;navyblue&quot;</span>), <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ksmooth"></span>
<img src="embed0023.png" alt="Kernel Smoothing" width="80%" />
<p class="caption">
Figure 3.34: Kernel Smoothing
</p>
</div>

<p>Note that our curve overlaps right on top of the curve fit produced by the <strong>ksmooth()</strong> function. It is an exact match; however, we can accomplish this only because we arbitrarily choose a bandwidth (<span class="math inline">\(h=7\)</span>), which is practically not optimal by hand.</p>
<p>Choosing an optimal bandwidth and the correct Kernel is a good exercise and study in <strong>Kernel regression or Kernel Smoothing</strong>.</p>
<p>For <strong>bandwidth selection</strong>, we have a few choices:</p>
<p><strong>First</strong>, we can use <strong>cross-validation</strong> techniques.</p>
<ul>
<li>LOOCV (Leave One Out Cross-Validation) </li>
<li>K-Fold CV (K-Fold Cross-Validation) </li>
</ul>
<p>The idea is to have an initial list of random bandwidths as data points for cross-validation. We then use <strong>mean squared error (MSE)</strong> to evaluate the cross-validation result. The result with the least <strong>MSE</strong> serves as the optimal bandwidth.  </p>
<p>The equation for <strong>MSE</strong> is shown below:</p>
<p><span class="math display">\[\begin{align}
MSE {}&amp;= \mathbb{E}\left[(f(x) - \hat{f}(x)^2\right] \\
&amp;= Bias(\hat{f}(x))^2 +  Var(\hat{f}(x)) \\
&amp;= \frac{1}{n}\sum_{i=1}^n\left(y_i - \hat{y}_i\right)^2
\end{align}\]</span></p>
<p>Note that <strong>MSE</strong> is measured based on its two components and how the trade-off plays along: <strong>Bias</strong> and <strong>Variance</strong>. We can find more discussion around adjusting the components in Chapter <strong>6</strong> (<strong>Statistical Computation</strong>) under the <strong>Significance of Regression</strong> Section and Chapter <strong>9</strong> (<strong>Computational Learning I</strong>) under the <strong>Regularization</strong> Section.</p>
<p>For <strong>least MSE</strong> equation, we have:</p>
<p><span class="math display">\[\begin{align}
CV(bandwidth) = \underset{h}{\mathrm{argmin}}\ MSE(h)
\end{align}\]</span></p>
<p>We cover more of <strong>cross-validation</strong> topic in Chapter <strong>6</strong> (<strong>Statistical Computation</strong>) under <strong>Model Selection</strong> Section and Chapter <strong>9</strong> (<strong>Computational Learning I</strong>).</p>
<p>There are also other measurements adapted from <strong>MSE</strong>:</p>
<ul>
<li><strong>MISE</strong> - mean integrated squared error (Jones 1990). We simply integrate <strong>MSE</strong> like so:</li>
</ul>
<p><span class="math display">\[\begin{align}
MISE(h) {}&amp;= \int MSE(h) dx \\
&amp;= \int Bias(\hat{f}(x))^2 dx + \int Var(\hat{f}(x)) dx \\
&amp;= \int(f(x) - \hat{f}(x))^2  dx + + \int Var(\hat{f}(x)) dx
\end{align}\]</span></p>
<ul>
<li><strong>AMISE</strong> - asymptotic limit of mean integrated squared error (Scott 1992 and Wand and Jones 1995):</li>
</ul>
<p><span class="math display">\[\begin{align}
AMISE(h) = \frac{1}{Nh} R(K) + \frac{1}{4}h^4 u_2(K)^2 R(f&#39;&#39;)
\end{align}\]</span></p>
<p>A simplification (or derivation) of the above <strong>AMISE</strong> equation is as follows:</p>
<p><span class="math display">\[\begin{align}
h_{(AMISE)} = 
\left(\frac{R(K)^{\frac{1}{5}}}{\mu_2(K)^\frac{2}{5}R(f&#39;&#39;)^{\frac{1}{5}}}\right) n^{-\frac{1}{5}}
= \left(\frac{R(K)}{\mu_2(K)^2R(f&#39;&#39;)}\right)^{\frac{1}{5}} n^{-\frac{1}{5}}
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
R(K) = \int K(x)^2 dx,\ \ \ \ \ \ \ \ \
\mu_2 (K) = \int x^2K(x) dx,\ \ \ \ \ \ \ \ \ \
R(f&#39;&#39;) = \int f&#39;&#39;(y)^2 dy
\end{align}\]</span></p>
<p><strong>Second</strong>, for <strong>optimal bandwidth</strong> calculation, we can use the <strong>Silverman Rule of Thumb</strong>, also called <strong>Normal Rule of Thumb</strong>. For example, when using <strong>Gaussian kernel</strong>, we use the below equation (Silverman 1986, Scott 1992, Jones et al.Â 1996, Venables-Ripley 2002):</p>
<p><span class="math display">\[\begin{align}
h_{(optimal)} = \left(\frac{4 \hat{\sigma} ^5}{3n}\right)^{\frac{1}{5}} = 1.06 \hat{\sigma} n^{-\frac{1}{5}}
\end{align}\]</span>
A variant version was proposed (Silverman and Scott 1992) to avoid oversmoothing:</p>
<p><span class="math display">\[\begin{align}
h_{(optimal)} = 0.9 A n^{-\frac{1}{5}}\ \ \ \ \ \leftarrow\ \ \ \ \ \ 
A =  min\left(\hat{\sigma}, \frac{IQR(x)}{1.349}\right) 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\hat{\sigma}\)</span> is the sample standard deviation</li>
<li>IQR is <strong>inter-quartile range</strong></li>
</ul>
<p>The implementation of <strong>KDE</strong> and sample bandwidth choices are discussed in Chapter <strong>5</strong> (<strong>Probability and Distribution</strong>) under the <strong>Non-parametric distribution</strong> Section, in which we cover <strong>KDE</strong> in detail as part of estimating probability density.</p>
</div>
</div>
<div id="polynomial-optimization" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.9</span> Polynomial Optimization <a href="numericallinearalgebra.html#polynomial-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section introduces the basic concept of <strong>Linear Programming and Optimization</strong>. The importance of such a subject becomes apparent in <strong>Computational Learning</strong> Chapters (Volume III). We start with the following two examples of a <strong>convex 2-polytope</strong> graphs from Figure <a href="numericallinearalgebra.html#fig:convexquadratic">3.35</a>. The first graph has solutions within the <strong>feasible region</strong> bounded by four linear equations. The second graph has solutions within the <strong>feasible region</strong> bounded by a linear equation and a convex quadratic equation.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:convexquadratic"></span>
<img src="convexquadratic.png" alt="Linear Programming" width="100%" />
<p class="caption">
Figure 3.35: Linear Programming
</p>
</div>


<p>Our goal is to find the optimal value (a maximum and a minimum) of a given <strong>objective function</strong> - also called <strong>linear cost function</strong>. Our objective function is <strong>subject to inequality constraints</strong>in our case. Let us start with an objective function for the first graph subject to four inequality constraints corresponding to the four equations.</p>
<p><span class="math display">\[
\begin{array}{rl}
\mathbf{\text{objective:}} &amp; 3x + 4y   = z\\
\mathbf{\text{subject to:}} &amp;  y  \le -\frac{1}{3}x + 3\\
&amp; y  \ge -3x + 3\\
&amp; x  \le 3\\
&amp; y  \ge 0
\end{array}
\]</span>
Note that the first graph shows four intersections (corners) around the region. To solve the problem, let us use those intersecting points.</p>
<p><span class="math display">\[
\begin{array}{rrl}
(x, y): &amp; 3x + 4y &amp; \text{optimal value} \\
====== &amp; ========= &amp; ======== \\
(0,3)       :&amp;  3(0) + 4(3) =12  &amp; \\
(3, 2)  :&amp;  3(3) + 4(2) =17 &amp; \text{(maximum)} \\
(1,0)       :&amp;  3(1) + 4(0) =\ \ 3 &amp; \text{(minimum)} \\
(3,0)       :&amp;  3( 3) + 4(0) =\ \ 9 &amp;
\end{array}
\]</span></p>
<p>The <strong>maximum value</strong> for <strong>z</strong> is 17 at (3,2) and the <strong>minimum value</strong> is 3 at (1, 0).</p>
<div id="simplexmethod" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.9.1</span> Simplex Method<a href="numericallinearalgebra.html#simplexmethod" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Alternatively, rather than relying on a graph to visualize the <strong>feasible region</strong>, we can use the <strong>Simplex Method</strong> to find the optimal solution - maximum or minimum value of <strong>z</strong>. </p>
<p><strong>First</strong>, transform inequality equations into equality equations. To do this, we use the following guide:</p>
<ul>
<li>For <span class="math inline">\(\mathbf{\le}\)</span> inequality, we add a <strong>slack variable</strong> (<span class="math inline">\(s_i\)</span>) to the left hand side (LHS) of the inequality. </li>
<li>For <span class="math inline">\(\mathbf{\ge}\)</span> inequality, we add an <strong>excess or surplus variable</strong> (<span class="math inline">\(e_i\)</span>) and an <strong>artificial variable</strong> (<span class="math inline">\(a_i\)</span>) to the LHS of the inequality. </li>
<li>For <span class="math inline">\(\mathbf{=}\)</span> equality, we add an <strong>artificial variable</strong> (<span class="math inline">\(a_i\)</span>) to the LHS of the equality.</li>
<li>For each <strong>artificial variable</strong> added to the constraints, subtract a term from the objective function, namely (<span class="math inline">\(M \times a_i\)</span>), where <strong>M</strong> is an arbitrarily large number. If the objective function is for minimization, then we add the term instead.</li>
<li>If the right-hand side (RHS) constant is negative, multiply the inequality with <span class="math inline">\(\mathbf{(-1)}\)</span>.</li>
</ul>
<p><strong>Second</strong>, let us re-arrange our equations such that we have the variables located at the left-hand side (LHS) and the constant located at the right-hand side (RHS).</p>
<p><span class="math display">\[
\left\{
\begin{array}{lrrrrrrrr}
y &amp;\le&amp; -\frac{1}{3}x &amp;+&amp; 3\\
y &amp;\ge&amp; -3x &amp;+&amp; 3\\
x &amp;\le&amp; 3\\
3x &amp;+&amp; 4y &amp;=&amp; z
\end{array}
\right\}\rightarrow
\left\{
\begin{array}{rrrrrrrrrrrrrr}
\frac{1}{3}x &amp;+&amp; 1y &amp;\le&amp; 3\\
3x &amp;+&amp; 1y &amp;\ge&amp;  3\\
1x &amp;+&amp; 0y &amp;\le&amp; 3\\
3x &amp;+&amp; 4y &amp;=&amp; z
\end{array}
\right\}
\]</span></p>
<p><strong>Third</strong>, let us start adding variables. The <strong>slack</strong>, <strong>surplus</strong>, and <strong>artificial variables</strong> serve as placeholder buffers that indicate how much difference the left-hand side is from the right-hand side constant.</p>
<p><span class="math display">\[
\left\{
\begin{array}{llllllllllllllllllllr}
\frac{1}{3}x &amp;+&amp; 1y &amp;+&amp; s_1 &amp;=&amp; 3\\
3x &amp;+&amp; 1y  &amp;-&amp; e_1 &amp;=&amp;  3\\
1x &amp;+&amp;  0y &amp;+&amp; s_2 &amp;=&amp; 3\\
&amp;&amp; 3x &amp;+&amp;  4y &amp;=&amp; z
\end{array}
\right\}
\rightarrow
\left\{
\begin{array}{lllllllllllllllllllllr}
1x &amp;+&amp; 3y &amp;+&amp; s_1 &amp;&amp;&amp;=&amp; 9\\
3x &amp;+&amp; 1y &amp;-&amp; e_1 &amp;+&amp; a_1 &amp;=&amp;  3\\
1x &amp;+&amp;  0y &amp;+&amp; s_2 &amp;&amp; &amp;=&amp; 3\\
 3x &amp;+&amp;  4y &amp;-&amp;   Ma_1 &amp;&amp; &amp;=&amp; z
\end{array}
\right\}
\]</span></p>
<p>There are a few notes to mention here. The first note is that the value of <span class="math inline">\(\mathbf{a_1}\)</span> equals <span class="math inline">\(-3x - 1y + e_1 + 3\)</span>. Therefore, <strong>z</strong> can be expanded as such (note the use of Big M):</p>
<p><span class="math display">\[
\begin{array}{lll}
3x + 4y - M(-3x - 1y + e_1 + 3) &amp;= z \\
3x + 4y + 3Mx + My - Me_1 - 3M &amp;= z\\
(3+3M)x + (4+M)y - Me_1 - 3M &amp;= z\\
z -(3+3M)x -(4+M)y + Me_1  &amp;= -3M
\end{array}
\]</span></p>
<p>The second note is that we have added four new constraints to cover the four basic variables:</p>
<p><span class="math display">\[
s_1, s_2, e_1, a_1 \ge 0
\]</span></p>
<p>Lastly, introducing an <strong>artificial variable</strong> to the equations avoids us from ending into an infeasible solution, for example when we add <strong>surplus variables</strong>. Because of the existence of <strong>artificial variables</strong> in the problem, let us use the <strong>Big M Method</strong> and avoid a <strong>two-phase approach</strong>. The <strong>M</strong> indicates a large number that acts as a buffer variable to avoid an infeasible solution. </p>
<p><strong>Fourth</strong>, we can now create an initial <strong>augmented matrix</strong> called <strong>simplex tableau</strong> based on the transformed equalities. See Figure <a href="numericallinearalgebra.html#fig:simplextableau">3.36</a>. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simplextableau"></span>
<img src="simplextableau.png" alt="Simplex Method (Maximization)" width="100%" />
<p class="caption">
Figure 3.36: Simplex Method (Maximization)
</p>
</div>
<p>In the figure, the initial tableau shows that column <strong>x</strong> has the most negative number, which becomes our <strong>entering</strong> variable. <span class="math inline">\(\mathbf{A_1}\)</span> becomes our departing variable (based on the ratio test - divide <strong>b</strong> by <strong>x</strong> to get the row with the smallest positive quotient). Here, the <strong>pivot element</strong> is 3 - meaning all values in the column need to be zeroed out except the pivot element. We perform transformation using the equations under the <strong>Oper</strong> column for rows, namely <strong>S1</strong>, <strong>S2</strong>, and <strong>Z</strong>. Doing so gets us the <strong>1st transformation tableau</strong>. Then, the <strong>pivot element</strong> is transformed to 1 with the rest of the rows. Then, under the <strong>Base</strong> column of the <strong>1st transformed tableau</strong>, we see <span class="math inline">\(\mathbf{X}\)</span> row taking the place of <span class="math inline">\(\mathbf{A_1}\)</span>. We then repeat the process. We see that column <strong>Y</strong> has the most negative number, and the smallest positive quotient falls under row <span class="math inline">\(\mathbf{S_1}\)</span> - the departing variable. The <strong>pivot element</strong> is also <strong>8/3</strong>. With that, we run the iteration until we reach the final transformation, giving the following conclusion:</p>
<p>The optimal (maximal) solution is <span class="math inline">\(\mathbf{z} = 17\)</span> at (3,2) with the following <strong>base</strong> values:</p>
<p><span class="math display">\[
x = 3\ \ \ \ \ \ \  
y = 2\ \ \ \ \ \ \  
s_1 = 0\ \ \ \ \ \ \ 
s_2 = 0\ \ \ \ \ \ \ 
e_1 = 8\ \ \ \ \ \ \ 
a_1 = 0
\]</span>
If our <strong>objective function</strong> is set for a minimizaton problem, then we add <span class="math inline">\(Ma_i\)</span> rather than subtract.</p>
<p><span class="math display">\[
\begin{array}{lll}
3x + 4y + M(-3x - 1y + e_1 + 3) &amp;= z \\
3x + 4y - 3Mx - My + Me_1 + 3M &amp;= z\\
(3-3M)x + (4-M)y + Me_1 + 3M &amp;= z\\
z -(3-3M)x -(4-M)y - Me_1  &amp;= 3M 
\end{array}
\]</span>
From there, we follow the same <strong>Big M Method</strong>. See Figure <a href="numericallinearalgebra.html#fig:simplextableau1">3.37</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simplextableau1"></span>
<img src="simplextableau1.png" alt="Simplex Method (Minimization)" width="100%" />
<p class="caption">
Figure 3.37: Simplex Method (Minimization)
</p>
</div>
<p>The optimal (minimal) solution is <span class="math inline">\(\mathbf{z} = 3\)</span> at (1,0) with the following <strong>base</strong> values:</p>
<p><span class="math display">\[
x = 1\ \ \ \ \ \ \  
y = 0\ \ \ \ \ \ \  
s_1 = 8\ \ \ \ \ \ \ 
s_2 = 2\ \ \ \ \ \ \ 
e_1 = 0\ \ \ \ \ \ \ 
a_1 = 0
\]</span></p>
<p>The same approach applies to the second graph, which has a convex quadratic equation. We just need to analyze a given optimization problem statement and see if there is a solution based on the given two constraints.</p>
<p><span class="math display">\[
\mathbf{\text{subject to: }} y \le 2\ \text{and}\ y \ge \frac{1}{2}x^2
\]</span></p>
</div>
<div id="dualsimplex" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.9.2</span> Dual Simplex<a href="numericallinearalgebra.html#dualsimplex" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, let us get more familiar with the <strong>Theory of Duality</strong> in which our original problem is in <strong>Primal form</strong>, which can be transformed into its <strong>Dual form</strong>.    </p>
<p>For <strong>constrained minimization problem</strong>, we have the following <strong>Canonical-Primal</strong> formulation:</p>
<p><span class="math display">\[
\begin{array}{llll}
\mathbf{\text{minimize:}} &amp; \mathbf{c}^T\mathbf{x} = z &amp;\leftarrow\ \ \ \sum_{j=1}^n c_j x_j = z\\
\mathbf{\text{subject to:}} &amp; A \mathbf{x} \ge \mathbf{b} &amp;\leftarrow\ \ \ \sum_{j=1}^n a_{ij} x_j \ge \mathbf{b}_i &amp;\forall i = 1,..,m\\
 &amp; \mathbf{x} \ge \mathbf{0}  &amp;\leftarrow\ \ \ \mathbf{x}_j \ge \mathbf{0} &amp; \forall j = 1,..,n
 \end{array}
\]</span></p>
<p>where <strong>m</strong> is the number of constraints, and <strong>n</strong> is the number of decision variables.</p>
<p>For <strong>constrained maximization problem</strong>, we have the following <strong>Canonical-Dual</strong> formulation:</p>
<p><span class="math display">\[
\begin{array}{llll}
\mathbf{\text{maximize:}} &amp; \mathbf{b}^T\mathbf{y} = z &amp;\leftarrow\ \ \ \sum_{i=1}^m b_i y_i = z\\
\mathbf{\text{subject to:}} &amp; A^T \mathbf{y} \le \mathbf{c} &amp;\leftarrow\ \ \ \sum_{i=1}^m a_{ij} y_i \le \mathbf{c}_j &amp;\forall j = 1,..,n\\
 &amp; \mathbf{y} \ge \mathbf{0}  &amp;\leftarrow\ \ \ \mathbf{y}_i \ge \mathbf{0} &amp; \forall i = 1,..,m
 \end{array}
\]</span>
A more general and comprehensive guide is expressed in Figure <a href="numericallinearalgebra.html#fig:duality">3.38</a> <span class="citation">(Ekeocha R.J. et al. <a href="bibliography.html#ref-ref1210r">2018</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:duality"></span>
<img src="duality.png" alt="Duality Theory" width="80%" />
<p class="caption">
Figure 3.38: Duality Theory
</p>
</div>
<p>To illustrate, let us use the same example as we demonstrated recently. Here, we <strong>maximize</strong> the objective function - in our case, this characterizes the <strong>Dual formulation</strong> of our problem. Let us ensure our inequalities are transformed into <span class="math inline">\(\le\)</span> constraints. </p>
<p><span class="math display">\[
\begin{array}{rlllll}
\mathbf{\text{objective function}}&amp;:\ 3y_1 + 4y_2 = z\ \ \ \text{(maximize)}\\
\mathbf{\text{subject to}}&amp;:\ +1y_1 + +3y_2 \le +9\\
&amp;:\ -3y_1 + - 1y_2  \le -3\\
&amp;:\ +1y_1 + +0y_2 \le +3\\
\end{array}
\]</span>
The constraints above are re-arranged from their original form. Note that we also modified the variable names to follow the rules mentioned above - so not to confuse, we have <strong>y1</strong> for <strong>x</strong> and <strong>y2</strong> for <strong>y</strong>:</p>
<p><span class="math display">\[
\left\{
\begin{array}{lrrrrrrrr}
y &amp;\le&amp; -\frac{1}{3}x &amp;+&amp; 3\\
y &amp;\ge&amp; -3x &amp;+&amp; 3\\
x &amp;\le&amp; 3\\
\end{array}
\right\}\rightarrow
\left\{
\begin{array}{rrrrrrrrrrrrrr}
1y_1 &amp;+&amp; 3y_2 &amp;\le&amp; 9\\
-3y_1 &amp;+&amp; -1y_2 &amp;\le&amp;  -3\\
1y_1 &amp;+&amp; 0y_2 &amp;\le&amp; 3\\
\end{array}
\right\}
\]</span></p>
<p>Based on the rule above, let us discover the <strong>Primal formulation of the problem</strong>. Then, to write our <strong>Dual Problem</strong> to its <strong>Primal formulation</strong>, we perform the following transposition of the <strong>coefficients</strong>:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rrrrrrrrrrrrrr}
1 &amp;+&amp; 3 &amp;\le&amp; 9\\
-3 &amp;+&amp; -1 &amp;\le&amp;  -3\\
1 &amp;+&amp; 0 &amp;\le&amp; 3\\
3 &amp;+&amp; + 4 &amp;=&amp; 0
\end{array}
\right\}\rightarrow
\left\{
\begin{array}{rrrrrrrrrrrrrr}
1 &amp;+&amp; -3 &amp;+&amp; 1 &amp;\ge&amp; 3\\
3 &amp;+&amp; -1 &amp;+&amp; 0 &amp;\ge&amp; 4\\
9 &amp;+&amp; -3 &amp;+&amp; 3 &amp;\ge&amp; 0\\
\end{array}
\right\}
\]</span></p>
<p>From here, we can now express the <strong>Primal formulation</strong> like so:</p>
<p><span class="math display">\[
\begin{array}{rlllll}
\mathbf{\text{objective function}}&amp;:\ 9x_1 + -3x_2 + 3x_3 = z\ \ \ \text{(minimize)}\\
\mathbf{\text{subject to}}&amp;:\ 1x_1 + -3x_2 + 1x_3 \ge 3\\
&amp;:\ 3x_1 + -1x_2 + 0x_3 \ge 4\\
\end{array}
\]</span></p>
<p>We can then use <strong>Simplex Method</strong> to solve for both <strong>maximal</strong> and <strong>minimal</strong> values.</p>
<p>A good case of an optimization problem has to do with blending pastry ingredients, which is a typical toy example. Our goal is to minimize cost (the primal problem) subject to a minimum amount of ingredients. On the other hand, the dual problem is to maximize the number of ingredients subject to a maximum allowable cost.</p>
<p>We leave readers to investigate the <strong>standard forms</strong> of an optimization problem that involves one of the optimal forms to be <strong>unrestricted</strong>. Refer to fig <a href="numericallinearalgebra.html#fig:duality">3.38</a>.</p>
</div>
<div id="primaldual" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.9.3</span> Primal-Dual Formulation<a href="numericallinearalgebra.html#primaldual" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us now contemplate a case in which our equations are non-linear. As we move to <strong>Lagrangian Multiplier</strong> methods, this becomes apparent in the next section. For intuition, let us review Figure <a href="numericallinearalgebra.html#fig:primaldual">3.39</a>. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:primaldual"></span>
<img src="primaldual.png" alt="Primal-Dual Problem" width="80%" />
<p class="caption">
Figure 3.39: Primal-Dual Problem
</p>
</div>
<p>A <strong>weak duality</strong> theorem is subject to the following such that the <strong>objective function</strong> being maximized is less or equal to the <strong>objective function</strong> being minimized.:</p>
<p><span class="math display">\[\begin{align}
\underbrace{\text{max:}\ \mathbf{b}^T \mathbf{x}}_{D(\alpha)} \le \underbrace{\text{min:}\  \mathbf{c}^T\mathbf{y}}_{P(w)}
\end{align}\]</span></p>
<p>A <strong>strong duality</strong> happens when the <strong>duality gap</strong> equates to zero such that, by optimizing <span class="math inline">\(\mathbf{w}^*\)</span> and <span class="math inline">\(\mathbf{\alpha}^*\)</span>, we arrive at the objective functions being equal. </p>
<p><span class="math display">\[\begin{align}
P(w^*) = D(\alpha^*)
\end{align}\]</span></p>
<p>We cover more of this topic in Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under the <strong>Non-Linear SVM</strong> section.</p>
</div>
<div id="lagrange-multiplier" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.9.4</span> Lagrange Multiplier <a href="numericallinearalgebra.html#lagrange-multiplier" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before switching context to a new area, let us introduce <strong>Lagrange Multiplier</strong>, denoted by the symbol lambda (<span class="math inline">\(\lambda\)</span>). For intuition, let us use Figure <a href="numericallinearalgebra.html#fig:lagrangemultiplier">3.40</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lagrangemultiplier"></span>
<img src="lagrangemultiplier.png" alt="Lagrange Multiplier" width="60%" />
<p class="caption">
Figure 3.40: Lagrange Multiplier
</p>
</div>
<p>The goal is to find the point called <strong>extremum</strong> at which a function, namely <span class="math inline">\(f(x)\)</span>, is maximized up to (but barely touching) a level curve (contour) of a constraining function, namely <span class="math inline">\(g(x)\)</span>. We deal with vectors of the functions called gradients denoted as <span class="math inline">\(\nabla\)</span> as these gradients are perpendicular (or normal) to their respective tangent lines and are scaled using the <strong>Lagrange multiplier</strong> <span class="math inline">\(\lambda\)</span>. We have a <strong>maximizing/minimizing function</strong> and a <strong>constraining function</strong>, scaled using <strong>Lagrange multiplier</strong>.</p>
<p><span class="math display">\[
\underbrace{f(x)}_\text{max/minimizing function}\ \ \ \ \ \ \ \   
\underbrace{g(x)}_\text{inequality constraint}\ \ \ \ \ \ \ \   
\underbrace{h(x)}_\text{equality constraint}
\]</span></p>
<p>Mathematically, we are merely solving systems of equations. Here, there is only one constraint.</p>
<p><span class="math display">\[\begin{align}
\nabla f(X) = \lambda_1 \times  \nabla g(X) ,\ \ \ \ \ \ g(X) = c
\end{align}\]</span></p>
<p>The equation for multiple constraints is written as:</p>
<p><span class="math display">\[\begin{align}
\nabla f(X) = \sum_{i=1}^n \lambda^{(1)}_i \times  \nabla g_i(X) + \sum_{j=1}^m \lambda^{(2)}_j \times  \nabla h_j(X),
\end{align}\]</span></p>
<p><span class="math display">\[
\ \ \ \ \ \ \ where\ \ \ \ 
\begin{array}{ll}
\forall i: \ g_i(X) = c^{(a)}_i\\
\forall j: \ h_j(X) = c^{(b)}_j
\end{array}
\]</span></p>
<p>The <strong>Lagrange multiplier</strong> allows us to find the <strong>extremum</strong> or <strong>critical point</strong> as it allows the function f(X) to be in multiples of the constraining function g(X). To find the <strong>extremum</strong>, we use the following <strong>Lagrangian function</strong>:</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}(X,  \lambda) =  f(X) - \sum_{i=1}^n \lambda_i \times  \left( g_i(X) - c_i\right) 
\end{align}\]</span></p>
<p>From there, we perform derivatives on the lagrangian function:</p>
<p><span class="math display">\[\begin{align}
\nabla_{X,Y,Z} \mathcal{L}(X, Y, Z) = 0
\end{align}\]</span></p>
<p>That is to say, perform partial derivatives with respect to each of the variables, namely, <span class="math inline">\(x, y, z\)</span>, setting each equation to zero:</p>
<p><span class="math display">\[
\frac{\partial}{\partial X} \mathcal{L}(X, Y,Z) = 0
\ \ \ \ \ \ \ \ \ \
\frac{\partial}{\partial Y} \mathcal{L}(X, Y, Z) = 0
\ \ \ \ \ \ \ \ \ \
\frac{\partial}{\partial Z} \mathcal{L}(X, Y, Z) = 0
\]</span></p>
<p>To illustrate, suppose we need to maximize <span class="math inline">\(f(x,y,z)\)</span> subject to two constraints <span class="math inline">\(g_1(x,y,z) = 0\)</span> and <span class="math inline">\(g_2(x,y,z) = 0\)</span>:</p>
<p><span class="math display">\[
f(x, y, z) = x - y^2 
\ \ \ \ \ \ \ \ \ \ 
g_1(x, y, z) \equiv y - z^2  = 0
\ \ \ \ \ \ \ \ \ \ 
g_2(x, y, z) \equiv  z - x^2   = 0
\]</span>
The gradient of <span class="math inline">\(f(x,y, z)\)</span> is derived as.</p>

<p><span class="math display">\[\begin{align*}
\frac{\partial}{\partial x} f(x,y,z) {}&amp;= \frac{\partial}{\partial x}  \left(x - y^2  \right) = 1\\
\frac{\partial}{\partial y} f(x,y,z) &amp;= \frac{\partial}{\partial y}  \left(x - y^2 \right) = -2y\\
\frac{\partial}{\partial z} f(x,y,z) &amp;= \frac{\partial}{\partial z}  \left(x - y^2 \right) = 0\\
\nabla f(x,y,z) &amp;= \left[\begin{array}{r} 1 \\ -2y \\ 0 \end{array} \right]
\end{align*}\]</span>
</p>
<p>The gradient of <span class="math inline">\(g_1(x,y, z)\)</span> is derived as.</p>

<p><span class="math display">\[\begin{align*}
\frac{\partial}{\partial x} g_1(x,y,z) {}&amp;= \frac{\partial}{\partial x}  (y - z^2) = 0\\
\frac{\partial}{\partial y} g_1(x,y,z) &amp;= \frac{\partial}{\partial x}  (y - z^2) = 1\\
\frac{\partial}{\partial z} g_1(x,y,z) &amp;= \frac{\partial}{\partial x}  (y - z^2) = -2z\\
\nabla g_1(x,y, z) &amp;= \left[\begin{array}{r} 0 \\1\\-2z \end{array}\right]
\end{align*}\]</span>
</p>
<p>The gradient of <span class="math inline">\(g_2(x,y, z)\)</span> is derived as.</p>

<p><span class="math display">\[\begin{align*}
\frac{\partial}{\partial x} g_2(x,y,z) {}&amp;= \frac{\partial}{\partial x}  \left(z - x^2\right) = -2x\\
\frac{\partial}{\partial y} g_2(x,y,z) &amp;= \frac{\partial}{\partial y}  \left(z - x^2\ \right) = 0\\
\frac{\partial}{\partial z} g_2(x,y,z) &amp;= \frac{\partial}{\partial z}  \left(z - x^2\right) = 1\\ 
\nabla g_2(x,y, z) &amp;= \left[\begin{array}{r} -2x \\ 0 \\ 1 \end{array} \right]
\end{align*}\]</span>
</p>
<p>To validate, we can derive the functions, e.g. <strong>f(x,y,z)</strong>, in R like so:</p>

<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb156-1" data-line-number="1">dx =<span class="st"> </span><span class="kw">D</span>(<span class="kw">expression</span>(x <span class="op">-</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span>), <span class="st">&quot;x&quot;</span>)</a>
<a class="sourceLine" id="cb156-2" data-line-number="2">dy =<span class="st"> </span><span class="kw">D</span>(<span class="kw">expression</span>(x <span class="op">-</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span>), <span class="st">&quot;y&quot;</span>)</a>
<a class="sourceLine" id="cb156-3" data-line-number="3">dz =<span class="st"> </span><span class="kw">D</span>(<span class="kw">expression</span>(x <span class="op">-</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span>), <span class="st">&quot;z&quot;</span>)</a>
<a class="sourceLine" id="cb156-4" data-line-number="4"><span class="kw">as.data.frame</span>(<span class="kw">t</span>(<span class="kw">c</span>(<span class="st">&quot;dx&quot;</span> =<span class="st"> </span>dx, <span class="st">&quot;dy&quot;</span> =<span class="st"> </span><span class="kw">paste0</span>(dy, <span class="dt">collapse=</span><span class="st">&quot;&quot;</span>), <span class="st">&quot;dz&quot;</span> =<span class="st"> </span>dz)))</a></code></pre></div>
<pre><code>##   dx     dy dz
## 1  1 -2 * y  0</code></pre>

<p>Plugging in the gradients into our <strong>Lagrangian Function</strong>:</p>

<p><span class="math display">\[\begin{align*}
\nabla\mathcal{L}(x,y,z) {}&amp;\equiv \nabla f(x,y,z) - ( \lambda_1 \nabla g_1(x,y,z) + \lambda_2 \nabla g_2(x,y,z)) = 0 \\
&amp;\equiv \left[\begin{array}{r} 1 \\ -2y\\ 0 \end{array}\right] -
   \lambda_1 \left[\begin{array}{r} 0 \\ 1 \\ -2z \end{array} \right] -
   \lambda_2 \left[\begin{array}{r} -2x \\ 0 \\ 1 \end{array} \right] = 
   \left[\begin{array}{rrr} 0 \\ 0 \\ 0 \end{array}\right],\\
\end{align*}\]</span>
</p>
<p>we get the following:</p>
<p><span class="math display">\[
x = \frac{-1}{2\lambda_2}\ \ \ \ \ \ \ \
y = \frac{-\lambda_1}{2}\ \ \ \ \ \ \ \ \ \
z = \frac{\lambda_2}{2\lambda_1}
\]</span></p>
<p>We then formulate system of equations to solve for <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>:</p>
<p><span class="math display">\[\begin{align*}
g_1(x,y,z) = y - z^2 {}&amp;= \frac{-\lambda_1}{2} - \left(\frac{\lambda_2}{2\lambda_1}\right)^2 \\
g_2(x,y,z) = z - x^2 &amp;= \frac{\lambda_2}{2\lambda_1} - \left(\frac{-1}{2\lambda_2}\right)^2 
\end{align*}\]</span></p>
<p>Using a scientific calculator, the result would be:</p>
<p><span class="math display">\[
\lambda_1 = -0.6095068\ \ \ \ \ \ \ \ \lambda_2 = -0.6729501
\]</span></p>
<p>From there, we can solve for <span class="math inline">\(x, y, z\)</span>:</p>
<p><span class="math display">\[\begin{align*}
x {}&amp;= \frac{-1}{2\lambda_2} = \frac{-1}{2 \times (-0.6729501)} = 0.7429971\\
y &amp;= \frac{-\lambda_1}{2} = \frac{-(-0.6095068)}{2} = 0.3047534\\
z &amp;= \frac{\lambda_2}{2\lambda_1} = \frac{-0.6729501}{2\times (-0.6095068)} = 0.5520448
\end{align*}\]</span></p>
<p>Therefore, the maximum value of our function f(x,y,z) subject to the two constraints <span class="math inline">\(g_1(x,y,z)=0\)</span> and <span class="math inline">\(g_2(x,y,z) = 0\)</span> is 0.6501225 at the <strong>critical point</strong> (0.7429971, 0.3047534, 0.5520448).</p>
<p>As complementary or prerequisite, we leave readers to investigate <strong>Fenchel Conjugate</strong>, <strong>Conjugate Duality</strong>, and <strong>Convex Analysis</strong>.</p>
</div>
<div id="karush-khun-tucker-conditions" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.9.5</span> Karush-Khun-Tucker Conditions <a href="numericallinearalgebra.html#karush-khun-tucker-conditions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the previous section, we gave a taste of optimization by illustrating the use of <strong>Lagrange multipliers</strong> with two constraints. In this section, we introduce <strong>Karush-Khun-Tucker (KKT)</strong> conditions. The idea is to optimize (e.g.Â minimize / maximize) a <strong>continuously differentiable</strong> objective function with KKT constraints. For example, we have an objective function, namely <span class="math inline">\(f(x^*)\)</span>, and two constraints, namely <span class="math inline">\(h(x^*) = 0\)</span> and <span class="math inline">\(g(x^*) \le 0\)</span> where <span class="math inline">\(x^*\)</span> is the <strong>optimal solution of the problem</strong> so that we have the following:
<span class="math display">\[\begin{align}
min\ f(x^*)\ \ \ \ \ such\ that\ \ \ \ h(x^*) = 0\ \ and\ \ \ g(x^*) \le 0
\end{align}\]</span></p>
<p>Similar to our previous discussion around <strong>Lagrange Multipliers</strong>, we write the objective function and constraints into a single <strong>Lagrangian</strong> equation like so:</p>
<p><span class="math display">\[\begin{align}
L(x^*, \lambda_1, \lambda_2) = f(x^*) + {\lambda_1}^T h(x^*) +  {\lambda_2}^T g(x^*)
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
x^*, \lambda_1, \lambda_2\ \ \  \text{are vectors and  } \lambda_1, \lambda_2 \text{ are dual variables}
\]</span></p>
<p>additionally, we have the following sum of weighted functions:</p>
<p><span class="math display">\[\begin{align}
{\lambda_1}^T h(x^*) {}&amp;= \sum_{j=1}^m \lambda_{1_j} h_j(x^*)\\
{\lambda_2}^T g(x^*) &amp;= \sum_{i=1}^n \lambda_{2_i} g_i(x^*)
\end{align}\]</span></p>
<p>We then optimize by differentiation:</p>
<p><span class="math display">\[\begin{align}
\nabla L(x^*, \lambda_1, \lambda_2) = \nabla f(x^*) + {\lambda_1}^T \nabla h(x^*) + {\lambda_2}^T \nabla g(x^*) {}&amp;= 0 \\
\nonumber \\
\frac{\partial L}{\partial x^*} =
\frac{\partial f}{\partial x^*} +
\lambda_1\frac{\partial h}{\partial x^*} +
\lambda_2\frac{\partial g}{\partial x^*} &amp;= 0
\end{align}\]</span></p>
<p>In the specific case above, the <strong>KKT</strong> conditions are:</p>
<p><span class="math display">\[
\begin{array}{lrl}
\text{* Optimality (Stationarity) condition} &amp; \nabla L(x_i^*, \lambda1, \lambda_2) = 0, &amp; i=1,...,n\\
\text{* Primal Feasibility conditions} &amp; h_j(x^*) = 0,  &amp; j=1,...,m \\
  &amp;  g_i(x^*) \le 0, &amp; i=1,...,n \\
\text{* Dual Feasibility condition} &amp; \lambda_i \ge 0, &amp; i=1,...,l \\
\text{* Complementary Slackness condition} &amp; {\lambda_{2_i}}^Tg_i(x^*)  =  0, &amp; i=1,...,n \\
\end{array} 
\]</span></p>
<p>We cover the case of <strong>SVM</strong> in applying <strong>KKT</strong> constraints in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>), including topics around <strong>Primal</strong> and <strong>Dual</strong> formulation.</p>
</div>
</div>
<div id="summary-1" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.10</span> Summary<a href="numericallinearalgebra.html#summary-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In dealing with approximations by way of iteration, it helps to pay close attention to convergence speed. There are optimization techniques that can be used to improve convergence speed. We will explore these optimization solutions in the Machine Learning and Deep Learning chapters.
We have shown the mechanics of approximation by showing different stationary iterative solutions for each type of problem, dealing specifically with systems of equations that do not depend on change or time. In the next chapter, we review methods used for dynamic memory-less systems. After which, we also review methods used for probabilistic memory-based systems.</p>
<p>Systems that are considered dynamic are dependent on changes in time.</p>
<p>Systems that are considered memory-based are dependent on state changes.</p>
<p>Dynamic memory-less (stateless) systems are discussed in the Calculus chapter.</p>
<p>Probabilistic memory-based (stateless) systems are discussed in the Bayesian Chapter.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linearalgebra.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="numericalcalculus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
