<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Computational Learning I | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Computational Learning I | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Computational Learning I | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2023-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian2.html"/>
<link rel="next" href="machinelearning2.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#acknowledgment-and-motivations"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#caveat"><i class="fa fa-check"></i><b>0.2</b> Caveat</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i><b>0.3</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.4" data-path="mathematical-notation.html"><a href="mathematical-notation.html#notation"><i class="fa fa-check"></i><b>0.4</b> Notation</a></li>
<li class="chapter" data-level="0.5" data-path="mathematical-notation.html"><a href="mathematical-notation.html#number-system"><i class="fa fa-check"></i><b>0.5</b> Number System</a></li>
<li class="chapter" data-level="0.6" data-path="mathematical-notation.html"><a href="mathematical-notation.html#implementation"><i class="fa fa-check"></i><b>0.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numericalmethods.html"><a href="numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="numericalmethods.html"><a href="numericalmethods.html#closed-form-equation"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="numericalmethods.html"><a href="numericalmethods.html#analytical-and-numerical-solutions"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="numericalmethods.html"><a href="numericalmethods.html#significant-figures"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="numericalmethods.html"><a href="numericalmethods.html#accuracy"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="numericalmethods.html"><a href="numericalmethods.html#precision"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="numericalmethods.html"><a href="numericalmethods.html#stability-and-sensitivity"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="numericalmethods.html"><a href="numericalmethods.html#stiffness-and-implicitness"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="numericalmethods.html"><a href="numericalmethods.html#conditioning-and-posedness"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="linearalgebra.html"><a href="linearalgebra.html#system-of-linear-equations"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="linearalgebra.html"><a href="linearalgebra.html#scalar-vector-and-matrix-tensor"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition-and-multiplication"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="linearalgebra.html"><a href="linearalgebra.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="linearalgebra.html"><a href="linearalgebra.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linearalgebra.html"><a href="linearalgebra.html#magnitude-direction-unit-vectors"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-combination-and-independence"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="linearalgebra.html"><a href="linearalgebra.html#space-span-and-basis"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="linearalgebra.html"><a href="linearalgebra.html#determinants"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="linearalgebra.html"><a href="linearalgebra.html#minors-cofactors-and-adjugate-forms"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="linearalgebra.html"><a href="linearalgebra.html#inverse-form-and-row-echelon-form"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-transformations"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="linearalgebra.html"><a href="linearalgebra.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="linearalgebra.html"><a href="linearalgebra.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="linearalgebra.html"><a href="linearalgebra.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="linearalgebra.html"><a href="linearalgebra.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="linearalgebra.html"><a href="linearalgebra.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="linearalgebra.html"><a href="linearalgebra.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="linearalgebra.html"><a href="linearalgebra.html#rank-and-nullity"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="linearalgebra.html"><a href="linearalgebra.html#singularity-and-triviality"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="linearalgebra.html"><a href="linearalgebra.html#orthogonality-and-orthonormality"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="linearalgebra.html"><a href="linearalgebra.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-reconstruction-using-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="linearalgebra.html"><a href="linearalgebra.html#diagonalizability-of-a-matrix"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="linearalgebra.html"><a href="linearalgebra.html#trace-of-a-square-matrix"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="linearalgebra.html"><a href="linearalgebra.html#algebraic-and-geometric-multiplicity"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="linearalgebra.html"><a href="linearalgebra.html#types-of-matrices"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-factorization"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="linearalgebra.html"><a href="linearalgebra.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="linearalgebra.html"><a href="linearalgebra.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="linearalgebra.html"><a href="linearalgebra.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="linearalgebra.html"><a href="linearalgebra.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="linearalgebra.html"><a href="linearalgebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="linearalgebra.html"><a href="linearalgebra.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="linearalgebra.html"><a href="linearalgebra.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="linearalgebra.html"><a href="linearalgebra.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="linearalgebra.html"><a href="linearalgebra.html#software-libraries"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="linearalgebra.html"><a href="linearalgebra.html#summary"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#iteration-and-convergence"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-root-and-fixed-point-by-iteration"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-solutions-to-systems-of-eqs-by-iteration-ax-b"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialregression"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-polynomial-functions-by-series-expansion"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialinterpolation"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialsmoothing"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-optimization"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#summary-1"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numericalcalculus.html"><a href="numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#introductory-calculus"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-integration"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-differentiation"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-ordinary-differential-equations"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="numericalcalculus.html"><a href="numericalcalculus.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="numericalcalculus.html"><a href="numericalcalculus.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="numericalcalculus.html"><a href="numericalcalculus.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="numericalcalculus.html"><a href="numericalcalculus.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="numericalcalculus.html"><a href="numericalcalculus.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="numericalcalculus.html"><a href="numericalcalculus.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-functional-differential-equations"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-partial-differential-equations"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-fourier-series-and-transform"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#summary-2"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="numericalprobability.html"><a href="numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="numericalprobability.html"><a href="numericalprobability.html#approximation-based-on-random-chances"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="numericalprobability.html"><a href="numericalprobability.html#distribution"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="numericalprobability.html"><a href="numericalprobability.html#mass-and-density"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="numericalprobability.html"><a href="numericalprobability.html#probability"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-density-function-pdf"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-mass-function-pmf"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="numericalprobability.html"><a href="numericalprobability.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="numericalprobability.html"><a href="numericalprobability.html#special-functions"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="numericalprobability.html"><a href="numericalprobability.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="numericalprobability.html"><a href="numericalprobability.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="numericalprobability.html"><a href="numericalprobability.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="numericalprobability.html"><a href="numericalprobability.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="numericalprobability.html"><a href="numericalprobability.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="numericalprobability.html"><a href="numericalprobability.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="numericalprobability.html"><a href="numericalprobability.html#distributiontypes"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="numericalprobability.html"><a href="numericalprobability.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="numericalprobability.html"><a href="numericalprobability.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="numericalprobability.html"><a href="numericalprobability.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="numericalprobability.html"><a href="numericalprobability.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="numericalprobability.html"><a href="numericalprobability.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="numericalprobability.html"><a href="numericalprobability.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="numericalprobability.html"><a href="numericalprobability.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="numericalprobability.html"><a href="numericalprobability.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="numericalprobability.html"><a href="numericalprobability.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="numericalprobability.html"><a href="numericalprobability.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="numericalprobability.html"><a href="numericalprobability.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="numericalprobability.html"><a href="numericalprobability.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="numericalprobability.html"><a href="numericalprobability.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="numericalprobability.html"><a href="numericalprobability.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="numericalprobability.html"><a href="numericalprobability.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="numericalprobability.html"><a href="numericalprobability.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="numericalprobability.html"><a href="numericalprobability.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="numericalprobability.html"><a href="numericalprobability.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="numericalprobability.html"><a href="numericalprobability.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="numericalprobability.html"><a href="numericalprobability.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="numericalprobability.html"><a href="numericalprobability.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="numericalprobability.html"><a href="numericalprobability.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="numericalprobability.html"><a href="numericalprobability.html#summary-3"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="statistics.html"><a href="statistics.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="statistics.html"><a href="statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="statistics.html"><a href="statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="statistics.html"><a href="statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="statistics.html"><a href="statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="statistics.html"><a href="statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="statistics.html"><a href="statistics.html#inferential-statistics"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="statistics.html"><a href="statistics.html#the-significance-of-difference"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="statistics.html"><a href="statistics.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="statistics.html"><a href="statistics.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="statistics.html"><a href="statistics.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="statistics.html"><a href="statistics.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="statistics.html"><a href="statistics.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="statistics.html"><a href="statistics.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="statistics.html"><a href="statistics.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearson’s Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="statistics.html"><a href="statistics.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="statistics.html"><a href="statistics.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="statistics.html"><a href="statistics.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="statistics.html"><a href="statistics.html#post-hoc-analysis"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="statistics.html"><a href="statistics.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="statistics.html"><a href="statistics.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="statistics.html"><a href="statistics.html#multiple-comparison-tests"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="statistics.html"><a href="statistics.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="statistics.html"><a href="statistics.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="statistics.html"><a href="statistics.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="statistics.html"><a href="statistics.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="statistics.html"><a href="statistics.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="statistics.html"><a href="statistics.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="statistics.html"><a href="statistics.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="statistics.html"><a href="statistics.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="statistics.html"><a href="statistics.html#statistical-modeling"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="statistics.html"><a href="statistics.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="statistics.html"><a href="statistics.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="statistics.html"><a href="statistics.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="statistics.html"><a href="statistics.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="statistics.html"><a href="statistics.html#regression-analysis"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="statistics.html"><a href="statistics.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="statistics.html"><a href="statistics.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="statistics.html"><a href="statistics.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="statistics.html"><a href="statistics.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="statistics.html"><a href="statistics.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="statistics.html"><a href="statistics.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="statistics.html"><a href="statistics.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="statistics.html"><a href="statistics.html#the-significance-of-regression"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="statistics.html"><a href="statistics.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="statistics.html"><a href="statistics.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="statistics.html"><a href="statistics.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="statistics.html"><a href="statistics.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="statistics.html"><a href="statistics.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="statistics.html"><a href="statistics.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="statistics.html"><a href="statistics.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="statistics.html"><a href="statistics.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="statistics.html"><a href="statistics.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="statistics.html"><a href="statistics.html#inference-for-regression"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="statistics.html"><a href="statistics.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="statistics.html"><a href="statistics.html#summary-4"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian.html"><a href="bayesian.html#probability-1"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian.html"><a href="bayesian.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian.html"><a href="bayesian.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian.html"><a href="bayesian.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian.html"><a href="bayesian.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="bayesian.html"><a href="bayesian.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian.html"><a href="bayesian.html#probability-rules"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian.html"><a href="bayesian.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian.html"><a href="bayesian.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian.html"><a href="bayesian.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="bayesian.html"><a href="bayesian.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="bayesian.html"><a href="bayesian.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="bayesian.html"><a href="bayesian.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="bayesian.html"><a href="bayesian.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="bayesian.html"><a href="bayesian.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="bayesian.html"><a href="bayesian.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian.html"><a href="bayesian.html#bayes-theorem"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian.html"><a href="bayesian.html#naïve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naïve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian.html"><a href="bayesian.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="bayesian.html"><a href="bayesian.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian.html"><a href="bayesian.html#conjugacy"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian.html"><a href="bayesian.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian.html"><a href="bayesian.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian.html"><a href="bayesian.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="bayesian.html"><a href="bayesian.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="bayesian.html"><a href="bayesian.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="bayesian.html"><a href="bayesian.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="bayesian.html"><a href="bayesian.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="bayesian.html"><a href="bayesian.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="bayesian.html"><a href="bayesian.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="bayesian.html"><a href="bayesian.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="bayesian.html"><a href="bayesian.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="bayesian.html"><a href="bayesian.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian.html"><a href="bayesian.html#information-theory"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian.html"><a href="bayesian.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian.html"><a href="bayesian.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian.html"><a href="bayesian.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian.html"><a href="bayesian.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian.html"><a href="bayesian.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian.html"><a href="bayesian.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian.html"><a href="bayesian.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bayesian.html"><a href="bayesian.html#bayesianinference"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="bayesian.html"><a href="bayesian.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="bayesian.html"><a href="bayesian.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="bayesian.html"><a href="bayesian.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="bayesian.html"><a href="bayesian.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian2.html"><a href="bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-models"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="bayesian2.html"><a href="bayesian2.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="bayesian2.html"><a href="bayesian2.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="bayesian2.html"><a href="bayesian2.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="bayesian2.html"><a href="bayesian2.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="bayesian2.html"><a href="bayesian2.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="bayesian2.html"><a href="bayesian2.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="bayesian2.html"><a href="bayesian2.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="bayesian2.html"><a href="bayesian2.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="bayesian2.html"><a href="bayesian2.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="bayesian2.html"><a href="bayesian2.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="bayesian2.html"><a href="bayesian2.html#simulation-and-sampling"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="bayesian2.html"><a href="bayesian2.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="bayesian2.html"><a href="bayesian2.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="bayesian2.html"><a href="bayesian2.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="bayesian2.html"><a href="bayesian2.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="bayesian2.html"><a href="bayesian2.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="bayesian2.html"><a href="bayesian2.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-analysis"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian2.html"><a href="bayesian2.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian2.html"><a href="bayesian2.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian2.html"><a href="bayesian2.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="bayesian2.html"><a href="bayesian2.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian2.html"><a href="bayesian2.html#summary-5"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="machinelearning1.html"><a href="machinelearning1.html#observation-and-measurement"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="machinelearning1.html"><a href="machinelearning1.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="machinelearning1.html"><a href="machinelearning1.html#input-data"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="machinelearning1.html"><a href="machinelearning1.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="machinelearning1.html"><a href="machinelearning1.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="machinelearning1.html"><a href="machinelearning1.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="machinelearning1.html"><a href="machinelearning1.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="machinelearning1.html"><a href="machinelearning1.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="machinelearning1.html"><a href="machinelearning1.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="machinelearning1.html"><a href="machinelearning1.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="machinelearning1.html"><a href="machinelearning1.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machinelearning1.html"><a href="machinelearning1.html#primitive-methods"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="machinelearning1.html"><a href="machinelearning1.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="machinelearning1.html"><a href="machinelearning1.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="machinelearning1.html"><a href="machinelearning1.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="machinelearning1.html"><a href="machinelearning1.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="machinelearning1.html"><a href="machinelearning1.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="machinelearning1.html"><a href="machinelearning1.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="machinelearning1.html"><a href="machinelearning1.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="machinelearning1.html"><a href="machinelearning1.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="machinelearning1.html"><a href="machinelearning1.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="machinelearning1.html"><a href="machinelearning1.html#distance-metrics"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="machinelearning1.html"><a href="machinelearning1.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="machinelearning1.html"><a href="machinelearning1.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="machinelearning1.html"><a href="machinelearning1.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="machinelearning1.html"><a href="machinelearning1.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="machinelearning1.html"><a href="machinelearning1.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="machinelearning1.html"><a href="machinelearning1.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="machinelearning1.html"><a href="machinelearning1.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="machinelearning1.html"><a href="machinelearning1.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="machinelearning1.html"><a href="machinelearning1.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="machinelearning1.html"><a href="machinelearning1.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="machinelearning1.html"><a href="machinelearning1.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="machinelearning1.html"><a href="machinelearning1.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="machinelearning1.html"><a href="machinelearning1.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="machinelearning1.html"><a href="machinelearning1.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="machinelearning1.html"><a href="machinelearning1.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="machinelearning1.html"><a href="machinelearning1.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="machinelearning1.html"><a href="machinelearning1.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="machinelearning1.html"><a href="machinelearning1.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="machinelearning1.html"><a href="machinelearning1.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="machinelearning1.html"><a href="machinelearning1.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="machinelearning1.html"><a href="machinelearning1.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="machinelearning1.html"><a href="machinelearning1.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="machinelearning1.html"><a href="machinelearning1.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="machinelearning1.html"><a href="machinelearning1.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="machinelearning1.html"><a href="machinelearning1.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="machinelearning1.html"><a href="machinelearning1.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureengineering"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="machinelearning1.html"><a href="machinelearning1.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="machinelearning1.html"><a href="machinelearning1.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="machinelearning1.html"><a href="machinelearning1.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="machinelearning1.html"><a href="machinelearning1.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="machinelearning1.html"><a href="machinelearning1.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="machinelearning1.html"><a href="machinelearning1.html#general-modeling"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="machinelearning1.html"><a href="machinelearning1.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="machinelearning1.html"><a href="machinelearning1.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="machinelearning1.html"><a href="machinelearning1.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="machinelearning1.html"><a href="machinelearning1.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="machinelearning1.html"><a href="machinelearning1.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="machinelearning1.html"><a href="machinelearning1.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="machinelearning1.html"><a href="machinelearning1.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="machinelearning1.html"><a href="machinelearning1.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="machinelearning1.html"><a href="machinelearning1.html#supervised-vs.unsupervised-learning"><i class="fa fa-check"></i><b>9.8</b> Supervised vs. Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="machinelearning1.html"><a href="machinelearning1.html#summary-6"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="machinelearning2.html"><a href="machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="machinelearning2.html"><a href="machinelearning2.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="machinelearning2.html"><a href="machinelearning2.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="machinelearning2.html"><a href="machinelearning2.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="machinelearning2.html"><a href="machinelearning2.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="machinelearning2.html"><a href="machinelearning2.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="machinelearning2.html"><a href="machinelearning2.html#binary-classification-supervised"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="machinelearning2.html"><a href="machinelearning2.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="machinelearning2.html"><a href="machinelearning2.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="machinelearning2.html"><a href="machinelearning2.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="machinelearning2.html"><a href="machinelearning2.html#multi-class-classification-supervised"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="machinelearning2.html"><a href="machinelearning2.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="machinelearning2.html"><a href="machinelearning2.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="machinelearning2.html"><a href="machinelearning2.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="machinelearning2.html"><a href="machinelearning2.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="machinelearning2.html"><a href="machinelearning2.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="machinelearning3.html"><a href="machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="machinelearning3.html"><a href="machinelearning3.html#clustering-unsupervised"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="machinelearning3.html"><a href="machinelearning3.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="machinelearning3.html"><a href="machinelearning3.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="machinelearning3.html"><a href="machinelearning3.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="machinelearning3.html"><a href="machinelearning3.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="machinelearning3.html"><a href="machinelearning3.html#meta-learning"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="machinelearning3.html"><a href="machinelearning3.html#natural-language-processing-nlp"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="machinelearning3.html"><a href="machinelearning3.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="machinelearning3.html"><a href="machinelearning3.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="machinelearning3.html"><a href="machinelearning3.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="machinelearning3.html"><a href="machinelearning3.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="machinelearning3.html"><a href="machinelearning3.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="machinelearning3.html"><a href="machinelearning3.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="machinelearning3.html"><a href="machinelearning3.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="machinelearning3.html"><a href="machinelearning3.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-forecasting"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="machinelearning3.html"><a href="machinelearning3.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="machinelearning3.html"><a href="machinelearning3.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="machinelearning3.html"><a href="machinelearning3.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="machinelearning3.html"><a href="machinelearning3.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="machinelearning3.html"><a href="machinelearning3.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="machinelearning3.html"><a href="machinelearning3.html#recommender-systems"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deeplearning1.html"><a href="deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="deeplearning1.html"><a href="deeplearning1.html#simple-perceptron"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="deeplearning1.html"><a href="deeplearning1.html#adaptive-linear-neuron-adaline"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="deeplearning1.html"><a href="deeplearning1.html#multi-layer-perceptron-mlp"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="deeplearning1.html"><a href="deeplearning1.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="deeplearning1.html"><a href="deeplearning1.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="deeplearning1.html"><a href="deeplearning1.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="deeplearning1.html"><a href="deeplearning1.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="deeplearning1.html"><a href="deeplearning1.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="deeplearning1.html"><a href="deeplearning1.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="deeplearning1.html"><a href="deeplearning1.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="deeplearning1.html"><a href="deeplearning1.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="deeplearning1.html"><a href="deeplearning1.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="deeplearning1.html"><a href="deeplearning1.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="deeplearning1.html"><a href="deeplearning1.html#convolutional-neural-network-cnn"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="deeplearning1.html"><a href="deeplearning1.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="deeplearning1.html"><a href="deeplearning1.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="deeplearning1.html"><a href="deeplearning1.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="deeplearning1.html"><a href="deeplearning1.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="deeplearning1.html"><a href="deeplearning1.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="deeplearning1.html"><a href="deeplearning1.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="deeplearning1.html"><a href="deeplearning1.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="deeplearning1.html"><a href="deeplearning1.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="deeplearning1.html"><a href="deeplearning1.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="deeplearning1.html"><a href="deeplearning1.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="deeplearning1.html"><a href="deeplearning1.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="deeplearning2.html"><a href="deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="deeplearning2.html"><a href="deeplearning2.html#residual-network-resnet"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="deeplearning2.html"><a href="deeplearning2.html#recurrent-neural-network-rnn"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="deeplearning2.html"><a href="deeplearning2.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="deeplearning2.html"><a href="deeplearning2.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="deeplearning2.html"><a href="deeplearning2.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-rnn"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-bidirectional-rnn"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-neural-network-tnn"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="deeplearning2.html"><a href="deeplearning2.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="deeplearning2.html"><a href="deeplearning2.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="deeplearning2.html"><a href="deeplearning2.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="deeplearning2.html"><a href="deeplearning2.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="deeplearning2.html"><a href="deeplearning2.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="deeplearning2.html"><a href="deeplearning2.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="deeplearning2.html"><a href="deeplearning2.html#applications-using-tnn-and-rnn"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="deeplearning2.html"><a href="deeplearning2.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="deeplearning2.html"><a href="deeplearning2.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="deeplearning2.html"><a href="deeplearning2.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="deeplearning2.html"><a href="deeplearning2.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="deeplearning2.html"><a href="deeplearning2.html#generative-adversarial-network-gan"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-reinforcement-network-dqn"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="deeplearning2.html"><a href="deeplearning2.html#summary-8"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="distributedcomputation.html"><a href="distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#integration-and-interoperability"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#ml-pipelines"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-standards"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="distributedcomputation.html"><a href="distributedcomputation.html#general-summary"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>15</b> Appendix</a><ul>
<li class="chapter" data-level="15.1" data-path="appendix.html"><a href="appendix.html#appendix-a"><i class="fa fa-check"></i><b>15.1</b> Appendix A</a><ul>
<li class="chapter" data-level="15.1.1" data-path="appendix.html"><a href="appendix.html#trigonometry"><i class="fa fa-check"></i><b>15.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="15.1.2" data-path="appendix.html"><a href="appendix.html#logarithms"><i class="fa fa-check"></i><b>15.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="15.1.3" data-path="appendix.html"><a href="appendix.html#category-theory"><i class="fa fa-check"></i><b>15.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="appendix.html"><a href="appendix.html#appendix-b"><i class="fa fa-check"></i><b>15.2</b> Appendix B</a><ul>
<li class="chapter" data-level="15.2.1" data-path="appendix.html"><a href="appendix.html#on-random-chances"><i class="fa fa-check"></i><b>15.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="15.2.2" data-path="appendix.html"><a href="appendix.html#on-replacements"><i class="fa fa-check"></i><b>15.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="15.2.3" data-path="appendix.html"><a href="appendix.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>15.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="15.2.4" data-path="appendix.html"><a href="appendix.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>15.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="15.2.5" data-path="appendix.html"><a href="appendix.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>15.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="15.2.6" data-path="appendix.html"><a href="appendix.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>15.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="15.2.7" data-path="appendix.html"><a href="appendix.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>15.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="appendix.html"><a href="appendix.html#appendix-c"><i class="fa fa-check"></i><b>15.3</b> Appendix C</a></li>
<li class="chapter" data-level="15.4" data-path="appendix.html"><a href="appendix.html#appendix-d"><i class="fa fa-check"></i><b>15.4</b> Appendix D</a><ul>
<li class="chapter" data-level="15.4.1" data-path="appendix.html"><a href="appendix.html#lubridate-library"><i class="fa fa-check"></i><b>15.4.1</b> Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machinelearning1" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 9</span> Computational Learning I<a href="machinelearning1.html#machinelearning1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
            autoNumber: "AMS",
            formatNumber: function (n) {return '9.'+n}
      } 
  }
});
</script>
<p>Volumes I and II of this book uncover many tricks and techniques to approximate everyday phenomena. In Volume III, we introduce how machines learn to approximate by training. We call this <strong>Machine Learning (ML)</strong>. Our focus in this chapter limits only to some of the more pervasive and ubiquitous ML techniques that are readily introduced in other literature. Furthermore, unlike the previous chapters, starting with this chapter, we emphasize the mechanics - the operation and process used.</p>
<p>We try to accompany the algorithm with code using the R language. However, it is important to point out that the collection of code in this chapter (and in this entire book) may contain only vanilla implementations of the corresponding mathematics and algorithms presented. It should be noted, therefore, that when we describe the implementation as <strong>example</strong>, it is to be viewed and taken that the implementations are naively created and thus are not meant for production use as they are not at par with a good production code, favoring software engineering approaches over coding the intuition behind the algorithms. A quality production code is better structured and equipped with advanced optimization features with better dynamic programming.</p>
<p>Before we cover machine learning concepts, we begin the chapter by outlining a few precursory concepts around <strong>Input Data</strong>, <strong>Metrics</strong>, and <strong>Exploratory Data Analysis (EDA)</strong> followed by <strong>Feature Engineering</strong></p>
<div id="observation-and-measurement" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.1</span> Observation and Measurement<a href="machinelearning1.html#observation-and-measurement" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our five senses serve as biological measuring tools, giving us the ability to perceive the world; hence, the ability to capture and record measurements. However, our human senses can only estimate more frequently than pinpoint measurements accurately. So if we line up a team of professional basketball players and try to get the height of each one of them just by visual senses, note it down on a piece of paper, and calculate the average height, we may be able to guess that the average height is probably around 6 feet 5 inches.</p>
<p>As a recap, we end up with the following steps:</p>
<ul>
<li>sample a team of basketball players</li>
<li>visually look at their height</li>
<li>estimate their height</li>
<li>predict the average height</li>
</ul>
<p>That is perhaps the most rudimentary level of practicing data science. On the other hand, a scientist relies on measuring devices to capture measurements and record them for analysis. Following an experiment, the scientist repeats the test and further cross-references results with data points from experiments of other colleagues.</p>
<p>When it comes to population data or sampled data, we deal primarily with a collection of observed measurements. We may treat this collection as our data set. Note that we may interchangeably use the term <strong>observations</strong> with <strong>collection</strong>. Now, it is crucial to recognize that our observed data becomes the only knowledge we have of the whole entity or phenomenon we are observing when it comes to observations. To our best measurement, we gauge things by how much we know - based on how much data we sample.</p>
<p>Therefore, it is essential to know the level of measurements we use, the different measurements available, the methods and algorithms exposed to us, and most importantly, the interpretation of the results.</p>
<p>In truth, there is no right or wrong about how we play our observations and measurements. For example, if we get an accuracy of 20%, settling on this initial result may be unwise. Instead, it is a common approach to admit that the result is only the beginning. That means that we have to proceed further with our experiment by re-adjusting, re-formulating, and re-introducing new ways, methods, and parameters to improve our experiment. If there is one leading objective, it is about reaching the actual value based on improved (or adjusted) iterations of our experiment, whether the actual value is known or not. Moreover, it is about sharing with others, hoping that we have a greater chance of hitting the target by collaborating with others. The idea is to find common grounds - to find consistency, stability, and accuracy in our results.</p>
<div id="levels-of-measurements" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.1.1</span> Levels of Measurements<a href="machinelearning1.html#levels-of-measurements" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us start with three kinds of variables that may seem common in everyday datasets: Categorical, Continuous, and Discrete variables.</p>
<ul>
<li><strong>Categorical</strong> variable - is a variable that holds any finite number of categories or groups.</li>
<li><strong>Continuous</strong> variable - is a variable that holds any infinite numerical value.</li>
<li><strong>Discrete</strong> variable - is a variable that can be categorical or continuous.</li>
</ul>
<p>If a discrete variable holds integer numbers (meaning, not a fraction), we consider this as a continuous variable.</p>
<p>e.g. -1, 2, 8, 20, 40</p>
<p>If a discrete variable holds integer numbers but is constrained within a finite number of integer values, we consider this as a categorical variable.</p>
<p>e.g. three colors with their corresponding assigned numbers: 1 = red, 2 = green, 3 = blue</p>
</div>
<div id="levels-of-categorical-measurements" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.1.2</span> Levels of Categorical measurements<a href="machinelearning1.html#levels-of-categorical-measurements" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can classify categorical variables into three kinds: Nominal, Ordinal, and Dichotomous.</p>
<ul>
<li><strong>Ordinal</strong> variable - is a variable that holds categorical values that follow an order.</li>
<li><strong>Nominal</strong> variable - is a variable that holds categorical values that do not follow an order.</li>
<li><strong>Dichotomous (Binary) </strong> variable - is a variable that holds categorical values that have opposites.</li>
</ul>
<p>If a categorical variable follows an order, it is an ordinal variable. A value is either followed by another value, preceded by another value, or both.</p>
<table>
<caption><span id="tab:nice-variables3">Table 9.1: </span>Ordinal Variable</caption>
<thead>
<tr class="header">
<th align="left">Ordinal</th>
<th align="left">Size</th>
<th align="left">Expectations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">First</td>
<td align="left">X-Small</td>
<td align="left">Low expectation</td>
</tr>
<tr class="even">
<td align="left">Second</td>
<td align="left">Small</td>
<td align="left">Just right</td>
</tr>
<tr class="odd">
<td align="left">Third</td>
<td align="left">Medium</td>
<td align="left">High Expectation</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">Large</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">X-Large</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>If a categorical variable does not follow an order, it is a nominal variable. A value cannot be preceded or followed by another value.</p>
<table>
<caption><span id="tab:nice-variables4">Table 9.2: </span>Nominal Variable</caption>
<thead>
<tr class="header">
<th align="left">Mood</th>
<th align="left">Color</th>
<th align="left">Means of Transportation</th>
<th align="left">Branch of Government</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Angry</td>
<td align="left">Red</td>
<td align="left">Bus</td>
<td align="left">Judiciary</td>
</tr>
<tr class="even">
<td align="left">Happy</td>
<td align="left">Blue</td>
<td align="left">Train</td>
<td align="left">Legislative</td>
</tr>
<tr class="odd">
<td align="left">Sad</td>
<td align="left">Green</td>
<td align="left">Plane</td>
<td align="left">Executive</td>
</tr>
<tr class="even">
<td align="left">Excited</td>
<td align="left">Orange</td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>If a categorical variable can hold one of any binary values, it is a dichotomous (binary) variable.</p>
<table>
<caption><span id="tab:nice-variables5">Table 9.3: </span>Binary Variable</caption>
<thead>
<tr class="header">
<th align="left">Answer</th>
<th align="left">Facts</th>
<th align="left">Coin</th>
<th align="left">Transmission</th>
<th align="left">Tide</th>
<th align="left">Binary</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Yes</td>
<td align="left">True</td>
<td align="left">Head</td>
<td align="left">Automatic</td>
<td align="left">High</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">No</td>
<td align="left">False</td>
<td align="left">Tail</td>
<td align="left">Manual</td>
<td align="left">Low</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
</div>
<div id="levels-of-continuous-measurements" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.1.3</span> Levels of Continuous measurements<a href="machinelearning1.html#levels-of-continuous-measurements" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Continuous variables can be classified further into two: Interval and Ratio.</p>
<ul>
<li><strong>Interval</strong> variable - is a variable whose numerical value is measured based on an interval scale.</li>
<li><strong>Ratio</strong> variable - is a variable whose numerical value is measured based on an interval scale with a pre-determined zero value.</li>
</ul>
<p>If the context is about height, then a 6 ft height is an example of an observed value of a ratio variable.</p>
<p>If the context is about ranking, e.g., ranking between 1 and 10, then 10 is an example of an observed value of an interval variable.</p>
</div>
<div id="discrete-vs-continuous-measurements" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.1.4</span> Discrete vs Continuous measurements<a href="machinelearning1.html#discrete-vs-continuous-measurements" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we cover the difference between discrete and continuous numerical variables. Consider Figure <a href="machinelearning1.html#fig:discrete">9.1</a> illustrating both discrete and continuous variables.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:discrete"></span>
<img src="discrete.png" alt="Discrete and Continuous" width="70%" />
<p class="caption">
Figure 9.1: Discrete and Continuous
</p>
</div>
<p>A continuous variable can hold any numerical values between 3 and 4. Divide the space between 3 and 4 and arrive at 3.5. Divide between 3.5 and 4 and get a value of 3.75. We can continue dividing, always arriving at a fraction between the two numbers. We can keep dividing infinitely.</p>
<p>Discrete variables hold numerical values such as integers that we cannot break into smaller values. It can hold any whole number or counting number such as 1, 2, 3, 4, and on. We know there is nothing in between each of the counting numbers. On the contrary, it can also hold fraction numbers as long as the numbers hold specific values and the set of numbers is finite.</p>
</div>
</div>
<div id="input-data" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.2</span> Input Data<a href="machinelearning1.html#input-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Data</strong> is defined as any observable and measurable entity. The term <strong>entity</strong> is important. Most of the dictionary definitions from the net carry attributes that describe an entity (Cambridge, collins, oxford, business, Merriam-Webster et al. - 2020). As examples:</p>
<ul>
<li>separate and independent</li>
<li>distinct and identifiable</li>
<li>self-contained</li>
<li>real being or existence</li>
</ul>
<p>Data is everything that is observable, measurable, and quantifiable.</p>
<p><strong>Input</strong> is data that is fed into a computer.</p>
<div id="structured-data" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.2.1</span> Structured Data<a href="machinelearning1.html#structured-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the early days of computers, the proliferation of database systems has contributed to the idea of normalizing data. At that time, systems designers (SDs) were brought in to help in <strong>computerizing</strong> or <strong>modernizing</strong> a company’s manual systems.</p>
<p>One of the many things that SDs do is collect data by surveying and interviewing companies looking to modernize their processes. The survey starts by asking simple questions about:</p>
<ul>
<li>Company profile (name, functions, processes, policies, responsibilities)</li>
<li>Product profile (stock-keeping units)</li>
<li>Customer profile (market-focused)</li>
</ul>
<p>In principle and practice, SDs use a universal modeling language (UML) to model and design systems. All the data being collected are mapped and translated into diagrams and tables. That makes data much more structured in a sense.</p>
<p>For the sake of explanation, and to make it simpler, suppose we have a simple “buy and sell” system:</p>
<ul>
<li>merchant sells a product</li>
<li>customer buys a product</li>
</ul>
<p>Already, there are 5 <strong>things</strong> (or items) to know about this system:</p>
<ol style="list-style-type: decimal">
<li>Merchant</li>
<li>Customer</li>
<li>Product</li>
<li>Sell</li>
<li>Buy</li>
</ol>
<p>One can classify the five items into three categories: The actors, the actions, and the objects acted upon by the actions.</p>
<table>
<caption><span id="tab:nice-actors">Table 9.4: </span>Actors and Objects</caption>
<thead>
<tr class="header">
<th align="left">Actors</th>
<th align="left">Actions</th>
<th align="left">Objects</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Merchant</td>
<td align="left">Sell</td>
<td align="left">Product</td>
</tr>
<tr class="even">
<td align="left">Customer</td>
<td align="left">Buy</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>In this simple system design, we perform some normalization using two simple and generic categories: Entities and Actions</p>
<table>
<caption><span id="tab:nice-buy">Table 9.5: </span>Categories</caption>
<thead>
<tr class="header">
<th align="left">Entities</th>
<th align="left">Actions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Merchant</td>
<td align="left">Sell</td>
</tr>
<tr class="even">
<td align="left">Customer</td>
<td align="left">Buy</td>
</tr>
<tr class="odd">
<td align="left">Product</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>We then form relationships amongst the categories. See Figure <a href="machinelearning1.html#fig:buysell">9.2</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:buysell"></span>
<img src="buysell.png" alt="Buy and Sell" width="50%" />
<p class="caption">
Figure 9.2: Buy and Sell
</p>
</div>
<p>What do we do if a merchant sells a product to multiple customers? How do we distinguish or identify one customer from another? We can answer that by first identifying customers by their name, contact information, and certain customer types.</p>
<p><strong>Entity:</strong> Customer</p>
<p><strong>Properties:</strong></p>
<ul>
<li>Name</li>
<li>Contact</li>
<li>Type</li>
</ul>
<p>Next, if the merchant has many products to sell, how do we distinguish or identify one product from another? We can identify products by their product name, product type, and product price.</p>
<p><strong>Entity:</strong> Product</p>
<p><strong>Properties:</strong></p>
<ul>
<li>Name</li>
<li>Type</li>
<li>Price</li>
<li>Inventory</li>
</ul>
<p>Similarly, we can identify a merchant by the following properties:</p>
<p><strong>Entity:</strong> Merchant</p>
<p><strong>Properties:</strong></p>
<ul>
<li>Name</li>
<li>Location</li>
<li>Owner</li>
</ul>
<p>It helps to understand that when we talk about individual / unique customers such as John Doe or Jane Smith, we refer to instances of a customer entity. In other words, entities contain instances ( some literature may call them records, others call them tuples ).</p>
<table>
<caption><span id="tab:nice-customer">Table 9.6: </span>Customers</caption>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Contact</th>
<th align="left">Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">John Doe</td>
<td align="left">1.650.nnn.nnn1</td>
<td align="left">Regular Customer</td>
</tr>
<tr class="even">
<td align="left">Jane Smith</td>
<td align="left">1.510.nnn.nnn2</td>
<td align="left">Walk-in Customer</td>
</tr>
</tbody>
</table>
<p>Two things can be said about the detail above. John Doe and Jane Smith are instances of the Customer entity.</p>
<table>
<caption><span id="tab:nice-product">Table 9.7: </span>Products</caption>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Type</th>
<th align="left">Price</th>
<th align="left">Inventory</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Table</td>
<td align="left">Furniture</td>
<td align="left">$100.00</td>
<td align="left">200</td>
</tr>
<tr class="even">
<td align="left">Chair</td>
<td align="left">Furniture</td>
<td align="left">$50.00</td>
<td align="left">100</td>
</tr>
</tbody>
</table>
<p>Table and Chair are instances of the Product entity.</p>
<table>
<caption><span id="tab:nice-merchant">Table 9.8: </span>Merchants</caption>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Location</th>
<th align="left">Owner</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ACME</td>
<td align="left">California</td>
<td align="left">Rich Joe</td>
</tr>
</tbody>
</table>
<p>ACME is an instance of a merchant entity</p>
<p>Now, what characterizes an instance? In the case of a customer entity, a customer instance is characterized based on customer name, contact, and customer type. Characterizing an instance gives a unique personality to an instance, e.g., John Doe is a separate individual from Jane Smith, which means that he has all the characteristics of being a John Doe. Characters here are called attributes, which means that instances are described based on attributes. In subsequent sections, we will be delving more into other translations of attributes such as signals, features, predictor variables, and independent variables.</p>
<p>How do we represent the <strong>action</strong>? Action can mean many things. For example, it can mean an event or an activity. In this specific case, we have transactions as an event. Here is a sample of a transaction:</p>
<pre><code>Transaction:   customer buys a product</code></pre>
<p>Given a transaction in which John Doe bought a single furniture table at $100.00 on January 1, 2018, how do we want the transaction instance to look like:</p>
<p>The transaction record may look like this:</p>
<table>
<caption><span id="tab:nice-trans1">Table 9.9: </span>Transactions</caption>
<thead>
<tr class="header">
<th align="left">Customer</th>
<th align="left">Action</th>
<th align="left">Product</th>
<th align="left">Amount</th>
<th align="left">Quantity</th>
<th align="left">Date</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">John Doe</td>
<td align="left">Buy</td>
<td align="left">Table</td>
<td align="left">$100.00</td>
<td align="left">1</td>
<td align="left">Jan 01, 2018</td>
</tr>
</tbody>
</table>
<p><strong>Storage of information</strong></p>
<p>Given a complete picture of entities and transactions for our business and their related properties, we now have a model that we can translate into a schema that is nothing more than just a representation of the model we constructed. This schema will contain tables (representing the entities and transactions) and table columns (representing the entity properties and transaction properties). This schema will reside in a database system.</p>
<p><strong>Creating tables</strong></p>
<p>Given a structure for our entities, e.g., Customers, Products, Merchants, and Transactions, we now should be able to capture instances of each entity into its corresponding database table. We first have to create tables into a SQL-based database system to do that. Below is a simple database command that creates the tables.</p>

<pre><code>Create Customer table:   
  create table customers(
        name text,
        contact text,
        type text
        )

Create Product table:   
  create table products(
        name text,
        type text,
        price number,
        inventory number
        )

Create Merchant table:   
  create table merchants(
        name text,
        location text,
        owner number
        )

Create Transaction table:   
  create table transactions(
          customer text, 
          action text, 
          product text,
          amount number,
          quantity number,
          date timestamp
          )</code></pre>

<p><strong>Storing a record</strong></p>
<p>To store a transaction record into the <strong>TRANSACTIONS</strong> table.</p>

<pre><code>Record Transaction:   
    insert into 
    transactions(&quot;John Doe&quot;, &quot;Buy&quot;, &quot;Table&quot;, 100, 1, &quot;Jan 01, 2018&quot;)</code></pre>

<p>The table has a transaction record of John Doe, who bought a furniture table for $100.00.</p>
<p>If John Doe buys multiple tables, the transaction table will have many more transaction records.</p>
<table>
<caption><span id="tab:nice-trans2">Table 9.10: </span>Transactions</caption>
<thead>
<tr class="header">
<th align="left">Customer</th>
<th align="left">Action</th>
<th align="left">Product</th>
<th align="left">Amount</th>
<th align="left">Quantity</th>
<th align="left">Date</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">John Doe</td>
<td align="left">Buy</td>
<td align="left">Table</td>
<td align="left">$100.00</td>
<td align="left">1</td>
<td align="left">Jan 01, 2018</td>
</tr>
<tr class="even">
<td align="left">John Doe</td>
<td align="left">Buy</td>
<td align="left">Table</td>
<td align="left">$100.00</td>
<td align="left">3</td>
<td align="left">Jan 02, 2018</td>
</tr>
<tr class="odd">
<td align="left">Jane Smith</td>
<td align="left">Buy</td>
<td align="left">Chair</td>
<td align="left">$50.00</td>
<td align="left">3</td>
<td align="left">Jan 03, 2018</td>
</tr>
</tbody>
</table>
<p>Here, John Doe bought four furniture tables within two consecutive dates. Jane Smith bought three furniture chairs on Jan 3, 2018.</p>
<p><strong>Structured Query Language (SQL):</strong></p>
<p>SQL is a language used to interact with a database system. The customer, product, merchant, and transaction tables reside in a database system. Each table serves as a repository of records. For example, the customer record of John Doe is recorded in the customer table in the database system. To request the database system for the customer record of John Doe, we have to issue an SQL statement.</p>
<p><strong>SQL statement:</strong> select * from customers where name = ‘John Doe.’</p>
<p>The database system will output the following record based on the SQL statement above. Notice the asterisk ’*’, which asks the database system to output all the properties or attributes of the customer record.</p>
<table>
<caption><span id="tab:nice-sqlcustomer">Table 9.11: </span>John Doe Record</caption>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Contact</th>
<th align="left">Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">John Doe</td>
<td align="left">1.650.nnn.nnn1</td>
<td align="left">Regular Customer</td>
</tr>
</tbody>
</table>
<p>The following SQL statement selects only the name of a customer named ‘John Doe’:</p>
<p><strong>SQL statement:</strong> select <strong>name</strong> from customers where name = ‘John Doe’</p>
<p>The database system will output only the name of the John Doe record based on the SQL statement above.</p>
<table>
<caption><span id="tab:nice-sqlname">Table 9.12: </span>John Doe Record</caption>
<thead>
<tr class="header">
<th align="left">Name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">John Doe</td>
</tr>
</tbody>
</table>
<p><strong>On Data Integrity:</strong></p>
<p>Database systems have to follow the ACID rule.</p>
<ul>
<li><strong>A</strong>tomicity - all relevant changes should be committed, or not at all.</li>
<li><strong>C</strong>onsistency - the state of the resulting data should remain in full consistently and not partial.</li>
<li><strong>I</strong>solation - competing transactions should process data in isolation.</li>
<li><strong>D</strong>urability - the correct state of data should be preserved even after failures.</li>
</ul>
<p>One example of dealing with data integrity is when a customer, e.g., John Doe, bought one furniture table on Jan 01, 2018. As soon as the transaction was recorded into the transaction table, the question would be, did we update the product table to reflect the transaction, meaning the inventory for the table should then be 199 units instead of 200 units?</p>
<table>
<caption><span id="tab:nice-product2">Table 9.13: </span>Products</caption>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Type</th>
<th align="left">Price</th>
<th align="left">Inventory</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Table</td>
<td align="left">Furniture</td>
<td align="left">$100.00</td>
<td align="left"><strong>199</strong></td>
</tr>
<tr class="even">
<td align="left">Chair</td>
<td align="left">Furniture</td>
<td align="left">$50.00</td>
<td align="left">100</td>
</tr>
</tbody>
</table>
<p><strong>On Referential Integrity and Relationships:</strong></p>
<p>The relational nature of database systems is based on referential integrity. It means that the relationship between two entities needs to be protected. Therefore, constraints have to be put in place to protect the relationship. As an example of a constraint, a transaction record should not exist if there is no product to sell in the first place. In other words, it is not possible to contain transaction records whose product attributes belong to products not recorded in the product table. That is because the transaction record references a non-existing product. That breaks the referential integrity.</p>
<p>To keep referential integrity intact, we need a way for both transaction records and product records to reference each other. In this case, we use a reference id.</p>
<table>
<caption><span id="tab:nice-trans3">Table 9.14: </span>Transactions</caption>
<thead>
<tr class="header">
<th align="left">Customer</th>
<th align="left">Action</th>
<th align="left"><strong>Product Id</strong></th>
<th align="left">Amount</th>
<th align="left">Quantity</th>
<th align="left">Date</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">John Doe</td>
<td align="left">Buy</td>
<td align="left"><strong>T12</strong></td>
<td align="left">$100.00</td>
<td align="left">1</td>
<td align="left">Jan 01, 2018</td>
</tr>
<tr class="even">
<td align="left">John Doe</td>
<td align="left">Buy</td>
<td align="left"><strong>T12</strong></td>
<td align="left">$100.00</td>
<td align="left">3</td>
<td align="left">Jan 02, 2018</td>
</tr>
<tr class="odd">
<td align="left">Jane Smith</td>
<td align="left">Buy</td>
<td align="left"><strong>C09</strong></td>
<td align="left">$50.00</td>
<td align="left">3</td>
<td align="left">Jan 03, 2018</td>
</tr>
</tbody>
</table>
<p>A new “product id” attribute is used as a reference id to refer to a specific product, replacing the “product” attribute.</p>
<table>
<caption><span id="tab:nice-product3">Table 9.15: </span>Products</caption>
<thead>
<tr class="header">
<th align="left"><strong>Product Id</strong></th>
<th align="left">Name</th>
<th align="left">Type</th>
<th align="left">Price</th>
<th align="left">Inventory</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>T12</strong></td>
<td align="left">Table</td>
<td align="left">Furniture</td>
<td align="left">$100.00</td>
<td align="left"><strong>199</strong></td>
</tr>
<tr class="even">
<td align="left"><strong>C09</strong></td>
<td align="left">Chair</td>
<td align="left">Furniture</td>
<td align="left">$50.00</td>
<td align="left">100</td>
</tr>
</tbody>
</table>
<p>A new “product id” attribute is added as a reference id to reference the product.</p>
<p>With regards to referential integrity, certain relationships need to be considered when designing a system:</p>
<table>
<caption><span id="tab:nice-referential">Table 9.16: </span>Referential Integrity</caption>
<colgroup>
<col width="16%" />
<col width="83%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Relationship</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Parent-child</td>
<td align="left">A child entity references a parent entity</td>
</tr>
<tr class="even">
<td align="left">One-to-One</td>
<td align="left">An instance of one entity references an instance of another</td>
</tr>
<tr class="odd">
<td align="left">One-to-Many</td>
<td align="left">An instance of one entity references many instances of another</td>
</tr>
<tr class="even">
<td align="left">Many-to-One</td>
<td align="left">Many instances of one entity references an instance of another</td>
</tr>
<tr class="odd">
<td align="left">Many-to-Many</td>
<td align="left">Many instances of one entity references many instances of another</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:relationship"></span>
<img src="relationship.png" alt="Relationship" width="70%" />
<p class="caption">
Figure 9.3: Relationship
</p>
</div>
<p>The moment we begin to design relationships of individual entities, we are now modeling a schema. A schema is a model - a structured representation of entities and actions and how each is inter-connected via references.</p>
<p>The inter-connected relationship can become complex if our schema consists of multiple entities and action tables.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:complexrelationship"></span>
<img src="complex_relationship.png" alt="Schema with complex relationships" width="50%" />
<p class="caption">
Figure 9.4: Schema with complex relationships
</p>
</div>
<p>Therefore, a designer of a system should be meticulous and aware of many things. A bad design could get costly, especially when the project is at the implementation stage. Any missed detail in the design may require redoing the entire implementation to fit the missing requirement or relationship between entities.</p>
<p>We have seen how data can be categorized into entities and attributes when it comes to data. We also have shown how we have created a table for Merchant data and a separate table for Customer data. Imagine if we do not do that, it may turn out to be this:</p>

<table>
<caption><span id="tab:nice-unnormalize2">Table 9.17: </span>Un-normalized Table</caption>
<thead>
<tr class="header">
<th align="left">Customer</th>
<th align="left">Contact</th>
<th align="left">CustType</th>
<th align="left">Prod</th>
<th align="left">Type</th>
<th align="left">Price</th>
<th align="left">Stock</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">John Doe</td>
<td align="left">1.650.nnn.nnn1</td>
<td align="left">Regular</td>
<td align="left">Table</td>
<td align="left">Furniture</td>
<td align="left">$100.00</td>
<td align="left">200</td>
</tr>
</tbody>
</table>

<p>That table is what we call denormalized form. In contrast, the Merchant table and Product table are the normalized forms. When one suggests normalizing a schema, we ensure relevant attributes of one entity are grouped in a table so that instances of such entities get recorded into the table along with the corresponding attributes. A denormalized table has unrelated attributes all mixed into one table. The table becomes a flat table, therefore. Not to say that it is a bad design, but it may not necessarily fit a relational model - one where we have the data integrity and referential integrity constraints in place as possibly the minimum requirement.</p>
<p>This book will not cover all other advanced topics in system or schema design because there is a whole book that covers database or system design. Topics such as normalization and indexing, database optimization and advanced SQL statements, and many other exciting topics have their rightful place in another more extensive discussion. While data science requires deep comfortable knowledge of database systems, data science does not stop there. We will visit this requirement in a later chapter of the book.</p>
</div>
<div id="non-structured-data" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.2.2</span> Non-Structured Data<a href="machinelearning1.html#non-structured-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have just introduced a way of organizing data into a structure. This time, let us now talk about unstructured data. The following items below are examples of unstructured data:</p>
<ul>
<li>news articles</li>
<li>blogs</li>
<li>comments or feedbacks</li>
<li>customer reviews</li>
<li>customer complaints</li>
<li>product descriptions</li>
<li>event descriptions</li>
<li>alert notifications</li>
<li>diagnostic logs</li>
<li>system logs</li>
<li>audit logs</li>
</ul>
<p>There is one common property shared by the data above. They are free text-based format. As such, they are unstructured.</p>
<p>Mining information from unstructured data can be challenging. In most cases, processing unstructured data requires parsing and pattern matching. The crude way to do this is to split a text string into individual words and start analyzing each word.</p>
<p>So here is an example - every time a user visits a website, the site logs the visit in this format.</p>
<blockquote>
[January 01, 2018] index.html 500
</blockquote>
<p>The log text can be parsed and can be split into the following representations:</p>
<ul>
<li>Date</li>
<li>URL</li>
<li>Status Code</li>
</ul>
<p>Of course, we can convert the parsed data into a structured format and save it into a relational database system table:</p>
<table>
<caption><span id="tab:nice-weblog">Table 9.18: </span>Website Log</caption>
<thead>
<tr class="header">
<th align="left">Date</th>
<th align="left">URL</th>
<th align="left">Status Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">January 01, 2018</td>
<td align="left">index.html</td>
<td align="left">500</td>
</tr>
</tbody>
</table>
<p>Otherwise, we can handle the unstructured data in a non-SQL structured format. For example, we can convert the parsed data into XML format or JSON format:</p>
<p><strong>XML format:</strong></p>
<blockquote>
<weblog>
<date>January 01, 2018</date>
<rl>index.html</url>
<statuscode>500</statuscode>
</weblog>
</blockquote>
<p><strong>JSON format:</strong></p>
<blockquote>
{ “weblog” :
{ “date” : “January 01, 2018”,
“url” : “index.html”,
“statuscode” : 500
}
}
</blockquote>
<p>We can then store the XML or JSON version of the data into a non-SQL database system as a document.</p>
<p>If we prefer to query the name of the customer using XML, we can use the following SQL statement:</p>
<p><strong>SQL statement:</strong></p>
<p>select XMLELEMENT(“Customer”, XMLELEMENT(“Name”, name))
from customers where name = ‘John Doe’</p>
<p><strong>XML Result:</strong></p>
<blockquote>
<Customer>
<Name>John Doe</Name>
</Customer>
</blockquote>
<p>If we prefer to query the name of the customer using JSON, we can use the following SQL statement:</p>
<p><strong>SQL statement:</strong></p>
<p>select JSON_QUERY(name, “$.Customer.Name”)
from customers where name = ‘John Doe’</p>
<p><strong>JSON Result:</strong></p>
<blockquote>
{ “Customer” :
{ “Name” : “John Doe” }
}
</blockquote>
<p>Structured Query Language (SQL) follows ISO SQL standards. ISO SQL standards are continuously and actively extended to support other features. For example, ISO SQL:2003/2006 supports XML documents, and after that, ISO SQL:2016 now also supports JSON documents. Many relational database systems follow SQL ISO standards. Ideally, we want to choose one that supports ISO SQL:2016.</p>
<p>A quick summary:</p>
<p>Structured data allows us to use SQL statements to retrieve records kept in normalized tables stored in relational database systems.</p>
<p>On the other hand, unstructured data allows us to store data as documents instead of tables in “non-relational or no-SQL” based database systems. Data is documented in extensible markup language (XML) format or javascript object notation (JSON) format. We treat data as a document having no relationship with other documents.</p>
<p>One important concept to emphasize is the <strong>key-value</strong> pair for lookup useful for unstructured data. Most <strong>in-memory</strong> systems utilize some <strong>memory cache</strong> technology for lookups. We leave readers to investigate <strong>memory cache</strong>.</p>
</div>
<div id="statistical-data" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.2.3</span> Statistical Data<a href="machinelearning1.html#statistical-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have been covering ‘entities’ and ‘attributes’ to describe data in the context of systems design. Here, we extend a few more terms from what we already have in Chapter <strong>6</strong> (<strong>Statistical Computation</strong>).</p>
<p>We introduce <strong>population</strong>, <strong>samples</strong>, <strong>observations</strong>, and <strong>variables</strong>.</p>
<p>A <strong>population</strong> represents the entirety or totality of entities of interest, e.g., the world’s human population.</p>
<p>A <strong>sample</strong> represents a selected portion of a population; Hence, sampling a population means collecting a subset of the population for statistical analysis.</p>
<p>An <strong>observation</strong> is the act of recording and measuring facts. Here, by just omitting the act of observing, we can refer to observed facts as observations.</p>
<p>A <strong>variable</strong> represents the ingredients or conditions capable of change. Other terms such as <strong>covariates</strong> or <strong>variates</strong> are related but do not necessarily equate to the term <strong>variable</strong> - to be explained more in the topic of random variables.</p>
<p>To hear someone say that there are <strong>other angles</strong> to consider, <strong>other factors</strong> to consider, or <strong>other variables</strong> to consider only means we have to consider more variables than what is presented on hand.</p>
<p>To consider all variables for a given sample means to account for all factors of a given sample.</p>
<p>Therefore, we need to consider variables (or factors) such as humidity, temperature, and wind velocity to study climate change. All three variables may contribute to climate change. However, there may be other variables to evaluate as well.</p>
<p>Does human progress contribute to climate change? How do we measure human progress? Here, we need to consider all measurable observations of human progress.</p>
<table>
<caption><span id="tab:niceclimatechange">Table 9.19: </span>Climate Change</caption>
<thead>
<tr class="header">
<th align="left">Date</th>
<th align="left">Humidity (%)</th>
<th align="left">Temperature (F)</th>
<th align="left">Wind Velocity (mph)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">January 01, 2018</td>
<td align="left">20</td>
<td align="left">70</td>
<td align="left">30</td>
</tr>
<tr class="even">
<td align="left">January 02, 2018</td>
<td align="left">13</td>
<td align="left">100</td>
<td align="left">10</td>
</tr>
</tbody>
</table>
<p>In Table <a href="machinelearning1.html#tab:niceclimatechange">9.19</a>, there are two observations of a given sample of climate change data. The sample bears three variables: <strong>Humidity</strong> measured in percentage (%), <strong>Temperature</strong> measured in Fahrenheit (F), and <strong>Wind Velocity</strong> measured in miles per hour (mph).</p>
<p>Notice the use of the term <strong>contribute</strong> when evaluating statistical data. We say that there are three variables (Humidity, Temperature, Wind Velocity) that contribute to climate change. The change in climate depends on observations of the three variables. In statistics, we do refer to climate change as a dependent variable and the other three variables as independent variables. We also refer to climate change as a response variable and the other three variables as predictor variables. And also, we refer to climate change as explained variable and the other three variables as explanatory variables.</p>
<table>
<caption><span id="tab:nicevariables">Table 9.20: </span>Variables</caption>
<thead>
<tr class="header">
<th align="left">Answer</th>
<th align="left">Question</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Dependent variable</td>
<td align="left">Independent variables</td>
</tr>
<tr class="even">
<td align="left">Response (Outcome) variable</td>
<td align="left">Predictor variables</td>
</tr>
<tr class="odd">
<td align="left">Explained variable</td>
<td align="left">Explanatory variables</td>
</tr>
</tbody>
</table>
<p>Response variables, Dependent variables, and Explained variables can be interchangeable. Other authors may not be strict in the subtle differences. For example, we can say that one variable affects another variable. In other cases, we say that one variable explains another variable. In some instances, we say that one variable predicts the outcome of another variable. The use of such terms may depend on the context of Statistics, Mathematics, or Machine Learning.</p>
<p>It may be fair to say that whichever variable is the center of focus or the main study, then that is one of the three: dependent, explained, or response variable. On the other hand, whichever variable effects, contributes to, or explains the study’s outcome is one of the three: independent, explanatory, or predictor variable(s).</p>
<p>We focus on measurable observations of a sample or population in statistical data. However, this does not mean that other data types are not qualified for statistical analysis. There are ways to translate or convert multimedia data into measurable data - the translation and conversion of data are part of pre-processing data, which we will cover at some point in the book.</p>
</div>
<div id="real-time-and-near-real-time-data" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.2.4</span> Real-Time and Near Real-Time Data<a href="machinelearning1.html#real-time-and-near-real-time-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Real-Time</strong> data is one with the property of being immediately available and often associated with being streamed with no delay. Data is stored in memory and streamed directly to a system for real-time processing to achieve close to zero latency.</p>
<p><strong>Near Real-Time</strong> data is one with the property of not being immediately available but with a short delay. Such data are stored physically on a disk prior to being delivered. The latency added is measured against the time it takes to store and retrieve the data.</p>
</div>
<div id="oltp-and-datawarehouse" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.2.5</span> OLTP and Datawarehouse<a href="machinelearning1.html#oltp-and-datawarehouse" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>OLTP</strong> data is online transaction data that requires processing. Response for an online request is expected to have no delay (from a user perspective), e.g., depositing money to a bank represents an online transaction and cannot have a long delay for response to reach a customer as soon as the deposit is committed.</p>
<p><strong>Data warehouse</strong> data is processed offline in batches; it often requires jobs scheduled during off-business hours, e.g., processing payroll for a big organization every month. The chances are that data warehouse (DW) engineers undergo three standard ETL methods to process DW data: extract, transform, and load. They mean literally just that. Extract raw data. Transform the extracted raw data into a more summarized form, then load it into a DW database for reporting and analysis.</p>
</div>
<div id="data-lake" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.2.6</span> Data lake<a href="machinelearning1.html#data-lake" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Data lake</strong> is a terminology often associated with Big Data. The premise behind the name explains how the volume of data is growing extremely large and complex in that it requires new methods to process such data. That is where map-reduce procedures come to play. That is also where distributed systems enter the picture. Moreover, that is where online streaming technologies also get more benefits.</p>
</div>
<div id="natural-language-nl" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.2.7</span> Natural Language (NL)<a href="machinelearning1.html#natural-language-nl" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With natural language, data is in the form of text which requires parsing, structuring, and analyzing to gain context and some meaning.</p>
<p>For example, taken from the NIPS dataset (as of 2018), we form a cloud of words (rendered using <strong>wordcloud</strong> R package) from a string set of text and identify the most common topic. See Figure <a href="machinelearning1.html#fig:wordcloud">9.5</a> as an illustration. As it turns out, <strong>neural network</strong> seems to be one of the common topics in the word cloud.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:wordcloud"></span>
<img src="wordcloud.png" alt="Wordcloud" width="50%" />
<p class="caption">
Figure 9.5: Wordcloud
</p>
</div>
</div>
<div id="multimedia-md" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.2.8</span> Multimedia (MD)<a href="machinelearning1.html#multimedia-md" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we narrow the scope of <strong>Multimedia</strong> around photo images, sound, music, and video.</p>
<p>Taken from the MNIST dataset (Y Lecun et al.), a handwritten digit is fed into a <strong>machine learning</strong> algorithm to predict and classify the digit. See Figure <a href="machinelearning1.html#fig:mnist">9.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mnist"></span>
<img src="mnist_digits.png" alt="MNIST dataset" width="60%" />
<p class="caption">
Figure 9.6: MNIST dataset
</p>
</div>
</div>
</div>
<div id="primitive-methods" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.3</span> Primitive Methods<a href="machinelearning1.html#primitive-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, as part of manipulating or translating data, we introduce (what we call for the lack of a better word) the primitive methods. Because we find these elementary and primitive methods scattered across a spectrum of topics or areas, usually only hinted at or abstracted, it may be essential to gather and discuss them in this book and expose them from abstraction, favoring applied intuition more than the mathematical intuition of it. In manipulating or translating data (whether directly or indirectly), there are about a number of primitive methods to cover, namely <strong>weighting</strong>, <strong>binning</strong>, <strong>discretizing</strong>, <strong>stratifying</strong>, <strong>smoothing</strong>, <strong>normalizing</strong>, <strong>standardizing</strong>, <strong>centering</strong>, <strong>scaling</strong>, <strong>transforming</strong>, and <strong>regularizing</strong>. These are perhaps the most common and useful elementary and primitive methods of processing data. Gradually, we develop them into discussions that explore the more involved methods in later chapters.</p>
<div id="weighting" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.3.1</span> Weighting<a href="machinelearning1.html#weighting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can explain the concept of <strong>weighting</strong> using a balance scale. Suppose we load each side of a scale with a stone and find that the scale does not balance. To balance the scale, we add small pebbles on each side. These added pebbles are what we call <strong>weights</strong>. We add extra weights to meet some level of balance.</p>
<p>Similarly, <strong>weights</strong> (or coefficients) are needed to put additional value to data. In <strong>ML</strong>, we parameterize our model to fit our data by adjusting the parameters (weights). In Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>), the idea of <strong>beta</strong> (<span class="math inline">\(\beta\)</span>) <strong>coefficients</strong> is introduced under <strong>Least Squares</strong> using the formula:</p>
<p><span class="math display">\[\begin{align}
\hat{y} = \beta_0 + \beta_i x_i \label{eqn:eqnnumber150}
\end{align}\]</span></p>
<p>The two coefficients, <span class="math inline">\(\beta_0\)</span>, and <span class="math inline">\(\beta_1\)</span>, can be regarded as <strong>weights</strong>.</p>
<p>We shall see that when it comes to <strong>Machine Learning</strong> and <strong>Deep Neural Networks</strong>, all our discussions throughout the rest of the book are based upon optimizing the weights or parameters via some iteration in conjunction with a <strong>learning rate</strong> hyperparameter. The idea is that we can use the adjusted weights as models for inference. Usually, we pair such parameters with corresponding predictor variables (in statistics) or extracted features (in machine learning). Such variables can either be discrete, continuous, or categorical.</p>
<p>Now, there are cases when we need to pre-process data before <strong>weighing</strong>. Unlike <strong>weights</strong>, here we deal with the data itself and not the coefficients. Data is denoted by <span class="math inline">\(x_i\)</span> in Equation <span class="math inline">\(\ref{eqn:eqnnumber150}\)</span>. All the primitive methods listed above except <strong>weighing</strong> and <strong>regularization</strong> can be used to adjust (or manipulate) data, e.g., <span class="math inline">\(x_i\)</span>. Both <strong>weighing</strong> and <strong>regularization</strong>, on the other hand, are used to adjust the coefficients. Specifically, <strong>regularization</strong> indirectly controls (punishes or rewards) the coefficients via an added regularizer in the <strong>Loss function</strong> (we discuss this in a later section).</p>
</div>
<div id="smoothing" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.3.2</span> Smoothing<a href="machinelearning1.html#smoothing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Smoothing is a primitive method in data science that deals with noise and fluctuations. In dealing with data, we may perceive some data to be outliers. They have random variations outside the norm. One way to deal with noise is just manually removing the noise or outlier. Figure <a href="machinelearning1.html#fig:smoothdata">9.7</a> illustrates a smoothed data.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:smoothdata"></span>
<img src="DS_files/figure-html/smoothdata-1.png" alt="Smoothing" width="70%" />
<p class="caption">
Figure 9.7: Smoothing
</p>
</div>
<p>For a simple demonstration, our simple crude way of smoothing data (which may not be the best way) is to scale down the data to subdue the effect of outliers. Here, we perform a cube root. See Table <a href="machinelearning1.html#tab:fluctuation2">9.21</a>.</p>
<table>
<caption><span id="tab:fluctuation2">Table 9.21: </span>Smoothing</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">X1</th>
<th align="right">X2</th>
<th align="right">X3</th>
<th align="right">X4</th>
<th align="right">X5</th>
<th align="right">X6</th>
<th align="right">X7</th>
<th align="right">X8</th>
<th align="right">X9</th>
<th align="right">X10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">X (noisy)</td>
<td align="right">27</td>
<td align="right">27</td>
<td align="right">8</td>
<td align="right">-1</td>
<td align="right">0</td>
<td align="right">8</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">-8</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">X (smoothed)</td>
<td align="right">3</td>
<td align="right">3</td>
<td align="right">2</td>
<td align="right">-1</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">-2</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>We can eliminate noise via smoothing. On the other hand, we show later how we can scale back data to expose real noise (highest peak), which could surface when scaled and become candidates for analysis. That is another way to expose outliers and make them more visible by scaling up (e.g., raise to the power of 2).</p>
</div>
<div id="normalizing" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.3.3</span> Normalizing<a href="machinelearning1.html#normalizing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Normalizing is mapping values to the range between 0 and 1. See Figure <a href="machinelearning1.html#fig:normalize">9.9</a>. To normalize, we can use the below formula:</p>
<p><span class="math display">\[\begin{align}
normalized\ x = \dfrac{x-min(x)}{max(x)-min(x)}
\end{align}\]</span></p>
<p>As an example, see Table <a href="machinelearning1.html#tab:normtab">9.22</a>.</p>
<p>In a more complex dataset, normalizing a dataset to a range between 0 to 1 is, in essence, virtually stripping off the units. So, for example, by measuring two variables (weight and height) where the weight comes in lbs (pounds) and height comes in ft (feet), we can normalize the values of both variables so that they show the same normalized range of values between 0 and 1 - and thus ignoring the units (lbs and ft) completely.</p>
<table>
<caption><span id="tab:normtab">Table 9.22: </span>Normalization</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">X1</th>
<th align="right">X2</th>
<th align="right">X3</th>
<th align="right">X4</th>
<th align="right">X5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">X</td>
<td align="right">50</td>
<td align="right">100.0</td>
<td align="right">150.0</td>
<td align="right">250.0</td>
<td align="right">300</td>
</tr>
<tr class="even">
<td align="left">Normalized</td>
<td align="right">0</td>
<td align="right">0.2</td>
<td align="right">0.4</td>
<td align="right">0.8</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb962"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb962-1" data-line-number="1">x1 =<span class="st">  </span><span class="kw">sort</span>( <span class="kw">runif</span>(<span class="dv">20</span>, <span class="dt">min=</span><span class="op">-</span><span class="dv">50</span>, <span class="dt">max=</span><span class="dv">150</span>)  )</a>
<a class="sourceLine" id="cb962-2" data-line-number="2">x2 =<span class="st"> </span><span class="dv">500</span> <span class="co"># simulate outlier</span></a>
<a class="sourceLine" id="cb962-3" data-line-number="3">x3 =<span class="st">  </span><span class="kw">sort</span>( <span class="kw">runif</span>(<span class="dv">20</span>, <span class="dt">min=</span><span class="dv">151</span>, <span class="dt">max=</span><span class="dv">250</span>) )</a>
<a class="sourceLine" id="cb962-4" data-line-number="4">x =<span class="st">   </span><span class="kw">c</span>(x1, x2,  x3) </a>
<a class="sourceLine" id="cb962-5" data-line-number="5">normalized_x =<span class="st"> </span>(x <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(x)) <span class="op">/</span><span class="st"> </span>( <span class="kw">max</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(x))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normalizeprim"></span>
<img src="DS_files/figure-html/normalizeprim-1.png" alt="Normalization" width="70%" />
<p class="caption">
Figure 9.8: Normalization
</p>
</div>
<p>In Figure <a href="machinelearning1.html#fig:normalizeprim">9.8</a> as can be seen, the scale of the original data ranges between 0-500. With normalization, the scale is now between 0 and 1. That also shows that outliers may still be visible even in their normalized version.</p>
<p>Additionally, there is also what we call <strong>mean normalization</strong>, which has the following equation: </p>
<p><span class="math display">\[\begin{align}
mean.norm.x = \dfrac{x-mean(x)}{max(x)-min(x)}
\end{align}\]</span></p>
<p>However, we can also treat this normalization as a type of standardization.</p>
</div>
<div id="standardizing" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.3.4</span> Standardizing <a href="machinelearning1.html#standardizing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Standardizing is the process of mapping values to the range between -N and N, e.g., between -1 and 1, where zero is always the mean (zero-mean). See Figure <a href="machinelearning1.html#fig:normalize">9.9</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normalize"></span>
<img src="normalize_standardize.png" alt="Normalize vs Sandardize" width="65%" />
<p class="caption">
Figure 9.9: Normalize vs Sandardize
</p>
</div>
<p>The formula to standardize (e.g. z-score normalization):</p>
<p><span class="math display">\[\begin{align}
standardized\ x = \dfrac{x-mean(x)}{stddev(x)}
\end{align}\]</span></p>
<p>As an example, see Table <a href="machinelearning1.html#tab:standardo">9.23</a>.</p>
<table>
<caption><span id="tab:standardo">Table 9.23: </span>Standardization</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">X1</th>
<th align="right">X2</th>
<th align="right">X3</th>
<th align="right">X4</th>
<th align="right">X5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">X</td>
<td align="right">50.00</td>
<td align="right">100.00</td>
<td align="right">150.00</td>
<td align="right">250.00</td>
<td align="right">300.00</td>
</tr>
<tr class="even">
<td align="left">Standardized</td>
<td align="right">-1.16</td>
<td align="right">-0.68</td>
<td align="right">-0.19</td>
<td align="right">0.77</td>
<td align="right">1.25</td>
</tr>
</tbody>
</table>
<p>Note that our values are now in units of one standard deviation.</p>
<p>Also, note that we also can use the function <strong>scale(.)</strong> to achieve the same standardization by scaling (including centering):</p>
<div class="sourceCode" id="cb963"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb963-1" data-line-number="1"><span class="kw">t</span>(<span class="kw">scale</span>(<span class="dt">x =</span> x, <span class="dt">scale=</span><span class="ot">TRUE</span>, <span class="dt">center=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>##        [,1]    [,2]    [,3]   [,4]  [,5]
## [1,] -1.157 -0.6751 -0.1929 0.7716 1.254
## attr(,&quot;scaled:center&quot;)
## [1] 170
## attr(,&quot;scaled:scale&quot;)
## [1] 103.7</code></pre>
<p>If we do not center, then we have the following outcome:</p>
<div class="sourceCode" id="cb965"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb965-1" data-line-number="1"><span class="kw">t</span>(<span class="kw">scale</span>(<span class="dt">x =</span> x, <span class="dt">scale=</span><span class="ot">TRUE</span>, <span class="dt">center=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>##        [,1]   [,2]   [,3]  [,4]  [,5]
## [1,] 0.2309 0.4619 0.6928 1.155 1.386
## attr(,&quot;scaled:scale&quot;)
## [1] 216.5</code></pre>
<p>In a more complex dataset, similar to normalization, trying to standardize a dataset to a range between -N and N is, in essence, also virtually stripping off the units. See Figure <a href="machinelearning1.html#fig:centerstandard">9.11</a>.</p>
<div class="sourceCode" id="cb967"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb967-1" data-line-number="1">x1 =<span class="st">  </span><span class="kw">sort</span>( <span class="kw">runif</span>(<span class="dv">20</span>, <span class="dt">min=</span><span class="op">-</span><span class="dv">50</span>, <span class="dt">max=</span><span class="dv">150</span>)  )</a>
<a class="sourceLine" id="cb967-2" data-line-number="2">x2 =<span class="st"> </span><span class="dv">500</span> <span class="co"># simulate outlier</span></a>
<a class="sourceLine" id="cb967-3" data-line-number="3">x3 =<span class="st">  </span><span class="kw">sort</span>( <span class="kw">runif</span>(<span class="dv">20</span>, <span class="dt">min=</span><span class="dv">151</span>, <span class="dt">max=</span><span class="dv">250</span>) )</a>
<a class="sourceLine" id="cb967-4" data-line-number="4">x =<span class="st">   </span><span class="kw">c</span>(x1, x2,  x3) </a>
<a class="sourceLine" id="cb967-5" data-line-number="5">standardized_x =<span class="st"> </span>(x <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x)) <span class="op">/</span><span class="st"> </span><span class="kw">sd</span>(x)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:standardprim"></span>
<img src="DS_files/figure-html/standardprim-1.png" alt="Standardization" width="70%" />
<p class="caption">
Figure 9.10: Standardization
</p>
</div>
<p>To re-emphasize, in the normalization method, the normalized values are within the range of 0 and 1. However, the values revolve around 0 in the standardization method because of the mean and standard deviation. So, for example, we see values between -1 and 1 or between -2 and 2. In other words, the range for standardized values is between -N and N.</p>
<p>Also, notice that the outlier is still visible while the data has been standardized.</p>
</div>
<div id="centering" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.3.5</span> Centering <a href="machinelearning1.html#centering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Centering is a simple primitive that transforms values to get closer to the zero-mean where the average value is centered - this is called centering on the mean.</p>
<p>Here is the formula:</p>
<p><span class="math display">\[centered\ x = x - mean(x)\]</span></p>
<p>Between <strong>centering</strong> and <strong>standardizing</strong>, centering does not require the transformed value to be divided by standard deviation. However, standardizing can also be considered another form of centering. The only difference is that centering is not scaled.</p>
<p>Let us now compare centering and standardizing in a table.</p>
<table>
<caption><span id="tab:centered">Table 9.24: </span>Standardization vs Centering</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">X1</th>
<th align="right">X2</th>
<th align="right">X3</th>
<th align="right">X4</th>
<th align="right">X5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">X</td>
<td align="right">50.00</td>
<td align="right">100.00</td>
<td align="right">150.00</td>
<td align="right">250.00</td>
<td align="right">300.00</td>
</tr>
<tr class="even">
<td align="left">Standardized</td>
<td align="right">-1.16</td>
<td align="right">-0.68</td>
<td align="right">-0.19</td>
<td align="right">0.77</td>
<td align="right">1.25</td>
</tr>
<tr class="odd">
<td align="left">Centered</td>
<td align="right">-120.00</td>
<td align="right">-70.00</td>
<td align="right">-20.00</td>
<td align="right">80.00</td>
<td align="right">130.00</td>
</tr>
</tbody>
</table>
<p>Since centering does not scale, it sometimes makes it a challenge to compare plots between centering and standardization since the gap between their values is significant, as seen in Figure <a href="machinelearning1.html#fig:centerstandard">9.11</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:centerstandard"></span>
<img src="DS_files/figure-html/centerstandard-1.png" alt="Centering" width="70%" />
<p class="caption">
Figure 9.11: Centering
</p>
</div>
</div>
<div id="scaling-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.3.6</span> Scaling <a href="machinelearning1.html#scaling-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Smoothing, normalizing, standardizing are all methods of scaling.</p>
<p>Other methods of scaling are logarithmic and exponential scaling.</p>
<p>The formula to scale using log(X):</p>
<p><span class="math display">\[scaled\ x = \log({x})\]</span></p>
<p>In some cases, our dataset reflects some exponentially increasing behavior. In this case, we use the logarithm to scale large numbers within manageable ranges. For example, notice the range drops from 0-8000 to 0-9.</p>
<table>
<caption><span id="tab:fluctuation4">Table 9.25: </span>Scaling</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">X1</th>
<th align="right">X2</th>
<th align="right">X3</th>
<th align="right">X4</th>
<th align="right">X5</th>
<th align="right">X6</th>
<th align="right">X7</th>
<th align="right">X8</th>
<th align="right">X9</th>
<th align="right">X10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">X</td>
<td align="right">12.0</td>
<td align="right">12.0</td>
<td align="right">12.0</td>
<td align="right">13.0</td>
<td align="right">13.0</td>
<td align="right">13.0</td>
<td align="right">13.0</td>
<td align="right">13.0</td>
<td align="right">13.0</td>
<td align="right">14.0</td>
</tr>
<tr class="even">
<td align="left">log(X)</td>
<td align="right">2.5</td>
<td align="right">2.5</td>
<td align="right">2.5</td>
<td align="right">2.6</td>
<td align="right">2.6</td>
<td align="right">2.6</td>
<td align="right">2.6</td>
<td align="right">2.6</td>
<td align="right">2.6</td>
<td align="right">2.6</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fluctuation4"></span>
<img src="DS_files/figure-html/fluctuation4-1.png" alt="Scaling" width="70%" />
<p class="caption">
Figure 9.12: Scaling
</p>
</div>
<p>Also, sometimes, it helps to know which log scaling to use.</p>
<table>
<caption><span id="tab:scaling1">Table 9.26: </span>Logarithms</caption>
<thead>
<tr class="header">
<th align="right">X</th>
<th align="right"><span class="math inline">\(log(X)\)</span></th>
<th align="right"><span class="math inline">\(log2(X)\)</span></th>
<th align="right"><span class="math inline">\(log3(X)\)</span></th>
<th align="right"><span class="math inline">\(log4(X)\)</span></th>
<th align="right"><span class="math inline">\(log5(X)\)</span></th>
<th align="right"><span class="math inline">\(log10(X)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1e+00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">1e+01</td>
<td align="right">2.30</td>
<td align="right">3.32</td>
<td align="right">2.10</td>
<td align="right">1.66</td>
<td align="right">1.43</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">1e+02</td>
<td align="right">4.61</td>
<td align="right">6.64</td>
<td align="right">4.19</td>
<td align="right">3.32</td>
<td align="right">2.86</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">1e+03</td>
<td align="right">6.91</td>
<td align="right">9.97</td>
<td align="right">6.29</td>
<td align="right">4.98</td>
<td align="right">4.29</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">1e+04</td>
<td align="right">9.21</td>
<td align="right">13.29</td>
<td align="right">8.38</td>
<td align="right">6.64</td>
<td align="right">5.72</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="right">1e+05</td>
<td align="right">11.51</td>
<td align="right">16.61</td>
<td align="right">10.48</td>
<td align="right">8.30</td>
<td align="right">7.15</td>
<td align="right">5</td>
</tr>
<tr class="odd">
<td align="right">1e+06</td>
<td align="right">13.82</td>
<td align="right">19.93</td>
<td align="right">12.58</td>
<td align="right">9.97</td>
<td align="right">8.58</td>
<td align="right">6</td>
</tr>
<tr class="even">
<td align="right">1e+07</td>
<td align="right">16.12</td>
<td align="right">23.25</td>
<td align="right">14.67</td>
<td align="right">11.63</td>
<td align="right">10.01</td>
<td align="right">7</td>
</tr>
<tr class="odd">
<td align="right">1e+08</td>
<td align="right">18.42</td>
<td align="right">26.58</td>
<td align="right">16.77</td>
<td align="right">13.29</td>
<td align="right">11.45</td>
<td align="right">8</td>
</tr>
<tr class="even">
<td align="right">1e+09</td>
<td align="right">20.72</td>
<td align="right">29.90</td>
<td align="right">18.86</td>
<td align="right">14.95</td>
<td align="right">12.88</td>
<td align="right">9</td>
</tr>
</tbody>
</table>
<p>Plotting different logarithms, let us see the result of each case.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scaling"></span>
<img src="DS_files/figure-html/scaling-1.png" alt="Scaling by Logarithms" width="70%" />
<p class="caption">
Figure 9.13: Scaling by Logarithms
</p>
</div>
<p>The range of the different logarithms falls within the range 0 and 20 for exponential values from 0 through 1e+9. So, if the ranges do not have significant differences (even within a range), using one of those logarithmic methods for scaling may not be a concern.</p>
</div>
<div id="transforming" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.3.7</span> Transforming<a href="machinelearning1.html#transforming" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Transformation is a generalized method that extends scaling. Such examples of transformation are log transformation, Box-Cox transformation, and clipping.</p>
<p>Similar to scaling, an example of transformation is when we raise the value of data to a power of 2 or 3.</p>
<p><span class="math display">\[
transformed \ x = x^2\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ transformed \ x = x^3
\]</span></p>
<p>Other power transformation is shown in Table <a href="machinelearning1.html#tab:transformation1">9.27</a>.</p>
<table>
<caption><span id="tab:transformation1">Table 9.27: </span>Power of Exponents</caption>
<thead>
<tr class="header">
<th align="right">X</th>
<th align="right"><span class="math inline">\(X^{1/4}\)</span></th>
<th align="right"><span class="math inline">\(X^{1/3}\)</span></th>
<th align="right"><span class="math inline">\(X^{1/2}\)</span></th>
<th align="right"><span class="math inline">\(X^0\)</span></th>
<th align="right"><span class="math inline">\(X^2\)</span></th>
<th align="right"><span class="math inline">\(X^3\)</span></th>
<th align="right"><span class="math inline">\(X^4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1.0</td>
<td align="right">0.25</td>
<td align="right">0.33</td>
<td align="right">0.5</td>
<td align="right">1</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
</tr>
<tr class="even">
<td align="right">1.2</td>
<td align="right">0.30</td>
<td align="right">0.40</td>
<td align="right">0.6</td>
<td align="right">1</td>
<td align="right">1.44</td>
<td align="right">1.73</td>
<td align="right">2.07</td>
</tr>
<tr class="odd">
<td align="right">1.4</td>
<td align="right">0.35</td>
<td align="right">0.47</td>
<td align="right">0.7</td>
<td align="right">1</td>
<td align="right">1.96</td>
<td align="right">2.74</td>
<td align="right">3.84</td>
</tr>
<tr class="even">
<td align="right">1.6</td>
<td align="right">0.40</td>
<td align="right">0.53</td>
<td align="right">0.8</td>
<td align="right">1</td>
<td align="right">2.56</td>
<td align="right">4.10</td>
<td align="right">6.55</td>
</tr>
<tr class="odd">
<td align="right">1.8</td>
<td align="right">0.45</td>
<td align="right">0.60</td>
<td align="right">0.9</td>
<td align="right">1</td>
<td align="right">3.24</td>
<td align="right">5.83</td>
<td align="right">10.50</td>
</tr>
<tr class="even">
<td align="right">2.0</td>
<td align="right">0.50</td>
<td align="right">0.67</td>
<td align="right">1.0</td>
<td align="right">1</td>
<td align="right">4.00</td>
<td align="right">8.00</td>
<td align="right">16.00</td>
</tr>
<tr class="odd">
<td align="right">2.2</td>
<td align="right">0.55</td>
<td align="right">0.73</td>
<td align="right">1.1</td>
<td align="right">1</td>
<td align="right">4.84</td>
<td align="right">10.65</td>
<td align="right">23.43</td>
</tr>
<tr class="even">
<td align="right">2.4</td>
<td align="right">0.60</td>
<td align="right">0.80</td>
<td align="right">1.2</td>
<td align="right">1</td>
<td align="right">5.76</td>
<td align="right">13.82</td>
<td align="right">33.18</td>
</tr>
<tr class="odd">
<td align="right">2.6</td>
<td align="right">0.65</td>
<td align="right">0.87</td>
<td align="right">1.3</td>
<td align="right">1</td>
<td align="right">6.76</td>
<td align="right">17.58</td>
<td align="right">45.70</td>
</tr>
<tr class="even">
<td align="right">2.8</td>
<td align="right">0.70</td>
<td align="right">0.93</td>
<td align="right">1.4</td>
<td align="right">1</td>
<td align="right">7.84</td>
<td align="right">21.95</td>
<td align="right">61.47</td>
</tr>
</tbody>
</table>
<p>Depending on the purpose, it may seem that using power (or order) of 4 can expose outliers.</p>
<p>In statistics and machine learning, one powerful transformation method is used: <strong>Box-Cox</strong> transformation. </p>
<p>Box-cox uses the following well-known formula:</p>
<p><span class="math display">\[
transformed \ x =\begin{cases}
\dfrac{x^\lambda - 1}{\lambda}\ &amp; \text{if } \lambda \neq 0 \\
\\
\log ({x})\ &amp; \text{if } \lambda = 0
\end{cases}
\]</span></p>
<table>
<caption><span id="tab:transformation3">Table 9.28: </span>Box Cox Transformation</caption>
<thead>
<tr class="header">
<th align="right">X-lambda</th>
<th align="right">0.3</th>
<th align="right">0.2</th>
<th align="right">0.1</th>
<th align="right">0</th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1.0</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.0</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
</tr>
<tr class="even">
<td align="right">1.2</td>
<td align="right">0.19</td>
<td align="right">0.19</td>
<td align="right">0.18</td>
<td align="right">0.18</td>
<td align="right">0.2</td>
<td align="right">0.22</td>
<td align="right">0.24</td>
</tr>
<tr class="odd">
<td align="right">1.4</td>
<td align="right">0.35</td>
<td align="right">0.35</td>
<td align="right">0.34</td>
<td align="right">0.34</td>
<td align="right">0.4</td>
<td align="right">0.48</td>
<td align="right">0.58</td>
</tr>
<tr class="even">
<td align="right">1.6</td>
<td align="right">0.50</td>
<td align="right">0.49</td>
<td align="right">0.48</td>
<td align="right">0.47</td>
<td align="right">0.6</td>
<td align="right">0.78</td>
<td align="right">1.03</td>
</tr>
<tr class="odd">
<td align="right">1.8</td>
<td align="right">0.64</td>
<td align="right">0.62</td>
<td align="right">0.61</td>
<td align="right">0.59</td>
<td align="right">0.8</td>
<td align="right">1.12</td>
<td align="right">1.61</td>
</tr>
<tr class="even">
<td align="right">2.0</td>
<td align="right">0.77</td>
<td align="right">0.74</td>
<td align="right">0.72</td>
<td align="right">0.69</td>
<td align="right">1.0</td>
<td align="right">1.50</td>
<td align="right">2.33</td>
</tr>
<tr class="odd">
<td align="right">2.2</td>
<td align="right">0.89</td>
<td align="right">0.85</td>
<td align="right">0.82</td>
<td align="right">0.79</td>
<td align="right">1.2</td>
<td align="right">1.92</td>
<td align="right">3.22</td>
</tr>
<tr class="even">
<td align="right">2.4</td>
<td align="right">1.00</td>
<td align="right">0.96</td>
<td align="right">0.91</td>
<td align="right">0.88</td>
<td align="right">1.4</td>
<td align="right">2.38</td>
<td align="right">4.27</td>
</tr>
<tr class="odd">
<td align="right">2.6</td>
<td align="right">1.11</td>
<td align="right">1.05</td>
<td align="right">1.00</td>
<td align="right">0.96</td>
<td align="right">1.6</td>
<td align="right">2.88</td>
<td align="right">5.53</td>
</tr>
<tr class="even">
<td align="right">2.8</td>
<td align="right">1.21</td>
<td align="right">1.14</td>
<td align="right">1.08</td>
<td align="right">1.03</td>
<td align="right">1.8</td>
<td align="right">3.42</td>
<td align="right">6.98</td>
</tr>
</tbody>
</table>
<p>Plotting <strong>Box-Cox</strong> Transformation. See Figure <a href="machinelearning1.html#fig:transformation4">9.14</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:transformation4"></span>
<img src="DS_files/figure-html/transformation4-1.png" alt="Box-Cox Transformation" width="70%" />
<p class="caption">
Figure 9.14: Box-Cox Transformation
</p>
</div>
<p>Compared to the “Power of Exponents” transformation, the Box-Cox transformation has a much more gradual ascend/descend than the “Power of Exponents” transformation, avoiding an aggressive exponential growth or increase.</p>
</div>
<div id="clipping" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.3.8</span> Clipping <a href="machinelearning1.html#clipping" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Clipping is another form of transformation. Clipping is a method of correcting the value of data based on constraints. Below is an example of how we clip the value. If the value is less than zero, then we assign the value of zero; otherwise, if the value is greater than 100, we assign the value of 100.</p>
<p><span class="math display">\[
clipped\ x = \begin{cases}
0 &amp; if\ x &lt; 0 \\
100 &amp; if\ x &gt; 100
\end{cases}
\]</span></p>
<p>In a way, this can help to remove outliers. We make sure values do not fall outside a given boundary range. Here, we use minimum and maximum values to set the boundaries. In the example above, we decided that our minimum value is zero, and our maximum value is 100.</p>
</div>
<div id="regularizing" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.3.9</span> Regularizing<a href="machinelearning1.html#regularizing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Regularization is about reward and penalty - it is about rewarding the right and penalizing the wrong. It is about tipping the scales to favor the right side (See Figure ). However, when it comes to <strong>ML</strong>, it can be used to demote the value of coefficients, as we shall see later.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:regularizing"></span>
<img src="regularizing.png" alt="Regularizing" width="60%" />
<p class="caption">
Figure 9.15: Regularizing
</p>
</div>
<p>Here is one very rudimentary but elegant formula:</p>
<p><span class="math display">\[\beta_0 * \lambda  + \beta_1 * (1 - \lambda ) = 1\]</span></p>
<p>Or</p>
<p><span class="math display">\[
\beta_0  * \frac{\lambda - 1}{\lambda}  + \beta_1 * \frac{1}{\lambda},\ \ \  
where\ 0 \le \lambda \le 1,\ \ \beta_0 = 1,\ and \ \beta_1 = 1
\]</span></p>
<p>We have two terms in the equation. Let us call them term A and term B.</p>
<p>A = <span class="math inline">\(\beta_0 * \lambda\)</span></p>
<p>B = <span class="math inline">\(\beta_1 * (1 - \lambda)\)</span></p>
<p>So then we have:</p>
<p><span class="math display">\[A + B = 1\]</span></p>
<p>For the simplest demonstration, we intentionally set the two beta coefficients (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>) to 1.</p>
<p>So how does it work? It works by <strong>sneaking</strong> in a parameter, the lambda (<span class="math inline">\(\lambda\)</span>), whose responsibility is to offset the balance of power. The lambda parameter attaches itself to the coefficients via multiplication, and thus it either promotes or maximizes; otherwise, it demotes or minimizes the influence of <strong>coefficients</strong>; in the process, it is an effective way of regulating the relevance of features - or in another way to see it, it is an effective way to reduce the complexity of our model. We shall cover that in a later section when we get to discuss <strong>Lasso</strong> and <strong>Ridge</strong> regularization.</p>
<p>We use the lambda parameter to introduce appropriate values to induce an effect, whether as a reward for or as a penalty against values or computations that may seem extreme or undesirable. Therefore, this parameter can also be considered a tunable parameter used to tune expressions in the direction we deem fit.</p>
<p>Let us test some values for the lambda in the range between 0 and 1:</p>
<table>
<caption><span id="tab:unnamed-chunk-450">Table 9.29: </span>Regularizing</caption>
<thead>
<tr class="header">
<th align="left">Lambda</th>
<th align="right">0</th>
<th align="right">0.1</th>
<th align="right">0.2</th>
<th align="right">0.3</th>
<th align="right">0.4</th>
<th align="right">0.5</th>
<th align="right">0.6</th>
<th align="right">0.7</th>
<th align="right">0.8</th>
<th align="right">0.9</th>
<th align="right">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>A</strong></td>
<td align="right">0</td>
<td align="right">0.1</td>
<td align="right">0.2</td>
<td align="right">0.3</td>
<td align="right">0.4</td>
<td align="right">0.5</td>
<td align="right">0.6</td>
<td align="right">0.7</td>
<td align="right">0.8</td>
<td align="right">0.9</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left"><strong>B</strong></td>
<td align="right">1</td>
<td align="right">0.9</td>
<td align="right">0.8</td>
<td align="right">0.7</td>
<td align="right">0.6</td>
<td align="right">0.5</td>
<td align="right">0.4</td>
<td align="right">0.3</td>
<td align="right">0.2</td>
<td align="right">0.1</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>Notice that if we assign values less than 0.5 to lambda, then the <span class="math inline">\(\beta_0\)</span> coefficient is penalized with small values, yielding smaller values for A. If we assign values greater than 0.5 to lambda, then the <span class="math inline">\(\beta_0\)</span> coefficient is rewarded with large values. The same happens with the <span class="math inline">\(\beta_1\)</span> coefficient in reverse.</p>
<p>However, what does regularization signify? Below is a plot using transformation by the power of three against the lambda (the regularizer)</p>
<p><span class="math display">\[x_1 * \lambda^3  + x_2 * (1 - \lambda )^3 = 1\]</span>
where</p>
<p><span class="math display">\[x_1 = 1\ and\ x_2 = 3\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-451"></span>
<img src="DS_files/figure-html/unnamed-chunk-451-1.png" alt="Regularizing" width="70%" />
<p class="caption">
Figure 9.16: Regularizing
</p>
</div>
<p>We should see the power of transformation and regularization working in tandem to influence data. That is where the value of regularization comes into play. For example, our expected values for A and B are between 0 and 1, yet there is a big gap between A and B. Using lambda to regularize, we now know that a lambda less than 0.3 does not have a significant impact on both terms A and B. However, once we can limit lambda between 0.3 through 1, we have a good chance that terms A and B can play along in the range 0 and 1.</p>
<p>What we have described is just a simple explanation of regularization. There are effective regularization solutions that are in use today. While they have contributed much, this is not to say that we should be limited to those solutions alone. If there is any way we can introduce a tunable parameter to regulate our model without overfitting or underfitting and yet be able to approach a good fit for our model or other relative purposes, it does deserve a good cheer, perhaps a lot more than that.</p>
<p>In linear regression, we will encounter the following generalized regularizations:</p>
<ul>
<li><p>Lasso (L1)</p></li>
<li><p>Ridge (L2)</p></li>
<li><p>Elastic Net</p></li>
</ul>
<p>Below is a list of other regularization solutions.</p>
<ul>
<li><p>Laplace regularization (L1)</p></li>
<li><p>Gauss regularization (L2)</p></li>
<li><p>Tikhonov regularization</p></li>
</ul>
<p>If and when we see any of these regularizations, it does mean one thing. They are there to influence the outcome by regulating through reward or penalty.</p>
</div>
</div>
<div id="distance-metrics" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.4</span> Distance Metrics<a href="machinelearning1.html#distance-metrics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we cover classic distance measurements. The most fundamental measurement has to do with distance. We measure the distance between data points and interpret the result depending on the context. The result of our distance computation may show closeness in proximity; otherwise may not. Interpretation of a close distance may come in relevance, significance, correlation, and others; otherwise, data points that are farther away may connote irrelevance, no significance, no correlation, or simply no relationship whatsoever.</p>
<p>Let us enumerate a few typical measurements.</p>
<div id="cosine-similarity" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.4.1</span> Cosine Similarity<a href="machinelearning1.html#cosine-similarity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have generated some normalized random numbers and scaled them to 10. Let us have 6 of those random numbers for measurement. See Figure <a href="machinelearning1.html#fig:csimilar">9.17</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:csimilar"></span>
<img src="DS_files/figure-html/csimilar-1.png" alt="Cosine Similarity" width="70%" />
<p class="caption">
Figure 9.17: Cosine Similarity
</p>
</div>
<p>There are many ways to compare the distance between those numbers. One such measurement is <strong>cosine similarity</strong>.</p>
<p><strong>Cosine similarity</strong> is mainly used to get the cosine distance between two vectors. We will see this more in use when we get to natural language processing (NLP).</p>
<p>The following formula applies to cosine similarity:</p>
<p><span class="math display">\[\begin{align}
Cosine\ Similarity = cos(\theta) = \frac{D_{1} \cdotp D_{2}}{||D_{1}||\ ||D_{2}||}
\end{align}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align}
dot\ product = D_{1} \cdotp D_{2} = \sum_{i=1}^{n} D_{1i} D_{2i}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
magnitude = ||D_{1}|| = \sqrt[2]{d_{1}^2 + ...+ d_{n}^2}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
magnitude = ||D_{2}|| = \sqrt[2]{d_{1}^2 + ...+ d_{n}^2}
\end{align}\]</span></p>
<p>Also, here is another equivalent formula:</p>
<p><span class="math display">\[\begin{align}
Cosine = cos(\theta) = \frac{adjacent}{hypothenus}\ ;\ \ \therefore \theta = cos^{-1}\frac{a}{h} = acos(cos(\theta))
\end{align}\]</span></p>
<p>We can plot an example. See Figure <a href="machinelearning1.html#fig:csimilar1">9.18</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:csimilar1"></span>
<img src="DS_files/figure-html/csimilar1-1.png" alt="Cosine Similarity" width="70%" />
<p class="caption">
Figure 9.18: Cosine Similarity
</p>
</div>
<p>Let us put the cosine similarity into action (see Table <a href="machinelearning1.html#tab:csimilar2">9.30</a>):</p>

<div class="sourceCode" id="cb968"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb968-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb968-2" data-line-number="2">v1 =<span class="st"> </span><span class="kw">round</span>( <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">min=</span><span class="dv">1</span>, <span class="dt">max=</span><span class="dv">10</span>) )</a>
<a class="sourceLine" id="cb968-3" data-line-number="3">v2 =<span class="st"> </span><span class="kw">round</span>( <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">min=</span><span class="dv">1</span>, <span class="dt">max=</span><span class="dv">10</span>) )</a>
<a class="sourceLine" id="cb968-4" data-line-number="4">dot_product =<span class="st"> </span>v1 <span class="op">%*%</span><span class="st"> </span>v2</a>
<a class="sourceLine" id="cb968-5" data-line-number="5">v1_magnitude =<span class="st"> </span><span class="kw">sqrt</span>( <span class="kw">sum</span>( v1 <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) )</a>
<a class="sourceLine" id="cb968-6" data-line-number="6">v2_magnitude =<span class="st"> </span><span class="kw">sqrt</span>( <span class="kw">sum</span>( v2 <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) )</a>
<a class="sourceLine" id="cb968-7" data-line-number="7">cosine_similarity =<span class="st"> </span>dot_product <span class="op">/</span><span class="st"> </span>( v1_magnitude <span class="op">*</span><span class="st"> </span>v2_magnitude )</a>
<a class="sourceLine" id="cb968-8" data-line-number="8">ds =<span class="st"> </span><span class="kw">t</span>( <span class="kw">data.frame</span>(v1,v2))</a>
<a class="sourceLine" id="cb968-9" data-line-number="9"><span class="kw">colnames</span>(ds) =<span class="st"> </span><span class="kw">c</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb968-10" data-line-number="10"><span class="kw">rownames</span>(ds) =<span class="kw">c</span> (<span class="st">&quot;**vector P**&quot;</span>, <span class="st">&quot;**vector Q**&quot;</span>)</a>
<a class="sourceLine" id="cb968-11" data-line-number="11">knitr<span class="op">::</span><span class="kw">kable</span>(</a>
<a class="sourceLine" id="cb968-12" data-line-number="12">  <span class="kw">head</span>(ds, <span class="dv">20</span>), <span class="dt">caption =</span> <span class="st">&quot;Jaccard Similarity&quot;</span>, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<table>
<caption><span id="tab:csimilar2">Table 9.30: </span>Jaccard Similarity</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">7</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>vector P</strong></td>
<td align="right">9</td>
<td align="right">7</td>
<td align="right">10</td>
<td align="right">6</td>
<td align="right">8</td>
<td align="right">9</td>
<td align="right">4</td>
<td align="right">7</td>
<td align="right">5</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="left"><strong>vector Q</strong></td>
<td align="right">9</td>
<td align="right">9</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">2</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">3</td>
</tr>
</tbody>
</table>

<p>Compute for Cosine Similarity:</p>
<p><span class="math display">\[\begin{align*}
P \cdotp Q = (9, 7, 10, 6, 8, 9, 4, 7, 5, 5)\ \cdotp (9, 9, 8, 6, 6, 6, 2, 6, 6, 3) = 457
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align}
||P|| = \sqrt[2]{|p_{1}^2  + ...+ p_{n}^2} = 22.9347
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
||Q|| = \sqrt[2]{|q_{1}^2  + ...+ q_{n}^2} = 20.4695
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
cos(\theta) = \frac{P \cdotp Q}{||P||\ ||Q||} = \frac{457}{22.9347 * 20.4695} = 0.9735
\end{align}\]</span></p>
</div>
<div id="manhattan-and-euclidean-distance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.4.2</span> Manhattan and Euclidean Distance  <a href="machinelearning1.html#manhattan-and-euclidean-distance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have listed the following common distance measurements:</p>
<p><span class="math display">\[\begin{align}
(L1\ norm) = D_{manhattan}(P,Q)=\sqrt[1]{|p_{1} - q_{1}|^1 + ...+ |p_{j}-q_{j}|^1} = \begin{pmatrix}\sum_{i=1}^{j} |p_{i} - q_{i}|\end{pmatrix}^{1/1}\\
(L2\ norm) = D_{euclidean}(P,Q)=\sqrt[2]{|p_{1} - q_{1}|^2 + ...+ |p_{j}-q_{j}|^2} = \begin{pmatrix}\sum_{i=1}^{j} |p_{i} - q_{i}|\end{pmatrix}^{1/2} 
\end{align}\]</span></p>
<p>Notice that the only difference between the two measurements is the radicals (see Figure <a href="machinelearning1.html#fig:csimilar3">9.19</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:csimilar3"></span>
<img src="DS_files/figure-html/csimilar3-1.png" alt="Euclidean and Manhattan  Distances" width="70%" />
<p class="caption">
Figure 9.19: Euclidean and Manhattan Distances
</p>
</div>
</div>
<div id="minkowski-and-chebyshev-supremum-distance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.4.3</span> Minkowski and Chebyshev (Supremum) Distance  <a href="machinelearning1.html#minkowski-and-chebyshev-supremum-distance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Minkowski</strong> and <strong>Supremum</strong> distance measurements also follow the same identical formula as <strong>Manhattan</strong> and <strong>Euclidean</strong> distance measurements. The only difference is also with the radicals.</p>
<p><span class="math display">\[\begin{align}
D_{minkowski}(P,Q)=\sqrt[n]{|p_{1} - q_{1}|^n + ...+ |p_{j}-q_{j}|^n} = \begin{pmatrix}\sum_{i=1}^{j} |p_{i} - q_{i}|\end{pmatrix}^{1/n} \\
\lim_{n \to \infty}D_{chebyshev}(P,Q)=\sqrt[n]{|p_{1} - q_{1}|^n + ...+ |p_{j}-q_{j}|^n} = \begin{pmatrix}\sum_{i=1}^{j} |p_{i} - q_{i}|\end{pmatrix}^{1/n} 
\end{align}\]</span></p>
<p>Let us compare the three popular distance measurements: manhattan, euclidean, minkowski (see Table <a href="machinelearning1.html#tab:mdistance">9.31</a>).</p>

<div class="sourceCode" id="cb969"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb969-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb969-2" data-line-number="2">v1 =<span class="st"> </span><span class="kw">round</span>( <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">min=</span><span class="dv">1</span>, <span class="dt">max=</span><span class="dv">10</span>) )</a>
<a class="sourceLine" id="cb969-3" data-line-number="3">v2 =<span class="st"> </span><span class="kw">round</span>( <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">min=</span><span class="dv">1</span>, <span class="dt">max=</span><span class="dv">10</span>) )</a>
<a class="sourceLine" id="cb969-4" data-line-number="4">n =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb969-5" data-line-number="5">Vmanhattan =<span class="st"> </span><span class="kw">abs</span>( v1 <span class="op">-</span><span class="st"> </span>v2 ) <span class="op">^</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb969-6" data-line-number="6">Veuclidean =<span class="st"> </span><span class="kw">abs</span>( v1 <span class="op">-</span><span class="st"> </span>v2 ) <span class="op">^</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb969-7" data-line-number="7">Vminkowski =<span class="st"> </span><span class="kw">abs</span>( v1 <span class="op">-</span><span class="st"> </span>v2 ) <span class="op">^</span><span class="st"> </span>n</a>
<a class="sourceLine" id="cb969-8" data-line-number="8">Sman =<span class="st"> </span><span class="kw">sum</span>( Vmanhattan )</a>
<a class="sourceLine" id="cb969-9" data-line-number="9">Seuc =<span class="st"> </span><span class="kw">sum</span>( Veuclidean )</a>
<a class="sourceLine" id="cb969-10" data-line-number="10">Smin =<span class="st"> </span><span class="kw">sum</span>( Vminkowski )</a>
<a class="sourceLine" id="cb969-11" data-line-number="11">Dman =<span class="st"> </span>Sman<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">1</span>);  Deuc =<span class="st"> </span>Seuc<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>);  Dmin =<span class="st"> </span>Smin<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb969-12" data-line-number="12">ds =<span class="st"> </span><span class="kw">t</span>( <span class="kw">data.frame</span>(v1,v2,</a>
<a class="sourceLine" id="cb969-13" data-line-number="13">                   <span class="kw">abs</span>( v1 <span class="op">-</span><span class="st"> </span>v2 ) <span class="op">^</span><span class="st"> </span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb969-14" data-line-number="14">                   <span class="kw">abs</span>( v1 <span class="op">-</span><span class="st"> </span>v2 ) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb969-15" data-line-number="15">                   <span class="kw">abs</span>( v1 <span class="op">-</span><span class="st"> </span>v2 ) <span class="op">^</span><span class="st"> </span>n ))</a>
<a class="sourceLine" id="cb969-16" data-line-number="16"><span class="kw">colnames</span>(ds) =<span class="st"> </span><span class="kw">c</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb969-17" data-line-number="17"><span class="kw">rownames</span>(ds) =<span class="kw">c</span> (<span class="st">&quot;**vector P**&quot;</span>, <span class="st">&quot;**vector Q**&quot;</span>,</a>
<a class="sourceLine" id="cb969-18" data-line-number="18">                 <span class="st">&quot;**manhattan $(Pi-Qi)^1$**&quot;</span>,</a>
<a class="sourceLine" id="cb969-19" data-line-number="19">                 <span class="st">&quot;**euclidean  $(Pi-Qi)^2$**&quot;</span>,</a>
<a class="sourceLine" id="cb969-20" data-line-number="20">                 <span class="st">&quot;**minkowski  $(Pi-Qi)^3$**&quot;</span>)</a>
<a class="sourceLine" id="cb969-21" data-line-number="21">knitr<span class="op">::</span><span class="kw">kable</span>(</a>
<a class="sourceLine" id="cb969-22" data-line-number="22">  <span class="kw">head</span>(ds, <span class="dv">20</span>), <span class="dt">caption =</span> <span class="st">&quot;Distance Measurements&quot;</span>, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<table>
<caption><span id="tab:mdistance">Table 9.31: </span>Distance Measurements</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">7</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>vector P</strong></td>
<td align="right">9</td>
<td align="right">7</td>
<td align="right">10</td>
<td align="right">6</td>
<td align="right">8</td>
<td align="right">9</td>
<td align="right">4</td>
<td align="right">7</td>
<td align="right">5</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="left"><strong>vector Q</strong></td>
<td align="right">9</td>
<td align="right">9</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">2</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="left"><strong>manhattan <span class="math inline">\((Pi-Qi)^1\)</span></strong></td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">3</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left"><strong>euclidean <span class="math inline">\((Pi-Qi)^2\)</span></strong></td>
<td align="right">0</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">0</td>
<td align="right">4</td>
<td align="right">9</td>
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td align="left"><strong>minkowski <span class="math inline">\((Pi-Qi)^3\)</span></strong></td>
<td align="right">0</td>
<td align="right">8</td>
<td align="right">8</td>
<td align="right">0</td>
<td align="right">8</td>
<td align="right">27</td>
<td align="right">8</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">8</td>
</tr>
</tbody>
</table>

<p>Compute for <strong>Manhattan Distance</strong>:</p>
<p><span class="math display">\[\begin{align}
D_{manhattan}(P, Q) = \sqrt[1]{sum(0, 2, 2, 0, 2, 3, 2, 1, 1, 2)} = \sqrt[1]{15} = 15
\end{align}\]</span></p>
<p>Compute for <strong>Euclidean Distance</strong>:</p>
<p><span class="math display">\[\begin{align}
D_{euclidean}(P, Q) = \sqrt[2]{sum(0, 4, 4, 0, 4, 9, 4, 1, 1, 4)} = \sqrt[2]{31} = 5.5678
\end{align}\]</span></p>
<p>Compute for <strong>Minkowski Distance</strong>:</p>
<p><span class="math display">\[\begin{align}
D_{minkowski}(P, Q) = \sqrt[3]{sum(0, 8, 8, 0, 8, 27, 8, 1, 1, 8)} = \sqrt[3]{69} = 4.1016
\end{align}\]</span></p>
</div>
<div id="jaccard-similarity-and-distance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.4.4</span> Jaccard (Similarity and Distance) <a href="machinelearning1.html#jaccard-similarity-and-distance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given two vectors and comparing, one of the many ways to handle distance similarity is using Jaccard similarity.</p>
<p><strong>Jaccard similarity</strong> formula:</p>
<p><span class="math display">\[\begin{align}
Jaccard\ Similarity = J(P,Q) =  \frac{\sum_{i=1}^{j} min(p_{i},q_{i})}{\sum_{i=1}^{j} max(p_{i},q_{i})}
\end{align}\]</span></p>
<p><strong>Jaccard distance</strong> formula:</p>
<p><span class="math display">\[\begin{align}
Jaccard\ Distance = D_{j}(P,Q) =  1 - J(P,Q)
\end{align}\]</span></p>
<p>Using the two formulas, we should be able to construct a simple example:</p>
<div class="sourceCode" id="cb970"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb970-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb970-2" data-line-number="2">v1 =<span class="st"> </span><span class="kw">round</span>( <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">min=</span><span class="dv">1</span>, <span class="dt">max=</span><span class="dv">10</span>) )</a>
<a class="sourceLine" id="cb970-3" data-line-number="3">v2 =<span class="st"> </span><span class="kw">round</span>( <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">min=</span><span class="dv">1</span>, <span class="dt">max=</span><span class="dv">10</span>) )</a>
<a class="sourceLine" id="cb970-4" data-line-number="4">m =<span class="st">  </span><span class="kw">matrix</span>( <span class="kw">c</span>(v1, v2), <span class="dt">nrow=</span><span class="dv">2</span>, <span class="dt">ncol=</span><span class="kw">length</span>(v1), <span class="dt">byrow=</span><span class="ot">TRUE</span>)  </a>
<a class="sourceLine" id="cb970-5" data-line-number="5">Jmin=<span class="kw">apply</span>(m, <span class="dv">2</span>, min)</a>
<a class="sourceLine" id="cb970-6" data-line-number="6">Jmax=<span class="kw">apply</span>(m, <span class="dv">2</span>, max)</a>
<a class="sourceLine" id="cb970-7" data-line-number="7">Jsimilarity =<span class="st"> </span><span class="kw">sum</span>(Jmin)<span class="op">/</span><span class="kw">sum</span>(Jmax)</a>
<a class="sourceLine" id="cb970-8" data-line-number="8">Jdistance=<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Jsimilarity</a>
<a class="sourceLine" id="cb970-9" data-line-number="9">ds =<span class="st"> </span><span class="kw">t</span>( <span class="kw">data.frame</span>(v1,v2, Jmin, Jmax))</a>
<a class="sourceLine" id="cb970-10" data-line-number="10"><span class="kw">colnames</span>(ds) =<span class="st"> </span><span class="kw">c</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb970-11" data-line-number="11"><span class="kw">rownames</span>(ds) =<span class="kw">c</span> (<span class="st">&quot;**vector P**&quot;</span>, <span class="st">&quot;**vector Q**&quot;</span>, <span class="st">&quot;**min**&quot;</span>, <span class="st">&quot;**max**&quot;</span>)</a>
<a class="sourceLine" id="cb970-12" data-line-number="12">knitr<span class="op">::</span><span class="kw">kable</span>(</a>
<a class="sourceLine" id="cb970-13" data-line-number="13">  <span class="kw">head</span>(ds, <span class="dv">20</span>), <span class="dt">caption =</span> <span class="st">&#39;Jaccard Similarity&#39;</span>,</a>
<a class="sourceLine" id="cb970-14" data-line-number="14">  <span class="dt">booktabs =</span> <span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb970-15" data-line-number="15">)</a></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-454">Table 9.32: </span>Jaccard Similarity</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">7</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>vector P</strong></td>
<td align="right">9</td>
<td align="right">7</td>
<td align="right">10</td>
<td align="right">6</td>
<td align="right">8</td>
<td align="right">9</td>
<td align="right">4</td>
<td align="right">7</td>
<td align="right">5</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="left"><strong>vector Q</strong></td>
<td align="right">9</td>
<td align="right">9</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">2</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="left"><strong>min</strong></td>
<td align="right">9</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">2</td>
<td align="right">6</td>
<td align="right">5</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="left"><strong>max</strong></td>
<td align="right">9</td>
<td align="right">9</td>
<td align="right">10</td>
<td align="right">6</td>
<td align="right">8</td>
<td align="right">9</td>
<td align="right">4</td>
<td align="right">7</td>
<td align="right">6</td>
<td align="right">5</td>
</tr>
</tbody>
</table>
<p>Compute for Jaccard Similarity:</p>
<p><span class="math display">\[\begin{align}
J(P, Q) = \frac{sum(9, 7, 8, 6, 6, 6, 2, 6, 5, 3)}{sum(9, 9, 10, 6, 8, 9, 4, 7, 6, 5)} = 0.7945
\end{align}\]</span></p>
<p>Compute for Jaccard Distance:</p>
<p><span class="math display">\[\begin{align}
D_{j} = 1 - 0.7945 = 0.2055
\end{align}\]</span></p>
</div>
<div id="hamming-distance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.4.5</span> Hamming Distance <a href="machinelearning1.html#hamming-distance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Hamming distance</strong> measures the distance between texts. In terms of calculation, it computes the number of iterations to replace letters between texts. The shortest iteration is the Hamming distance. For two-bit texts, it calculates the number of positions in the two texts that differ. The Hamming distance is three in the example below because positions 2, 6, and 7 are different between the two-bit strings.</p>
<p><span class="math display">\[
\begin{array}{l}
bit.string.1 = 0 1 1 0 0 1 1 0\\
bit.string.2 = 1 0 1 0 0 0 0 0
\end{array}
\]</span></p>
</div>
<div id="mahalanobis-distance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.4.6</span> Mahalanobis Distance <a href="machinelearning1.html#mahalanobis-distance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Mahalanobis distance</strong> is a distance measure between a point ( or distribution) to another distribution. It reduces the scale (variance) calculation into <strong>Euclidean distance</strong>.</p>
</div>
<div id="precision-and-accuracy" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.4.7</span> Precision and Accuracy  <a href="machinelearning1.html#precision-and-accuracy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Often, we are asked to make crucial decisions. Decisions are made based on facts. If there is lacking facts, it definitely can post a challenge. Sometimes, facts do not come in full and may come only partially. However, then sometimes, some factual data are contrary to the truth. Therefore, we need to separate true positives from false positives and true negatives from false negatives.</p>
<p>Given the responsibility to decide, we often want to be able to decide as accurately and precisely as possible. So, what can we do now?</p>
<p>To answer that question, let us have a close look at the Figure <a href="machinelearning1.html#fig:precision">9.20</a>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:precision"></span>
<img src="truth_table.png" alt="Actual vs Predict Table" width="70%" />
<p class="caption">
Figure 9.20: Actual vs Predict Table
</p>
</div>
<p>Now, there are a few metrics that will help in our decisions, all derived from Table .</p>
<ul>
<li><strong>Precision</strong> is a measure of the ratio (or proportion) of subjects (samples) that are <strong>correctly</strong> identified (or labeled) as positive out of all the subjects (samples) that are predicted as positive.</li>
</ul>
<p><span class="math display">\[\begin{align}
Precision = \frac{True\ Positive}{(True\ Positive) + (False\ Positive)} = \frac{TP}{TP + FP}
\end{align}\]</span></p>
<ul>
<li><strong>Accuracy</strong> is a measure of the ratio (or proportion) of subjects (samples) that are <strong>correctly</strong> identified (or labeled) as positive or negative out of all the subjects (samples) predicted.</li>
</ul>
<p><span class="math display">\[\begin{align}
Accuracy = \frac{TP + TN}{TP + FP + FN + TN}
\end{align}\]</span></p>
<ul>
<li><strong>Sensitivity (Recall)</strong>, also called <strong>probability of prediction (or detection)</strong>, is a measure of the ratio (or proportion) of subjects (samples) that are <strong>correctly</strong> identified (or predicted or labeled) as positive, out of all subjects that are labeled as actually relevant.  </li>
</ul>
<p><span class="math display">\[\begin{align}
\text{Sensitivity} = \frac{True\ Positive}{(True\ Positive) + (False\ Negative)} = \frac{TP}{TP + FN}
\end{align}\]</span></p>
<ul>
<li><strong>Specificity</strong> is a measure of the ratio (or proportion) of subjects (samples) that are <strong>correctly</strong> identified (or predicted or labeled) as negative out of all subjects that are labeled as not actually relevant. </li>
</ul>
<p><span class="math display">\[\begin{align}
\text{Specificity} = \frac{True\ Negative}{(True\ Negative) + (False\ Positive)} = \frac{TN}{TN + FP}
\end{align}\]</span></p>
<ul>
<li><strong>F1 Score</strong> as a measure that weighs <strong>Recall</strong> and <strong>Precision</strong> when there is an imbalance of class distribution. </li>
</ul>
<p><span class="math display">\[\begin{align}
\text{F1 Score} = \frac{2*(Recall * Precision)}{(Recall + Precision)} = \frac{2RP}{R+P}
\end{align}\]</span></p>
<ul>
<li><strong>Error Rate</strong> is a measure of inaccuracy. </li>
</ul>
<p><span class="math display">\[\begin{align}
\text{Error Rate} = 1 - \text{Accuracy}
\end{align}\]</span></p>
<ul>
<li><strong>Jaccard Index(JI)</strong>, also called <strong>Jaccard Similarity Coefficient</strong>, measures the interaction of two sets over their union. </li>
</ul>
<p><span class="math display">\[\begin{align}
\text{Jaccard Index} = \frac{|A \cap B|}{ |A\cup B|}
\end{align}\]</span></p>
<p>    On the other hand, <strong>JI</strong> is also the ratio (or proportion) of subjects (samples) that are <strong>correctly</strong> identified (or predicted or labeled) as positive out of all subjects predicted, excluding those labeled as true negative.</p>
<p><span class="math display">\[\begin{align}
\text{Jaccard Index} = \frac{TP}{TP + FP + FN}
\end{align}\]</span></p>
</div>
<div id="auc-on-roc" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.4.8</span> AUC on ROC <a href="machinelearning1.html#auc-on-roc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use <strong>receiver operating characteristic (ROC)</strong> to measure the performance of prediction or classification by evaluating the <strong>area under the curve (AUC)</strong> on <strong>ROC</strong>. <strong>ROC</strong> is generated using the <strong>True Positive Rate (TPR)</strong> and <strong>False Positive Rate (FPR)</strong> measures:</p>
<p><span class="math display">\[\begin{align}
\underbrace{\text{TPR} = \frac{TP}{TP + FN}}_\text{sensitivity}
\ \ \ \ \ \ \ \ 
\underbrace{\text{FPR} = \frac{FP}{FP + TN}}_\text{1 - specificity}
\end{align}\]</span></p>
<p>To illustrate, let us simulate a binomial process (e.g., tossing a coin 400 times). Let us try to achieve 50% of success - that the coin lands on heads with a 50% probability of success. Hopefully, we can get 200 for heads and 200 for tails using a random seed of 2020 - though that may not always be the case. So let us see if we are close enough.</p>

<div class="sourceCode" id="cb971"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb971-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb971-2" data-line-number="2">N=<span class="dv">400</span></a>
<a class="sourceLine" id="cb971-3" data-line-number="3">actual    =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n=</span>N, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span><span class="fl">0.50</span>)</a>
<a class="sourceLine" id="cb971-4" data-line-number="4">heads =<span class="st"> </span><span class="kw">length</span>(<span class="kw">which</span>(actual <span class="op">==</span><span class="st"> </span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb971-5" data-line-number="5">tails =<span class="st"> </span><span class="kw">length</span>(<span class="kw">which</span>(actual <span class="op">==</span><span class="st"> </span><span class="dv">0</span>))</a>
<a class="sourceLine" id="cb971-6" data-line-number="6"><span class="kw">c</span>(<span class="st">&quot;Heads&quot;</span> =<span class="st"> </span>heads, <span class="st">&quot;Tails&quot;</span> =<span class="st"> </span>tails)</a></code></pre></div>
<pre><code>## Heads Tails 
##   206   194</code></pre>

<p>We get an outcome with 206 heads and 194 tails - let us take note of this as our <strong>ground truth</strong>.</p>
<p>Now, let us simulate a perfect prediction (just merely cloning the outcome of 400 tosses). Our <strong>confusion table</strong> looks like so:</p>

<div class="sourceCode" id="cb973"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb973-1" data-line-number="1">predicted =<span class="st"> </span>actual</a>
<a class="sourceLine" id="cb973-2" data-line-number="2"><span class="kw">table</span>(<span class="kw">as.factor</span>(predicted), <span class="kw">as.factor</span>(actual), </a>
<a class="sourceLine" id="cb973-3" data-line-number="3">      <span class="dt">dnn =</span> <span class="kw">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Actual&quot;</span>))</a></code></pre></div>
<pre><code>##          Actual
## Predicted   0   1
##         0 194   0
##         1   0 206</code></pre>

<p>Alternatively, we can use <strong>confusionMatrix(.)</strong> from the <strong>caret</strong> library to help us get some of the metrics namely, <strong>accuracy</strong>, <strong>sensitivity</strong>, <strong>specificity</strong>, etc. </p>

<div class="sourceCode" id="cb975"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb975-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb975-2" data-line-number="2"><span class="kw">confusionMatrix</span>(<span class="kw">as.factor</span>(predicted), <span class="kw">as.factor</span>(actual))</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 194   0
##          1   0 206
##                                     
##                Accuracy : 1         
##                  95% CI : (0.991, 1)
##     No Information Rate : 0.515     
##     P-Value [Acc &gt; NIR] : &lt;2e-16    
##                                     
##                   Kappa : 1         
##                                     
##  Mcnemar&#39;s Test P-Value : NA        
##                                     
##             Sensitivity : 1.000     
##             Specificity : 1.000     
##          Pos Pred Value : 1.000     
##          Neg Pred Value : 1.000     
##              Prevalence : 0.485     
##          Detection Rate : 0.485     
##    Detection Prevalence : 0.485     
##       Balanced Accuracy : 1.000     
##                                     
##        &#39;Positive&#39; Class : 0         
## </code></pre>

<p>Notice that most metrics such as <strong>accuracy</strong>, <strong>sensitivity</strong>, and <strong>specificity</strong> all show a value of 1. That indicates we have a perfect prediction.</p>
<p>However, suppose our outcome turns out to be far from being perfect. Let us simulate three separate exercises and show the confusion table.</p>

<div class="sourceCode" id="cb977"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb977-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb977-2" data-line-number="2">pred1 =<span class="st"> </span>exercise1.prediction =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n=</span>N, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span><span class="fl">0.90</span>)</a>
<a class="sourceLine" id="cb977-3" data-line-number="3">pred2 =<span class="st"> </span>exercise2.prediction =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n=</span>N, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span><span class="fl">0.70</span>)</a>
<a class="sourceLine" id="cb977-4" data-line-number="4">pred3 =<span class="st"> </span>exercise3.prediction =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n=</span>N, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span><span class="fl">0.20</span>)</a></code></pre></div>

<p>The first set of predictions follows a sample binomial distribution with a 90% probability of success that the coin lands on heads. The second set shows a 70% probability, and the third set shows a 20% probability.</p>
<p>Let us show the <strong>confusion table</strong> for the first exercise using its prediction and the <strong>ground truth</strong>:</p>

<div class="sourceCode" id="cb978"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb978-1" data-line-number="1">(<span class="dt">conf.tab =</span> <span class="kw">table</span>(<span class="kw">as.factor</span>(pred1), <span class="kw">as.factor</span>(actual), </a>
<a class="sourceLine" id="cb978-2" data-line-number="2">                  <span class="dt">dnn =</span> <span class="kw">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Actual&quot;</span>)))</a></code></pre></div>
<pre><code>##          Actual
## Predicted   0   1
##         0  19  24
##         1 175 182</code></pre>

<p>The table shows that our prediction has 182 true positives (<strong>TP</strong>), 175 false positives (<strong>FP</strong>), 19 true negatives (<strong>TN</strong>), and 24 false negatives (<strong>FN</strong>).</p>
<p>In terms of metrics, the <strong>specificity</strong> is 0.098 based on 19 / (19 + 175).</p>
<p>The <strong>sensitivity</strong> is 0.883 based on 182 / (182 + 24).</p>
<p>The <strong>accuracy</strong> is 0.502 based on (182 + 19) / (19 + 175 + 24 + 182).</p>
<p>The <strong>true positive rate (TPR)</strong> is 0.883 based on 182 / (182 + 24). This is also equivalent to <strong>sensitivity</strong>.</p>
<p>The <strong>false positive rate (TPR)</strong> is 0.902 based on 175 / (175 + 19). This is also equivalent to <strong>(1-specificity)</strong>.</p>
<p>Let us show the <strong>confusion table</strong> for the second exercise using its prediction and the <strong>ground truth</strong>:</p>

<div class="sourceCode" id="cb980"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb980-1" data-line-number="1">(<span class="dt">conf.tab =</span> <span class="kw">table</span>(<span class="kw">as.factor</span>(pred2), <span class="kw">as.factor</span>(actual), </a>
<a class="sourceLine" id="cb980-2" data-line-number="2">                  <span class="dt">dnn =</span> <span class="kw">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Actual&quot;</span>)))</a></code></pre></div>
<pre><code>##          Actual
## Predicted   0   1
##         0  52  63
##         1 142 143</code></pre>

<p>The table shows that our prediction has 143 true positives (<strong>TP</strong>), 142 false positives (<strong>FP</strong>), 52 true negatives (<strong>TN</strong>), and 63 false negatives (<strong>FN</strong>).</p>
<p>In terms of metrics, the <strong>specificity</strong> is 0.268 based on 52 / (52 + 142).</p>
<p>The <strong>sensitivity</strong> is 0.694 based on 143 / (143 + 63).</p>
<p>The <strong>accuracy</strong> is 0.488 based on (143 + 52) / (52 + 142 + 63 + 143).</p>
<p>The <strong>true positive rate (TPR)</strong> is 0.694 based on 143 / (143 + 63). This is also equivalent to <strong>sensitivity</strong>.</p>
<p>The <strong>false positive rate (TPR)</strong> is 0.732 based on 142 / (142 + 52). This is also equivalent to <strong>(1-specificity)</strong>.</p>
<p>Finally, let us show the <strong>confusion table</strong> for the third exercise using its prediction and the <strong>ground truth</strong>:</p>

<div class="sourceCode" id="cb982"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb982-1" data-line-number="1">(<span class="dt">conf.tab =</span> <span class="kw">table</span>(<span class="kw">as.factor</span>(pred3), <span class="kw">as.factor</span>(actual), </a>
<a class="sourceLine" id="cb982-2" data-line-number="2">                  <span class="dt">dnn =</span> <span class="kw">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Actual&quot;</span>)))</a></code></pre></div>
<pre><code>##          Actual
## Predicted   0   1
##         0 151 158
##         1  43  48</code></pre>

<p>The table shows that our prediction has 48 true positives (<strong>TP</strong>), 43 false positives (<strong>FP</strong>), 151 true negatives (<strong>TN</strong>), and 158 false negatives (<strong>FN</strong>).</p>
<p>In terms of metrics, the <strong>specificity</strong> is 0.778 based on 151 / (151 + 43).</p>
<p>The <strong>sensitivity</strong> is 0.233 based on 48 / (48 + 158).</p>
<p>The <strong>accuracy</strong> is 0.498 based on (48 + 151) / (151 + 43 + 158 + 48).</p>
<p>The <strong>true positive rate (TPR)</strong> is 0.233 based on 48 / (48 + 158). This is also equivalent to <strong>sensitivity</strong>.</p>
<p>The <strong>false positive rate (TPR)</strong> is 0.222 based on 43 / (43 + 151). This is also equivalent to <strong>(1-specificity)</strong>.</p>
<p>We showed three different sets of predictions and how they are interpreted in the table.Let us get a more complete and detailed interpretation using <strong>confusionMatrix(.)</strong> function. Here, we use the predictions from the third exercise and compare the metrics below.</p>

<div class="sourceCode" id="cb984"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb984-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb984-2" data-line-number="2"><span class="kw">confusionMatrix</span>(<span class="kw">as.factor</span>(pred3), <span class="kw">as.factor</span>(actual))</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 151 158
##          1  43  48
##                                         
##                Accuracy : 0.498         
##                  95% CI : (0.447, 0.548)
##     No Information Rate : 0.515         
##     P-Value [Acc &gt; NIR] : 0.774         
##                                         
##                   Kappa : 0.011         
##                                         
##  Mcnemar&#39;s Test P-Value : 8.92e-16      
##                                         
##             Sensitivity : 0.778         
##             Specificity : 0.233         
##          Pos Pred Value : 0.489         
##          Neg Pred Value : 0.527         
##              Prevalence : 0.485         
##          Detection Rate : 0.378         
##    Detection Prevalence : 0.772         
##       Balanced Accuracy : 0.506         
##                                         
##        &#39;Positive&#39; Class : 0             
## </code></pre>

<p>For <strong>Multi-Classification</strong>, let us defer the subject in Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under <strong>AdaBoost</strong> Section in which we cover confusion matrix for multi-classes.</p>
<p>Now, in terms of <strong>AUC on ROC</strong>, to get an intuition around the concept, it helps to tailor our dataset such that we construct four different predictions coming from four separate processes.</p>

<div class="sourceCode" id="cb986"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb986-1" data-line-number="1">N =<span class="st"> </span><span class="dv">1000</span> </a>
<a class="sourceLine" id="cb986-2" data-line-number="2">pos =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, N); neg  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb986-3" data-line-number="3">pred =<span class="st"> </span><span class="kw">c</span>(pos, neg)</a>
<a class="sourceLine" id="cb986-4" data-line-number="4">norm1 =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;mean&quot;</span> =<span class="st"> </span><span class="fl">2.0</span>, <span class="st">&quot;sd&quot;</span> =<span class="st"> </span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb986-5" data-line-number="5">norm2 =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;mean&quot;</span> =<span class="st"> </span><span class="fl">1.9</span>, <span class="st">&quot;sd&quot;</span> =<span class="st"> </span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb986-6" data-line-number="6">norm3 =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;mean&quot;</span> =<span class="st"> </span><span class="fl">1.7</span>, <span class="st">&quot;sd&quot;</span> =<span class="st"> </span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb986-7" data-line-number="7">norm4 =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;mean&quot;</span> =<span class="st"> </span><span class="fl">1.5</span>, <span class="st">&quot;sd&quot;</span> =<span class="st"> </span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb986-8" data-line-number="8">norm5 =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;mean&quot;</span> =<span class="st"> </span><span class="fl">1.3</span>, <span class="st">&quot;sd&quot;</span> =<span class="st"> </span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb986-9" data-line-number="9">pos.prob =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dt">mean=</span>norm1<span class="op">$</span>mean, <span class="dt">sd=</span>norm1<span class="op">$</span>sd) </a>
<a class="sourceLine" id="cb986-10" data-line-number="10">neg.prob =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb986-11" data-line-number="11">neg.prob[[<span class="dv">1</span>]] =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dt">mean=</span>norm2<span class="op">$</span>mean, <span class="dt">sd=</span>norm2<span class="op">$</span>sd) </a>
<a class="sourceLine" id="cb986-12" data-line-number="12">neg.prob[[<span class="dv">2</span>]] =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dt">mean=</span>norm3<span class="op">$</span>mean, <span class="dt">sd=</span>norm3<span class="op">$</span>sd)  </a>
<a class="sourceLine" id="cb986-13" data-line-number="13">neg.prob[[<span class="dv">3</span>]] =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dt">mean=</span>norm4<span class="op">$</span>mean, <span class="dt">sd=</span>norm4<span class="op">$</span>sd) </a>
<a class="sourceLine" id="cb986-14" data-line-number="14">neg.prob[[<span class="dv">4</span>]] =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dt">mean=</span>norm5<span class="op">$</span>mean, <span class="dt">sd=</span>norm5<span class="op">$</span>sd) </a></code></pre></div>

<p>Let us review the four plots in Figure <a href="machinelearning1.html#fig:aucroc">9.21</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:aucroc"></span>
<img src="DS_files/figure-html/aucroc-1.png" alt="Observation vs Expectation" width="100%" />
<p class="caption">
Figure 9.21: Observation vs Expectation
</p>
</div>
<p>To understand the four plots, we have two distributions. The left-side blue distribution indicates all positives, and the right-side red distribution indicates all negatives. We put in place two thresholds. The left-side blue threshold marks the cut-off for the positive distribution, and the right-side red threshold marks the cut-off for the negative distribution. As the positive distribution moves towards the left side, most of its region gets into the <strong>false-positive</strong> territory. As the negative distribution moves towards the right side, most of its region gets into the <strong>false-negative</strong>. For simplicity, we made the positive distribution constant (not moving) and moved the negative distribution further away from its cut-off, meaning, as it moves farther away, it gets lesser to the <strong>false-negative</strong> territory. That also means improvement of <strong>AUC</strong>.</p>
<p>Another interpretation without using the cut-off is that the more overlap between both distributions, the lesser the <strong>AUC</strong> becomes - prediction power lessens.</p>
<p>Let us use <strong>roc(.)</strong> function in a 3rd-party library called <strong>pROC</strong> to illustrate. See Figure <a href="machinelearning1.html#fig:roc">9.22</a></p>

<div class="sourceCode" id="cb987"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb987-1" data-line-number="1"><span class="kw">library</span>(pROC)</a>
<a class="sourceLine" id="cb987-2" data-line-number="2">color =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;black&quot;</span> ,<span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb987-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>) , <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb987-4" data-line-number="4">     <span class="dt">ylab=</span><span class="st">&quot;True Positive Rate (Sensitivity)&quot;</span>, </a>
<a class="sourceLine" id="cb987-5" data-line-number="5">     <span class="dt">xlab=</span><span class="st">&quot;False Positive Rate (1 - Specificity)&quot;</span>,</a>
<a class="sourceLine" id="cb987-6" data-line-number="6">     <span class="dt">main=</span><span class="st">&quot;AUC on ROC&quot;</span>)</a>
<a class="sourceLine" id="cb987-7" data-line-number="7">auc =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>)</a>
<a class="sourceLine" id="cb987-8" data-line-number="8"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>) {</a>
<a class="sourceLine" id="cb987-9" data-line-number="9">  prob =<span class="st"> </span><span class="kw">c</span>(pos.prob, neg.prob[[i]])</a>
<a class="sourceLine" id="cb987-10" data-line-number="10">  roc =<span class="st"> </span><span class="kw">roc</span>(pred <span class="op">~</span><span class="st"> </span>prob, <span class="dt">quiet=</span><span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb987-11" data-line-number="11">  TPR =<span class="st"> </span>roc<span class="op">$</span>sensitivities</a>
<a class="sourceLine" id="cb987-12" data-line-number="12">  FPR =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>roc<span class="op">$</span>specificities</a>
<a class="sourceLine" id="cb987-13" data-line-number="13">  <span class="kw">lines</span>(FPR, TPR, <span class="dt">col=</span>color[i])</a>
<a class="sourceLine" id="cb987-14" data-line-number="14">  auc[i] =<span class="st"> </span><span class="kw">round</span>(roc<span class="op">$</span>auc,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb987-15" data-line-number="15">}</a>
<a class="sourceLine" id="cb987-16" data-line-number="16"><span class="kw">abline</span>( <span class="dt">a=</span><span class="dv">0</span>, <span class="dt">b=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb987-17" data-line-number="17"><span class="kw">legend</span>(<span class="fl">0.4</span>, <span class="fl">0.3</span>, </a>
<a class="sourceLine" id="cb987-18" data-line-number="18">    <span class="dt">legend=</span><span class="kw">c</span>( </a>
<a class="sourceLine" id="cb987-19" data-line-number="19">      <span class="kw">paste0</span>(<span class="st">&quot;predict.1 (auc=&quot;</span>,auc[<span class="dv">1</span>],<span class="st">&quot;)&quot;</span>), </a>
<a class="sourceLine" id="cb987-20" data-line-number="20">      <span class="kw">paste0</span>(<span class="st">&quot;predict.2 (auc=&quot;</span>,auc[<span class="dv">2</span>],<span class="st">&quot;)&quot;</span>),</a>
<a class="sourceLine" id="cb987-21" data-line-number="21">      <span class="kw">paste0</span>(<span class="st">&quot;predict.3 (auc=&quot;</span>,auc[<span class="dv">3</span>],<span class="st">&quot;)&quot;</span>), </a>
<a class="sourceLine" id="cb987-22" data-line-number="22">      <span class="kw">paste0</span>(<span class="st">&quot;predict.4 (auc=&quot;</span>,auc[<span class="dv">4</span>],<span class="st">&quot;)&quot;</span>)), </a>
<a class="sourceLine" id="cb987-23" data-line-number="23">    <span class="dt">col=</span>color,  <span class="dt">pch=</span><span class="dv">20</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb987-24" data-line-number="24"><span class="kw">legend</span>(<span class="fl">0.60</span>, <span class="fl">0.67</span>, </a>
<a class="sourceLine" id="cb987-25" data-line-number="25">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;0.90 - 1.00 = excellent&quot;</span>,  <span class="st">&quot;0.80 - 0.90 = good&quot;</span>, </a>
<a class="sourceLine" id="cb987-26" data-line-number="26">              <span class="st">&quot;0.70 - 0.80 = fair&quot;</span>,   <span class="st">&quot;0.60 - 0.70 = poor&quot;</span>,</a>
<a class="sourceLine" id="cb987-27" data-line-number="27">              <span class="st">&quot;0.50 - 0.60 = fail&quot;</span> ),</a>
<a class="sourceLine" id="cb987-28" data-line-number="28">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>),  <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">16</span>,<span class="dv">16</span>,<span class="dv">16</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:roc"></span>
<img src="DS_files/figure-html/roc-1.png" alt="AUC on ROC" width="70%" />
<p class="caption">
Figure 9.22: AUC on ROC
</p>
</div>

<p>To put a better interpretation of the <strong>AUC</strong>, we map the score like so:</p>

<div class="sourceCode" id="cb988"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb988-1" data-line-number="1">score &lt;-<span class="st"> </span><span class="cf">function</span>(auc) {</a>
<a class="sourceLine" id="cb988-2" data-line-number="2">    <span class="cf">if</span>(auc <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.60</span>) <span class="kw">return</span>(<span class="st">&quot;fail&quot;</span>); </a>
<a class="sourceLine" id="cb988-3" data-line-number="3">    <span class="cf">if</span>(auc <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.60</span> <span class="op">&amp;&amp;</span><span class="st"> </span>auc <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.70</span>) <span class="kw">return</span>(<span class="st">&quot;poor&quot;</span>) </a>
<a class="sourceLine" id="cb988-4" data-line-number="4">    <span class="cf">if</span>(auc <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.70</span> <span class="op">&amp;&amp;</span><span class="st"> </span>auc <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.80</span>) <span class="kw">return</span>(<span class="st">&quot;fair&quot;</span>) </a>
<a class="sourceLine" id="cb988-5" data-line-number="5">    <span class="cf">if</span>(auc <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.80</span> <span class="op">&amp;&amp;</span><span class="st"> </span>auc <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.90</span>) <span class="kw">return</span>(<span class="st">&quot;good&quot;</span>) </a>
<a class="sourceLine" id="cb988-6" data-line-number="6">    <span class="cf">if</span>(auc <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.90</span> <span class="op">&amp;&amp;</span><span class="st"> </span>auc <span class="op">&lt;=</span><span class="st"> </span><span class="fl">1.00</span>) <span class="kw">return</span>(<span class="st">&quot;excellent&quot;</span>) </a>
<a class="sourceLine" id="cb988-7" data-line-number="7">}</a></code></pre></div>

<p>Based on the AUC result, <strong>prediction 1</strong> is scored as poor. It means that the process producing the <strong>predictions</strong> does not provide enough prediction power to produce outcomes close to the actual values. For the other predictions, we have the following <strong>AUC</strong>.</p>

<div class="sourceCode" id="cb989"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb989-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;prediction 1&quot;</span> =<span class="st"> </span>auc[<span class="dv">1</span>], <span class="st">&quot;prediction 2&quot;</span>=auc[<span class="dv">2</span>], </a>
<a class="sourceLine" id="cb989-2" data-line-number="2">  <span class="st">&quot;prediction 3&quot;</span> =<span class="st"> </span>auc[<span class="dv">3</span>], <span class="st">&quot;prediction 4&quot;</span>=auc[<span class="dv">4</span>] )</a></code></pre></div>
<pre><code>## prediction 1 prediction 2 prediction 3 prediction 4 
##        0.644        0.866        0.963        0.995</code></pre>

<p>We cover <strong>Feature Selection</strong> later in one of the sections ahead using another two 3rd-party packages called <strong>ROCR</strong> and <strong>randomForest</strong>.</p>
</div>
</div>
<div id="exploratory-data-analysis" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.5</span> Exploratory Data Analysis<a href="machinelearning1.html#exploratory-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Exploratory Data Analysis (EDA)</strong> aims to study the underlying structure of data and to discover patterns and associations, including effects and influences. We have covered the nature of data distribution in previous chapters. We also covered statistical and bayesian analysis around data.</p>
<p>This section introduces a few concepts in <strong>Data Mining</strong> to supplement our understanding of <strong>Exploratory Analysis</strong>. <strong>Data Mining</strong> is a superset course that includes <strong>Exploratory Data Analysis (EDA)</strong>.</p>
<p>Though there are many more topics in <strong>Data Mining</strong> that are not possible to cover in this book, let us frame the idea of <strong>Data Mining</strong> in the context of only the following topics:</p>
<ul>
<li>Data Cleaning</li>
<li>Association</li>
<li>Pattern Discovery</li>
<li>Null Invariance</li>
<li>Correlation and Collinearity</li>
<li>Covariance</li>
<li>Missingness and Imputation</li>
<li>Others (e.g., Outliers, Leverage, Influence, Data Leakage, and Confounding Variables)</li>
</ul>
<div id="data-cleaning-wrangling" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.1</span> Data Cleaning (Wrangling)  <a href="machinelearning1.html#data-cleaning-wrangling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Data Cleaning</strong> is essential before handling Exploratory Data Analysis methods. Specialists in Data Analysis offer best practices to address this area <span class="citation">(e.g., Osborne, J. W. <a href="bibliography.html#ref-ref1502j">2013</a>)</span>. Notwithstanding a thorough coverage, let us instead briefly list eight dimensions of Data Quality that can be used as a guide to know if our Data is <strong>Clean</strong>. Other literature may list only six dimensions of different combinations out of the eight dimensions. We reference articles that cover them <span class="citation">(Elgabry O. <a href="bibliography.html#ref-ref1502o">2019</a>; Gupta A. <a href="bibliography.html#ref-ref1509a">2021</a>)</span>:</p>
<ul>
<li>Accuracy - Data is as close to its true value.</li>
<li>Completeness - Data is as comprehensive.</li>
<li>Consistency - Values of the same Data are the same everywhere.</li>
<li>Validity - Data is within the prescribed constraints.</li>
<li>Timeliness - Data is available as required.</li>
<li>Uniformity - Data is uniform everywhere in terms of presentation.</li>
<li>Uniqueness - Data is not duplicated anywhere.</li>
<li>Integrity - Data references are intact (See Referential Integrity in Structured Data)</li>
</ul>
<p>The quality of data can also be improved during the gathering, filtering, and transforming (ETL) of data.</p>
</div>
<div id="association" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.2</span> Association<a href="machinelearning1.html#association" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To explain the intuition behind <strong>Association</strong>, suppose we shop online and make a few orders (corresponding to a few transactions recorded into our database). Each order has a list of ordered items. Our goal is to find the most frequent set of ordered items called <strong>itemset</strong> across a set of transactions. We can use any of three <strong>algorithms</strong> starting with <strong>Apriori Algorithm</strong> (note here that we refer to a set of transactions as <strong>transactional database</strong>).</p>
<p><strong>Apriori Algorithm</strong> </p>
<p>To explain <strong>Apriori Algorithm</strong>, let us use Figure <a href="machinelearning1.html#fig:apriori">9.23</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:apriori"></span>
<img src="apriori.png" alt="Apriori Algorithm" width="85%" />
<p class="caption">
Figure 9.23: Apriori Algorithm
</p>
</div>
<p>Figure <a href="machinelearning1.html#fig:apriori">9.23</a> illustrates the classic technique called <strong>Apriori algorithm</strong> used to find the most frequent itemsets <span class="citation">(Srikant, R., &amp; Agrawal, R. <a href="bibliography.html#ref-ref378">1996</a>)</span>. In the figure, we notice multiple scans. Each scan results in a table of itemsets with corresponding frequency. For example, in table C1, we see an itemset (group) that contains only item A. It shows that this item in the first itemset counts 3. As we go to the next scan, we only consider itemsets with a frequency equal to or greater than the minimum support. We then perform the same count, but this time, notice that table C2 has itemsets (groups) that contain two items.</p>
<p>The <strong>Apriori Algorithm</strong> is described below:</p>
<p><span class="math display">\[
\begin{array}{l}
L_1 = \text{\{frequent 1-itemset\}}\\
\text{For k in 2,...   repeat the following}\\
\ \ \ \ \ \ \ \ C_k\ \text{= candidate k-itemsets from }L_{k-1}\\
\ \ \ \ \ \ \ \ \text{For t in 1,... repeat the following}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{tx = transaction[t]}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{For each k-itemset in tx, repeat the following}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{count frequency}\\
\ \ \ \ \ \ \ \ L_k\ \text{= All candidate k-itemsets from } C_k \text{ with minimum support/frequency}\\
\end{array}
\]</span></p>
<p>Grouping items into itemsets based on frequency is one type of <strong>association</strong>, and it follows specific <strong>association rules</strong> as a measure for the effectiveness of the <strong>association</strong> (note that we cover more measures in the <strong>Null-Invariant</strong> section).</p>
<p><strong>Association rules</strong> </p>
<ul>
<li><strong>Support</strong> measures the frequency or number of occurrences of an itemset. An <strong>absolute support</strong> measures the number of occurrences. A <strong>relative support</strong> measures the fraction of the number of transactions in which the itemset occurs out of all transactions. For example, item <strong>A</strong> in Figure <a href="machinelearning1.html#fig:apriori">9.23</a> is a 1-itemset with absolute Support of 3 and relative Support of 60% because it shows occurrences in three transactions out of five transactions.</li>
</ul>
<p><span class="math display">\[ 
Relative\ Support = P(A) = S(A) = 
\frac{\text{transaction in which A occurs}}{\text{total transactions}}=  \frac{3}{5} = 0.60 
\]</span></p>
<ul>
<li><strong>Confidence</strong> measures the probability that an itemset, say a 2-itemset containing <strong>A</strong> and <strong>B</strong>, will occur conditioned on the occurrence of an itemset, say <strong>A</strong>. It measures the fraction of the number of transactions in which <strong>A</strong> and <strong>B</strong> occur out of all transactions in which <strong>A</strong> occurs. For example, in Figure <a href="machinelearning1.html#fig:apriori">9.23</a>, there are three transactions in which <strong>B</strong> occurs. There is one transaction in which both <strong>A</strong> and <strong>B</strong> occur. Therefore, calculating the <strong>confidence</strong> of <strong>A</strong> and <strong>B</strong> to occur together in the same transaction, conditioned on <strong>B</strong>, we have:</li>
</ul>
<p><span class="math display">\[\begin{align*}
Confidence &amp;= \frac{P(A,B)}{P(A)} = C(A \rightarrow B) = \frac{\text{transactions in which A and B occur}}{\text{transactions in which A occurs}} \\
&amp;= \frac{1}{3} = 0.33 
\end{align*}\]</span></p>
<ul>
<li><strong>Lift</strong> measures the ratio of confidence over expected confidence. For example, if <strong>A</strong> and <strong>B</strong> will occur conditioned on the occurrence of an item, say <strong>A</strong>; then the expected confidence is the relative support of <strong>B</strong>.</li>
</ul>
<p><span class="math display">\[
Lift = L(A,B) = \frac{P(A,B)}{P(A)P(B)} = 
\frac{C(A \rightarrow B)}{S(B)} = 
\frac{
\frac{\text{tx in which A and B occur}}{\text{tx in which B occurs}} 
}{
\frac{\text{tx in which B occurs}}{\text{total transactions}}
} =  \frac{ \frac{1}{3}}{\frac{3}{5}} = 0.56
\]</span></p>
<p>The candidate itemsets are selected based on the rules above. For example, in Figure <a href="machinelearning1.html#fig:apriori">9.23</a>, we have chosen a <strong>minimum support</strong> of 2 to limit the itemsets based on that frequency limit. The end result shows that our 3-itemset with the most number of occurrences is <span class="math inline">\(\{A, C, D\}\)</span>.</p>
<p>Note that <strong>Apriori algorithm</strong> requires a repeated scan of the transactional database to generate candidate itemsets. In the next portion of this section, we cover two other alternatives. One of the two algorithms eliminates the need to scan the database repeatedly.</p>
<p><strong>FP-growth Algorithm</strong> </p>
<p>The <strong>Frequent Pattern (FP) growth algorithm</strong>, also called <strong>FP-Growth algorithm</strong>, is proposed by Han J. et al. <span class="citation">(<a href="bibliography.html#ref-ref368">2000</a>)</span> in that it avoids candidate generation found in <strong>Apriori algorithm</strong> when generating frequent itemsets. It scans transactions in a database to arrange the candidate itemsets in descending order and then makes one final scan to construct a compact FP-tree structure based on the ordered itemsets. See Figure <a href="machinelearning1.html#fig:fpgrowth">9.24</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fpgrowth"></span>
<img src="fpgrowth.png" alt="FP-Growth Data" width="85%" />
<p class="caption">
Figure 9.24: FP-Growth Data
</p>
</div>
<p>The algorithm proceeds as follows (granting our minimum support equals 2):</p>
<p><strong>First</strong>, generate a frequent 1-itemset by scanning the transaction database.</p>
<p><span class="math display">\[
\text{\{O:5\},\{A:4\},\{C:4\},\{R:4\},\{D:2\},\{E:2\},\{P:2\},\{B:2\},\{F:2\}}
\]</span></p>
<p><strong>Second</strong>, generate an <strong>ordered</strong> list of k-itemsets by re-scanning the transaction database for each transaction based on the 1-itemset generated. See the first table in Figure <a href="machinelearning1.html#fig:fpgrowth">9.24</a> for the <strong>(Ordered) itemsets</strong>.</p>
<p><strong>Third</strong> with the <strong>ordered</strong> k-itemsets, construct the <strong>FP-tree</strong>. For example, starting with the first transaction <strong>TID=101</strong>, we create a tree with the <strong>Null {}</strong> node being the root. </p>
<p><span class="math display">\[
\text{\{\}}\rightarrow \text{\{O:1\}}\rightarrow \text{\{A:1\}}
\rightarrow \text{\{C:1\}}\rightarrow \text{\{R:1\}}\rightarrow \text{\{B:1\}}
\]</span></p>
<p>Then, we read the second transaction <strong>TID=102</strong> and continue to build the tree.</p>
<p><span class="math display">\[
\text{\{\}}\rightarrow \text{\{O:2\}}\rightarrow \text{\{A:2\}}
\underset{\text{\{D:1\}}\rightarrow \text{\{E:1\}}\rightarrow \text{\{P:1\}}\rightarrow \text{\{B:1\}}}{ \underset{\downarrow}{ \rightarrow \text{\{C:2\}}\rightarrow \text{\{R:2\}}\rightarrow \text{\{B:1\}}}}
\]</span></p>
<p>Notice that while we traverse the tree, we increment the node corresponding to each occurring item in the current transaction. We also branch out to a new node if an occurring item reaches a node with no other neighboring node corresponding to the item. In our case, for the second transaction, {R:2} is the last matching node. So we branch out by creating a new node, namely {D:1}, and proceed with building the branch.</p>
<p>Then, we read the third transaction <strong>TID=103</strong> and continue to build the tree:</p>
<p><span class="math display">\[
\underset{\{D:1\}\rightarrow \{E:1\} \rightarrow \{P:1\} }{ \underset{\downarrow}{ \text{\{\}} \rightarrow \text{\{O:3\}}\rightarrow }}
 \text{\{A:2\}}
\underset{\text{\{D:1\}}\rightarrow \text{\{E:1\}}\rightarrow \text{\{P:1\}}\rightarrow \text{\{B:1\}}}{ \underset{\downarrow}{ \rightarrow \text{\{C:2\}}\rightarrow \text{\{R:2\}}\rightarrow \text{\{B:1\}}}}
\]</span></p>
<p>Here, we create a new branch from {O:3}.</p>
<p>We continue processing the next transactions. The <strong>FP-tree</strong> is shown in Figure <a href="machinelearning1.html#fig:fpgrowth">9.24</a>. </p>
<p><strong>Fourth</strong>, we create a <strong>conditional pattern base</strong> table column. We use the <strong>FP-tree</strong> to populate the conditional pattern base. For each of the 1-itemset, we traverse the tree to the root and build the k-itemset based on the traversed path. For example, the 1-itemset <strong>{F}</strong> forms a path to the root, namely <strong>{O, A, C, R:2}</strong>. That is a new 4-itemset. The number <strong>2</strong> in the new 4-itemset corresponds to the number in the node of the 1-itemset <strong>{F}</strong>, namely <strong>{F:2}</strong>. In another example, there are two paths for 1-itemset <strong>{E}</strong>. The first one forms a path to the root, namely <strong>{O, A, C, R, D:1}</strong>, and the second one forms another path to the root, namely <strong>{O, D:1}</strong>.</p>
<p><strong>Finally</strong>, we use the <strong>conditional pattern base</strong> column to generate our <strong>frequent patterns</strong> by intersecting the ordered itemsets and its 1-itemset for each unique item:</p>

<p><span class="math display">\[\begin{align*}
\text{A-conditional = }&amp;\text{\{A,O:4\}} \\
\text{C-conditional = }&amp;\text{\{O,C:4\},\{A,C:4\},\{O,A,C:4\}}\\
\text{R-conditional = }&amp;\text{\{O,R:4\},\{A,R:4\},\{C,R:4\},\{O,A,R:4\},\{O,C,R:4\},\{O,A,C,R:4\}}\\
\text{D-conditional = }&amp;\text{\{O,D:2\}}\\
\text{E-conditional = }&amp;\text{\{O,E:2\},\{D,E:2\}, \{O,D,E:2\}}\\
\text{P-conditional = }&amp;\text{\{O,P:2\},\{D,P:2\},\{E,P:2\},\{O,D,P:2\},\{O,E,P:2\},,\{O,D,E,P:2\}}\\
\text{B-conditional = }&amp;\text{\{O,B:2\},\{A,B:2\},\{C,B:2\},\{R,B:2\},\{O,A,B:2\},\{O,C,B:2\},\{O,R,B:2\}}\\
&amp; \text{\{A,R,B:2\},\{O,A,C,B:2\},\{O,A,R,B:2\},\{A,C,R,B:2\},\{O,C,R,B:2\}}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
&amp;\text{\{O,A,C,R,B:2\}}\\
\text{F-conditional = }&amp;\text{\{O,F:2\},\{A,F:2\},\{C,F:2\},\{R,F:2\},\{O,A,F:2\},\{O,C,F:2\},\{O,R,F:2\}}\\
&amp;\text{\{A,R,F:2\},\{O,A,C,F:2\},\{O,A,R,F:2\},\{A,C,R,F:2\},\{O,C,R,F:2\}}\\
&amp;\text{\{O,A,C,R,F:2\}}
\end{align*}\]</span>
</p>
<p>For <strong>D-conditional</strong>, <strong>E-conditional</strong>, and <strong>P-conditional</strong>, we discard any pattern having any of the following items <strong>&lt;A, B, C&gt;</strong> because the patterns will generate a frequency below the minimum.</p>
<p>Similarly, for <strong>B-conditional</strong>, we discard any pattern having <strong>&lt;D, E, P&gt;</strong>.</p>
<p><strong>Eclat Algorithm</strong> </p>
<p>The <strong>Equivalence Class Transformation (Eclat) algorithm</strong> is a Depth First Search algorithm that uses a vertically transposed data format.</p>
<p>The algorithm proceeds as follows:</p>
<p><strong>First</strong>, get the transaction list for each item. For example, for item <strong>A</strong>, we have the following list of transactions: <strong>101, 103, 104</strong>. See Figure <a href="machinelearning1.html#fig:eclat">9.25</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:eclat"></span>
<img src="eclat.png" alt="Eclat Data" width="60%" />
<p class="caption">
Figure 9.25: Eclat Data
</p>
</div>
<p><strong>Second</strong>, list the frequently occurring patterns by intersecting the transaction list of each item with the transaction list of the other items. For example, suppose our minimum support equals 2. We then have the following intersection:</p>
<p><span class="math display">\[
\begin{array}{llllll}
A \cap B &amp;= \text{\{104\}} &amp; A \cap C \cap D &amp;= \text{\{104,103\}} &amp; A \cap C \cap D \cap E &amp;= \text{\{103\}}\\
A \cap C &amp;= \text{\{101,103\}} &amp; A \cap C \cap E &amp;= \text{\{103\}} \\
A \cap D &amp;= \text{\{101,103\}} &amp; A \cap D \cap E &amp;= \text{\{103\}} \\
A \cap E &amp;= \text{\{103\}}
\end{array}
\]</span></p>
<p><strong>Third</strong>, with minimum support of 2, we reduce the list to the following:</p>
<p><span class="math display">\[
\begin{array}{llll}
A \cap C &amp;= \text{\{101,103\}} &amp; A \cap C \cap D &amp;= \text{\{104,103\}}\\
A \cap D &amp;= \text{\{101,103\}}
\end{array} 
\]</span></p>
<p>Therefore the below itemsets are more likely to occur than other associations:</p>
<p><span class="math display">\[
\text{\{A,C\}, \{A,D\}, \{A,C,D\}}
\]</span>
We leave readers to investigate variants and enhancements made to the algorithms mentioned above, particularly in computational costs such as space savings by compression.</p>
</div>
<div id="pattern-discovery" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.3</span> Pattern Discovery<a href="machinelearning1.html#pattern-discovery" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we introduce algorithms that deal with items not only for their association but also to discover patterns that have sequential characteristics. Unlike a transaction database in which we have a list of transactions identified by transaction IDs, each containing an unordered list of items, we operate on a database with sequences identified by sequence IDs, each containing a sequentially-ordered list of items. We refer to a set of sequences as <strong>sequential database</strong>). See Figure <a href="machinelearning1.html#fig:sequential">9.26</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sequential"></span>
<img src="sequential.png" alt="Sequential Data" width="60%" />
<p class="caption">
Figure 9.26: Sequential Data
</p>
</div>
<p>For example, the sequence with ID equal to 102 has 7 elements.</p>

<p><span class="math display">\[
\text{&lt;(ABC)DEF(GH)(IJ)K&gt;}\ \ \rightarrow
\]</span>
<span class="math display">\[
\underbrace{
\underbrace{\text{&lt;(ABC)}}_{\begin{array}{c}\text{1st}\\ \text{element}\\ \text{(unordered)}\end{array}}
\underbrace{
\underbrace{\text{D}}_{\begin{array}{c}\text{2nd}\\ \text{element}\end{array}}
\underbrace{\text{E}}_{\begin{array}{c}\text{3rd}\\ \text{element}\end{array}}
\underbrace{\text{F}}_{\begin{array}{c}\text{4th}\\ \text{element}\end{array}}
}_\text{(ordered)}
\underbrace{\text{(GH)}}_{\begin{array}{c}\text{5th}\\ \text{element}\\ \text{(unordered)}\end{array}}
\underbrace{\text{(IJ)}}_{\begin{array}{c}\text{6th}\\ \text{element}\\ \text{(unordered)}\end{array}}
\underbrace{\text{K&gt;}}_{\begin{array}{c}\text{7th}\\ \text{element}\\ \text{(ordered)}\end{array}}
}_\text{(ordered)} 
\]</span>
</p>
<p>Note the below alternative longer notation:</p>
<p><span class="math display">\[
\text{&lt;(ABC)DEF(GH)(IJ)K&gt;} \equiv
\text{&lt;(ABC)}\rightarrow
\text{D}\rightarrow\text{E}\rightarrow\text{F}\rightarrow\text{(GH)}\rightarrow\text{(IJ)}\rightarrow\text{K&gt;}
\]</span></p>
<p>Also, note that an unordered element produces a list of possible unordered combinations (vs ordered permutation). For example, any of the below combinations can represent the unordered element (ABC).</p>
<p><span class="math display">\[
(ABC) = \text{ (A), (B), (C), (AB), (AC), (CB), ()}
\]</span>
We also can interpret the element (ABC) to mean that a customer ordered items A, B, and C together. On the other hand, a second customer may order a subset of the pattern, e.g., items A and B together, items B and C together, or items A and C together. Then a third customer may order only item A or nothing at all.</p>
<p>So that if the element is part of a sequence like so, then the element can have any of the following sub-sequences:</p>
<p><span class="math display">\[
\underbrace{\text{&lt;D(A)E&gt;}}_\text{sub-sequence},
\underbrace{\text{&lt;D(AB)E&gt;}}_\text{sub-sequence},
\underbrace{\text{&lt;D(AC)E&gt;}}_\text{sub-sequence},
\underbrace{\text{&lt;D(CB)E&gt;}}_\text{sub-sequence},
\underbrace{\text{&lt;DE&gt;}}_\text{sub-sequence},...
\ \ \ \ \
\in \underbrace{\text{&lt;D(ABC)E&gt;}}_\text{sequence} 
\]</span>
The sub-sequence <strong>&lt; D(CB)E &gt;</strong> means that we see a pattern in which a customer made a sequence of orders in which the first order has item D followed by a second order with items C and B together, then followed by a third-order with item E.</p>
<p>The sub-sequence <strong>&lt; DE &gt;</strong> means that we see a pattern in which a customer made a sequence of orders in which the first order has item D followed by a second order with item E. there are no other orders made with any combination containing items A, B, and C together.</p>
<p>Let us now introduce a few algorithms that allow us to discover (or mine) sequential patterns:</p>
<p><strong>Generalized Sequential Patterns (GSP)</strong>  </p>
<p><strong>GSP</strong> uses <strong>Apriori algorithm</strong> to discover sequential patterns <span class="citation">(Srikant, R., &amp; Agrawal, R. <a href="bibliography.html#ref-ref378">1996</a>)</span>. The algorithm proceeds as such (assume the maximum length of a sequence of interest is N):</p>
<p><span class="math display">\[
\begin{array}{l}
F_1 = \text{\{frequent 1-sequence\}}\\
\text{For k in 2,...,N   repeat the following}\\
\ \ \ \ \ \ \ \ C_k\ \text{= candidate k-sequence from }F_{k-1}\\
\ \ \ \ \ \ \ \ \text{For i in 1,... repeat the following}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{sq = sequence[i]}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{For each k-sequence in sq, repeat the following}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{count frequency}\\
\ \ \ \ \ \ \ \ F_k\ \text{= All candidate k-sequence from } C_k \text{ with minimum support/frequency}\\
\end{array}
\]</span></p>
<p><strong>First</strong>, using Figure <a href="machinelearning1.html#fig:sequential">9.26</a>, let us generate the following <strong>length-1</strong> candidate subsequences:</p>
<p><span class="math display">\[\begin{align*}
\text{&lt;A&gt;:5},\ \text{&lt;B&gt;:5},\ \text{&lt;C&gt;:4},\ \text{&lt;D&gt;:3},\ \text{&lt;E&gt;:3},\ \text{&lt;F&gt;:3},\\ \text{&lt;G&gt;:1},\ \text{&lt;H&gt;:1},\ \text{&lt;I&gt;:1},\ \text{&lt;J&gt;:1},\ \text{&lt;K&gt;:1}
\end{align*}\]</span></p>
<p>From there, we keep subsequences that meet the minimum support (e.g. minsup = 2).</p>
<p><span class="math display">\[
\text{&lt;A&gt;:5},\ \ \text{&lt;B&gt;:5},\ \ \text{&lt;C&gt;:4},\ \ \text{&lt;D&gt;:3},\ \ \text{&lt;E&gt;:3},\ \ \text{&lt;F&gt;:3}
\]</span></p>
<p><strong>Second</strong>, let us mine for <strong>length-2</strong> sequential pattern using the list of items with minsup=2. Here, we construct a <strong>frequent item matrix</strong>. See Figure <a href="machinelearning1.html#fig:gsp">9.27</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gsp"></span>
<img src="gsp.png" alt="GSP (51 2-length sequence)" width="100%" />
<p class="caption">
Figure 9.27: GSP (51 2-length sequence)
</p>
</div>
<p>Based on the matrix, we see 17 <strong>length-2</strong> sequential patterns that meet the minsup = 2.</p>
<p><strong>Third</strong>, we proceed mining for <strong>length-3</strong> sequential patterns.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gsp3"></span>
<img src="gsp3.png" alt="GSP (6 3-length sequence)" width="45%" />
<p class="caption">
Figure 9.28: GSP (6 3-length sequence)
</p>
</div>
<p>That gives us the following <strong>length-3</strong> sequential patterns that meet the minsup = 2. A few of the patterns are listed below.</p>
<p><span class="math display">\[
\text{&lt;ABF&gt;,&lt;ACF&gt;,&lt;ADE&gt;,&lt;BCF&gt;,&lt;CDE&gt;},\ \ \cdots
\]</span>
We labeled the third column as <strong>3-item sequence</strong>, which is different from the <strong>length-3</strong> sequential pattern. For example, <strong>(AB)C</strong> is a <strong>3-item sequence</strong> but falls under <strong>length-2</strong> sequential patterns because it has only two elements, namely <strong>(AB)</strong> being an unordered element and <strong>C</strong>.</p>
<p><strong>Fourth</strong>, we continue to form all other <strong>length-k</strong> sequences until we reach a point where we do not have any other sequence that meets our minimum support of 2.</p>
<p><strong>Sequential Pattern Discovery using Equivalence classes (SPADE)</strong> </p>
<p><strong>SPADE</strong> is an alternative algorithm that uses <strong>ECLAT algorithm</strong> for its association <span class="citation">(Zaki M.J. <a href="bibliography.html#ref-ref372">2001</a>)</span>. The data is vertically-formatted for Depth First Search pattern discovery.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:spade"></span>
<img src="spade.png" alt="SPADE" width="90%" />
<p class="caption">
Figure 9.29: SPADE
</p>
</div>
<p>To illustrate the algorithm, we use Figure <a href="machinelearning1.html#fig:spade">9.29</a>.</p>
<p><strong>First</strong>, we scan the sequential database for all sequences (see table <strong>T1</strong>) to construct an element table (see table <strong>T2</strong>). For example, the first element of the first sequence corresponds to the following:</p>
<p><span class="math display">\[
\text{SID}^\text{(sequence id)} = 101,\ \ \ \ \ \ \ \
\text{EID}^\text{(element id)} = 1,\ \ \ \ \ \ \ \ \ \ 
\text{Item} = \text{A}
\]</span></p>
<p>The second element of the first sequence corresponds to the following:</p>
<p><span class="math display">\[
\text{SID}^\text{(sequence id)} = 101,\ \ \ \ \ \ \ \
\text{EID}^\text{(element id)} = 2,\ \ \ \ \ \ \ \ \ \ 
\text{Item} = \text{AB}
\]</span></p>
<p><strong>Second</strong>, we use table <strong>T2</strong> to construct our vertically-formatted <strong>length-2</strong> sequence table (see table <strong>T3</strong>). For example, item <strong>A</strong> is a <strong>length-1</strong> sequential pattern that has the following sequence ids and element ids:</p>
<p><span class="math display">\[
\text{length-1 sequence for A = }\left[
\begin{array}{ll}
101 &amp; 1\\101 &amp; 2\\102 &amp; 1\\103 &amp; 1\\ \vdots &amp; \vdots
\end{array}
\right]
\ \ \ \ \ \ \ \ \ \ \ \ 
\text{length-1 sequence for B = }\left[
\begin{array}{ll}
101 &amp; 2\\102 &amp; 1\\103 &amp; 2\\104 &amp; 1\\ \vdots &amp; \vdots
\end{array}
\right]
\]</span></p>
<p><strong>Third</strong>, we continue to use table <strong>T2</strong> to construct our next vertically-formatted <strong>length-2</strong> sequence table (see table <strong>T4</strong>). For example, item <strong>AB</strong> is a <strong>length-2</strong> sequential pattern. The first entry in the table corresponds to the first sequence with SID=101 containing item <strong>A</strong> being the first element (EID=1) in the sequence and item <strong>B</strong> being one of the combinations of the second element (<strong>AB</strong>) with EID=2 in the sequence.</p>
<p>Note that table <strong>T5</strong> is just an extension of table <strong>T4</strong> in which item <strong>BA</strong> is also a <strong>length-2</strong> sequential pattern. The entry in the table corresponds to the fifth sequential pattern with SID=105 containing item <strong>B</strong> being one of the combinations of the second element (<strong>AB</strong>) in the sequence and item <strong>A</strong> being one of the combinations of the fourth element (<strong>FA</strong>) in the sequence.</p>
<p><strong>Fourth</strong>, we construct our next vertically-formatted <strong>length-3</strong> sequence table using <strong>T2</strong>.
The pattern corresponds to the following entry:</p>
<p><span class="math display">\[
\text{ABC = }
\left[
\begin{array}{llll}
101 &amp; 1 &amp; 2 &amp; 3\\
105 &amp; 1 &amp; 2 &amp; 3
\end{array}
\right]
\ \ \ \ \ \ \ \
\text{CDE = }
\left[
\begin{array}{llll}
102 &amp; 1 &amp; 2 &amp; 3\\
104 &amp; 1 &amp; 2 &amp; 3
\end{array}
\right]
\ \ \ \ \ \ \ \ \cdots
\]</span></p>
<p>To interpret the <strong>length-3</strong> sequential pattern, we see a pattern in which a customer made a sequence of orders in which the first order has item A followed by a second order with item B and another order with item C. We see a frequency of two (one from SID=101 and the other from SID=105).</p>
<p><strong>Fifth</strong>, we continue to form all other <strong>length-k</strong> sequences until we reach a point where we do not have any other sequences that meet our minimum support of 2.</p>
<p><strong>Frequent Pattern-Projected Sequential Pattern (FreeSpan)</strong> </p>
<p><strong>FreeSpan</strong> uses a divide-and-conquer approach operating on projected subsequences based on <strong>length-k</strong> sequential patterns. These subsequences are the frequent pattern <strong>length-k projections</strong> <span class="citation">(Han J. et al. <a href="bibliography.html#ref-ref380">2000</a>)</span>.</p>
<p>To illustrate the <strong>FreeSpan</strong> algorithm, we proceed as follows:</p>
<p><strong>First</strong>, using the sequence table in Figure <a href="machinelearning1.html#fig:sequential">9.26</a>, we generate a frequent item list (or <strong>f_list</strong>) of frequent <strong>length-1</strong> sequential patterns that meet a minimum support of 2 (sorted in descending order):</p>
<p><span class="math display">\[
\text{&lt;A&gt;:5},\ \ \text{&lt;B&gt;:5},\ \ \text{&lt;C&gt;:4},\ \ \text{&lt;D&gt;:3},\ \ \text{&lt;E&gt;:3},\ \ \text{&lt;F&gt;:3}
\]</span></p>
<p><strong>Second</strong>, based on the generated <strong>length-1</strong> sequential pattern above, we construct our <strong>frequent item matrix</strong> <span class="citation">(Han J. et al. <a href="bibliography.html#ref-ref380">2000</a>)</span>. The rows and columns are ordered in descending order of support. See Figure <a href="machinelearning1.html#fig:freespan">9.30</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:freespan"></span>
<img src="freespan.png" alt="FreeSpan (1-projection)" width="65%" />
<p class="caption">
Figure 9.30: FreeSpan (1-projection)
</p>
</div>
<p>To explain, each cell in the matrix is represented by the intersection of two items and has the format (X, Y, Z) so that if the two items are <strong>A</strong> and <strong>B</strong>, then X is the number of occurrences for the sequential pattern <span class="math inline">\(\mathbf{\text{&lt;AB&gt;}}\)</span>, Y is the number of occurrences for the sequential pattern <span class="math inline">\(\mathbf{\text{&lt;BA&gt;}}\)</span>, and Z is the number of occurrence for the sequential pattern <span class="math inline">\(\mathbf{\text{&lt;(AB)&gt;}}\)</span>. For example, in Figure <a href="machinelearning1.html#fig:sequential">9.26</a>, for items <strong>A</strong> and <strong>B</strong>, the intersecting cell has (3,1,5) which comes from the following:</p>
<p>For X corresponding to <span class="math inline">\(\mathbf{\text{&lt;AB&gt;}}\)</span>:</p>
<p><span class="math display">\[
X:3\ \ \ \rightarrow 101:&lt;\underline{A}(A\underline{B})CF&gt;, 103:&lt;\underline{A}(A\underline{B})(ED)&gt;,
105:&lt;\underline{A}(A\underline{B})C(AF)&gt;
\]</span></p>
<p>For Y corresponding to <span class="math inline">\(\mathbf{\text{&lt;BA&gt;}}\)</span>:</p>
<p><span class="math display">\[
Y:1\ \ \ \rightarrow 105:&lt;A(A\underline{B})C(\underline{A}F)&gt;
\]</span></p>
<p>For Z corresponding to <span class="math inline">\(\mathbf{\text{&lt;(AB)&gt;}}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
Z:5\ \ \ {}&amp;\rightarrow 101:&lt;A(\underline{A}\underline{B})CF&gt;, 
102:&lt;(\underline{A}\underline{B}C)DEF(GH)(IJ)K&gt;, \\
\ \ \ &amp;\rightarrow 103:&lt;A(\underline{A}\underline{B})(ED)&gt;, 
104:&lt;(\underline{A}\underline{B}C)(CD)E&gt;,
105:&lt;A(\underline{A}\underline{B})C(AF)&gt;
\end{align*}\]</span></p>
<p>Finally, for the cell intersecting <strong>A</strong> (column-wise) and <strong>A</strong> (row-wise), we see the following combination:</p>
<p><span class="math display">\[
&lt;AA&gt;:3\ \ \ \rightarrow 101:&lt;\underline{A}(\underline{A}B)CF&gt;, 103:&lt;\underline{A}(\underline{A}B)(ED)&gt;,
105:&lt;\underline{A}(\underline{A}B)C(AF)&gt;
\]</span></p>
<p><strong>Third</strong>, let us find <strong>length-2</strong> sequential patterns based on the <strong>frequent item matrix</strong> (given a minsup = 2).</p>
<p><span class="math display">\[\begin{align*}
A{}&amp;:\ \ \ \ \ \text{&lt;AA&gt;:3}\\
B&amp;:\ \ \ \ \ \text{&lt;AB&gt;:3},\text{&lt;(AB)&gt;:5}\\
C&amp;:\ \ \ \ \ \text{&lt;AC&gt;:3},\text{&lt;(AC)&gt;:2},\text{&lt;BC&gt;:3},\text{&lt;(BC)&gt;:2}\\
D&amp;:\ \ \ \ \ \text{&lt;AD&gt;:3},\text{&lt;BD&gt;:3},\text{&lt;CD&gt;:2}\\
E&amp;:\ \ \ \ \ \text{&lt;AE&gt;:3},\text{&lt;BE&gt;:3},\text{&lt;CE&gt;:2}\\
F&amp;:\ \ \ \ \ \text{&lt;AF&gt;:3},\text{&lt;BF&gt;:3},\text{&lt;CF&gt;:3}\\
\end{align*}\]</span></p>
<p><strong>Fourth</strong>, we can continue with this approach for every smaller subsequences or projection with <strong>length-k</strong> sequential patterns. For example, for a <strong>length-3</strong> sequential patterns, we have:</p>
<p><span class="math display">\[
\text{&lt;ABC&gt;:2}\ \ \ \ \ \ \ \text{&lt;CDE&gt;:2}\ \ \ \ \ \cdots
\]</span></p>
<p><strong>Prefix-projected Sequential Pattern (PrefixSpan)</strong> </p>
<p><strong>PrefixSpan</strong> extends and enhances <strong>FreeSpan</strong> by reducing projections <span class="citation">(Pei J. et al. <a href="bibliography.html#ref-ref368_1">2004</a>)</span>.</p>
<p>Similar to <strong>Freespan</strong>, we use the sequence table in Figure <a href="machinelearning1.html#fig:sequential">9.26</a> to generate a list of <strong>length-1</strong> sequential patterns that meet minimum support of 2:</p>
<p><span class="math display">\[
\text{&lt;A&gt;:5},\ \ \text{&lt;B&gt;:5},\ \ \text{&lt;C&gt;:4},\ \ \text{&lt;D&gt;:3},\ \ \text{&lt;E&gt;:3},\ \ \text{&lt;F&gt;:3}
\]</span></p>
<p>From here, we construct our <strong>projection databases</strong> based on the <strong>length-1</strong> sequential patterns above. Then, we work our way from items that meet minimum support using a divide-and-conquer approach (by partitioning).</p>
<p><strong>First</strong>, we start with the initial <strong>length-1</strong> sequential patterns. For example, let us show the <strong>length-1</strong> prefix projections. We exclude G, H, I, J, and K as they do not meet the minimum support.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:prefixspan"></span>
<img src="prefixspan.png" alt="PrefixSpan (1-sequence projection)" width="90%" />
<p class="caption">
Figure 9.31: PrefixSpan (1-sequence projection)
</p>
</div>
<p>Note that sequences get pruned starting from the position of the first occurrence of the prefix to the beginning of the sequence. If the prefix is in an unordered element, the prefixed is replaced with an underscore &quot;_&quot; as a placeholder. For example, suppose our prefix is <strong>A</strong>, then we have the following three examples of an A-projection (A-suffix).</p>

<p><span class="math display">\[
\underbrace{\text{&lt;B(AC)D&gt;}}_\text{sequence} \rightarrow 
\underbrace{\text{&lt;(}\underline{}\text{C)D&gt;}}_\text{A-projection}
\ \ \ \ \ \ \ \
\underbrace{\text{&lt;B(CA)DF&gt;}}_\text{sequence} \rightarrow 
\underbrace{\text{&lt;DF&gt;}}_\text{A-projection}
\ \ \ \ \ \ \ \
\underbrace{\text{&lt;A(BC)EF&gt;}}_\text{sequence} \rightarrow 
\underbrace{\text{&lt;(BC)EF&gt;}}_\text{A-projection}
\]</span>
</p>
<p>Also, note that we highlighted the “C-projection” column in Figure <a href="machinelearning1.html#fig:prefixspan">9.31</a> for our next operation.</p>
<p><strong>Second</strong>, we now operate on <strong>2-sequential patterns</strong>. For example, based on the result above for the <strong>length-1</strong> sequence prefix patterns, let us use “C-prefix column” to generate a list of <strong>length-1</strong> sequential patterns that meet the minimum support of 2:</p>
<p><span class="math display">\[
\text{&lt;A&gt;:1},\ \ \text{&lt;B&gt;:0},\ \ \text{&lt;C&gt;:1},\ \ \text{&lt;D&gt;:2},\ \ \text{&lt;E&gt;:2},\ \ \text{&lt;F&gt;:3}
\]</span>
Therefore, we get the following minimum <strong>length-1</strong> sequential pattern from the <strong>C-projected</strong> column:</p>
<p><span class="math display">\[
\text{&lt;D&gt;:2},\ \ \text{&lt;E&gt;:2},\ \ \text{&lt;F&gt;:3}
\]</span></p>
<p>That corresponds to the following <strong>length-2</strong> sequential patterns:</p>
<p><span class="math display">\[
\text{&lt;CD&gt;:2}\ \ \ \ \ \
\text{&lt;CE&gt;:2}\ \ \ \ \ \
\text{&lt;CF&gt;:3}\ \ \ \ \ \
\]</span>
We can do the same for the other projections, e.g., <strong>A-projection</strong> and <strong>B-projection</strong>, to get the rest o the <strong>length-2</strong> sequential patterns.</p>
<p>To generate a <strong>length-2</strong> sequence projection from the <strong>C-projected</strong> column using the <strong>length-1</strong> sequential patterns, we get the following:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:prefixspan1"></span>
<img src="prefixspan1.png" alt="PrefixSpan (2-sequence projection)" width="50%" />
<p class="caption">
Figure 9.32: PrefixSpan (2-sequence projection)
</p>
</div>
<p><strong>Third</strong>, for a <strong>length-3</strong> sequential pattern, we can see in Figure <a href="machinelearning1.html#fig:prefixspan1">9.32</a> that given the <strong>CD-Projection</strong>, we have a sequential pattern that meets the minimum support because they both occur in the second and fourth sequences:</p>
<p><span class="math display">\[
\text{&lt;CDE&gt;:2}
\]</span></p>
<p>We can do the same for the other projections, e.g., <strong>AB-projection</strong> and <strong>AC-projection</strong>, to get the rest of the <strong>length-3</strong> sequential patterns. For example:</p>
<p><span class="math display">\[
\text{&lt;ABC&gt;:2}
\]</span></p>
<p><strong>Lastly</strong>, we can continue with the other <strong>length-k</strong> sequential patterns to get the complete list of sequential patterns.</p>
<p>Let us now show an implementation of <strong>Association and Pattern Discovery</strong> using three libraries. The first library is called <strong>arules</strong> and it provides functions to work on <strong>association</strong> and <strong>association rules</strong>. The second library is called <strong>arulesSequences</strong>, which allows operating on <strong>sequences</strong>.</p>
<p>To demonstrate the <strong>association</strong> and <strong>apriori</strong> algorithm, let us define our dataset and convert it into a transaction format. First, let us form a matrix with rows corresponding to a list of transaction orders and columns corresponding to a list of items.</p>

<div class="sourceCode" id="cb991"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb991-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;arules&quot;</span>)</a>
<a class="sourceLine" id="cb991-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb991-3" data-line-number="3">TX =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb991-4" data-line-number="4">trans =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, TX)</a>
<a class="sourceLine" id="cb991-5" data-line-number="5">alpha =<span class="st">  </span><span class="kw">toupper</span>(letters) </a>
<a class="sourceLine" id="cb991-6" data-line-number="6">orders =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>TX, <span class="dt">ncol=</span><span class="kw">length</span>(alpha), <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb991-7" data-line-number="7"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>TX) {</a>
<a class="sourceLine" id="cb991-8" data-line-number="8">  orders[i,] =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n=</span><span class="dv">26</span>, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span><span class="fl">0.15</span>)</a>
<a class="sourceLine" id="cb991-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb991-10" data-line-number="10"><span class="kw">colnames</span>(orders) =<span class="st"> </span>alpha</a>
<a class="sourceLine" id="cb991-11" data-line-number="11">orders_trans =<span class="st"> </span><span class="kw">as</span>(orders, <span class="st">&quot;transactions&quot;</span>)</a></code></pre></div>

<p>We use the <strong>inspect(.)</strong> function to inspect the content of our transaction orders.</p>

<div class="sourceCode" id="cb992"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb992-1" data-line-number="1"><span class="kw">inspect</span>(<span class="kw">head</span>(orders_trans))</a></code></pre></div>
<pre><code>##     items          
## [1] {Q,X,Y}        
## [2] {E,G,H,L,O,Q,R}
## [3] {A,M,O,T}      
## [4] {Z}            
## [5] {J,Q,V,W}      
## [6] {M,N,X}</code></pre>

<p>We use the <strong>summary(.)</strong> function for additional information about our transactional database. The function provides the most frequent 1-itemset, including quartile statistics.</p>

<div class="sourceCode" id="cb994"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb994-1" data-line-number="1"><span class="kw">summary</span>(orders_trans)</a></code></pre></div>
<pre><code>## transactions as itemMatrix in sparse format with
##  100 rows (elements/itemsets/transactions) and
##  26 columns (items) and a density of 0.1438 
## 
## most frequent items:
##       S       Q       R       U       M (Other) 
##      21      19      19      19      18     278 
## 
## element (itemset/transaction) length distribution:
## sizes
##  1  2  3  4  5  6  7  8 
##  8 18 21 23 11 14  4  1 
## 
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.00    2.00    4.00    3.74    5.00    8.00 
## 
## includes extended item information - examples:
##   labels
## 1      A
## 2      B
## 3      C</code></pre>

<p>Finally, if we need to view our data structure, we can use the <strong>str(.)</strong> function.</p>

<div class="sourceCode" id="cb996"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb996-1" data-line-number="1"><span class="kw">str</span>(orders_trans, <span class="dt">strict.width=</span><span class="st">&quot;wrap&quot;</span>)</a></code></pre></div>
<pre><code>## Formal class &#39;transactions&#39; [package &quot;arules&quot;] with 3 slots
## ..@ data :Formal class &#39;ngCMatrix&#39; [package &quot;Matrix&quot;] with 5 slots
## .. .. ..@ i : int [1:374] 16 23 24 4 6 7 11 14 16 17 ...
## .. .. ..@ p : int [1:101] 0 3 10 14 15 19 22 29 31 36 ...
## .. .. ..@ Dim : int [1:2] 26 100
## .. .. ..@ Dimnames:List of 2
## .. .. .. ..$ : NULL
## .. .. .. ..$ : NULL
## .. .. ..@ factors : list()
## ..@ itemInfo :&#39;data.frame&#39;: 26 obs. of 1 variable:
## .. ..$ labels: chr [1:26] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ...
## ..@ itemsetInfo:&#39;data.frame&#39;: 0 obs. of 0 variables</code></pre>

<p>To implement the <strong>Apriori algorithm</strong> and limit patterns, we use the <strong>apriori(.)</strong> function.</p>

<div class="sourceCode" id="cb998"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb998-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb998-2" data-line-number="2">apriori.output &lt;-<span class="st"> </span><span class="kw">apriori</span>(orders_trans, </a>
<a class="sourceLine" id="cb998-3" data-line-number="3">                    <span class="dt">parameter =</span> <span class="kw">list</span>(<span class="dt">supp=</span><span class="fl">0.001</span>, <span class="dt">conf=</span><span class="fl">0.8</span>,<span class="dt">maxlen=</span><span class="dv">10</span>))</a></code></pre></div>
<pre><code>## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport
##         0.8    0.1    1 none FALSE            TRUE
##  maxtime support minlen maxlen target  ext
##        5   0.001      1     10  rules TRUE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 0 
## 
## set item appearances ...[0 item(s)] done [0.00s].
## set transactions ...[26 item(s), 100 transaction(s)] done [0.00s].
## sorting and recoding items ... [26 item(s)] done [0.00s].
## creating transaction tree ... done [0.00s].
## checking subsets of size 1 2 3 4 5 6 7 8 done [0.00s].
## writing ... [3442 rule(s)] done [0.00s].
## creating S4 object  ... done [0.00s].</code></pre>

<p>We use the <strong>inspect(.)</strong> function one more time to inspect the result of apriori, displaying four measurements (e.g., support, confidence, coverage, and lift).</p>

<div class="sourceCode" id="cb1000"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1000-1" data-line-number="1"><span class="kw">inspect</span>(<span class="kw">head</span>(apriori.output))</a></code></pre></div>
<pre><code>##     lhs      rhs support confidence coverage lift  
## [1] {C,G} =&gt; {J} 0.01    1          0.01      7.692
## [2] {G,J} =&gt; {C} 0.01    1          0.01     12.500
## [3] {C,D} =&gt; {N} 0.01    1          0.01      6.667
## [4] {C,D} =&gt; {S} 0.01    1          0.01      4.762
## [5] {C,I} =&gt; {X} 0.01    1          0.01      7.692
## [6] {I,X} =&gt; {C} 0.01    1          0.01     12.500
##     count
## [1] 1    
## [2] 1    
## [3] 1    
## [4] 1    
## [5] 1    
## [6] 1</code></pre>

<p>If we want to limit the result only to a few items, we can use the following:</p>

<div class="sourceCode" id="cb1002"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1002-1" data-line-number="1">apriori.output &lt;-<span class="st"> </span><span class="kw">apriori</span>(orders_trans, <span class="dt">parameter =</span> </a>
<a class="sourceLine" id="cb1002-2" data-line-number="2">                          <span class="kw">list</span>(<span class="dt">supp=</span><span class="fl">0.001</span>, <span class="dt">conf=</span><span class="fl">0.8</span>,<span class="dt">maxlen=</span><span class="dv">10</span>),</a>
<a class="sourceLine" id="cb1002-3" data-line-number="3">                          <span class="dt">appearance=</span><span class="kw">list</span>(<span class="dt">lhs=</span><span class="kw">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;B&quot;</span>,<span class="st">&quot;C&quot;</span>)))</a></code></pre></div>
<pre><code>## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport
##         0.8    0.1    1 none FALSE            TRUE
##  maxtime support minlen maxlen target  ext
##        5   0.001      1     10  rules TRUE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 0 
## 
## set item appearances ...[3 item(s)] done [0.00s].
## set transactions ...[26 item(s), 100 transaction(s)] done [0.00s].
## sorting and recoding items ... [26 item(s)] done [0.00s].
## creating transaction tree ... done [0.00s].
## checking subsets of size 1 2 3 done [0.00s].
## writing ... [3 rule(s)] done [0.00s].
## creating S4 object  ... done [0.00s].</code></pre>
<div class="sourceCode" id="cb1004"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1004-1" data-line-number="1"><span class="kw">inspect</span>(<span class="kw">head</span>(apriori.output))</a></code></pre></div>
<pre><code>##     lhs      rhs support confidence coverage lift 
## [1] {B,C} =&gt; {X} 0.01    1          0.01     7.692
## [2] {B,C} =&gt; {M} 0.01    1          0.01     5.556
## [3] {A,B} =&gt; {P} 0.02    1          0.02     7.143
##     count
## [1] 1    
## [2] 1    
## [3] 2</code></pre>

<p>For sequences, let us demonstrate the use of the <strong>SPADE</strong> algorithm. First, let us construct our sequence database using Figure <a href="machinelearning1.html#fig:sequential">9.26</a>.</p>

<div class="sourceCode" id="cb1006"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1006-1" data-line-number="1"><span class="kw">library</span>(arulesSequences)</a>
<a class="sourceLine" id="cb1006-2" data-line-number="2">orders =<span class="st"> </span><span class="kw">list</span>(<span class="kw">c</span>(<span class="st">&quot;A&quot;</span>),<span class="kw">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;B&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;C&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;F&quot;</span>), </a>
<a class="sourceLine" id="cb1006-3" data-line-number="3">               <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;C&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;D&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;E&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;F&quot;</span>), </a>
<a class="sourceLine" id="cb1006-4" data-line-number="4">               <span class="kw">c</span>(<span class="st">&quot;G&quot;</span>,<span class="st">&quot;H&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;I&quot;</span>, <span class="st">&quot;J&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;K&quot;</span>),</a>
<a class="sourceLine" id="cb1006-5" data-line-number="5">               <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;D&quot;</span>, <span class="st">&quot;E&quot;</span>),</a>
<a class="sourceLine" id="cb1006-6" data-line-number="6">               <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;C&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;C&quot;</span>, <span class="st">&quot;D&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;E&quot;</span>),</a>
<a class="sourceLine" id="cb1006-7" data-line-number="7">               <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;C&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;F&quot;</span>))</a>
<a class="sourceLine" id="cb1006-8" data-line-number="8"><span class="kw">names</span>(orders) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;T&quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">21</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1006-9" data-line-number="9">trans.seq =<span class="st"> </span><span class="kw">as</span>(orders, <span class="st">&quot;transactions&quot;</span>)</a>
<a class="sourceLine" id="cb1006-10" data-line-number="10"><span class="kw">transactionInfo</span>(trans.seq)<span class="op">$</span>sequenceID =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,</a>
<a class="sourceLine" id="cb1006-11" data-line-number="11">                                          <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb1006-12" data-line-number="12"><span class="kw">transactionInfo</span>(trans.seq)<span class="op">$</span>eventID =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">1</span>,</a>
<a class="sourceLine" id="cb1006-13" data-line-number="13">                                       <span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb1006-14" data-line-number="14">trans.seq</a></code></pre></div>
<pre><code>## transactions in sparse format with
##  21 transactions (rows) and
##  11 items (columns)</code></pre>

<p>Let us then inspect our sequential data. This should match table <strong>T2</strong> in Figure <a href="machinelearning1.html#fig:spade">9.29</a>.</p>

<div class="sourceCode" id="cb1008"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1008-1" data-line-number="1"><span class="kw">inspect</span>(trans.seq)</a></code></pre></div>
<pre><code>##      items   transactionID sequenceID eventID
## [1]  {A}     T1            1          1      
## [2]  {A,B}   T2            1          2      
## [3]  {C}     T3            1          3      
## [4]  {F}     T4            1          4      
## [5]  {A,B,C} T5            2          1      
## [6]  {D}     T6            2          2      
## [7]  {E}     T7            2          3      
## [8]  {F}     T8            2          4      
## [9]  {G,H}   T9            2          5      
## [10] {I,J}   T10           2          6      
## [11] {K}     T11           2          7      
## [12] {A}     T12           3          1      
## [13] {A,B}   T13           3          2      
## [14] {D,E}   T14           3          3      
## [15] {A,B,C} T15           4          1      
## [16] {C,D}   T16           4          2      
## [17] {E}     T17           4          3      
## [18] {A}     T18           5          1      
## [19] {A,B}   T19           5          2      
## [20] {C}     T20           5          3      
## [21] {A,F}   T21           5          4</code></pre>

<p>Finally, we use <strong>cspade(.)</strong> function to demonstrate <strong>SPADE</strong> algorithm. We mine sequential patterns that meet our minimum support (in this case, we are using the relative support of 0.40).</p>
<p>For a <strong>length-2</strong> sequential pattern, we obtain the following:</p>

<div class="sourceCode" id="cb1010"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1010-1" data-line-number="1">spade.output =<span class="st"> </span><span class="kw">cspade</span>(trans.seq,<span class="dt">parameter =</span> </a>
<a class="sourceLine" id="cb1010-2" data-line-number="2">                    <span class="kw">list</span>(<span class="dt">support =</span> <span class="fl">0.40</span>),</a>
<a class="sourceLine" id="cb1010-3" data-line-number="3">                    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">verbose =</span> <span class="ot">FALSE</span>))</a></code></pre></div>

<p>Let us review how many sequential patterns have the support of 1,2, and 3:</p>

<div class="sourceCode" id="cb1011"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1011-1" data-line-number="1">supports  =<span class="st"> </span><span class="kw">size</span>(spade.output)</a>
<a class="sourceLine" id="cb1011-2" data-line-number="2"><span class="kw">c</span>(<span class="st">&quot;length-1&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(supports <span class="op">==</span><span class="st"> </span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb1011-3" data-line-number="3">  <span class="st">&quot;length-2&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(supports <span class="op">==</span><span class="st"> </span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb1011-4" data-line-number="4">  <span class="st">&quot;length-3&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(supports <span class="op">==</span><span class="st"> </span><span class="dv">3</span>))</a></code></pre></div>
<pre><code>## length-1 length-2 length-3 
##       10       25       16</code></pre>

<p>For a <strong>length-1</strong> sequential pattern, we can obtain by running the following:</p>

<div class="sourceCode" id="cb1013"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1013-1" data-line-number="1">spade.output =<span class="st"> </span><span class="kw">cspade</span>(trans.seq,<span class="dt">parameter =</span> </a>
<a class="sourceLine" id="cb1013-2" data-line-number="2">                    <span class="kw">list</span>(<span class="dt">support =</span> <span class="fl">0.40</span>, <span class="dt">maxsize=</span><span class="dv">1</span>, <span class="dt">maxlen=</span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb1013-3" data-line-number="3">                    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">verbose =</span> <span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb1013-4" data-line-number="4"><span class="kw">as</span>(spade.output, <span class="st">&quot;data.frame&quot;</span>)</a></code></pre></div>
<pre><code>##   sequence support
## 1    &lt;{A}&gt;     1.0
## 2    &lt;{B}&gt;     1.0
## 3    &lt;{C}&gt;     0.8
## 4    &lt;{D}&gt;     0.6
## 5    &lt;{E}&gt;     0.6
## 6    &lt;{F}&gt;     0.6</code></pre>

<p>For a <strong>length-2</strong> sequential pattern, we obtain the following list using the support:</p>

<div class="sourceCode" id="cb1015"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1015-1" data-line-number="1">spade.output =<span class="st"> </span><span class="kw">cspade</span>(trans.seq,<span class="dt">parameter =</span> </a>
<a class="sourceLine" id="cb1015-2" data-line-number="2">                    <span class="kw">list</span>(<span class="dt">support =</span> <span class="fl">0.40</span>),</a>
<a class="sourceLine" id="cb1015-3" data-line-number="3">                    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">verbose =</span> <span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb1015-4" data-line-number="4">sub.spade =<span class="st"> </span><span class="kw">subset</span>(spade.output, supports <span class="op">==</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1015-5" data-line-number="5"><span class="kw">head</span>(<span class="kw">as</span>(sub.spade , <span class="st">&quot;data.frame&quot;</span>), <span class="dt">n =</span> <span class="dv">15</span>) <span class="co"># display only the first 15</span></a></code></pre></div>
<pre><code>##         sequence support
## 7      &lt;{A},{F}&gt;     0.6
## 8      &lt;{B},{F}&gt;     0.6
## 9      &lt;{C},{F}&gt;     0.6
## 16   &lt;{A,B},{F}&gt;     0.6
## 20     &lt;{A},{E}&gt;     0.6
## 21     &lt;{B},{E}&gt;     0.6
## 22     &lt;{C},{E}&gt;     0.4
## 23     &lt;{D},{E}&gt;     0.4
## 31   &lt;{B,C},{E}&gt;     0.4
## 32   &lt;{A,C},{E}&gt;     0.4
## 33 &lt;{A,B,C},{E}&gt;     0.4
## 34   &lt;{A,B},{E}&gt;     0.6
## 35     &lt;{A},{D}&gt;     0.6
## 36     &lt;{B},{D}&gt;     0.6
## 37     &lt;{C},{D}&gt;     0.4</code></pre>

<p>For a <strong>length-3</strong> sequential pattern, we obtain the following:</p>

<div class="sourceCode" id="cb1017"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1017-1" data-line-number="1">spade.output =<span class="st"> </span><span class="kw">cspade</span>(trans.seq,<span class="dt">parameter =</span> </a>
<a class="sourceLine" id="cb1017-2" data-line-number="2">                    <span class="kw">list</span>(<span class="dt">support =</span> <span class="fl">0.40</span>, <span class="dt">maxsize =</span> <span class="dv">3</span>, <span class="dt">maxlen =</span> <span class="dv">3</span>),</a>
<a class="sourceLine" id="cb1017-3" data-line-number="3">                    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">verbose =</span> <span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb1017-4" data-line-number="4">sub.spade =<span class="st"> </span><span class="kw">subset</span>(spade.output, <span class="kw">size</span>(spade.output) <span class="op">==</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1017-5" data-line-number="5"><span class="kw">as</span>(sub.spade , <span class="st">&quot;data.frame&quot;</span>)</a></code></pre></div>
<pre><code>##             sequence support
## 10     &lt;{B},{C},{F}&gt;     0.4
## 11     &lt;{A},{C},{F}&gt;     0.4
## 12   &lt;{A,B},{C},{F}&gt;     0.4
## 14     &lt;{A},{B},{F}&gt;     0.4
## 15   &lt;{A},{A,B},{F}&gt;     0.4
## 16     &lt;{A},{A},{F}&gt;     0.4
## 21     &lt;{C},{D},{E}&gt;     0.4
## 22     &lt;{B},{D},{E}&gt;     0.4
## 23     &lt;{A},{D},{E}&gt;     0.4
## 24   &lt;{B,C},{D},{E}&gt;     0.4
## 25   &lt;{A,C},{D},{E}&gt;     0.4
## 26 &lt;{A,B,C},{D},{E}&gt;     0.4
## 27   &lt;{A,B},{D},{E}&gt;     0.4
## 44     &lt;{A},{B},{C}&gt;     0.4
## 45   &lt;{A},{A,B},{C}&gt;     0.4
## 46     &lt;{A},{A},{C}&gt;     0.4</code></pre>

<p>Apart from <strong>sequential patterns</strong>, part of mining data is to mine <strong>graph patterns</strong>. Here, we leave readers to investigate <strong>Graph pattern mining</strong>.</p>
</div>
<div id="null-invariance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.4</span> Null Invariance <a href="machinelearning1.html#null-invariance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, let us extend the concept of <strong>Association rules</strong> as discussed in a previous section and introduce <strong>Interestingness measures</strong>, which provide the strength of association of items. </p>
<p>Suppose we sell items <strong>A</strong>, <strong>B</strong>, and <strong>C</strong>. While we know that customers may order any combination of the three items, we are interested in the association only between item <strong>A</strong> and item <strong>B</strong>. For example, it may help to know the possibility that when customers order item <strong>A</strong>, they also order item <strong>B</strong>. Alternatively, perhaps, customers are interested only in item <strong>C</strong> but not both items <strong>A</strong> and <strong>B</strong> - these orders without items <strong>A</strong> and <strong>B</strong> are called the <strong>null transactions</strong> with respect to <strong>A</strong> and <strong>B</strong>. To illustrate further, let us use Figure <a href="machinelearning1.html#fig:nulltrans">9.33</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nulltrans"></span>
<img src="nulltrans.png" alt="Datasets with Null Transactions" width="95%" />
<p class="caption">
Figure 9.33: Datasets with Null Transactions
</p>
</div>
<p>Analyzing the orders in table <strong>T1</strong>, we see that there are orders containing item <strong>A</strong> and item <strong>B</strong>, making 850 orders. There are orders containing only item <strong>A</strong> but not <strong>B</strong> making 965 orders. Finally, there are orders containing only item <strong>B</strong> but not <strong>A</strong>, making 750. Moreover, some orders do not contain both items <strong>A</strong> and <strong>B</strong> - the <strong>null transactions</strong> - making a total of 80.</p>
<p>Let us compute for the <strong>Support</strong> and <strong>Confidence</strong> using <strong>T1</strong>.</p>

<p><span class="math display">\[
\begin{array}{lll}
S(B) = \frac{1600}{2645} = 0.6049  &amp;
S(A \rightarrow B) = \frac{850}{2645} = 0.3214 &amp;
C(A \rightarrow B) = \frac{850}{1815} = 0.4683 \\
&amp; S(\overline{A} \rightarrow B) = \frac{750}{2645} = 0.2836 &amp;
C(\overline{A} \rightarrow B) = \frac{750}{830} = 0.9036
\end{array}
\]</span>
</p>
<p>The above measures show that there is a confidence level of 46.83% for item <strong>B</strong> being ordered along with item <strong>A</strong>, with the association having the support of 32.14%. However, we see a higher confidence level of 90.36% for item <strong>B</strong> being on its own without item <strong>A</strong>.</p>
<p>The strength of association can also be measured using the <strong>Lift</strong> measure in which a value less than one reflects a negative correlation; otherwise, we see a positive correlation. For example, below, we see a <strong>Lift</strong> value less than one; thus, we see a negative correlation.</p>

<p><span class="math display">\[
L(A,B) = \frac{C(A \rightarrow B)}{S(B)} = \frac{850/1815}{1600/2645} = 0.7742
\]</span>
</p>
<p>Another measure of association is the <strong>Chi-squared </strong> <span class="math inline">\((X^2)\)</span>. Given the corresponding expected values, a <strong>Chi-squared</strong> value equal to zero means no correlations between items; otherwise, there is a positive or negative correlation depending on the sign of the value. </p>
<p>Using the <strong>expected values</strong>:</p>

<p><span class="math display">\[\begin{align*}
\mathbb{E}(A,B) {}&amp;= \frac{A \times B}{\text{total tx}} = \frac{1815 \times 1600}{2645} = 1097.921\\
\mathbb{E}(\overline{A},B) &amp;= \frac{ \overline{A} \times B}{\text{total tx}} = \frac{830 \times 1600}{2645} = 502.0794\\
\mathbb{E}(A,\overline{B}) &amp;= \frac{ A \times \overline{B}}{\text{total tx}} = \frac{1815 \times 1045}{2645} = 717.0794\\
\mathbb{E}(\overline{A},\overline{B}) &amp;= \frac{\overline{A} \times \overline{B}}{\text{total tx}} = \frac{830 \times 1045}{2645} = 327.9206\\
\end{align*}\]</span>
</p>
<p>we compute for the <span class="math inline">\(X^2\)</span> like so:</p>

<p><span class="math display">\[\begin{align*}
X^2 {}&amp;= \sum_{i,j} \frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\
&amp;= 
\frac{(850 - 1097.921)^2}{1097.921} +
\frac{(750 - 502.0794)^2}{502.0794} +
\frac{(965 - 717.0794)^2}{717.0794} +
\frac{(80 - 327.9206)^2}{327.9206}\\
&amp;= 451.5558
\end{align*}\]</span>
</p>
<p>In terms of measuring <strong>similarity</strong> and <strong>distance</strong>, <strong>Interestingness</strong> of association can be measured using <strong>Cosine Similarity</strong>, <strong>Jaccard Coefficient</strong>, <strong>Jaccard Distance</strong>, <strong>Gini Index</strong>, and many others. For example, Figure <a href="machinelearning1.html#fig:interestingness">9.34</a> shows a partial list of <strong>Interestingness</strong> measures citing Tan P. et al. <span class="citation">(<a href="bibliography.html#ref-ref368_2">2002</a>)</span> and Han J. et al. <span class="citation">(<a href="bibliography.html#ref-ref370_2">2002</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:interestingness"></span>
<img src="interestingness.png" alt="Interestingness Measures" width="100%" />
<p class="caption">
Figure 9.34: Interestingness Measures
</p>
</div>
<p>Notice that there are 10000 <strong>null transactions</strong> in table <strong>T2</strong> compared to <strong>T1</strong> which has only 80 <strong>null transactions</strong>; yet, the computed values do not change (hence invariant) for <strong>Confidence</strong>, <strong>Kulczynski</strong>, <strong>Jaccard Coefficient/Distance</strong>, <strong>Cosine Similarity</strong>, and <strong>Imbalance Ratio</strong> measures.</p>
<p><strong>Null Invariance (NI)</strong> is a binary property of a given <strong>Interestingness</strong> measure that describes the effect of a change in the number of null transactions. For example, it helps to know if the value of our measure changes as the number of customer orders not containing items <strong>A</strong> and <strong>B</strong> increases. Here, we introduce the term <strong>Interestingness</strong> measure, a ranking criterion to determine the strength of association of items. Many <strong>Interestingness</strong> measures are used to rank the strength of association while also classified based on being <strong>Null-Invariant or not</strong>. Three of which are discussed in the previous section, namely <strong>Support</strong>, <strong>Confidence</strong>, and <strong>Lift</strong>. Additionally, the table in Figure <a href="machinelearning1.html#fig:interestingness">9.34</a> lists a few additional measures (J. Han et al. 2004; Pang-Ning Tan et al. 2002).</p>
<p>We only show three cases (tables <strong>T1</strong>, <strong>T2</strong>, <strong>T3</strong>); however, there are situations in which the number of transactions with respect to the combinations <span class="math inline">\((A,B), (\overline{A},B), (A,\overline{B}),(\overline{A},\overline{B})\)</span> vary across many cases. For that reason, we see disagreements in measures.</p>
<p>One measure that may help is the <strong>Imbalance Ratio (IR)</strong>. Along with <strong>Kulczynski</strong>, the <strong>Imbalance ratio (IR)</strong> determines both neutrality and imbalance of certain different datasets (J. Han 2004). An <strong>IR</strong> value closer to one indicates a strong imbalance. Moreover, a <strong>Kulczynski</strong> value of 0.5 consistent across multiple datasets indicates neutrality. Comparing tables <strong>T1</strong>, <strong>T2</strong>, <strong>T3</strong>, we see that they are all neutral based on the <strong>Kulczynski</strong> measure; however, only table <strong>T3</strong> is balanced based on the <strong>IR</strong> measure.  </p>
<p>For illustration, let us show a crude implementation of the measures listed in Figure <a href="machinelearning1.html#fig:interestingness">9.34</a>.</p>
<p>For reference, we provide the following probabilities for table <strong>T1</strong>:</p>

<p><span class="math display">\[
\begin{array}{l}
P(A,B) = \frac{850}{2645} = 0.32136 \\
P(\overline{A},B) = \frac{750}{2645} = 0.28355 \\
P(A,\overline{B}) = \frac{965}{2645} = 0.36484 \\
P(\overline{A},\overline{B}) = \frac{80}{2645} = 0.03025 
\end{array}
\ \ \ \ \ \ \ \ \ \ 
\begin{array}{l}
P(A) = \frac{1815}{2645} = 0.68620 \\
P(\overline{A}) = \frac{830}{2645} = 0.31380 \\
P(B) = \frac{1600}{2645}= 0.60491 \\
P(\overline{B}) = \frac{1045}{2645} = 0.39509
\end{array}
\]</span>
</p>
<p>Let us also compute for conditional probabilities (based on chain rule):</p>

<p><span class="math display">\[
\begin{array}{l}
P(A|B) = \frac{P(A,B)} {P(B)} = \frac{0.32136}{0.60491} = 0.53125\\
P(\overline{A}|B) = \frac{P(\overline{A},B)} {P(B)} = \frac{0.28355}{0.60491} = 0.46875\\
P(A|\overline{B}) = \frac{P(A,\overline{B})} {P(\overline{B})} = \frac{0.36484}{0.39509} = 0.92344\\
P(\overline{A}|\overline{B}) = \frac{P(\overline{A},\overline{B})} {P(\overline{B})} = \frac{0.03025}{0.39509} = 0.07656\\
\end{array}
\ \ \ \ \ \ \
\begin{array}{l}
P(B|A) = \frac{P(B,A)} {P(A)} = \frac{0.32136}{0.68620} = 0.46832\\
P(\overline{B}|A)  = \frac{P(\overline{B},A)} {P(A)} =  \frac{0.36484}{0.68620} = 0.53168\\
P(B|\overline{A})  = \frac{P(B,\overline{A})} {P(\overline{A})} =  \frac{0.28355}{0.31380} = 0.90360\\
P(\overline{B}|\overline{A})  = \frac{P(\overline{B},\overline{A})} {P(\overline{A})} =  \frac{0.03025}{0.31380} = 0.09640\\
\end{array}
\]</span>
</p>
<p>Here, the sum of the following probabilities should equal one:</p>

<p><span class="math display">\[\begin{align*}
1 {}&amp;= P(A,B) + P(\overline{A}, B) + P(A, \overline{B}) + P(\overline{A},\overline{B})\\
1 &amp;= P(A) + P(\overline{A})\\
1 &amp;= P(B) + P(\overline{B})\\
1 &amp;= P(A|B) + P(\overline{A}|B)\\
1 &amp;= P(A|\overline{B}) + P(\overline{A}|\overline{B})\\
1 &amp;= P(B|A) + P(\overline{B}|A)\\
1 &amp;= P(B|\overline{A}) + P(\overline{B}|\overline{A})\\
\end{align*}\]</span>
</p>
<p><strong>First</strong>, we construct the three matrices and define a few helper functions:</p>

<div class="sourceCode" id="cb1019"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1019-1" data-line-number="1">T1 =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">850</span>, <span class="dv">750</span>, <span class="dv">965</span>, <span class="dv">80</span>), <span class="dt">nrow=</span><span class="dv">2</span>, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1019-2" data-line-number="2">T2 =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">850</span>, <span class="dv">750</span>, <span class="dv">965</span>, <span class="dv">10000</span>), <span class="dt">nrow=</span><span class="dv">2</span>, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1019-3" data-line-number="3">T3 =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">850</span>, <span class="dv">850</span>, <span class="dv">850</span>, <span class="dv">10000</span>), <span class="dt">nrow=</span><span class="dv">2</span>, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1019-4" data-line-number="4">A.B   &lt;-<span class="st"> </span><span class="cf">function</span>(D) { D[<span class="dv">1</span>,<span class="dv">1</span>] }              <span class="co"># (A,B)</span></a>
<a class="sourceLine" id="cb1019-5" data-line-number="5">A_.B  &lt;-<span class="st"> </span><span class="cf">function</span>(D) { D[<span class="dv">1</span>,<span class="dv">2</span>] }              <span class="co"># (not A, B)</span></a>
<a class="sourceLine" id="cb1019-6" data-line-number="6">A.B_  &lt;-<span class="st"> </span><span class="cf">function</span>(D) { D[<span class="dv">2</span>,<span class="dv">1</span>] }              <span class="co"># (A, not B)</span></a>
<a class="sourceLine" id="cb1019-7" data-line-number="7">A_.B_ &lt;-<span class="st"> </span><span class="cf">function</span>(D) { D[<span class="dv">2</span>,<span class="dv">2</span>] }              <span class="co"># (not A, not B)</span></a>
<a class="sourceLine" id="cb1019-8" data-line-number="8">A     &lt;-<span class="st"> </span><span class="cf">function</span>(D) { <span class="kw">A.B</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A.B_</span>(D) }    <span class="co"># (A)</span></a>
<a class="sourceLine" id="cb1019-9" data-line-number="9">B     &lt;-<span class="st"> </span><span class="cf">function</span>(D) { <span class="kw">A_.B</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A.B</span>(D) }    <span class="co"># (B)</span></a>
<a class="sourceLine" id="cb1019-10" data-line-number="10">A_    &lt;-<span class="st"> </span><span class="cf">function</span>(D) { <span class="kw">A_.B</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A_.B_</span>(D) }  <span class="co"># (not A)</span></a>
<a class="sourceLine" id="cb1019-11" data-line-number="11">B_    &lt;-<span class="st"> </span><span class="cf">function</span>(D) { <span class="kw">A.B_</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A_.B_</span>(D) }  <span class="co"># (not B)</span></a>
<a class="sourceLine" id="cb1019-12" data-line-number="12">E &lt;-<span class="st"> </span><span class="cf">function</span>(D, X, Y) { <span class="kw">X</span>(D) <span class="op">*</span><span class="st"> </span><span class="kw">Y</span>(D) <span class="op">/</span><span class="st"> </span><span class="kw">Tot.Tx</span>(D) } <span class="co"># expectation</span></a>
<a class="sourceLine" id="cb1019-13" data-line-number="13">P &lt;-<span class="st"> </span><span class="cf">function</span>(D, X) { <span class="kw">X</span>(D) <span class="op">/</span><span class="st"> </span><span class="kw">Tot.Tx</span>(D) } <span class="co"># probability</span></a>
<a class="sourceLine" id="cb1019-14" data-line-number="14"> <span class="co"># conditional probability</span></a>
<a class="sourceLine" id="cb1019-15" data-line-number="15">P.cond &lt;-<span class="st"> </span><span class="cf">function</span>(D, X, Y) { <span class="kw">P</span>(D, X) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D, Y) }</a>
<a class="sourceLine" id="cb1019-16" data-line-number="16">Tot.Tx &lt;-<span class="st"> </span><span class="cf">function</span>(D) {  <span class="kw">A.B</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A_.B</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A.B_</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A_.B_</span>(D) }</a></code></pre></div>

<p><strong>Second</strong>, we define functions for the <strong>Interestingness</strong> measures:</p>

<div class="sourceCode" id="cb1020"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1020-1" data-line-number="1">support &lt;-<span class="st"> </span><span class="cf">function</span>(D, X) {  <span class="kw">X</span>(D) <span class="op">/</span><span class="st"> </span><span class="kw">Tot.Tx</span>(D) }</a>
<a class="sourceLine" id="cb1020-2" data-line-number="2">confidence &lt;-<span class="st"> </span><span class="cf">function</span>(D, X, Y) { <span class="kw">X</span>(D) <span class="op">/</span><span class="st"> </span><span class="kw">Y</span>(D) }</a>
<a class="sourceLine" id="cb1020-3" data-line-number="3">lift &lt;-<span class="st"> </span><span class="cf">function</span>(D, X, Y) { X <span class="op">/</span><span class="st"> </span>Y }</a>
<a class="sourceLine" id="cb1020-4" data-line-number="4">cosine.similarity &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-5" data-line-number="5">  <span class="kw">P</span>(D, A.B) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>( <span class="kw">P</span>(D, A) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D, B))</a>
<a class="sourceLine" id="cb1020-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb1020-7" data-line-number="7">jaccard.coefficient &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-8" data-line-number="8">  <span class="kw">P</span>(D, A.B) <span class="op">/</span><span class="st"> </span>(<span class="kw">P</span>(D,A) <span class="op">+</span><span class="st"> </span><span class="kw">P</span>(D,B) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D, A.B))</a>
<a class="sourceLine" id="cb1020-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb1020-10" data-line-number="10">jaccard.distance &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-11" data-line-number="11">  <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">jaccard.coefficient</span>(D)</a>
<a class="sourceLine" id="cb1020-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb1020-13" data-line-number="13">leverage &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-14" data-line-number="14">  <span class="kw">P</span>(D,A.B) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,B)</a>
<a class="sourceLine" id="cb1020-15" data-line-number="15">}</a>
<a class="sourceLine" id="cb1020-16" data-line-number="16">conviction &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-17" data-line-number="17">  <span class="kw">max</span>(</a>
<a class="sourceLine" id="cb1020-18" data-line-number="18">    (<span class="kw">P</span>(D,A) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,B_)) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,A.B_),</a>
<a class="sourceLine" id="cb1020-19" data-line-number="19">    (<span class="kw">P</span>(D,B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_)) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,A_.B)</a>
<a class="sourceLine" id="cb1020-20" data-line-number="20">  )</a>
<a class="sourceLine" id="cb1020-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb1020-22" data-line-number="22">chi.squared &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-23" data-line-number="23">  ( <span class="kw">A.B</span>(D) <span class="op">-</span><span class="st"> </span><span class="kw">E</span>(D,A,B) )<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="kw">E</span>(D,A,B) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-24" data-line-number="24"><span class="st">    </span>( <span class="kw">A_.B</span>(D) <span class="op">-</span><span class="st"> </span><span class="kw">E</span>(D,A_,B) )<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="kw">E</span>(D,A_,B) <span class="op">+</span></a>
<a class="sourceLine" id="cb1020-25" data-line-number="25"><span class="st">  </span>( <span class="kw">A.B_</span>(D) <span class="op">-</span><span class="st"> </span><span class="kw">E</span>(D,A,B_) )<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="kw">E</span>(D,A,B_)  <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-26" data-line-number="26"><span class="st">    </span>( <span class="kw">A_.B_</span>(D) <span class="op">-</span><span class="st"> </span><span class="kw">E</span>(D,A_,B_) )<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="kw">E</span>(D,A_,B_)</a>
<a class="sourceLine" id="cb1020-27" data-line-number="27">}</a>
<a class="sourceLine" id="cb1020-28" data-line-number="28">kappa &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-29" data-line-number="29">  ( <span class="kw">P</span>(D, A.B) <span class="op">+</span><span class="st"> </span><span class="kw">P</span>(D, A_.B_) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,B) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A_) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D, B_) ) <span class="op">/</span></a>
<a class="sourceLine" id="cb1020-30" data-line-number="30"><span class="st">    </span>( <span class="dv">1</span><span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,B) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A_) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D, B_))</a>
<a class="sourceLine" id="cb1020-31" data-line-number="31">}</a>
<a class="sourceLine" id="cb1020-32" data-line-number="32">gini.index &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-33" data-line-number="33">  <span class="kw">P</span>(D, A) <span class="op">*</span><span class="st"> </span>( <span class="kw">P.cond</span>(D, A.B, A)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">P.cond</span>(D, A.B_,A)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-34" data-line-number="34"><span class="st">  </span><span class="kw">P</span>(D, A_) <span class="op">*</span><span class="st"> </span>(<span class="kw">P.cond</span>(D, A_.B, A_)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">P.cond</span>(D, A_.B_, A_)<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-35" data-line-number="35"><span class="st">  </span><span class="kw">P</span>(D, B)<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D, B_)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1020-36" data-line-number="36">}</a>
<a class="sourceLine" id="cb1020-37" data-line-number="37">yule.Q &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-38" data-line-number="38">  ( <span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B_) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A_.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A.B_) ) <span class="op">/</span></a>
<a class="sourceLine" id="cb1020-39" data-line-number="39"><span class="st">  </span>( <span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B_) <span class="op">+</span><span class="st"> </span><span class="kw">P</span>(D,A_.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A.B_) )</a>
<a class="sourceLine" id="cb1020-40" data-line-number="40">}</a>
<a class="sourceLine" id="cb1020-41" data-line-number="41">yule.Y &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-42" data-line-number="42">  ( <span class="kw">sqrt</span>(<span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B_)) <span class="op">-</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">P</span>(D,A_.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A.B_)) ) <span class="op">/</span></a>
<a class="sourceLine" id="cb1020-43" data-line-number="43"><span class="st">  </span>( <span class="kw">sqrt</span>(<span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B_)) <span class="op">+</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">P</span>(D,A_.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A.B_)) )</a>
<a class="sourceLine" id="cb1020-44" data-line-number="44">}</a>
<a class="sourceLine" id="cb1020-45" data-line-number="45">kulczynski &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-46" data-line-number="46">  <span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">P.cond</span>(D,A.B,B) <span class="op">+</span><span class="st"> </span><span class="kw">P.cond</span>(D,A.B, A))</a>
<a class="sourceLine" id="cb1020-47" data-line-number="47">}</a>
<a class="sourceLine" id="cb1020-48" data-line-number="48">odds.ratio &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-49" data-line-number="49">  (<span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B_)) <span class="op">/</span><span class="st"> </span>(<span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B))</a>
<a class="sourceLine" id="cb1020-50" data-line-number="50">}</a>
<a class="sourceLine" id="cb1020-51" data-line-number="51">j.measure &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-52" data-line-number="52">  <span class="kw">max</span>(</a>
<a class="sourceLine" id="cb1020-53" data-line-number="53">  <span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">P.cond</span>(D,A.B,A) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,B), <span class="dv">2</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1020-54" data-line-number="54"><span class="st">  </span><span class="kw">P</span>(D,A.B_) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">P.cond</span>(D,A.B_,A) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,B_), <span class="dv">2</span>), </a>
<a class="sourceLine" id="cb1020-55" data-line-number="55">  <span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">P.cond</span>(D,A.B,B) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,B), <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-56" data-line-number="56"><span class="st">  </span><span class="kw">P</span>(D,A.B_) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">P.cond</span>(D,A_.B,B) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,A_), <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1020-57" data-line-number="57">}</a>
<a class="sourceLine" id="cb1020-58" data-line-number="58">imbalance.ratio &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-59" data-line-number="59">  <span class="kw">abs</span>(<span class="kw">support</span>(D,A) <span class="op">-</span><span class="st"> </span><span class="kw">support</span>(D,B)) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-60" data-line-number="60"><span class="st">  </span>( <span class="kw">support</span> (D,A) <span class="op">+</span><span class="st"> </span><span class="kw">support</span>(D,B) <span class="op">-</span><span class="st"> </span><span class="kw">support</span>(D,A.B))</a>
<a class="sourceLine" id="cb1020-61" data-line-number="61">}</a></code></pre></div>

<p><strong>Third</strong>, we then compute for the measures of the three matrices:</p>

<div class="sourceCode" id="cb1021"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1021-1" data-line-number="1">m =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span><span class="dv">17</span>, <span class="dt">ncol=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1021-2" data-line-number="2"><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>) {</a>
<a class="sourceLine" id="cb1021-3" data-line-number="3">  <span class="cf">if</span> (j <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { D =<span class="st"> </span>T1 } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1021-4" data-line-number="4">  <span class="cf">if</span> (j <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) { D =<span class="st"> </span>T2 } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1021-5" data-line-number="5">  <span class="cf">if</span> (j <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) { D =<span class="st"> </span>T3 }</a>
<a class="sourceLine" id="cb1021-6" data-line-number="6">  m[, j] =<span class="st"> </span><span class="kw">c</span>( <span class="kw">support</span>(D, A.B), <span class="kw">confidence</span>(D, A.B, A), </a>
<a class="sourceLine" id="cb1021-7" data-line-number="7">              <span class="kw">lift</span>(D, <span class="kw">confidence</span>(D, A.B, A), <span class="kw">support</span>(D,B) ),</a>
<a class="sourceLine" id="cb1021-8" data-line-number="8">              <span class="kw">chi.squared</span>(D), <span class="kw">kulczynski</span>(D), <span class="kw">leverage</span>(D),</a>
<a class="sourceLine" id="cb1021-9" data-line-number="9">              <span class="kw">jaccard.coefficient</span>(D), <span class="kw">jaccard.distance</span>(D),</a>
<a class="sourceLine" id="cb1021-10" data-line-number="10">              <span class="kw">cosine.similarity</span>(D), <span class="kw">conviction</span>(D),</a>
<a class="sourceLine" id="cb1021-11" data-line-number="11">              <span class="kw">kappa</span>(D), <span class="kw">gini.index</span>(D),  <span class="kw">yule.Q</span>(D),</a>
<a class="sourceLine" id="cb1021-12" data-line-number="12">              <span class="kw">yule.Y</span>(D), <span class="kw">odds.ratio</span>(D), <span class="kw">j.measure</span>(D),</a>
<a class="sourceLine" id="cb1021-13" data-line-number="13">              <span class="kw">imbalance.ratio</span>(D))</a>
<a class="sourceLine" id="cb1021-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb1021-15" data-line-number="15"><span class="kw">colnames</span>(m) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;T1&quot;</span>, <span class="st">&quot;T2&quot;</span>, <span class="st">&quot;T3&quot;</span>)</a>
<a class="sourceLine" id="cb1021-16" data-line-number="16"><span class="kw">rownames</span>(m) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Support&quot;</span>, <span class="st">&quot;Confidence&quot;</span>, <span class="st">&quot;Lift&quot;</span>, <span class="st">&quot;Chi-Squared&quot;</span>,</a>
<a class="sourceLine" id="cb1021-17" data-line-number="17">                <span class="st">&quot;Kulczynski&quot;</span>, <span class="st">&quot;Leverage (Piatetsky-Shapiro)&quot;</span>, </a>
<a class="sourceLine" id="cb1021-18" data-line-number="18">                <span class="st">&quot;Jaccard Coefficient&quot;</span>,  <span class="st">&quot;Jaccard Distance&quot;</span>, </a>
<a class="sourceLine" id="cb1021-19" data-line-number="19">                <span class="st">&quot;Cosine Similarity&quot;</span>, <span class="st">&quot;Conviction&quot;</span>,</a>
<a class="sourceLine" id="cb1021-20" data-line-number="20">                <span class="st">&quot;Kappa&quot;</span>, <span class="st">&quot;Gini Index&quot;</span>, <span class="st">&quot;Yule Q&quot;</span>, <span class="st">&quot;Yule Y&quot;</span>,</a>
<a class="sourceLine" id="cb1021-21" data-line-number="21">                <span class="st">&quot;Odds Ratio&quot;</span>, <span class="st">&quot;J-Measure&quot;</span>, <span class="st">&quot;Imbalance Ratio&quot;</span>)</a>
<a class="sourceLine" id="cb1021-22" data-line-number="22"><span class="kw">round</span>(m, <span class="dv">3</span>)</a></code></pre></div>
<pre><code>##                                   T1       T2       T3
## Support                        0.321    0.068    0.068
## Confidence                     0.468    0.468    0.500
## Lift                           0.774    3.678    3.691
## Chi-Squared                  451.556 2219.674 2231.344
## Kulczynski                     0.500    0.500    0.500
## Leverage (Piatetsky-Shapiro)  -0.094    0.049    0.049
## Jaccard Coefficient            0.331    0.331    0.333
## Jaccard Distance               0.669    0.669    0.667
## Cosine Similarity              0.499    0.499    0.500
## Conviction                     0.743    1.825    1.729
## Kappa                         -0.407    0.419    0.422
## Gini Index                     0.082    0.039    0.042
## Yule Q                        -0.828    0.843    0.843
## Yule Y                        -0.531    0.548    0.549
## Odds Ratio                     0.107   13.333   11.765
## J-Measure                      0.151    0.073    0.074
## Imbalance Ratio                0.084    0.084    0.000</code></pre>

</div>
<div id="correlation-and-collinearity" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.5</span> Correlation and Collinearity  <a href="machinelearning1.html#correlation-and-collinearity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Correlation</strong> measures the strength of relation amongst variables characterized by the linear movement in a positive (ascending) or negative (descending) direction. Thus, <strong>Correlation</strong> is also termed <strong>collinearity</strong> in its linear Correlation. <strong>Multicollinearity</strong> is a more common term for Correlation amongst multiple variables.</p>
<p>In this section, we cover Correlation (or collinearity) between random variables - starting with two variables - by showing the plots that they form. It helps to visualize the Correlation between distributions as this becomes our baseline and foundation in comparing with other unusual, skewed, or out-of-the-ordinary distributions.</p>
<p>Here, we introduce <strong>Correlation Coefficient</strong> to measure the correlation strength between two variables. A measure of zero means there is no correlation between the variables. A measure of one means a solidly positive correlation; otherwise, it is a solidly negative correlation. Below are the two <strong>Pearson r correlation</strong> formulae that render the same result.</p>

<p><span class="math display">\[\begin{align}
Cor = \frac{n\sum(AB) - \sum{A} \sum{B}}
      {\sqrt{
        \left(n\sum A^2 - (\sum A)^2\right)
        \left(n\sum B^2 - (\sum B)^2\right)
       }}\ \ \ \ \ \ \ \ \text{range}: [-1,1]
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
Cor = \frac{\sum (A - \overline{A})(B - \overline{B})}
{ \sqrt{\sum (A - \overline{A})^2} \sqrt{\sum(B - \overline{B})^2}}\ \ \ \ \ \ \ \ \text{range}: [-1,1]
\end{align}\]</span>
</p>
<p>A simple implementation of the <strong>Correlation Coefficient</strong> is provided below:</p>

<div class="sourceCode" id="cb1023"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1023-1" data-line-number="1">correlation1 &lt;-<span class="st"> </span><span class="cf">function</span>(A,B) {</a>
<a class="sourceLine" id="cb1023-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(A)</a>
<a class="sourceLine" id="cb1023-3" data-line-number="3">  ( n <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>( A <span class="op">*</span><span class="st"> </span>B) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(A) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(B) ) <span class="op">/</span></a>
<a class="sourceLine" id="cb1023-4" data-line-number="4"><span class="st">  </span><span class="kw">sqrt</span>( ( n <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(A<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(A)<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st">  </span>( n <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(B<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(B)<span class="op">^</span><span class="dv">2</span>) )</a>
<a class="sourceLine" id="cb1023-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1023-6" data-line-number="6">correlation2 &lt;-<span class="st"> </span><span class="cf">function</span>(A,B) {</a>
<a class="sourceLine" id="cb1023-7" data-line-number="7">  <span class="kw">sum</span> ( (A <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(A)) <span class="op">*</span><span class="st"> </span>(B <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(B)) ) <span class="op">/</span></a>
<a class="sourceLine" id="cb1023-8" data-line-number="8"><span class="st">  </span>( <span class="kw">sqrt</span>(<span class="kw">sum</span>((A <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(A))<span class="op">^</span><span class="dv">2</span>)) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((B <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(B))<span class="op">^</span><span class="dv">2</span>))   )</a>
<a class="sourceLine" id="cb1023-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb1023-10" data-line-number="10">correlation =<span class="st"> </span>correlation1</a></code></pre></div>

<p>Using Figure <a href="machinelearning1.html#fig:c1">9.35</a>, we show a correlation chart between two random variables. It illustrates a solid increasing diagonal line. The correlation is 100% - given that we are using two duplicated random variables to simulate correlation. The two variables are dependent. As one increases, so does the other. The correlation coefficient is 1, suggesting a solidly positive correlation. The correlation coefficient for the descending line is -1, suggesting a solidly negative correlation. This case of having two variables with a correlation coefficient of -1 or 1 also means that one of the two variables can be representative of the other - and as each one duplicates the other, the chances are that we choose one of them for our purpose, whatever it may be.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c1"></span>
<img src="DS_files/figure-html/c1-1.png" alt="Very Strong Correlation (Uniform vs Uniform)" width="70%" />
<p class="caption">
Figure 9.35: Very Strong Correlation (Uniform vs Uniform)
</p>
</div>
<p>In reality, most datasets with some degree of correlation come with noise (perturbation or error). In a later section, we discuss data leakage, confounding variables, outliers, and missing values. As for the plot in Figure <a href="machinelearning1.html#fig:cc1">9.36</a>, we try to determine any <strong>linear</strong> correlation. The correlation coefficient is 0.9244, suggesting a strong positive correlation (due to the added noise).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cc1"></span>
<img src="DS_files/figure-html/cc1-1.png" alt="Very Strong Correlation (Uniform vs Uniform)" width="70%" />
<p class="caption">
Figure 9.36: Very Strong Correlation (Uniform vs Uniform)
</p>
</div>
<p>Using Figure <a href="machinelearning1.html#fig:c5">9.37</a>, we show a correlation between a <strong>uniform</strong> distribution and a <strong>normal</strong> distribution. The plot follows a vertical convergence of data to the center for the <strong>normal</strong> distribution and an even (or uniform) horizontal distribution of data. That illustrates very low - to no - correlation, given that we are using two different random variables of two different types of distributions. Any pair of random variables will have a low correlation if a low percentage of the data closely matches. The correlation coefficient is 0.0252, suggesting a very weak correlation. This dataset type represents a constant timeline in which every moment captures some random event with uncertainty. Our goal is to estimate the point of convergence (the center or mean) of such a random event in a time series.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c5"></span>
<img src="DS_files/figure-html/c5-1.png" alt="Very Weak Correlation (Uniform vs Normal)" width="70%" />
<p class="caption">
Figure 9.37: Very Weak Correlation (Uniform vs Normal)
</p>
</div>
<p>Using Figure <a href="machinelearning1.html#fig:c3">9.38</a>, we show a correlation chart between two random variables of <strong>uniform</strong> distribution. That illustrates no correlation, given that we are using two independent random variables. That also indicates that the correlation between two different ‘uniform’ distributions shows data points covering a rectangular region. The correlation coefficient is 0.0054.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c3"></span>
<img src="DS_files/figure-html/c3-1.png" alt="No Correlation (Uniform vs Uniform)" width="70%" />
<p class="caption">
Figure 9.38: No Correlation (Uniform vs Uniform)
</p>
</div>
<p>In the next figure, let us use <strong>RMSE</strong> to measure the relationship and compare it with <strong>correlation coefficient</strong>. A measure of zero indicates no perturbation.  </p>
<p><span class="math display">\[\begin{align}
RMSE = \sqrt{\frac{1}{n}\sum(A-B)^2}\ \ \ \ \ \ \ \ \text{range}: [0,\infty]
\end{align}\]</span></p>
<p>A simple implementation of <strong>RMSE</strong> is provided below:</p>

<div class="sourceCode" id="cb1024"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1024-1" data-line-number="1">rmse &lt;-<span class="st"> </span><span class="cf">function</span>(A,B) {</a>
<a class="sourceLine" id="cb1024-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(A)</a>
<a class="sourceLine" id="cb1024-3" data-line-number="3">  <span class="kw">sqrt</span>(<span class="kw">mean</span>((A <span class="op">-</span><span class="st"> </span>B)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1024-4" data-line-number="4">}</a></code></pre></div>

<p>Using Figure <a href="machinelearning1.html#fig:c1">9.35</a>, we show a correlation chart between two random variables of <strong>normal</strong> distribution. It illustrates a more dispersed behavior (variance) converging to the center (mean). The correlation coefficient is 0.01 which suggests non-linear correlation. However, the <strong>RMSE</strong> is 1.2868. A zero <strong>RMSE</strong> indicates that the two variables have zero perturbation. An RMSE of around 1.2 indicates a standard deviation close to 1. In other words, RMSE increases as variance (error/perturbation) increases. Let us cover <strong>covariance</strong> later in the next section.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c2"></span>
<img src="DS_files/figure-html/c2-1.png" alt="Weak Correlation (Normal vs Normal)" width="70%" />
<p class="caption">
Figure 9.39: Weak Correlation (Normal vs Normal)
</p>
</div>

<p>Categorically, Figure <a href="machinelearning1.html#fig:c6">9.40</a> shows a correlation chart between a <strong>uniform</strong> distribution and a <strong>categorical</strong> distribution stratified from the same <strong>uniform</strong> distribution. Because we are using the same uniform distribution to simulate categorical distribution, notice a plot of 3 vertical lines based on the three numeric categories (1,2,3). The distribution of categories is not spread evenly - because of the uniform distribution. Category 1 is spread across data points from -1.5 to 0.5. Category 2 is spread across data points from -0.5 to 0.5. Category 3 is spread across data points from 0.5 to 1.5.</p>

<div class="sourceCode" id="cb1025"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1025-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1025-2" data-line-number="2">n=<span class="dv">100</span></a>
<a class="sourceLine" id="cb1025-3" data-line-number="3">x1 =<span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min=</span><span class="op">-</span><span class="fl">1.5</span>, <span class="dt">max=</span><span class="fl">1.5</span>)</a>
<a class="sourceLine" id="cb1025-4" data-line-number="4">x2 =<span class="st"> </span><span class="kw">stratify</span>(x1, <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), n) </a>
<a class="sourceLine" id="cb1025-5" data-line-number="5"><span class="kw">plot</span>(x2, x1, <span class="dt">main=</span><span class="st">&quot;Strong Category (Uniform vs Stratified Uniform)&quot;</span>, </a>
<a class="sourceLine" id="cb1025-6" data-line-number="6">     <span class="dt">ylab=</span><span class="st">&quot;X1 (Uniform)&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;X2 (Stratified Uniform)&quot;</span>, <span class="dt">pch=</span><span class="dv">20</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c6"></span>
<img src="DS_files/figure-html/c6-1.png" alt="Strong Category (Uniform vs Stratified Uniform)" width="70%" />
<p class="caption">
Figure 9.40: Strong Category (Uniform vs Stratified Uniform)
</p>
</div>

<p>Categorically, Figure <a href="machinelearning1.html#fig:c7">9.41</a> shows a correlation chart between a <strong>uniform</strong> distribution and a <strong>categorical</strong> distribution stratified from another <strong>uniform</strong> distribution. Because we are using different uniform distributions to simulate categorical distribution, notice a plot of 3 vertical lines based on the three numerical categories (1,2,3). The distribution of the categories is spread evenly across data points -1.5 to 1.5.</p>

<div class="sourceCode" id="cb1026"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1026-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1026-2" data-line-number="2">n=<span class="dv">100</span></a>
<a class="sourceLine" id="cb1026-3" data-line-number="3">x1 =<span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min=</span><span class="op">-</span><span class="fl">1.5</span>, <span class="dt">max=</span><span class="fl">1.5</span>)</a>
<a class="sourceLine" id="cb1026-4" data-line-number="4">x2 =<span class="st"> </span><span class="kw">stratify</span>(<span class="kw">runif</span>(n, <span class="dt">min=</span><span class="op">-</span><span class="fl">1.5</span>, <span class="dt">max=</span><span class="fl">1.5</span>), <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), n) </a>
<a class="sourceLine" id="cb1026-5" data-line-number="5"><span class="kw">plot</span>(x2, x1, <span class="dt">main=</span><span class="st">&quot;Uniform vs Stratified Uniform Non-Collinearity&quot;</span>, </a>
<a class="sourceLine" id="cb1026-6" data-line-number="6">     <span class="dt">ylab=</span><span class="st">&quot;Uniform&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Different Stratified Uniform&quot;</span>, <span class="dt">pch=</span><span class="dv">20</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c7"></span>
<img src="DS_files/figure-html/c7-1.png" alt="Uniform vs Stratified Uniform Non-Collinearity" width="70%" />
<p class="caption">
Figure 9.41: Uniform vs Stratified Uniform Non-Collinearity
</p>
</div>

<p>With all that said, let us use a function called <strong>pairs(.)</strong> to generate a single plot for all pair-wise combinations of multiple variables. We use the <strong>mtcars</strong> dataset with a select number of variables (mpg, cyl, disp, hp, drat, carb). See Figure <a href="machinelearning1.html#fig:col1">9.42</a>. Notice that the mpg-disp pair seems to show a negative correlation. The mpg-drat pair seems to show a positive correlation. The cyl-disp pair seems to have a categorical correlation, however.</p>

<div class="sourceCode" id="cb1027"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1027-1" data-line-number="1"><span class="kw">pairs</span>(mtcars[,<span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;cyl&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>, <span class="st">&quot;drat&quot;</span>, <span class="st">&quot;carb&quot;</span>)], <span class="dt">pch=</span><span class="dv">20</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:col1"></span>
<img src="DS_files/figure-html/col1-1.png" alt="Correlation pairs" width="100%" />
<p class="caption">
Figure 9.42: Correlation pairs
</p>
</div>

<p>When our dataset comes with multiple variables, we then use <strong>multicollinearity</strong> to measure correlation across multiple variables. To illustrate, let us introduce a third-party package in R called <strong>corrplot</strong>. We use <strong>corrplot(.)</strong> function to analyze <strong>collinearity</strong> in one plot similar to the <strong>pairs(.)</strong> function. See Figure <a href="machinelearning1.html#fig:cl1">9.43</a>. The color spectrum maps to the range [-1,1]. Colors closer to 1 indicate positive collinearity. Colors closer to -1 indicate negative collinearity. Otherwise, there is no collinearity. Here, we use the <strong>mtcars</strong> for our dataset, which comes with 11 variables.</p>

<div class="sourceCode" id="cb1028"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1028-1" data-line-number="1"><span class="kw">library</span>(corrplot)</a>
<a class="sourceLine" id="cb1028-2" data-line-number="2">mt &lt;-<span class="st"> </span><span class="kw">cor</span>(mtcars)</a>
<a class="sourceLine" id="cb1028-3" data-line-number="3"><span class="kw">corrplot</span>(mt, <span class="dt">method =</span> <span class="st">&quot;circle&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cl1"></span>
<img src="DS_files/figure-html/cl1-1.png" alt="Collinearity Matrix" width="50%" />
<p class="caption">
Figure 9.43: Collinearity Matrix
</p>
</div>

<p>Below, we select a few variables to compare collinearity.</p>

<div class="sourceCode" id="cb1029"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1029-1" data-line-number="1">mt =<span class="st"> </span>mtcars[,<span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;cyl&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>, <span class="st">&quot;drat&quot;</span>)]</a>
<a class="sourceLine" id="cb1029-2" data-line-number="2"><span class="kw">cor</span>(mt)</a></code></pre></div>
<pre><code>##          mpg     cyl    disp      hp    drat
## mpg   1.0000 -0.8522 -0.8476 -0.7762  0.6812
## cyl  -0.8522  1.0000  0.9020  0.8324 -0.6999
## disp -0.8476  0.9020  1.0000  0.7909 -0.7102
## hp   -0.7762  0.8324  0.7909  1.0000 -0.4488
## drat  0.6812 -0.6999 -0.7102 -0.4488  1.0000</code></pre>

<p>Notice that the cyl-hp pair has a collinearity coefficient of 0.8324475.</p>

<div class="sourceCode" id="cb1031"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1031-1" data-line-number="1">cyl =<span class="st"> </span>mtcars[,<span class="kw">c</span>(<span class="st">&quot;cyl&quot;</span>)]</a>
<a class="sourceLine" id="cb1031-2" data-line-number="2">hp =<span class="st"> </span>mtcars[, <span class="kw">c</span>(<span class="st">&quot;hp&quot;</span>)]</a>
<a class="sourceLine" id="cb1031-3" data-line-number="3"><span class="kw">correlation</span>(cyl,hp)</a></code></pre></div>
<pre><code>## [1] 0.8324</code></pre>

<p>Recall in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>) about <strong>Dot Product</strong> and <strong>Determinant</strong>. To measure <strong>Collinearity</strong>, we can also use <strong>Dot Product</strong> and <strong>Determinant</strong>. If two vectors have a zero determinant, then the two vectors are not only <strong>Collinear</strong>, they are <strong>100% collinear</strong>, and thus one is enough to represent the other. If the <strong>Dot Product</strong>, however, is zero, then the two vectors are orthogonal, and therefore there is no <strong>Collinearity</strong>; otherwise, there is some degree of <strong>Collinearity</strong>.</p>
</div>
<div id="covariance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.6</span> Covariance <a href="machinelearning1.html#covariance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Datasets with <strong>gaussian distribution</strong> may be measured using <strong>covariance</strong> instead of <strong>collinearity</strong>. Below is our covariance formula:</p>

<p><span class="math display">\[\begin{align}
\underbrace{Cov = \frac{\sum (A - \overline{A})(B - \overline{B})}{N}}_{\text{population covariance}}
\ \ \ \ \ \ \ \ 
\text{range}: [0,\infty]
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\underbrace{Cov = \frac{\sum (A - \overline{A})(B - \overline{B})}{(N-1)}}_{\text{sample covariance}}
\ \ \ \ \ \ \ \ 
\text{range}: [0,\infty]
\end{align}\]</span>
</p>
<p>A simple implementation of the sample <strong>Covariance</strong> is provided below:</p>

<div class="sourceCode" id="cb1033"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1033-1" data-line-number="1">covariance &lt;-<span class="st"> </span><span class="cf">function</span>(A,B) {</a>
<a class="sourceLine" id="cb1033-2" data-line-number="2">  N =<span class="st"> </span><span class="kw">length</span>(A)</a>
<a class="sourceLine" id="cb1033-3" data-line-number="3">  <span class="kw">sum</span> ( (A <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(A)) <span class="op">*</span><span class="st"> </span>(B <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(B)) ) <span class="op">/</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1033-4" data-line-number="4">}</a></code></pre></div>

<p>Let us use <strong>mtcars</strong> dataset and compare <strong>mpg</strong> and <strong>drat</strong> variables. We use three measures: Correlation, RMSE, and Covariance. Similar to RMSE, a high covariance indicates a high degree of deviation of the variables from the mean. A zero covariance means that all data points converge to the center.</p>

<div class="sourceCode" id="cb1034"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1034-1" data-line-number="1">mpg =<span class="st"> </span>mtcars[,<span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)]</a>
<a class="sourceLine" id="cb1034-2" data-line-number="2">drat =<span class="st"> </span>mtcars[,<span class="kw">c</span>(<span class="st">&quot;drat&quot;</span>)]</a>
<a class="sourceLine" id="cb1034-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;Correlation&quot;</span> =<span class="st"> </span><span class="kw">correlation</span>(mpg, drat),</a>
<a class="sourceLine" id="cb1034-4" data-line-number="4">  <span class="st">&quot;RMSE&quot;</span> =<span class="st"> </span><span class="kw">rmse</span>(mpg, drat),</a>
<a class="sourceLine" id="cb1034-5" data-line-number="5">  <span class="st">&quot;Covariance&quot;</span> =<span class="st"> </span><span class="kw">covariance</span>(mpg,drat))</a></code></pre></div>
<pre><code>## Correlation        RMSE  Covariance 
##      0.6812     17.4146      2.1951</code></pre>

</div>
<div id="outliers-leverage-influence" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.7</span> Outliers, Leverage, Influence   <a href="machinelearning1.html#outliers-leverage-influence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Chapter <strong>6</strong> (<strong>Statistical Computation</strong>), we covered the topic of <strong>Outliers, Leverage, and Influence</strong>.</p>
<p><strong>Outliers</strong> are data points with y values (response) that are outside the usual trend of the data.</p>
<p><strong>Leverage</strong> are data points with x values (predictors) outside the normal range of the data.</p>
<p><strong>Influence</strong> measures the effect (or Influence) of data points on the rest of the data.</p>

<div class="sourceCode" id="cb1036"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1036-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1036-2" data-line-number="2">beta0 =<span class="st"> </span><span class="fl">0.5</span></a>
<a class="sourceLine" id="cb1036-3" data-line-number="3">beta1 =<span class="st"> </span><span class="fl">1.2</span></a>
<a class="sourceLine" id="cb1036-4" data-line-number="4">x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">30</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1036-5" data-line-number="5">random_noise =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">30</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1036-6" data-line-number="6">expected_y =<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>random_noise</a>
<a class="sourceLine" id="cb1036-7" data-line-number="7">model =<span class="st"> </span><span class="kw">lm</span>(expected_y <span class="op">~</span><span class="st"> </span>x)</a>
<a class="sourceLine" id="cb1036-8" data-line-number="8"><span class="kw">plot</span>(expected_y <span class="op">~</span><span class="st"> </span>x,   <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="dv">1</span>, <span class="dt">main=</span><span class="st">&quot;Goodness of Fit&quot;</span>, </a>
<a class="sourceLine" id="cb1036-9" data-line-number="9">      <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Response&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Predictor&quot;</span>, </a>
<a class="sourceLine" id="cb1036-10" data-line-number="10">      <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">6</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">6</span>))</a>
<a class="sourceLine" id="cb1036-11" data-line-number="11"><span class="kw">abline</span>(model, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb1036-12" data-line-number="12">new_x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>, <span class="fl">4.5</span>)</a>
<a class="sourceLine" id="cb1036-13" data-line-number="13">new_y =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> new_x)) <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="fl">-.5</span>)</a>
<a class="sourceLine" id="cb1036-14" data-line-number="14"><span class="kw">points</span>(new_x, new_y, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb1036-15" data-line-number="15">       <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>))</a>
<a class="sourceLine" id="cb1036-16" data-line-number="16"><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">inset=</span>.<span class="dv">02</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.8</span>,</a>
<a class="sourceLine" id="cb1036-17" data-line-number="17">       <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Outlier (Influential)&quot;</span>, <span class="st">&quot;Leverage&quot;</span>, </a>
<a class="sourceLine" id="cb1036-18" data-line-number="18">                <span class="st">&quot;Leverage (Influential)&quot;</span>), </a>
<a class="sourceLine" id="cb1036-19" data-line-number="19">       <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>) )</a>
<a class="sourceLine" id="cb1036-20" data-line-number="20"></a>
<a class="sourceLine" id="cb1036-21" data-line-number="21">influence_x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb1036-22" data-line-number="22">influence_y =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(model, </a>
<a class="sourceLine" id="cb1036-23" data-line-number="23">                      <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> influence_x)) <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb1036-24" data-line-number="24">x1 =<span class="st"> </span><span class="kw">c</span>(x, influence_x)</a>
<a class="sourceLine" id="cb1036-25" data-line-number="25">y1 =<span class="st"> </span><span class="kw">c</span>(expected_y, influence_y)</a>
<a class="sourceLine" id="cb1036-26" data-line-number="26">influenced_model =<span class="st"> </span><span class="kw">lm</span> ( y1 <span class="op">~</span><span class="st"> </span>x1 )</a>
<a class="sourceLine" id="cb1036-27" data-line-number="27"><span class="kw">abline</span>(influenced_model, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1036-28" data-line-number="28"></a>
<a class="sourceLine" id="cb1036-29" data-line-number="29">influence_x =<span class="st"> </span><span class="kw">c</span>(<span class="fl">4.5</span>)</a>
<a class="sourceLine" id="cb1036-30" data-line-number="30">influence_y =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(model, </a>
<a class="sourceLine" id="cb1036-31" data-line-number="31">                      <span class="dt">newdata =</span>  <span class="kw">data.frame</span>(<span class="dt">x =</span> influence_x)) <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span>.<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb1036-32" data-line-number="32">x1 =<span class="st"> </span><span class="kw">c</span>(x, influence_x)</a>
<a class="sourceLine" id="cb1036-33" data-line-number="33">y1 =<span class="st"> </span><span class="kw">c</span>(expected_y, influence_y)</a>
<a class="sourceLine" id="cb1036-34" data-line-number="34">influenced_model =<span class="st"> </span><span class="kw">lm</span> ( y1 <span class="op">~</span><span class="st"> </span>x1 )</a>
<a class="sourceLine" id="cb1036-35" data-line-number="35"><span class="kw">abline</span>(influenced_model, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:influence"></span>
<img src="DS_files/figure-html/influence-1.png" alt="Outliers, Leverages, Influentials" width="70%" />
<p class="caption">
Figure 9.44: Outliers, Leverages, Influentials
</p>
</div>

<p>For example, Figure <a href="machinelearning1.html#fig:influence">9.44</a> illustrates three data points (red dot, black dot, blue dot). The red dot is an outlier with a significant influence as its slope is steeper, pulling the fitted line further away from the usual trend - the red dotted line.</p>
<p>The data point with blue dot color is not an outlier as its y value sticks to the trend; nonetheless, it is considered leverage since its x value is extremely away from the normal range of the other data points. Notice also that it slightly affects the slope, as can be seen by the blue dotted line.</p>
<p>On the other hand, the data point with the black dot color does not influence since it can keep the slope unchanged - the orange line - even though it is considered leverage as its x value is extremely away from the normal range of the other data.</p>
</div>
<div id="dominating-factors" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.8</span> Dominating Factors <a href="machinelearning1.html#dominating-factors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There may be situations in which we see categorical variables whose values are extremely uneven in that given two levels (categories), one level extremely dominates the other levels. See Figure <a href="machinelearning1.html#fig:dominating">9.45</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dominating"></span>
<img src="DS_files/figure-html/dominating-1.png" alt="Extreme Values" width="60%" />
<p class="caption">
Figure 9.45: Extreme Values
</p>
</div>
<p>Out of 1000 observations, 990 observations register a dominating value of 3.141593, whereas ten observations register a value of 2.718282. It would be good to trim (or remove) such variables if such extreme levels may not contribute.</p>
</div>
<div id="missingness-and-imputation" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.9</span> Missingness and Imputation  <a href="machinelearning1.html#missingness-and-imputation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we introduce the concept of missing data. One negative effect of missing data is that it reduces the statistical power and analyzability of observations. It also reduces the inference power of <strong>ML</strong> algorithms. For that reason, we need to be able to manage missing data.</p>
<p>There are three mechanisms of missingness that we need to be familiar with to manage missing data.</p>
<p><strong>MCAR</strong> (Missing completely at random) refers to missing data for no reason other than it being completely random. There is no dependency on the response or any observed or unobserved data. Therefore, this eliminates systematic bias. For example, given a list of observations, we simulate missing data by flipping a coin and removing values of observations if the coin lands on tails. In a real-world scenario, hardware storage may fail mechanically due to wear and tear, ,causing data loss. Without proper backup, some records may become corrupted and unrecoverable due to media failures or errors.</p>
<p><strong>MAR</strong> (Missing at random) refers to missing data for no reason other than it can be explained by other information (other factors). That is dependent on the observed data. For example, male participants may skip answering some questions in a survey. If we analyze the entire survey, we may find that 90% of females answered some specific questions that about 50% of male participants failed to answer. The missing data is not relevant to or dependent on the question itself; rather, it is dependent on the fact that the participant’s gender is male. Therefore, the missing data is dependent on other information in the observed data - in this case, gender is part of the survey (one of the questions in the survey).</p>
<p><strong>MNAR</strong> (Missing not at random) refers to missing data due to dependence on unobserved data or missing predictors. In the survey example above, the value of the missing data influences the probability of participants from willing to provide the value of the missing data due to some other reasons (that are not part of the survey - the unobserved data or missing predictor). One reason is if our survey includes personal or private information. Some questions may be skipped with no provided information. In such cases, sampled data may not become a true representative of the entire population because of bias.</p>
<p>To manage missing data, we introduce <strong>Imputation</strong>, a method to replace the missing data with estimates.</p>
<p>Indeed, the most simple solution for missing data is to ignore the records or perform <strong>row deletion</strong>. However, the consequence of deleting records is that it may create imbalances. For example, in the survey for <strong>MAR</strong>, 50% of male participants skipped answering some questions. Therefore, removing 50% of the male records creates <strong>BIAS</strong> towards female participants.</p>
<p>One form of <strong>imputation</strong> is to compute the average (the mean) across the observations. However, we need to be wary of the size of the missing data. To replace a large number of missing data using the average over a lesser number of available data creates <strong>invariability</strong>. We reduce variance because then data becomes closely similar to each other.</p>
<p>To solve this <strong>invariance</strong> problem, we can use the <strong>Hot Deck Method</strong>method. That is achieved by averaging a group of relevant predictors together and replacing the missing data only for those observations belonging to the respective group.</p>
<p>A better method but computationally costly is the <strong>Multiple Imputation</strong> method. That is achieved by performing <strong>regression (e.g., least-square) and prediction</strong> and then performing <strong>bayesian analysis</strong>. The idea is to generate <strong>M</strong> sub-samples from the overall observations - <strong>M</strong> being the number of imputations. For each sub-sample, we perform regression - fitting the data based on Least-Squares (as an example). We then use the regression model to predict all our missing data within the sub-sample. Next, we calculate the average or mean of the predicted values within each sub-sample. We repeat the process for all sub-samples. Afterwhich, we calculate the overall mean of all the sub-sample means. This overall mean becomes our replacement value. We also calculate the variance within each sub-sample to complement our value for discrepancies.</p>
<p>To illustrate, let us use a third-party R package called <strong>mice</strong> and use a dataset called <strong>airquality</strong>. Unfortunately, our dataset has missing data for the <strong>Ozone</strong> and <strong>Solar.R</strong> predictors:</p>

<div class="sourceCode" id="cb1037"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1037-1" data-line-number="1"><span class="co"># display only the first 10 observations.  </span></a>
<a class="sourceLine" id="cb1037-2" data-line-number="2"><span class="kw">head</span>(airquality, <span class="dt">n=</span> <span class="dv">10</span>)</a></code></pre></div>
<pre><code>##    Ozone Solar.R Wind Temp Month Day
## 1     41     190  7.4   67     5   1
## 2     36     118  8.0   72     5   2
## 3     12     149 12.6   74     5   3
## 4     18     313 11.5   62     5   4
## 5     NA      NA 14.3   56     5   5
## 6     28      NA 14.9   66     5   6
## 7     23     299  8.6   65     5   7
## 8     19      99 13.8   59     5   8
## 9      8      19 20.1   61     5   9
## 10    NA     194  8.6   69     5  10</code></pre>

<p>In the list, we see our <strong>Ozone</strong> predictor having two missing data (from the 1st displayed records) and two missing data for <strong>Solar.R</strong>. Assuming the <strong>missingness</strong> is <strong>MCAR</strong>. Using <strong>mice(.)</strong>, let us review some missing data patterns. Here, we exclude <strong>Month</strong> and <strong>Day</strong> columns. See Figure <a href="machinelearning1.html#fig:imputation">9.46</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:imputation"></span>
<img src="DS_files/figure-html/imputation-1.png" alt="Imputation" width="50%" />
<p class="caption">
Figure 9.46: Imputation
</p>
</div>
<pre><code>##     Wind Temp Solar.R Ozone   
## 111    1    1       1     1  0
## 35     1    1       1     0  1
## 5      1    1       0     1  1
## 2      1    1       0     0  2
##        0    0       7    37 44</code></pre>

<p>There are 153 observations in which the <strong>Zone</strong> predictor has 37 missing values and the <strong>Solar.R</strong> predictor has seven missing values.</p>
<p>Let us perform <strong>imputation</strong> using <strong>mice(.)</strong>. The function will run through 5 iterations for 2 imputations (m=2 sub-samples). We use <strong>Predictive mean matching (PMM)</strong> as our imputation method <span class="citation">(Morris T.P. et al. <a href="bibliography.html#ref-ref463">2014</a>)</span>. </p>

<div class="sourceCode" id="cb1040"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1040-1" data-line-number="1">imputed.datasets =<span class="st"> </span><span class="kw">mice</span>(missing.data, <span class="dt">m=</span><span class="dv">2</span>, <span class="dt">maxit=</span><span class="dv">5</span>, </a>
<a class="sourceLine" id="cb1040-2" data-line-number="2">                        <span class="dt">meth=</span><span class="st">&#39;pmm&#39;</span>, <span class="dt">seed=</span><span class="dv">2020</span>)</a></code></pre></div>
<pre><code>## 
##  iter imp variable
##   1   1  Ozone  Solar.R
##   1   2  Ozone  Solar.R
##   2   1  Ozone  Solar.R
##   2   2  Ozone  Solar.R
##   3   1  Ozone  Solar.R
##   3   2  Ozone  Solar.R
##   4   1  Ozone  Solar.R
##   4   2  Ozone  Solar.R
##   5   1  Ozone  Solar.R
##   5   2  Ozone  Solar.R</code></pre>
<div class="sourceCode" id="cb1042"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1042-1" data-line-number="1"><span class="kw">summary</span>(imputed.datasets)</a></code></pre></div>
<pre><code>## Class: mids
## Number of multiple imputations:  2 
## Imputation methods:
##   Ozone Solar.R    Wind    Temp 
##   &quot;pmm&quot;   &quot;pmm&quot;      &quot;&quot;      &quot;&quot; 
## PredictorMatrix:
##         Ozone Solar.R Wind Temp
## Ozone       0       1    1    1
## Solar.R     1       0    1    1
## Wind        1       1    0    1
## Temp        1       1    1    0</code></pre>

<p>To determine the imputed values calculated by <strong>mice(.)</strong>, we use the following format below. We use two imputations (m=2 sub-samples) and five iterations with seed=2020. The result below shows two dataset options from which to choose an imputed dataset. For example, the fifth observation with missing <strong>Solar.R</strong> value has an imputed value of 322 for the first dataset and an imputed value of 260 for the second dataset:</p>

<div class="sourceCode" id="cb1044"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1044-1" data-line-number="1"><span class="co"># display only the 1st 10 records</span></a>
<a class="sourceLine" id="cb1044-2" data-line-number="2"><span class="kw">head</span>( imputed.datasets<span class="op">$</span>imp<span class="op">$</span>Solar.R, <span class="dt">n =</span> <span class="dv">6</span>)</a></code></pre></div>
<pre><code>##      1   2
## 5   44 220
## 6  322 260
## 11   8   7
## 27   8  14
## 96 187 139
## 97  36 193</code></pre>

<p>Let us now complete the imputation by using <strong>complete(.)</strong> function. Below is the original dataset with missing data.</p>

<div class="sourceCode" id="cb1046"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1046-1" data-line-number="1"><span class="kw">head</span>(airquality, <span class="dt">n=</span><span class="dv">6</span>)</a></code></pre></div>
<pre><code>##   Ozone Solar.R Wind Temp Month Day
## 1    41     190  7.4   67     5   1
## 2    36     118  8.0   72     5   2
## 3    12     149 12.6   74     5   3
## 4    18     313 11.5   62     5   4
## 5    NA      NA 14.3   56     5   5
## 6    28      NA 14.9   66     5   6</code></pre>

<p>Let us choose the first dataset option for completing the imputation.</p>

<div class="sourceCode" id="cb1048"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1048-1" data-line-number="1">imputed.airquality =<span class="st"> </span><span class="kw">complete</span>(imputed.datasets,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1048-2" data-line-number="2"><span class="kw">head</span>(imputed.airquality, <span class="dt">n=</span><span class="dv">6</span>)</a></code></pre></div>
<pre><code>##   Ozone Solar.R Wind Temp
## 1    41     190  7.4   67
## 2    36     118  8.0   72
## 3    12     149 12.6   74
## 4    18     313 11.5   62
## 5    14      44 14.3   56
## 6    28     322 14.9   66</code></pre>

<p>Alternatively, we can choose the second dataset option, which produces different values for the missing data.</p>

<div class="sourceCode" id="cb1050"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1050-1" data-line-number="1">imputed.airquality =<span class="st"> </span><span class="kw">complete</span>(imputed.datasets,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1050-2" data-line-number="2"><span class="kw">head</span>(imputed.airquality, <span class="dt">n=</span><span class="dv">6</span>)</a></code></pre></div>
<pre><code>##   Ozone Solar.R Wind Temp
## 1    41     190  7.4   67
## 2    36     118  8.0   72
## 3    12     149 12.6   74
## 4    18     313 11.5   62
## 5    14     220 14.3   56
## 6    28     260 14.9   66</code></pre>

<p>We leave readers to also investigate <strong>Local Residual Draws (LRD)</strong>. </p>
</div>
<div id="confounding-variable" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.10</span> Confounding Variable <a href="machinelearning1.html#confounding-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To understand the concept of <strong>Confounding Variable</strong>, let us first review the linear formula below:</p>
<p><span class="math display">\[\begin{align}
y = \beta_0 + \beta_1 \cdot X + err
\end{align}\]</span></p>
<p>The <span class="math inline">\(\beta_0\)</span> represents an intercept, and <span class="math inline">\(\beta_1\)</span> is a coefficient (or a weight). The third term in the equation can be interpreted as noise (or maybe error) contributed by some unknown variable that is not included in the model. Because this variable is unknown, we treat it as noise. It has many interpretations, namely error, residual, perturbation, variation, or effect. This <strong>effect</strong> or <strong>residual</strong> is yet to be known. In such cases, this unknown variable (or a set of unknown variables) may be called a <strong>confounded</strong> variable if it contributes to noise (creating a non-zero covariance).</p>
<p>In exploring and analyzing our data, there are cases in which our data does not just come with one predictor variable but with multiple predictor variables. Take the Table <a href="machinelearning1.html#tab:climatechange1">9.33</a> listing contributing factors to climate change. There are three predictor variables: Humidity, Temperature, and Wind Velocity.</p>
<table>
<caption><span id="tab:climatechange1">Table 9.33: </span>Climate Change</caption>
<thead>
<tr class="header">
<th align="left">Date</th>
<th align="left">Humidity (%)</th>
<th align="left">Temperature (F)</th>
<th align="left">Wind Velocity (mph)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">January 01, 2018</td>
<td align="left">20</td>
<td align="left">70</td>
<td align="left">30</td>
</tr>
<tr class="even">
<td align="left">January 02, 2018</td>
<td align="left">13</td>
<td align="left">100</td>
<td align="left">10</td>
</tr>
</tbody>
</table>
<p>We use the three predictor variables to predict climate change (response variable). However, we know that the three predictors do not just influence climate change. Any or all of the following variables may contribute to climate change - deforestation, coal mining, greenhouse gases, and smog emissions. Notice that we just listed four more variables in the mix. Our task now is to study the effect of those additional predictor variables on climate change. That is where <strong>confounding variables</strong> may come into play. Mixing the right predictor variables is a challenge. However, if we mix unwanted input variables that distort our analysis, it is not in our desired position. A confounding variable tends to be described in terms of its bad significance. It means that its use in the model may cause chaos or distortion to the outcome. If so, in essence, a confounding variable is a bias variable - or an error variable, to put it simply. We commit an error in adding or excluding the variable if the variable distorts the outcome.</p>
</div>
<div id="data-leakage" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.11</span> Data Leakage <a href="machinelearning1.html#data-leakage" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Data Leakage</strong> has a similar concept as <strong>Confounding Variable</strong> in terms of the effect of distorting outcomes. However, in this section, we deal with observations instead of predictors. <strong>Data leakage</strong> is a condition that happens when unseen data (e.g., from our testing set) gets mixed into our training set (and vice versa). Contaminating our training set distorts our outcome. Moreover, if we use the outcome as proof in announcing bold claims, it certainly creates an unwanted position.</p>
<p>One way to determine <strong>data leakage</strong> is to analyze the correlation between our training and test set. Moreover, one way to avoid <strong>data leakage</strong> is to have a separate validation set to evaluate our final model.</p>
<p>We also cover <strong>data leakage</strong> under <strong>General Modeling</strong> section.</p>
</div>
<div id="one-hot-encoding" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.12</span> One Hot Encoding <a href="machinelearning1.html#one-hot-encoding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>One Hot Encoding</strong> is a method used in predictions when the dataset contains both categorical and numerical data. To fit our data well into our prediction model, we transform our categorical data into one with a vertical format. See Figure <a href="machinelearning1.html#fig:onehot">9.47</a>. The <strong>weather</strong> feature is translated into a set of bit vectors. Each bit vector represents a unique category in the <strong>weather</strong> feature. For example, we create a new <strong>Sunny</strong> feature, representing a bit vector with 1 for all weather patterns that fall under sunny weather; otherwise, the bit is set to zero. Similarly, we create a new feature called <strong>Rainy</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:onehot"></span>
<img src="onehot.png" alt="One Hot Encoding" width="70%" />
<p class="caption">
Figure 9.47: One Hot Encoding
</p>
</div>
<p>To illustrate in R code, let us first create a simple dataset. We include a categorical feature called <strong>Weather</strong> like so:</p>

<div class="sourceCode" id="cb1052"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1052-1" data-line-number="1">dataset =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb1052-2" data-line-number="2">    <span class="dt">Weather =</span> <span class="kw">c</span>(<span class="st">&quot;Sunny&quot;</span>, <span class="st">&quot;Rainy&quot;</span>, <span class="st">&quot;Rainy&quot;</span>, <span class="st">&quot;Sunny&quot;</span>, <span class="st">&quot;Sunny&quot;</span>),</a>
<a class="sourceLine" id="cb1052-3" data-line-number="3">    <span class="dt">Items =</span> <span class="kw">c</span>(<span class="st">&quot;A,C,D&quot;</span>, <span class="st">&quot;B,E&quot;</span>, <span class="st">&quot;A,C,D,E&quot;</span>, <span class="st">&quot;A,B&quot;</span>, <span class="st">&quot;B,F&quot;</span>),</a>
<a class="sourceLine" id="cb1052-4" data-line-number="4">    <span class="dt">Score =</span> <span class="kw">c</span>(<span class="fl">6.5</span>, <span class="fl">9.8</span>, <span class="fl">3.2</span>, <span class="fl">4.4</span>, <span class="fl">7.8</span>) </a>
<a class="sourceLine" id="cb1052-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb1052-6" data-line-number="6">dataset</a></code></pre></div>
<pre><code>##   Weather   Items Score
## 1   Sunny   A,C,D   6.5
## 2   Rainy     B,E   9.8
## 3   Rainy A,C,D,E   3.2
## 4   Sunny     A,B   4.4
## 5   Sunny     B,F   7.8</code></pre>

<p>We then perform <strong>One Hot encoding</strong> on the <strong>weather</strong> feature using <strong>model.matrix(.)</strong> function. That creates a dataset with a list of bit vectors.</p>

<div class="sourceCode" id="cb1054"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1054-1" data-line-number="1">onehot =<span class="st"> </span><span class="kw">model.matrix</span>( <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Weather , <span class="dt">data =</span> dataset )</a>
<a class="sourceLine" id="cb1054-2" data-line-number="2"><span class="kw">colnames</span>(onehot) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Rainy&quot;</span>, <span class="st">&quot;Sunny&quot;</span>)</a>
<a class="sourceLine" id="cb1054-3" data-line-number="3">onehot[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]</a></code></pre></div>
<pre><code>##   Rainy Sunny
## 1     0     1
## 2     1     0
## 3     1     0
## 4     0     1
## 5     0     1</code></pre>

<p>Our next step is to incorporate the bit vectors into the dataset using <strong>mutate(.)</strong> function from <strong>dplyr</strong> package:</p>

<div class="sourceCode" id="cb1056"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1056-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb1056-2" data-line-number="2">mutated.dataset =<span class="st"> </span><span class="kw">mutate</span>(dataset, <span class="dt">Weather=</span><span class="ot">NULL</span>, </a>
<a class="sourceLine" id="cb1056-3" data-line-number="3">                         <span class="dt">Rainy =</span> onehot[,<span class="dv">1</span>], <span class="dt">Sunny =</span> onehot[,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb1056-4" data-line-number="4">mutated.dataset [, <span class="kw">c</span>(<span class="st">&quot;Sunny&quot;</span>, <span class="st">&quot;Rainy&quot;</span>, <span class="st">&quot;Items&quot;</span>, <span class="st">&quot;Score&quot;</span>)]</a></code></pre></div>
<pre><code>##   Sunny Rainy   Items Score
## 1     1     0   A,C,D   6.5
## 2     0     1     B,E   9.8
## 3     0     1 A,C,D,E   3.2
## 4     1     0     A,B   4.4
## 5     1     0     B,F   7.8</code></pre>

</div>
<div id="winsorization-and-trimming" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.13</span> Winsorization and Trimming  <a href="machinelearning1.html#winsorization-and-trimming" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The concept of <strong>Winsorization</strong> and <strong>Trimming</strong> is best explained with a dataset that follows a <strong>Gaussian Normal distribution</strong>. We see the right-most tail and the left-most tail representing extreme outliers in a Normal distribution. Instead of stretching our dataset to cover both extreme outliers, it may be necessary at times to <strong>trim off</strong> those outliers - hence, the term <strong>trimming</strong>. For example, any data point below the 5% quartile or above the 95% quartile is excluded from the dataset. Our dataset covers only the range between the 5% quartile and 95% quartile.</p>
<p>Another way to handle outliers is by <strong>Winsorization</strong>. The idea is to take the values right at the exact 5% quartile and 95% quartile. We then use the two values to replace any outlier below the 5% or above the 95% quartile. This concept is almost similar to <strong>Missingness and Imputation</strong>, in which we replace missing values with the average values of our observation (granting we choose the simplest method). In the case of <strong>Winsorization</strong>, we replace extreme values with a cut-off value taken at specified boundaries.</p>
<p>To illustrate, let us apply <strong>winsorization</strong> in R code using a third-party package called <strong>DescTools</strong> along with its function <strong>Winsorize(.)</strong>. To achieve <strong>Winsorization</strong>, let us simulate a dataset evenly spread between -4 and 4. We then winsorize at -3 and 3.</p>

<div class="sourceCode" id="cb1058"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1058-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb1058-2" data-line-number="2"><span class="kw">library</span>(DescTools)</a>
<a class="sourceLine" id="cb1058-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1058-4" data-line-number="4">dataset =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dt">length.out=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb1058-5" data-line-number="5">dataset</a></code></pre></div>
<pre><code>##  [1] -4.0000 -3.1111 -2.2222 -1.3333 -0.4444  0.4444
##  [7]  1.3333  2.2222  3.1111  4.0000</code></pre>
<div class="sourceCode" id="cb1060"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1060-1" data-line-number="1"><span class="kw">Winsorize</span>(dataset, <span class="dt">minval=</span><span class="op">-</span><span class="fl">3.0</span>, <span class="dt">maxval =</span> <span class="fl">3.0</span> ) </a></code></pre></div>
<pre><code>##  [1] -3.0000 -3.0000 -2.2222 -1.3333 -0.4444  0.4444
##  [7]  1.3333  2.2222  3.0000  3.0000</code></pre>

</div>
<div id="discretization" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.14</span> Discretization <a href="machinelearning1.html#discretization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Discretization</strong>, also called <strong>Binning</strong>, is the method of replacing a continuous number with its discretized counterpart. We divide the continuous space into discrete intervals called <strong>bins</strong> and then assign continuous numbers to its corresponding <strong>bin</strong>. To illustrate, let us use <strong>discretize(.)</strong> function found in <strong>arules</strong> package.</p>

<div class="sourceCode" id="cb1062"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1062-1" data-line-number="1"><span class="kw">library</span>(arules)</a>
<a class="sourceLine" id="cb1062-2" data-line-number="2">data =<span class="st"> </span><span class="kw">round</span>( <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">12</span>), <span class="dv">2</span>) </a>
<a class="sourceLine" id="cb1062-3" data-line-number="3">discrete =<span class="st"> </span><span class="kw">discretize</span>(data, <span class="dt">method=</span><span class="st">&quot;frequency&quot;</span>, <span class="dt">breaks=</span><span class="dv">4</span>, </a>
<a class="sourceLine" id="cb1062-4" data-line-number="4">                      <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;C&quot;</span>, <span class="st">&quot;D&quot;</span>))</a>
<a class="sourceLine" id="cb1062-5" data-line-number="5">transformed.data =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb1062-6" data-line-number="6">  <span class="dt">continuous.form =</span> data,</a>
<a class="sourceLine" id="cb1062-7" data-line-number="7">  <span class="dt">discrete.form =</span> <span class="kw">c</span>(discrete),</a>
<a class="sourceLine" id="cb1062-8" data-line-number="8">  <span class="dt">label =</span> <span class="kw">factor</span>(discrete)</a>
<a class="sourceLine" id="cb1062-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb1062-10" data-line-number="10">transformed.data </a></code></pre></div>
<pre><code>##    continuous.form discrete.form label
## 1             0.00             1     A
## 2             0.09             1     A
## 3             0.18             1     A
## 4             0.27             2     B
## 5             0.36             2     B
## 6             0.45             2     B
## 7             0.55             3     C
## 8             0.64             3     C
## 9             0.73             3     C
## 10            0.82             4     D
## 11            0.91             4     D
## 12            1.00             4     D</code></pre>

<p>In our application of discretization, we use four intervals corresponding to four bins which are given labels (A, B, C, D). Another term for <strong>bin</strong> is <strong>category</strong> or <strong>level</strong>. Below, we use the function <strong>levels(.)</strong> to show the category used:</p>
<div class="sourceCode" id="cb1064"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1064-1" data-line-number="1"><span class="kw">levels</span>(discrete)</a></code></pre></div>
<pre><code>## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot;</code></pre>
<p>If we omit the label, then we see the intervals used for the discrete counterparts:</p>

<div class="sourceCode" id="cb1066"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1066-1" data-line-number="1">discrete =<span class="st"> </span><span class="kw">discretize</span>(data, <span class="dt">method=</span><span class="st">&quot;frequency&quot;</span>, <span class="dt">breaks=</span><span class="dv">4</span>, <span class="dt">labels  =</span> <span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb1066-2" data-line-number="2"><span class="kw">levels</span>(discrete)</a></code></pre></div>
<pre><code>## [1] &quot;[0,0.247)&quot;   &quot;[0.247,0.5)&quot; &quot;[0.5,0.752)&quot;
## [4] &quot;[0.752,1]&quot;</code></pre>

</div>
<div id="stratification" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.15</span> Stratification <a href="machinelearning1.html#stratification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Stratification</strong>, also called <strong>Segmenting</strong> or <strong>Stratifying</strong>, is a method to sort observations into groups (<strong>strata</strong>). An individual group is called the <strong>stratum</strong>.</p>
<p>To illustrate, we use a 3rd-party package called <strong>splitstackshape</strong>. A function to use for stratification is <strong>stratified(.)</strong>. Now, suppose we have the following dataset.</p>

<div class="sourceCode" id="cb1068"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1068-1" data-line-number="1"><span class="kw">library</span>(splitstackshape)</a>
<a class="sourceLine" id="cb1068-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1068-3" data-line-number="3">N=<span class="dv">10</span></a>
<a class="sourceLine" id="cb1068-4" data-line-number="4">dataset =<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">Weather =</span> <span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Sunny&quot;</span>, <span class="st">&quot;Rainy&quot;</span>), <span class="dt">size=</span>N, </a>
<a class="sourceLine" id="cb1068-5" data-line-number="5">                                       <span class="dt">replace=</span><span class="ot">TRUE</span>),  </a>
<a class="sourceLine" id="cb1068-6" data-line-number="6">                <span class="dt">Participation =</span> <span class="kw">rbinom</span>(<span class="dt">n =</span> N, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">prob=</span><span class="fl">0.50</span>) ) </a>
<a class="sourceLine" id="cb1068-7" data-line-number="7">dataset</a></code></pre></div>
<pre><code>##    Weather Participation
## 1    Rainy            57
## 2    Sunny            53
## 3    Rainy            60
## 4    Sunny            46
## 5    Sunny            53
## 6    Sunny            50
## 7    Sunny            54
## 8    Sunny            59
## 9    Sunny            47
## 10   Rainy            42</code></pre>

<p>Let us stratify the dataset using <strong>stratified(.)</strong>. Our goal is to sort the activities into sample groups of rainy and sunny weather. After grouping the observations, we then take a sample per group, given a specific sample size - it can be a fraction of the total size of the sample (here, we use 50%).</p>

<div class="sourceCode" id="cb1070"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1070-1" data-line-number="1">(<span class="dt">strata =</span> <span class="kw">stratified</span>(<span class="dt">indt=</span>dataset, <span class="dt">group=</span><span class="kw">c</span>(<span class="st">&quot;Weather&quot;</span>), <span class="dt">size=</span><span class="fl">0.50</span>))</a></code></pre></div>
<pre><code>##    Weather Participation
## 1:   Rainy            60
## 2:   Rainy            57
## 3:   Sunny            47
## 4:   Sunny            46
## 5:   Sunny            54
## 6:   Sunny            50</code></pre>

<p>To validate, let us startify the dataset by using a combination of functions from <strong>dplyr</strong> package, namely <strong>group_by(.)</strong> and <strong>sample_frac(.)</strong>.</p>

<div class="sourceCode" id="cb1072"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1072-1" data-line-number="1">strata =<span class="st"> </span>dataset <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(Weather) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_frac</span>(<span class="fl">0.50</span>)</a>
<a class="sourceLine" id="cb1072-2" data-line-number="2"><span class="kw">as.data.frame</span>(strata)</a></code></pre></div>
<pre><code>##   Weather Participation
## 1   Rainy            60
## 2   Rainy            42
## 3   Sunny            50
## 4   Sunny            54
## 5   Sunny            47
## 6   Sunny            53</code></pre>

<p>Our dataset is grouped by Weather. We can summarize each group using two functions.</p>
<p>using <strong>aggregate</strong>:</p>

<div class="sourceCode" id="cb1074"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1074-1" data-line-number="1"><span class="kw">aggregate</span>(dataset<span class="op">$</span>Participation, <span class="dt">by=</span><span class="kw">list</span>(<span class="dt">Weather=</span>dataset<span class="op">$</span>Weather), </a>
<a class="sourceLine" id="cb1074-2" data-line-number="2">          <span class="dt">FUN=</span>length)</a></code></pre></div>
<pre><code>##   Weather x
## 1   Rainy 3
## 2   Sunny 7</code></pre>

<p>using <strong>tapply</strong>:</p>
<div class="sourceCode" id="cb1076"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1076-1" data-line-number="1">(<span class="dt">group.size =</span> <span class="kw">tapply</span>(dataset<span class="op">$</span>Participation, dataset<span class="op">$</span>Weather, </a>
<a class="sourceLine" id="cb1076-2" data-line-number="2">                     <span class="dt">FUN=</span>length))</a></code></pre></div>
<pre><code>## Rainy Sunny 
##     3     7</code></pre>
<p>The <strong>Rainy</strong> group has a size of 3 and the <strong>Sunny</strong> group has a size 7. If we apply a sample fraction of 50%, then we have the following sample size per group:</p>
<p>Sample Size for Rainy group: 3 <span class="math inline">\(\times\)</span> 0.50 = 2.</p>
<p>Sample Size for Sunny group: 7 <span class="math inline">\(\times\)</span> 0.50 = 4.</p>
</div>
<div id="fine-and-coarse-classing" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.16</span> Fine and Coarse Classing<a href="machinelearning1.html#fine-and-coarse-classing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Dataset may come with continuous independent variables. Therefore, there may be cases when we need to transform continuous independent variables into a discrete form before classifying them. One method we can use for the transformation is <strong>Binning</strong>, which we covered in <strong>Computational Learning I</strong> under the <strong>EDA</strong> section. Here, we call such <strong>Binning</strong> either <strong>Fine Classing</strong> or <strong>Coarse Classing</strong>, which groups the independent variable into K groups. <strong>Fine Classing</strong> is binning features with manageable cardinality, e.g., 20 and less.</p>
<p>On the other hand, <strong>Coarse Classing</strong> tries to combine smaller <strong>Fine Classes</strong> into larger classes. A common method is called <strong>Weight of Evidence (WoE)</strong> and <strong>Information Value (IV)</strong> for interaction which <strong>flattens</strong> a distribution. For example, we can use log-odds to transform the dataset into a dichotomous dataset given a continuous distribution. Such methods try to address concerns about losing information during the transformation. </p>
<p><span class="math display">\[\begin{align}
\text{WoE}_j = \ \log_e(\text{Odds)} = \ \log_e \left[\frac{P(X = x_j|Y=1)}{P(X = x_j|Y = 0)}\right]
\end{align}\]</span></p>
<p><span class="math display">\[
\text{where Odds} = \frac{\% \text{ Positive}}{\% \text{ Negative}}
\]</span></p>
<p>Such formula is related to <strong>Naive Bayes</strong> in which we have the following:</p>
<p><span class="math display">\[\begin{align}
\underbrace{\log_e \left[\frac{P(Y=1|X = x_j)}{P(Y = 0|X = x_j)}\right]}_{\text{posterior}} = 
\underbrace{\log_e \left[\frac{P(Y=1)}{P(Y = 0)}\right]}_{\text{prior}} +
\underbrace{\log_e \left[\frac{P(X = x_j|Y=1)}{P(X = x_j|Y = 0)}\right]}_{\text{likelihood}}
\end{align}\]</span></p>
<p>where <span class="math inline">\(x_j \in B_i\)</span> such that <strong>B</strong> is a set of bins.</p>
<p>Because <strong>prior</strong> excludes the <strong>X</strong> feature, we can ignore the <strong>Prior</strong> and thus end with the <strong>WeO</strong> formula.</p>
<p>To then measure the predictive power of a feature, we can use <strong>IV</strong>:</p>
<p><span class="math display">\[\begin{align}
\begin{array}{lll}
\text{IV}_j &amp;= \text{WoE}_j \times (\% \text{Positive } - \% \text{Negative}) \\
&amp;= \text{WoE}_j \times \left(\sum_i^k \left[P(x_j \in B_i|Y=1) - P(x_j \in B_i|Y=0)\right]  \right)
\end{array} \label{eqn:eqnnumber400}
\end{align}\]</span></p>
<p>Based on <strong>IV</strong>, we can then interpret the predictive power like so (as an example):</p>

<p><span class="math display">\[
\begin{array}{ll}
0\% \le \text{IV} &lt; 20\% &amp; \text{Not Predictive (Exclude)} \\
20\% \le \text{IV} &lt; 40\% &amp; \text{Weak} \\
40\% \le \text{IV} &lt; 70\% &amp; \text{Medium} \\
70\% \le \text{IV} \le 100\% &amp; \text{Highly Predictive (Strong)} \\
\end{array}
\]</span>
</p>
<p>We leave readers to investigate further the use of <strong>WoE</strong> and <strong>IV</strong> in terms of feature transformation and selection, excluding the need to handle missing values, dummy variables, and outliers.</p>
</div>
<div id="embedding" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.17</span> Embedding <a href="machinelearning1.html#embedding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Other types of data that we also have to explore deal with pictures (images), videos, music, and natural languages (words and sentences). To process these types of data computationally, somehow, we need to transform or map them into their numerical representation, structured in the form of matrices or vectors). Such an <strong>interpretable</strong> representation of a particular data type is called <strong>Embedding</strong>.</p>
<p><strong>Embedding</strong> has many definitions on the net (a few of which are):</p>
<ul>
<li>a mapping from high dimensional space to lower-dimensional space.</li>
<li>a transformation of discrete (categorical) data into its continuous numerical representation.</li>
</ul>
<p>A few examples of embeddings are:</p>
<ul>
<li>images are mapped to a 3-D matrix (e.g., convnet).</li>
<li>words are mapped into a vector (e.g., word2vec).</li>
<li>medical codes are mapped into multi-layered representation (e.g., med2vec).</li>
</ul>
<p>In this section, let us use a simple embedding from a bag of words to a vector.</p>
<p><strong>First</strong>, let us define a few terms:</p>
<ul>
<li><strong>Corpus</strong> - a collection of documents or texts (written or spoken).</li>
<li><strong>Documents</strong> - a collection of thoughts (that are informational).</li>
<li><strong>Dictionary</strong> - a collection of unique words (with lexical and semantic meaning).</li>
<li><strong>Term</strong> - is another synonym for word.</li>
</ul>
<p><strong>Second</strong>, let us collect a corpus of documents like so:</p>
<ul>
<li>Document 1 - Tomorrow will be another day, but today is a great day.</li>
<li>Document 2 - I have a great day today.</li>
<li>Document 3 - It is a wonderful day.</li>
</ul>
<p><strong>Third</strong>, let us construct our dictionary of words (terms) and count the occurrence per word. See Figure <a href="machinelearning1.html#fig:corpus">9.48</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:corpus"></span>
<img src="corpus.png" alt="Embedding (Vector Space Model)" width="85%" />
<p class="caption">
Figure 9.48: Embedding (Vector Space Model)
</p>
</div>
<p>We introduce <strong>stop words</strong> (table <strong>T1</strong>) in the example to note that certain words are common enough to be filtered out as they may not add value (instead of add distortion or skewness) to our analysis. Depending on necessity, our list of <strong>stop words</strong> may vary.</p>
<p>Table <strong>T2</strong> represents our dictionary - a list of vocabulary.</p>
<p><strong>Fourth</strong>, let us map the three documents based on our generated vocabulary. Three documents are mapped to a <strong>term-document matrix</strong> (see table <strong>T3</strong>) - this simple embedding is called <strong>vector space model (VSM)</strong>.</p>
<p><strong>Finally</strong>, the use of embedding becomes apparent when we search for a document given a query. For example, suppose we are searching for a document, and we have the following search query:</p>
<p><span class="math display">\[
\text{Query = &quot;great day today&quot;}.
\]</span></p>
<p>Let us discuss two simple solutions to help us rank our documents and retrieve the document with the highest rank.</p>
<p>The first solution is called <strong>simple VSM</strong>, which counts for the existence of unique terms in each document. So, for example, there are three unique terms in our query that exist in D1 and D2. Moreover, only one unique term in our query exists in D3.</p>
<p><span class="math display">\[
\text{search(Query, D1) = 3}\ \ \ \ \ \ \ \
\text{search(Query, D2) = 3}\ \ \ \ \ \ \ \ 
\text{search(Query, D3) = 1}
\]</span>
Therefore, the documents we need to retrieve are D1 and D2.</p>
<p>A better approach is called <strong>Term-frequency (TF) weighing</strong>, which accounts for duplicate terms. For example:</p>
<p><span class="math display">\[
\text{search(Query, D1)} = \underbrace{1}_{\text{great}} + \underbrace{2}_{\text{day}} + \underbrace{1}_{\text{today}} = 4
\]</span></p>
<p><span class="math display">\[
\text{search(Query, D2)} = \underbrace{1}_{\text{great}} + \underbrace{1}_{\text{day}} + \underbrace{1}_{\text{today}} = 3
\]</span></p>
<p>Therefore, the document we need to retrieve is D1.</p>
<p>In Chapter <strong>11</strong> (<strong>Computational Learning III</strong>), we introduce <strong>TF-IDF</strong> and <strong>Cosine Similarity</strong>, which are covered in <strong>Text Mining</strong>, complementing and enhancing document ranking.</p>
<p>We leave readers to investigate <strong>word2vec</strong>, <strong>med2vec</strong>, and <strong>convnet</strong> for some well-known <strong>embeddings</strong>.</p>
</div>
</div>
<div id="featureengineering" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.6</span> Feature Engineering<a href="machinelearning1.html#featureengineering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The essential primary ingredient in computational learning - or machine learning <strong>(ML)</strong> - is the input. We define <strong>input</strong> as any observable and measurable property of an entity or phenomenon (Wikipedia). Our goal is to study and analyze the <strong>input</strong> - a raw material (a raw metric) - and determine its importance and relevance. Any <strong>input</strong> extracted and selected based on domain knowledge is called a candidate <strong>feature</strong>. That is where <strong>Feature Engineering</strong> comes into play. </p>
<div id="machine-learning-features" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.1</span> Machine Learning Features<a href="machinelearning1.html#machine-learning-features" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Machine Learning, we introduce terms such as <strong>Feature(s)</strong> and <strong>Label(s)</strong>, <strong>Input(s)</strong> and <strong>output(s)</strong>. See Table <a href="machinelearning1.html#tab:nicevariables1">9.34</a> for equivalent terms.</p>
<table>
<caption><span id="tab:nicevariables1">Table 9.34: </span>Variables</caption>
<thead>
<tr class="header">
<th align="left">Answer</th>
<th align="left">Question</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Dependent variable</td>
<td align="left">Independent variables</td>
</tr>
<tr class="even">
<td align="left">Response variable (Variate)</td>
<td align="left">Predictor variables (Regressor / Covariate)</td>
</tr>
<tr class="odd">
<td align="left">Explained variable</td>
<td align="left">Explanatory variables</td>
</tr>
<tr class="even">
<td align="left">Label</td>
<td align="left">Features</td>
</tr>
<tr class="odd">
<td align="left">Output</td>
<td align="left">Inputs</td>
</tr>
</tbody>
</table>
<p>Barring statistics for a moment, we can treat features as traits, characteristics, or properties of entities. They describe the profile or personality of entities. For example, the weather has properties such as humidity, temperature, and wind velocity. Another example is a person with unique traits and characteristics that embody the personality of the individual. With machine learning data, we also need to give a name for the person. In other words, we need to label the personality. In essence, a label represents an object’s attributes. While we explain this concept using objects, machine learning data is not limited to just characterizing an object. We can treat features also in terms of inputs that result in an output (the label). As in statistics, any input may have the quality of being observable and measurable and thus can result in a definitive output that can be labeled. For example, humidity, temperature, and wind velocity are inputs (features) in our study of the effect of climate change. The effect of climate change represents the focus of our study and our labeling efforts.</p>
<p>Recall that while we are interested in data that is only observable and measurable entities, we prefer to narrow down data into features that contribute significance and relevance for analysis in data science. Here, it is important to note that not all such attributes or features can <strong>contribute</strong> or <strong>influence</strong>; therefore, we have algorithms that reduce the number of <strong>Features</strong> to those more <strong>relevant</strong>. </p>
<p>There seems to be a consensus about the lack of a unified formal definition of <strong>Feature Engineering</strong>. If so, perhaps we can prescribe our overarching definition with no intention to solicit merit. Let us first describe the basic premise of <strong>Engineering</strong>, which is a systematic and creative application of scientific, mathematical, theoretical, and empirical knowledge (sourced from britannica.com, linkengineering.org, bccampus.ca) to produce consumables (usable products) from raw materials. With that said, it seems sensible to regard <strong>Feature Engineering</strong> as both methodological (bearing the application of math and science) and ideological (bearing the demand for artful creativity) in the context of creating, selecting or extracting, and transforming features, bearing in mind the domain for which we apply such processes. One has to develop a system of methodologies describing the creation and reduction of features from a set of raw inputs unique to a domain. The goal is to eliminate <strong>redundant</strong> and <strong>irrelevant</strong> information. Furthermore, the intent is to enhance the predictive power of models, increase the accuracy of output, and enhance the performance of algorithms, among many other benefits.</p>
<p>In this section, our view of <strong>Feature Engineering</strong> covers a system of three overlapping methodologies: <strong>Construction</strong>, <strong>Selection</strong>, and <strong>Transformation</strong> of <strong>Features</strong>. In some cases, these methodologies are more ideological, and thus other literature follows specific descriptions, categories, or order. However, ultimately, we follow the <strong>GIGO</strong> philosophy - garbage in, garbage out. In other words, we are more unified to believe that a solid, relevant feature provides more valuable and informative data that guides us toward better decision choices.</p>
</div>
<div id="dimensionality-reduction" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.2</span> Dimensionality Reduction <a href="machinelearning1.html#dimensionality-reduction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Dimension</strong> in <strong>ML</strong> refers to the number of features in a dataset (whether they are relevant, irrelevant, or redundant). A highly dimensional dataset means that the dataset contains a high number of features. Therefore, it is sensical to reduce the number of dimensions in our dataset to a minimum - this is called <strong>dimensionality reduction</strong>.</p>
<p>The following sections cover strategies that allow us to perform such dimensionality reduction, starting with <strong>PCA</strong>.</p>
</div>
<div id="principal-component-analysis" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.3</span> Principal Component Analysis  <a href="machinelearning1.html#principal-component-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Principal Component Analysis (PCA)</strong> is widely discussed and explained on the net as one of the many dimensionality reduction methods. Here, we may have to recall concepts such as <strong>Eigenvectors</strong>, <strong>EigenValues</strong>, and <strong>Singular Valued Decomposition (SVD)</strong> discussed in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>). In addition, the concept of <strong>Covariance</strong> is also needed. It is briefly covered under the <strong>EDA</strong> section in this Chapter and Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>) under the <strong>Types of Matrices</strong> Section.</p>
<p>To get the intuition behind <strong>PCA</strong>, it helps to compare it with <strong>Ordinary Least Square (OLS)</strong>, which we covered in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) (see Figure <a href="machinelearning1.html#fig:olspca">9.49</a>). In <strong>OLS</strong>, we try to find the best fit through the data points by computing the <strong>minimum sum square</strong> of all <strong>vertical</strong> distances between the data points and the line. In other words, we <strong>compute for the least residual</strong>. The line becomes the <strong>model</strong> upon which we predict values of <strong>Y</strong> based on a given <strong>X</strong>. In <strong>PCA</strong>, it is essential to note in the figure that we are using <strong>X1</strong> and <strong>X2</strong> axes to indicate that we are instead of comparing two independent variables. Conceptually, in <strong>PCA</strong>, we also try to find the best fit of the line through the data points; however, the <strong>fitted line</strong> is a projection that represents the spread (or variance) of the data points being projected. The goal is to find the largest variance by computing the <strong>maximum sum square</strong> of all distances between the projected points and the point of origin, with the corresponding <strong>Principal axis</strong>.</p>
<p>Equivalently, we can also compute the least square distance of the orthogonal projection to the corresponding <strong>Principal axis</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:olspca"></span>
<img src="ols_pca.png" alt="OLS vs PCA" width="100%" />
<p class="caption">
Figure 9.49: OLS vs PCA
</p>
</div>
<p>To illustrate, let us first generate our own dataset:</p>

<div class="sourceCode" id="cb1078"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1078-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2019</span>)</a>
<a class="sourceLine" id="cb1078-2" data-line-number="2">N =<span class="st"> </span><span class="dv">25</span></a>
<a class="sourceLine" id="cb1078-3" data-line-number="3">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb1078-4" data-line-number="4">dataset =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">sample</span>(range, <span class="dt">size=</span>N<span class="op">*</span><span class="dv">4</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>), <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">4</span>) </a>
<a class="sourceLine" id="cb1078-5" data-line-number="5"><span class="kw">colnames</span>(dataset) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;feature1&quot;</span>, <span class="st">&quot;feature2&quot;</span>, <span class="st">&quot;feature3&quot;</span>, <span class="st">&quot;feature4&quot;</span>)</a>
<a class="sourceLine" id="cb1078-6" data-line-number="6"><span class="kw">head</span>(dataset)</a></code></pre></div>
<pre><code>##      feature1 feature2 feature3 feature4
## [1,]       77        2       31      100
## [2,]       72       60        9       13
## [3,]       31       95        4       52
## [4,]       62       63       35       34
## [5,]        6       46       52       77
## [6,]        5       65       38        5</code></pre>

<p><strong>Second</strong>, we perform centering and scaling for our dataset by standardization. Note that our scaling and centering are column-wise. See <strong>standardization</strong> in <strong>Primitive Analytical Methods</strong> section. Hereafter, we only use centering. We defer scaling for now and illustrate the need to manually scale to <strong>unit eigenvector</strong> in further discussion later.</p>
<div class="sourceCode" id="cb1080"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1080-1" data-line-number="1"><span class="kw">head</span>((<span class="dt">std.dataset =</span> <span class="kw">scale</span>(dataset, <span class="dt">center=</span><span class="ot">TRUE</span>, <span class="dt">scale=</span><span class="ot">FALSE</span>))[,])</a></code></pre></div>
<pre><code>##      feature1 feature2 feature3 feature4
## [1,]    32.24   -46.24   -10.52    43.16
## [2,]    27.24    11.76   -32.52   -43.84
## [3,]   -13.76    46.76   -37.52    -4.84
## [4,]    17.24    14.76    -6.52   -22.84
## [5,]   -38.76    -2.24    10.48    20.16
## [6,]   -39.76    16.76    -3.52   -51.84</code></pre>
<p><strong>Third</strong>, for simple explanation of <strong>PCA</strong>, we use feature1 and feature2.</p>
<div class="sourceCode" id="cb1082"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1082-1" data-line-number="1">X =<span class="st"> </span>std.sample =<span class="st"> </span>std.dataset[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)]</a>
<a class="sourceLine" id="cb1082-2" data-line-number="2"><span class="kw">head</span>(X)</a></code></pre></div>
<pre><code>##      feature1 feature2
## [1,]    32.24   -46.24
## [2,]    27.24    11.76
## [3,]   -13.76    46.76
## [4,]    17.24    14.76
## [5,]   -38.76    -2.24
## [6,]   -39.76    16.76</code></pre>
<p><strong>Fourth</strong>, we compute for the <strong>Principal components</strong> using the <strong>svd(.)</strong> function. Now, recall that we covered <strong>Single Valued Decomposition (SVD)</strong> under <strong>Matrix Factorization</strong> in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>). Here, we use the following <strong>SVD</strong> equation:</p>
<p><span class="math display">\[\begin{align}
X = UDV^T
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li>X is the matrix (e.g., our dataset - centered and scaled).</li>
<li>U holds the left <strong>eigenvectors</strong>, also called the <strong>loading</strong> matrix.</li>
<li>D is the standard deviation (diagonal) for rotation.</li>
<li><span class="math inline">\(V\)</span> holds the right <strong>eigenvectors</strong>, also called the <strong>scores</strong> matrix.</li>
</ul>

<div class="sourceCode" id="cb1084"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1084-1" data-line-number="1">svd.model =<span class="st"> </span><span class="kw">svd</span>(std.sample)</a>
<a class="sourceLine" id="cb1084-2" data-line-number="2">(<span class="dt">D =</span> svd.model<span class="op">$</span>d)</a></code></pre></div>
<pre><code>## [1] 159.9 140.2</code></pre>
<div class="sourceCode" id="cb1086"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1086-1" data-line-number="1">(<span class="dt">U =</span> svd.model<span class="op">$</span>u)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,] <span class="co"># show only first 5 rows</span></a></code></pre></div>
<pre><code>##          [,1]     [,2]
## [1,]  0.20133  0.32999
## [2,] -0.12804  0.15314
## [3,] -0.24445 -0.20766
## [4,] -0.12396  0.07883
## [5,]  0.09718 -0.25373</code></pre>
<div class="sourceCode" id="cb1088"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1088-1" data-line-number="1">(<span class="dt">V =</span> svd.model<span class="op">$</span>v)</a></code></pre></div>
<pre><code>##         [,1]    [,2]
## [1,] -0.3467  0.9380
## [2,] -0.9380 -0.3467</code></pre>

<p>We then compute the <strong>PCs</strong> using the following formula:</p>

<p><span class="math display">\[\begin{align}
PC = X \cdot V 
\end{align}\]</span></p>
<div class="sourceCode" id="cb1090"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1090-1" data-line-number="1">(<span class="dt">PC =</span> X <span class="op">%*%</span><span class="st"> </span>V)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,] <span class="co"># show only first 5 rows</span></a></code></pre></div>
<pre><code>##        [,1]   [,2]
## [1,]  32.19  46.27
## [2,] -20.47  21.47
## [3,] -39.09 -29.12
## [4,] -19.82  11.05
## [5,]  15.54 -35.58</code></pre>

<p>The second way is to compute for the <strong>covariance</strong> of our dataset and then use the <strong>eigen(.)</strong> function to compute for the <strong>EigenVectors</strong> and <strong>EigenValues</strong> using the <strong>covariance matrix</strong>. The covariance between the two features is computed using the following formula (for a large number of features, it may help to decompose the matrix using <strong>Cholesky Factorization</strong>): </p>
<p><span class="math display">\[\begin{align}
Cov(X) = \frac{(X - \bar{X})^T(X - \bar{X})}{N - 1}
\end{align}\]</span></p>
<div class="sourceCode" id="cb1092"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1092-1" data-line-number="1">X.mean =<span class="st"> </span><span class="kw">apply</span>(X, <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1092-2" data-line-number="2">(<span class="dt">cov.features =</span> ( <span class="kw">t</span>(X <span class="op">-</span><span class="st"> </span>X.mean) <span class="op">%*%</span><span class="st"> </span>(X <span class="op">-</span><span class="st"> </span>X.mean)  ) <span class="op">/</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</a></code></pre></div>
<pre><code>##          feature1 feature2
## feature1   848.86    80.06
## feature2    80.06  1035.86</code></pre>
<p>The same covariance can be computed using the <strong>cov(.)</strong> function.</p>
<div class="sourceCode" id="cb1094"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1094-1" data-line-number="1">(<span class="dt">cov.features =</span> <span class="kw">cov</span>(std.sample))</a></code></pre></div>
<pre><code>##          feature1 feature2
## feature1   848.86    80.06
## feature2    80.06  1035.86</code></pre>
<p>We first compute for the <strong>EigenValues</strong> and <strong>EigenVectors</strong>.</p>

<div class="sourceCode" id="cb1096"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1096-1" data-line-number="1">p.eigen =<span class="st"> </span><span class="kw">eigen</span>(cov.features)</a>
<a class="sourceLine" id="cb1096-2" data-line-number="2">(<span class="dt">eigenVal =</span> p.eigen<span class="op">$</span>values)  <span class="co"># EigenValues</span></a></code></pre></div>
<pre><code>## [1] 1065.4  819.3</code></pre>
<div class="sourceCode" id="cb1098"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1098-1" data-line-number="1">(<span class="dt">eigenVec =</span> p.eigen<span class="op">$</span>vectors) <span class="co"># EigenVectors</span></a></code></pre></div>
<pre><code>##        [,1]    [,2]
## [1,] 0.3467 -0.9380
## [2,] 0.9380  0.3467</code></pre>

<p>Then we compute for the <strong>PCs</strong>:</p>
<div class="sourceCode" id="cb1100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1100-1" data-line-number="1">(<span class="dt">PC =</span> X <span class="op">%*%</span><span class="st"> </span>eigenVec)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,] <span class="co"># show only first 5 rows</span></a></code></pre></div>
<pre><code>##        [,1]   [,2]
## [1,] -32.19 -46.27
## [2,]  20.47 -21.47
## [3,]  39.09  29.12
## [4,]  19.82 -11.05
## [5,] -15.54  35.58</code></pre>
<p>Note that the expanded version of the <strong>PCA</strong> formula is as such:</p>
<p><span class="math display">\[\begin{align}
PC = X \cdot V = \sum_i^n X_i \times V_i 
\end{align}\]</span></p>
<p>In this case, it is easy to just perform the computation with two features:</p>

<div class="sourceCode" id="cb1102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1102-1" data-line-number="1">V =<span class="st"> </span>loadings =<span class="st"> </span>eigenVec</a>
<a class="sourceLine" id="cb1102-2" data-line-number="2">PC1 =<span class="st"> </span>X[,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1102-3" data-line-number="3">PC2 =<span class="st"> </span>X[,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb1102-4" data-line-number="4">(<span class="dt">PC =</span> <span class="kw">cbind</span>(PC1, PC2))[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##         PC1    PC2
## [1,] -32.19 -46.27
## [2,]  20.47 -21.47
## [3,]  39.09  29.12
## [4,]  19.82 -11.05
## [5,] -15.54  35.58</code></pre>

<p>The third way is using the <strong>prcomp(.)</strong> function which readily computes for the <strong>PCs</strong>.</p>
<div class="sourceCode" id="cb1104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1104-1" data-line-number="1">(<span class="dt">pca.model =</span> <span class="kw">prcomp</span>(X))</a></code></pre></div>
<pre><code>## Standard deviations (1, .., p=2):
## [1] 32.64 28.62
## 
## Rotation (n x k) = (2 x 2):
##              PC1     PC2
## feature1 -0.3467  0.9380
## feature2 -0.9380 -0.3467</code></pre>
<p>We can get the <strong>PCs</strong> via the <strong>PCA model</strong> produced:</p>
<div class="sourceCode" id="cb1106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1106-1" data-line-number="1">(<span class="dt">scores =</span> pca.model<span class="op">$</span>x)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,] <span class="co"># show only first 5 rows</span></a></code></pre></div>
<pre><code>##         PC1    PC2
## [1,]  32.19  46.27
## [2,] -20.47  21.47
## [3,] -39.09 -29.12
## [4,] -19.82  11.05
## [5,]  15.54 -35.58</code></pre>
<p>The same <strong>PCA model</strong> has the <strong>Eigenvectors</strong> which we can use to also compute for <strong>PCA</strong>:</p>
<div class="sourceCode" id="cb1108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1108-1" data-line-number="1">V =<span class="st"> </span>loadings =<span class="st"> </span>pca.model<span class="op">$</span>rotation <span class="co"># eigenvectors</span></a>
<a class="sourceLine" id="cb1108-2" data-line-number="2">PC1 =<span class="st"> </span>X[,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1108-3" data-line-number="3">PC2 =<span class="st"> </span>X[,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb1108-4" data-line-number="4">(<span class="dt">PC =</span> <span class="kw">cbind</span>(PC1, PC2))[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##         PC1    PC2
## [1,]  32.19  46.27
## [2,] -20.47  21.47
## [3,] -39.09 -29.12
## [4,] -19.82  11.05
## [5,]  15.54 -35.58</code></pre>
<p>We can now see that a <strong>Principal Component</strong> makes up of a linear combination - a dot product of the data points and corresponding eigenvectors. We will expound on this idea later.</p>
<p><strong>Fifth</strong>, let us plot the model (see Figure <a href="machinelearning1.html#fig:pca1">9.50</a>). The idea is to find the maximum deviation (or variance) of the two features. We maximize the distance between the <strong>projected points</strong> to the point of origin.</p>

<div class="sourceCode" id="cb1110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1110-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">pty=</span><span class="st">&quot;s&quot;</span>)</a>
<a class="sourceLine" id="cb1110-2" data-line-number="2"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1110-3" data-line-number="3"><span class="co">### Plotting the first Principal Component (PC1)</span></a>
<a class="sourceLine" id="cb1110-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">80</span>,<span class="dv">80</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">100</span>,<span class="dv">80</span>), </a>
<a class="sourceLine" id="cb1110-5" data-line-number="5">      <span class="dt">xlab=</span><span class="st">&quot;feature1 (X1)&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;feature2 (X2)&quot;</span>, </a>
<a class="sourceLine" id="cb1110-6" data-line-number="6">     <span class="dt">main=</span><span class="st">&quot;Maximum Variance (PC1)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1110-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1110-8" data-line-number="8"><span class="co">### Intercept and Slope of PC1</span></a>
<a class="sourceLine" id="cb1110-9" data-line-number="9">pc1.intrcpt =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1110-10" data-line-number="10">pc1.slope =<span class="st"> </span>eigenVec[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">/</span>eigenVec[<span class="dv">1</span>,<span class="dv">1</span>] </a>
<a class="sourceLine" id="cb1110-11" data-line-number="11"><span class="kw">lines</span>(X[,<span class="dv">1</span>], pc1.slope <span class="op">*</span><span class="st"> </span>X[,<span class="dv">1</span>], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1110-12" data-line-number="12"><span class="co">### Projection to PC1</span></a>
<a class="sourceLine" id="cb1110-13" data-line-number="13">slope.orthog =<span class="st"> </span><span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>pc1.slope)</a>
<a class="sourceLine" id="cb1110-14" data-line-number="14">intrcpt.orthog =<span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>slope.orthog <span class="op">*</span><span class="st"> </span>X[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1110-15" data-line-number="15">x.proj =<span class="st"> </span>pc1.x.proj =<span class="st"> </span>(pc1.intrcpt <span class="op">-</span><span class="st"> </span>intrcpt.orthog) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1110-16" data-line-number="16"><span class="st">                      </span>(slope.orthog <span class="op">-</span><span class="st"> </span>pc1.slope) </a>
<a class="sourceLine" id="cb1110-17" data-line-number="17">y.proj =<span class="st"> </span>pc1.y.proj =<span class="st"> </span>pc1.slope <span class="op">*</span><span class="st"> </span>x.proj  <span class="op">+</span><span class="st"> </span>pc1.intrcpt</a>
<a class="sourceLine" id="cb1110-18" data-line-number="18"><span class="kw">segments</span>(X[,<span class="dv">1</span>], X[,<span class="dv">2</span>], x.proj, y.proj, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1110-19" data-line-number="19"><span class="co">### Plot the Points</span></a>
<a class="sourceLine" id="cb1110-20" data-line-number="20"><span class="kw">points</span>(X, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-21" data-line-number="21"><span class="kw">points</span>(x.proj, y.proj, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-22" data-line-number="22"><span class="kw">points</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb1110-23" data-line-number="23"><span class="co">### Legend and PC1 label</span></a>
<a class="sourceLine" id="cb1110-24" data-line-number="24"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, </a>
<a class="sourceLine" id="cb1110-25" data-line-number="25">    <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Projected Points&quot;</span>, <span class="st">&quot;Point of Origin&quot;</span>),</a>
<a class="sourceLine" id="cb1110-26" data-line-number="26">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;navyblue&quot;</span>),  <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">16</span>, <span class="dv">20</span>, <span class="dv">20</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-27" data-line-number="27"><span class="kw">text</span>(<span class="op">-</span><span class="dv">55</span>,<span class="dv">60</span>, <span class="dt">label=</span><span class="st">&quot;PC1&quot;</span>)</a>
<a class="sourceLine" id="cb1110-28" data-line-number="28"></a>
<a class="sourceLine" id="cb1110-29" data-line-number="29"><span class="co">### Plotting the second Principal Component (PC2)</span></a>
<a class="sourceLine" id="cb1110-30" data-line-number="30"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">80</span>,<span class="dv">80</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">100</span>,<span class="dv">80</span>), </a>
<a class="sourceLine" id="cb1110-31" data-line-number="31">      <span class="dt">xlab=</span><span class="st">&quot;feature1 (X1)&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;feature2 (X2)&quot;</span>, </a>
<a class="sourceLine" id="cb1110-32" data-line-number="32">     <span class="dt">main=</span><span class="st">&quot;Maximum Variance (PC2)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1110-33" data-line-number="33"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1110-34" data-line-number="34"><span class="co">### Intercept and Slope of PC2</span></a>
<a class="sourceLine" id="cb1110-35" data-line-number="35">pc2.intrcpt =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1110-36" data-line-number="36">pc2.slope =<span class="st"> </span>eigenVec[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>eigenVec[<span class="dv">1</span>,<span class="dv">2</span>] </a>
<a class="sourceLine" id="cb1110-37" data-line-number="37"><span class="kw">lines</span>(X[,<span class="dv">2</span>], pc2.slope <span class="op">*</span><span class="st"> </span>X[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1110-38" data-line-number="38"><span class="co">### Projection to PC2</span></a>
<a class="sourceLine" id="cb1110-39" data-line-number="39">slope.orthog =<span class="st"> </span><span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>pc2.slope)</a>
<a class="sourceLine" id="cb1110-40" data-line-number="40">intrcpt.orthog =<span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>slope.orthog <span class="op">*</span><span class="st"> </span>X[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1110-41" data-line-number="41">x.proj =<span class="st"> </span>pc2.x.proj =<span class="st"> </span>(pc2.intrcpt <span class="op">-</span><span class="st"> </span>intrcpt.orthog) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1110-42" data-line-number="42"><span class="st">                      </span>(slope.orthog <span class="op">-</span><span class="st"> </span>pc2.slope) </a>
<a class="sourceLine" id="cb1110-43" data-line-number="43">y.proj =<span class="st"> </span>pc2.y.proj =<span class="st"> </span>pc2.slope <span class="op">*</span><span class="st"> </span>x.proj  <span class="op">+</span><span class="st"> </span>pc2.intrcpt</a>
<a class="sourceLine" id="cb1110-44" data-line-number="44"><span class="kw">segments</span>(X[,<span class="dv">1</span>], X[,<span class="dv">2</span>], x.proj, y.proj, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1110-45" data-line-number="45"><span class="co">### Plot the Points</span></a>
<a class="sourceLine" id="cb1110-46" data-line-number="46"><span class="kw">points</span>(X, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-47" data-line-number="47"><span class="kw">points</span>(x.proj, y.proj, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-48" data-line-number="48"><span class="kw">points</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb1110-49" data-line-number="49"><span class="co">### Legend and PC2 label</span></a>
<a class="sourceLine" id="cb1110-50" data-line-number="50"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, </a>
<a class="sourceLine" id="cb1110-51" data-line-number="51">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;Projected Points&quot;</span>, <span class="st">&quot;Point of Origin&quot;</span>),</a>
<a class="sourceLine" id="cb1110-52" data-line-number="52">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;red&quot;</span>),  <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">16</span>, <span class="dv">20</span>, <span class="dv">20</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-53" data-line-number="53"><span class="kw">text</span>(<span class="op">-</span><span class="dv">50</span>,<span class="op">-</span><span class="dv">40</span>, <span class="dt">label=</span><span class="st">&quot;PC2&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pca1"></span>
<img src="DS_files/figure-html/pca1-1.png" alt="Principal Component Analysis (PCA)" width="100%" />
<p class="caption">
Figure 9.50: Principal Component Analysis (PCA)
</p>
</div>

<p>Here, it helps to know the terminologies used in <strong>SVD</strong> and <strong>PCA</strong>. We start by describing <strong>Principal Component</strong>. Geometrically, a <strong>Principal Component (PC1)</strong> is represented by a <strong>Principal axis</strong>, oriented in the direction that maximizes the variance (given two features). We also term this <strong>Principal direction</strong>. Data points are projected to this <strong>Principal axis</strong>. Then, the <strong>sum of squared distance (SSD)</strong> of all the projected points from the point of origin along the <strong>Principal axis</strong> is calculated. See below:</p>
<p><span class="math display">\[\begin{align}
\text{SSD}_{max} = \underset{ssd}{max} \left\{ \sum_i^n d_i^2 \right\}=  \underset{ssd}{max} \left\{d_1^2 + d_2^2 + d_3^2 + ... + d_n^2 \right\}
\end{align}\]</span></p>
<p>To get the maximum SSD, we may need to visualize the geometric representation of the <strong>Principal Component</strong> as it rotates around the point of origin until such that we find the maximum variance.</p>
<p>A second <strong>Principal Component (PC2)</strong> inherently gets created, which is also geometrically represented by a <strong>Principal axis</strong> that is orthogonal (or perpendicular) to the primary <strong>Principal axis (PC1)</strong>. Equivalently, we get the sum squared distance of the <strong>projected points</strong> from the point of origin along the <strong>orthogonal axis</strong> by using the SSD formula.</p>
<p><strong>Sixth</strong>, let us compute for the <strong>unit vectors</strong>. Note that the orientation of the segments that make up both <strong>PC1</strong> and<strong>PC2</strong> is described by the slopes of their respective lines (axis). We know that a slope consists of the <strong>rise</strong> over the <strong>run</strong>, which corresponds to the y vector and x vector (also called <strong>PC Scores</strong> or <strong>Loading Scores</strong> ).</p>
<p><span class="math display">\[\begin{align}
\text{slope} = \frac{\text{rise}}{\text{run}} = \frac{\text{y vector}}{\text{x vector}} 
\end{align}\]</span></p>
<p>In this specific case, we only focus on our two dimensions (e.g. two features).</p>

<div class="sourceCode" id="cb1111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1111-1" data-line-number="1"><span class="kw">colnames</span>(V) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;EigenVec for PC1&quot;</span>, <span class="st">&quot;EigenVec for PC2&quot;</span>)</a>
<a class="sourceLine" id="cb1111-2" data-line-number="2"><span class="kw">rownames</span>(V) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Score 1 (a)&quot;</span>, <span class="st">&quot;Score 2 (b)&quot;</span>)</a>
<a class="sourceLine" id="cb1111-3" data-line-number="3">V</a></code></pre></div>
<pre><code>##             EigenVec for PC1 EigenVec for PC2
## Score 1 (a)          -0.3467           0.9380
## Score 2 (b)          -0.9380          -0.3467</code></pre>

<p>The slope of our linear combinations (our PCs) becomes:</p>

<div class="sourceCode" id="cb1113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1113-1" data-line-number="1">pc1.slope =<span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">1</span>] </a>
<a class="sourceLine" id="cb1113-2" data-line-number="2">pc2.slope =<span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">2</span>] <span class="op">/</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">2</span>] </a>
<a class="sourceLine" id="cb1113-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;Slope for PC1&quot;</span> =<span class="st"> </span>pc1.slope, <span class="st">&quot;Slope for PC2&quot;</span> =<span class="st"> </span>pc2.slope)</a></code></pre></div>
<pre><code>## Slope for PC1 Slope for PC2 
##        0.3696       -2.7054</code></pre>

<p>Now suppose for <strong>PC1</strong>, we have a=-0.3467 and b=-0.938; therefore, a classic <strong>Pythagorean Theorem</strong> gives us the linear combination as <span class="math inline">\(c = \sqrt{a^2 + b^2} =\)</span> 1. This is the length (or magnitude value) of our Eigenvector for <strong>PC1</strong>.</p>
<p>To compute the <strong>unit vector</strong> of the x and y vectors, including the <strong>Eigenvector</strong>, we divide all the vector components by the <strong>Eigenvector</strong> length. This makes the <strong>Eigenvector</strong> a <strong>Unit Eigenvector</strong> equal to 1, e.g. <span class="math inline">\(\text{unit eigenvector } =\)</span> 1/1. The y and x unit vectors have -0.938 and -0.3467, respectively. It means that for every 0.3467 unit decrease of <strong>feature1</strong>, we get -0.938 unit increase of <strong>feature2</strong>.</p>
<pre><code>##       X-vec       Y-vec Eigenvector 
##     -0.3467     -0.9380      1.0000</code></pre>
<p>Similarly, for <strong>PC2</strong>, we have the following:</p>
<pre><code>##       X-vec       Y-vec Eigenvector 
##      0.9380     -0.3467      1.0000</code></pre>
<p>If an <strong>Eigenvector</strong> is scaled down to a <strong>unit vector</strong>, we call this <strong>Singular vector</strong> for the <strong>PC1</strong>. On the other hand, the <strong>Singular value</strong> for <strong>PC1</strong> is computed by taking the root square of its SSD.</p>
<p><span class="math display">\[\begin{align}
\text{singular value} = \sqrt{\text{SSD}}
\end{align}\]</span></p>
<p>Below is the implementation of the <strong>singular value</strong> computation in R code:</p>

<div class="sourceCode" id="cb1117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1117-1" data-line-number="1">SSD &lt;-<span class="st"> </span><span class="cf">function</span>(a, b, eigenvector) {</a>
<a class="sourceLine" id="cb1117-2" data-line-number="2">  s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1117-3" data-line-number="3">  a.unit =<span class="st"> </span>a <span class="op">/</span><span class="st"> </span>eigenvector</a>
<a class="sourceLine" id="cb1117-4" data-line-number="4">  b.unit =<span class="st"> </span>b <span class="op">/</span><span class="st"> </span>eigenvector</a>
<a class="sourceLine" id="cb1117-5" data-line-number="5">  c =<span class="st"> </span><span class="kw">sqrt</span>(a.unit<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>b.unit<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1117-6" data-line-number="6">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) { s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span>(c[i] <span class="op">-</span><span class="st"> </span><span class="dv">0</span>)<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb1117-7" data-line-number="7">  s  </a>
<a class="sourceLine" id="cb1117-8" data-line-number="8">}</a></code></pre></div>

<p>The <strong>singular values</strong> for <strong>PC1</strong> and <strong>PC2</strong> are the following:</p>

<div class="sourceCode" id="cb1118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1118-1" data-line-number="1">PC1.ssd =<span class="st"> </span><span class="kw">SSD</span>(pc1.x.proj, pc1.y.proj, pc1.eigenvector)</a>
<a class="sourceLine" id="cb1118-2" data-line-number="2">PC2.ssd =<span class="st"> </span><span class="kw">SSD</span>(pc2.x.proj, pc2.y.proj, pc2.eigenvector)</a>
<a class="sourceLine" id="cb1118-3" data-line-number="3">SV =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb1118-4" data-line-number="4">  <span class="dt">ssd =</span> <span class="kw">rbind</span>(PC1.ssd, PC2.ssd),</a>
<a class="sourceLine" id="cb1118-5" data-line-number="5">  <span class="dt">singular.value =</span> <span class="kw">rbind</span>(<span class="kw">sqrt</span>(PC1.ssd), <span class="kw">sqrt</span>(PC2.ssd)),</a>
<a class="sourceLine" id="cb1118-6" data-line-number="6">  <span class="dt">variance =</span> <span class="kw">rbind</span>(PC1.ssd<span class="op">/</span>(N<span class="dv">-1</span>), PC2.ssd<span class="op">/</span>(N<span class="dv">-1</span>)),</a>
<a class="sourceLine" id="cb1118-7" data-line-number="7">  <span class="dt">std.dev =</span> <span class="kw">rbind</span>(<span class="kw">sqrt</span>(PC1.ssd<span class="op">/</span>(N<span class="dv">-1</span>)), <span class="kw">sqrt</span>(PC2.ssd<span class="op">/</span>(N<span class="dv">-1</span>))),</a>
<a class="sourceLine" id="cb1118-8" data-line-number="8">  <span class="dt">proportion =</span> pca.model<span class="op">$</span>sdev <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(pca.model<span class="op">$</span>sdev) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb1118-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb1118-10" data-line-number="10"><span class="kw">rownames</span>(SV) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;PC1&quot;</span>, <span class="st">&quot;PC2&quot;</span>)</a>
<a class="sourceLine" id="cb1118-11" data-line-number="11">SV</a></code></pre></div>
<pre><code>##       ssd singular.value variance std.dev proportion
## PC1 25571          159.9   1065.4   32.64      53.28
## PC2 19662          140.2    819.3   28.62      46.72</code></pre>

<p>We can validate the <strong>standard deviation</strong> using the <strong>PCA.model</strong> we generated previously:</p>

<div class="sourceCode" id="cb1120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1120-1" data-line-number="1">pca.model<span class="op">$</span>sdev</a></code></pre></div>
<pre><code>## [1] 32.64 28.62</code></pre>

<p>For the proportionality of the <strong>Principal Components</strong>, we see that <strong>PC1</strong> has a higher proportion at 53.2796%. See Figure <a href="machinelearning1.html#fig:pcaproportion1">9.51</a>.</p>

<div class="sourceCode" id="cb1122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1122-1" data-line-number="1">PC.proportions =<span class="st"> </span>SV<span class="op">$</span>proportion</a>
<a class="sourceLine" id="cb1122-2" data-line-number="2"><span class="kw">barplot</span>(PC.proportions, <span class="dt">las=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1122-3" data-line-number="3">        <span class="dt">xlab=</span><span class="st">&quot;Principal Components&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Variance&quot;</span>,</a>
<a class="sourceLine" id="cb1122-4" data-line-number="4">        <span class="dt">col=</span><span class="st">&quot;white&quot;</span>,</a>
<a class="sourceLine" id="cb1122-5" data-line-number="5">        <span class="dt">names.arg=</span><span class="kw">paste</span>(<span class="st">&quot;PC&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(PC.proportions)))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcaproportion1"></span>
<img src="DS_files/figure-html/pcaproportion1-1.png" alt="PC Proportions" width="70%" />
<p class="caption">
Figure 9.51: PC Proportions
</p>
</div>

<p><strong>Seventh</strong>, for two features, we can rotate the respective <strong>Principal axis</strong> of their <strong>PCs</strong> to align them along the vertical and horizontal axes. The data is also rotated. See Figure <a href="machinelearning1.html#fig:pca2">9.52</a>.</p>

<div class="sourceCode" id="cb1123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1123-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">80</span>,<span class="dv">80</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">100</span>,<span class="dv">80</span>), </a>
<a class="sourceLine" id="cb1123-2" data-line-number="2">      <span class="dt">xlab=</span><span class="st">&quot;feature1 (X1)&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;feature2 (X2)&quot;</span>, </a>
<a class="sourceLine" id="cb1123-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Rotated Principal Components&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1123-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1123-5" data-line-number="5"><span class="co">### Plot the Points</span></a>
<a class="sourceLine" id="cb1123-6" data-line-number="6"><span class="kw">points</span>(PC, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">1.0</span>)</a>
<a class="sourceLine" id="cb1123-7" data-line-number="7"><span class="kw">points</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb1123-8" data-line-number="8"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb1123-9" data-line-number="9"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1123-10" data-line-number="10"><span class="co">### Legend and PC2 label</span></a>
<a class="sourceLine" id="cb1123-11" data-line-number="11"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, </a>
<a class="sourceLine" id="cb1123-12" data-line-number="12">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;Principal Axis (PC1)&quot;</span>, <span class="st">&quot;Principal Axis (PC2)&quot;</span>),</a>
<a class="sourceLine" id="cb1123-13" data-line-number="13">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>),  <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">16</span>, <span class="dv">20</span>, <span class="dv">20</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1123-14" data-line-number="14"><span class="kw">text</span>(<span class="dv">5</span>, <span class="dv">-80</span>, <span class="dt">label=</span><span class="st">&quot;(PC2)&quot;</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1123-15" data-line-number="15"><span class="kw">text</span>(<span class="dv">60</span>,  <span class="dv">5</span>, <span class="dt">label=</span><span class="st">&quot;(PC1)&quot;</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pca2"></span>
<img src="DS_files/figure-html/pca2-1.png" alt="Rotated Principal Components" width="70%" />
<p class="caption">
Figure 9.52: Rotated Principal Components
</p>
</div>

<p>It is important to note that, for a dataset with three features, we get three <strong>Principal axes</strong> that are all orthogonal to each other; thus, geometrically, we see a 3D representation of the <strong>Principal components</strong>.</p>
<p>For larger datasets with more features, we can quickly generate a covariance matrix derived from our original dataset to review deviations between features quickly. Additionally, because covariance is commutative, the matrix is therefore symmetric in which the upper and lower triangular areas are equal. Because of this, we can use <strong>Cholesky Decomposition</strong>, which we introduced in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>) to extract the upper portion of the matrix. </p>
<div class="sourceCode" id="cb1124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1124-1" data-line-number="1"><span class="co"># Get the covariance, then transform to cholesky matrix</span></a>
<a class="sourceLine" id="cb1124-2" data-line-number="2">(<span class="dt">chol.matrix =</span> <span class="kw">chol</span>( <span class="kw">cov</span>(dataset)) )</a></code></pre></div>
<pre><code>##          feature1 feature2 feature3 feature4
## feature1    29.14    2.748   -2.249   0.6465
## feature2     0.00   32.067    3.051  -3.8405
## feature3     0.00    0.000   30.959   8.1306
## feature4     0.00    0.000    0.000  27.1806</code></pre>
<p>We can then use <strong>eigen(.)</strong> function as before. However, here, we show two functions that we can use for <strong>PCA</strong>, namely <strong>prcomp(.)</strong> and <strong>princomp(.)</strong>.</p>
<div class="sourceCode" id="cb1126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1126-1" data-line-number="1">(<span class="dt">pca.model =</span> <span class="kw">prcomp</span>(dataset, <span class="dt">tol=</span><span class="fl">0.10</span>, <span class="dt">center =</span> <span class="ot">TRUE</span>, <span class="dt">scale =</span> <span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>## Standard deviations (1, .., p=4):
## [1] 1.1305 1.0455 0.9975 0.7962
## 
## Rotation (n x k) = (4 x 4):
##              PC1      PC2     PC3     PC4
## feature1  0.1670  0.46545 -0.8299 -0.2584
## feature2  0.1668  0.81206  0.3543  0.4327
## feature3 -0.6672  0.35077  0.2515 -0.6070
## feature4 -0.7065 -0.02958 -0.3500  0.6144</code></pre>
<p>Below is the summary of the <strong>PCA</strong> outcome:</p>
<div class="sourceCode" id="cb1128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1128-1" data-line-number="1"><span class="kw">summary</span>(pca.model)</a></code></pre></div>
<pre><code>## Importance of components:
##                         PC1   PC2   PC3   PC4
## Standard deviation     1.13 1.046 0.997 0.796
## Proportion of Variance 0.32 0.273 0.249 0.158
## Cumulative Proportion  0.32 0.593 0.842 1.000</code></pre>
<p>We also use <strong>screeplot(.)</strong> function from <strong>graphics</strong> library to visualize <strong>PCA</strong>. See Figure <a href="machinelearning1.html#fig:pcaproportion2">9.53</a> for the screeplot.</p>
<div class="sourceCode" id="cb1130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1130-1" data-line-number="1"><span class="kw">require</span>(graphics)</a>
<a class="sourceLine" id="cb1130-2" data-line-number="2"><span class="kw">screeplot</span>(pca.model, <span class="dt">npcs =</span> <span class="dv">5</span>, <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;lines&quot;</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcaproportion2"></span>
<img src="DS_files/figure-html/pcaproportion2-1.png" alt="PC Proportions" width="70%" />
<p class="caption">
Figure 9.53: PC Proportions
</p>
</div>
<p>Alternatively, we can use <strong>princomp(.)</strong> function. See Figure <a href="machinelearning1.html#fig:pcaproportion2">9.53</a> for the screeplot.</p>
<div class="sourceCode" id="cb1131"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1131-1" data-line-number="1">std.dataset =<span class="st"> </span><span class="kw">scale</span>(dataset, <span class="dt">center=</span><span class="ot">TRUE</span>, <span class="dt">scale=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1131-2" data-line-number="2">(pca.model &lt;-<span class="st"> </span><span class="kw">princomp</span>(std.dataset, <span class="dt">cor =</span> <span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>## Call:
## princomp(x = std.dataset, cor = TRUE)
## 
## Standard deviations:
## Comp.1 Comp.2 Comp.3 Comp.4 
## 1.1305 1.0455 0.9975 0.7962 
## 
##  4  variables and  25 observations.</code></pre>
<div class="sourceCode" id="cb1133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1133-1" data-line-number="1"><span class="kw">screeplot</span>(pca.model, <span class="dt">npcs =</span> <span class="dv">5</span>, <span class="dt">type =</span> <span class="st">&quot;lines&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcaproportion3"></span>
<img src="DS_files/figure-html/pcaproportion3-1.png" alt="PC Proportions" width="70%" />
<p class="caption">
Figure 9.54: PC Proportions
</p>
</div>
<p><strong>Independent Component Analysis (ICA)</strong> is similar to <strong>PCA</strong>. We leave readers to investigate further on this topic.</p>
</div>
<div id="linear-discriminant-analysis-lda" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.4</span> Linear Discriminant Analysis (LDA)  <a href="machinelearning1.html#linear-discriminant-analysis-lda" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We extend the concept of <strong>Dimensionality Reduction</strong> compared to <strong>PCA</strong> by introducing <strong>Linear Discriminant Analysis (LDA)</strong>. With <strong>LDA</strong>, we assume that our dataset is classified (or labeled). Our goal is to determine the maximum separation of the classes. In other words, we <strong>discriminate</strong> data points such that they are separated into groups relative to their respective <strong>moments</strong>, e.g., mean (<span class="math inline">\(\mu\)</span>) and variance (<span class="math inline">\(\sigma\)</span>). Here, we use Figure <a href="machinelearning1.html#fig:lda">9.55</a> to illustrate the idea.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lda"></span>
<img src="lda.png" alt="Linear Discriminant Analysis (LDA)" width="70%" />
<p class="caption">
Figure 9.55: Linear Discriminant Analysis (LDA)
</p>
</div>
<p>The figure shows an <strong>Eigenvector</strong> representing the space unto which data points are projected. The projected data points are assumed to follow a multimodal distribution with multinomial outcomes. In the case of our figure, we see a projection of two <strong>Gaussian distribution</strong> with respective means (<span class="math inline">\(\mu_1, \mu_2\)</span>) and covariances (<span class="math inline">\(\sigma_1, \sigma_2\)</span>). In <strong>LDA</strong>, similar to <strong>PCA</strong>, our goal is to find the best <strong>fit</strong>. However, a <strong>best fit</strong> represents a line (a projected vector) with maximum separability and a minimum overlap of classes.</p>
<p>Note that in <strong>PCA</strong>, our goal is to derive the <strong>Principal components</strong>, e.g. <strong>PC1</strong> and <strong>PC2</strong>. In <strong>LDA</strong>, our goal is to derive the <strong>Linear Discriminants</strong>, e.g. <strong>LD1</strong> and <strong>LD2</strong>.</p>
<p>To illustrate, we use the <strong>iris</strong> dataset, which is common in explaining <strong>LDA</strong>.</p>

<div class="sourceCode" id="cb1134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1134-1" data-line-number="1">features =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>,<span class="st">&quot;Sepal.Width&quot;</span>,<span class="st">&quot;Petal.Length&quot;</span>,<span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1134-2" data-line-number="2">lda.dataset =<span class="st"> </span>iris[,features]</a>
<a class="sourceLine" id="cb1134-3" data-line-number="3">lda.dataset =<span class="st"> </span><span class="kw">as.matrix</span>(lda.dataset)</a>
<a class="sourceLine" id="cb1134-4" data-line-number="4">N =<span class="st"> </span><span class="kw">nrow</span>(lda.dataset)</a>
<a class="sourceLine" id="cb1134-5" data-line-number="5"><span class="kw">head</span>(lda.dataset) <span class="co"># show only first 5 observations</span></a></code></pre></div>
<pre><code>##      Sepal.Length Sepal.Width Petal.Length Petal.Width
## [1,]          5.1         3.5          1.4         0.2
## [2,]          4.9         3.0          1.4         0.2
## [3,]          4.7         3.2          1.3         0.2
## [4,]          4.6         3.1          1.5         0.2
## [5,]          5.0         3.6          1.4         0.2
## [6,]          5.4         3.9          1.7         0.4</code></pre>

<p>Here, our <strong>iris</strong> dataset is labeled - in other words, each observation is already classified as either <strong>Setosa</strong>, <strong>Versicolor</strong>, or <strong>Virginica</strong> depending on the <strong>Sepal</strong> and <strong>Petal</strong> properties.</p>

<div class="sourceCode" id="cb1136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1136-1" data-line-number="1">labels =<span class="st"> </span><span class="kw">levels</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)])</a>
<a class="sourceLine" id="cb1136-2" data-line-number="2">K =<span class="st"> </span><span class="kw">length</span>(labels)</a>
<a class="sourceLine" id="cb1136-3" data-line-number="3">labels</a></code></pre></div>
<pre><code>## [1] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;</code></pre>

<p><strong>First</strong>, using the <strong>Fisher Linear Discriminant</strong> method, our initial step is to compute for the mean of each features based on labels:</p>

<p><span class="math display">\[\begin{align}
\mu_k = \frac{1}{n_k} \sum_{x \in D_k} x
\ \ \ \ \ \ \ \ where:\ 
\begin{array}{ll}
\mathbf{k} &amp; \text{kth class}\\
\mathbf{D} &amp; \text{dataset}\\
\end{array} \label{eqn:eqnnumber401}
\end{align}\]</span></p>
<div class="sourceCode" id="cb1138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1138-1" data-line-number="1">mu.k =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>K, <span class="dt">ncol=</span><span class="kw">length</span>(features), <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1138-2" data-line-number="2"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1138-3" data-line-number="3">  idx =<span class="st"> </span><span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)] <span class="op">==</span><span class="st"> </span>labels[k])</a>
<a class="sourceLine" id="cb1138-4" data-line-number="4">  mu.k[k,] =<span class="st"> </span><span class="kw">apply</span>(lda.dataset[idx,] , <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1138-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1138-6" data-line-number="6"><span class="kw">colnames</span>(mu.k) =<span class="st"> </span>features</a>
<a class="sourceLine" id="cb1138-7" data-line-number="7"><span class="kw">rownames</span>(mu.k) =<span class="st"> </span>labels</a>
<a class="sourceLine" id="cb1138-8" data-line-number="8">mu.k</a></code></pre></div>
<pre><code>##            Sepal.Length Sepal.Width Petal.Length Petal.Width
## setosa            5.006       3.428        1.462       0.246
## versicolor        5.936       2.770        4.260       1.326
## virginica         6.588       2.974        5.552       2.026</code></pre>

<p>Note that the whole dataset assumes to follow a multi-modal Gaussian distribution with three modes, representing three means, e.g., <span class="math inline">\(\mu_1, \mu_2, \mu_3\)</span>, and three variances. Our goal is to project the data points to lower-dimensional spaces - a set of projections (represented by eigenvectors).</p>
<p><strong>Second</strong>, we compute for two types of <strong>variances</strong>. The first variance is the <strong>variance within each class</strong>. The second variance is the <strong>variance between classes</strong>. The idea is to maximize the separation of classes. That can be achieved if the variance between classes is larger than the variance within each class, allowing each data point to be closer together in a class and further away from other classes.</p>
<p>Here, we compute for the <strong>within-class scatter matrix</strong> <span class="math inline">\(\mathbf{S_w}\)</span> like so:</p>

<p><span class="math display">\[\begin{align}
\mathbf{S_w} = \sum_{k=1}^{K} S_k
\ \ \ \ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ \ 
S_k = \sum_{x \in D_k} \left(x - \mu_k\right)\left(x - \mu_k\right)^T 
\end{align}\]</span></p>
<div class="sourceCode" id="cb1140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1140-1" data-line-number="1">within.class &lt;-<span class="st"> </span><span class="cf">function</span>(x, labels) {</a>
<a class="sourceLine" id="cb1140-2" data-line-number="2">  s_k =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1140-3" data-line-number="3">  <span class="cf">for</span> (k <span class="cf">in</span> labels) {</a>
<a class="sourceLine" id="cb1140-4" data-line-number="4">    idx =<span class="st"> </span><span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)] <span class="op">==</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1140-5" data-line-number="5">    mu =<span class="st"> </span><span class="kw">apply</span>(x[idx,] , <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1140-6" data-line-number="6">    s_i =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1140-7" data-line-number="7">    <span class="cf">for</span> (i <span class="cf">in</span> idx) {</a>
<a class="sourceLine" id="cb1140-8" data-line-number="8">      s_i =<span class="st"> </span>s_i <span class="op">+</span><span class="st">  </span>(x[i,] <span class="op">-</span><span class="st"> </span>mu) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x[i,] <span class="op">-</span><span class="st"> </span>mu) </a>
<a class="sourceLine" id="cb1140-9" data-line-number="9">    }</a>
<a class="sourceLine" id="cb1140-10" data-line-number="10">    s_k =<span class="st"> </span>s_k <span class="op">+</span><span class="st"> </span>s_i</a>
<a class="sourceLine" id="cb1140-11" data-line-number="11">  }</a>
<a class="sourceLine" id="cb1140-12" data-line-number="12">  s_k</a>
<a class="sourceLine" id="cb1140-13" data-line-number="13">}</a>
<a class="sourceLine" id="cb1140-14" data-line-number="14">S_w =<span class="st"> </span><span class="kw">within.class</span>(lda.dataset, labels)</a>
<a class="sourceLine" id="cb1140-15" data-line-number="15"><span class="kw">rownames</span>(S_w) =<span class="st"> </span>features</a>
<a class="sourceLine" id="cb1140-16" data-line-number="16">S_w</a></code></pre></div>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length       38.956      13.630       24.625       5.645
## Sepal.Width        13.630      16.962        8.121       4.808
## Petal.Length       24.625       8.121       27.223       6.272
## Petal.Width         5.645       4.808        6.272       6.157</code></pre>

<p><strong>Third</strong>, we compute for the <strong>between-class scatter matrix</strong> <span class="math inline">\(\mathbf{S_b}\)</span> with the given total mean per class.</p>

<p><span class="math display">\[\begin{align}
\mathbf{S_b} = \sum_{k=1}^{K} n_k  \left(\mu_k - \mu\right)\left(\mu_k - \mu\right)^T
\ \ \ \ \ \ where: \ \ \mu\ \text{is total mean per class}
\end{align}\]</span></p>
<div class="sourceCode" id="cb1142"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1142-1" data-line-number="1">(<span class="dt">mu =</span> <span class="kw">apply</span>(lda.dataset, <span class="dv">2</span>, mean)) <span class="co"># mean(lda.dataset))</span></a></code></pre></div>
<pre><code>## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##        5.843        3.057        3.758        1.199</code></pre>
<div class="sourceCode" id="cb1144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1144-1" data-line-number="1">between.class &lt;-<span class="st"> </span><span class="cf">function</span>(x, labels) {</a>
<a class="sourceLine" id="cb1144-2" data-line-number="2">  s_k =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1144-3" data-line-number="3">  <span class="cf">for</span> (k <span class="cf">in</span> labels) {</a>
<a class="sourceLine" id="cb1144-4" data-line-number="4">    idx =<span class="st"> </span><span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)] <span class="op">==</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1144-5" data-line-number="5">    n.k =<span class="st"> </span><span class="kw">length</span>(idx)</a>
<a class="sourceLine" id="cb1144-6" data-line-number="6">    mu.k =<span class="st"> </span><span class="kw">apply</span>(lda.dataset[idx,] , <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1144-7" data-line-number="7">    s_k =<span class="st"> </span>s_k <span class="op">+</span><span class="st"> </span>n.k <span class="op">*</span><span class="st"> </span>(mu.k <span class="op">-</span><span class="st"> </span>mu) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(mu.k <span class="op">-</span><span class="st"> </span>mu)</a>
<a class="sourceLine" id="cb1144-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb1144-9" data-line-number="9">  s_k</a>
<a class="sourceLine" id="cb1144-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb1144-11" data-line-number="11">S_b =<span class="st"> </span><span class="kw">between.class</span>(lda.dataset, labels)</a>
<a class="sourceLine" id="cb1144-12" data-line-number="12"><span class="kw">rownames</span>(S_b) =<span class="st"> </span>features</a>
<a class="sourceLine" id="cb1144-13" data-line-number="13">S_b</a></code></pre></div>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length        63.21      -19.95       165.25       71.28
## Sepal.Width        -19.95       11.34       -57.24      -22.93
## Petal.Length       165.25      -57.24       437.10      186.77
## Petal.Width         71.28      -22.93       186.77       80.41</code></pre>

<p><strong>Fourth</strong>, to derive the best fit, we need to minimize the following objective function:</p>

<p><span class="math display">\[\begin{align}
\mathcal{J}(\mathbf{\bar{v}}) = \frac{(\bar{\mu}_1 - \bar{\mu}_2)^2}{s_1^2 + s_2^2}  
= \frac{\mathbf{\vec{v}}^T S_b \mathbf{\vec{v}}}{\mathbf{\vec{v}}^T S_w \mathbf{\vec{v}}} \label{eqn:eqnnumber150a}
\end{align}\]</span>
</p>
<p>For a good understanding of Equation (<span class="math inline">\(\ref{eqn:eqnnumber150a}\)</span>) and to explain its formulation, let us consider discussing projections and how to find the best projection. For that, we start with the idea that any data point can be projected in the direction of a vector, e.g., <span class="math inline">\(\mathbf{\vec{v}}\)</span>, using the following formula:</p>
<p><span class="math display">\[\begin{align}
\tilde{x}_i^{(\text{projected point)}} = \mathbf{\vec{v}}^T x_i 
\end{align}\]</span></p>
<p>A class of data points has a mean that can also be projected like so:</p>
<p><span class="math display">\[\begin{align}
\tilde{\mu}_k^{(\text{projected mean)}} = \mathbf{\vec{v}}^T \mu_k  
\end{align}\]</span></p>
<p>where <strong>k</strong> is a class and <span class="math inline">\(\mathbf{\vec{v}}\)</span> is the projected line.</p>
<p>We can compare such projected mean <span class="math inline">\(\tilde{\mu}_1\)</span> of a class <strong>k=1</strong> with the projected mean <span class="math inline">\(\tilde{\mu}_2\)</span> of another class <strong>k=2</strong> in terms of distance. The best projection is based on the largest distance between classes (with respect to their means).</p>
<p>Next, we also account for the variance of each class using the following formula (with basic derivation included):</p>

<p><span class="math display">\[\begin{align}
\tilde{s}_k^2 &amp;= \sum_{x \in D_k} \left(\tilde{x}_i - \tilde{\mu}_k\right)^2\\
&amp;=  \sum_{x \in D_k} \left(\mathbf{\vec{v}}^T x_i - \mathbf{\vec{v}}^T \mu_k\right)^2\\
&amp;=\sum_{x \in D_k} \mathbf{\vec{v}}^T  \left(x - \mu_k\right)\left(x - \mu_k\right)^T \mathbf{\vec{v}}\\
&amp; = \mathbf{\vec{v}}^T S_k \mathbf{\vec{v}}
\end{align}\]</span>
</p>
<p>Therefore:</p>
<p><span class="math display">\[\begin{align}
\sum \tilde{s}_k^2 = \mathbf{\vec{v}}^T S_w \mathbf{\vec{v}} 
\end{align}\]</span></p>
<p>Similarly, starting with a 2 class configuration, we can derive the following:</p>
<p><span class="math display">\[\begin{align}
\left(\tilde{\mu}_1 - \tilde{\mu}_2\right)^2 = \left( \mathbf{\vec{v}}^T \mu_1 - \mathbf{\vec{v}}^T\tilde{\mu}_2\right)^2 = \mathbf{\vec{v}}^T S_b \mathbf{\vec{v}} 
\end{align}\]</span></p>
<p>Now to minimize the objective function, namely <span class="math inline">\(\mathbf{\mathcal{J}}(\mathbf{\vec{v}})\)</span>, with respect to vector <span class="math inline">\(\mathbf{\vec{v}}\)</span>, we use partial derivatives:</p>

<p><span class="math display">\[\begin{align}
\frac{ \partial \mathbf{\mathcal{J}}(\mathbf{\vec{v}}) } {\partial \mathbf{\vec{v}}}
=  
\frac{ \partial  } {\partial \mathbf{\vec{v}}}
\left[\frac{\mathbf{\vec{v}}^T S_b \mathbf{\vec{v}}}{\mathbf{\vec{v}}^T S_w \mathbf{\vec{v}}}\right]
\end{align}\]</span>
</p>
<p>Setting the derivative of the objective function with respect to <span class="math inline">\(\mathbf{\vec{v}}\)</span> to zero, we get:</p>

<p><span class="math display">\[\begin{align}
\underbrace{S_b \mathbf{\vec{v}} = \lambda S_w \mathbf{\vec{v}} }_{\text{Generalized Eigenvalue Problem}}
\ \ \ \ \leftarrow \ \ \ \ \ \
S_b \mathbf{\vec{v}} - \lambda S_w \mathbf{\vec{v}} = 0
\ \ \ \ \ \ \ where\ \  \lambda = \frac{\mathbf{\vec{v}}^T S_b \mathbf{\vec{v}}}{\mathbf{\vec{v}}^T S_w \mathbf{\vec{v}}}
\end{align}\]</span>
</p>
<p>For a two-class configuration, the derived projection becomes:</p>
<p><span class="math display">\[\begin{align}
\mathbf{\vec{v}} = S_w^{-1} \left(\mu_1 - \mu_2\right)
\end{align}\]</span></p>
<p>For multi-class configuration, recall in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>) the <strong>Eigen equation</strong> and the derivation of the <strong>Eigenvalue</strong> <span class="math inline">\(\lambda\)</span> using determinant and echelon form:</p>
<p><span class="math display">\[\begin{align}
(A - \lambda I) = 0\ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ \ A \mathbf{\vec{v}} = \lambda \mathbf{\vec{v}}
\end{align}\]</span></p>
<p>Equivalently, we have the following expression:</p>
<p><span class="math display">\[\begin{align}
\underbrace{(S_w^{-1}S_b )\mathbf{\vec{v}} = \lambda \mathbf{\vec{v}}}_{A \mathbf{\vec{v}} = \lambda \mathbf{\vec{v}}}
\ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ \ S_b \mathbf{\vec{v}} = \lambda S_w \mathbf{\vec{v}} 
\end{align}\]</span></p>
<p><strong>Fifth</strong>, to then find the projection of datasets to the new axis, we use the following expression:</p>
<p><span class="math display">\[\begin{align}
y = \mathbf{\vec{v}}^T x 
\end{align}\]</span></p>
<p>Let us compute for the eigenvalues and corresponding eigenvectors (we choose the two highest eigenvalues):</p>
<div class="sourceCode" id="cb1146"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1146-1" data-line-number="1">e =<span class="st"> </span><span class="kw">eigen</span>(<span class="kw">solve</span>(S_w) <span class="op">%*%</span><span class="st"> </span>S_b)</a>
<a class="sourceLine" id="cb1146-2" data-line-number="2">idx =<span class="st"> </span><span class="kw">sort</span>( e<span class="op">$</span>values, <span class="dt">index.return =</span> <span class="ot">TRUE</span>, <span class="dt">decreasing=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1146-3" data-line-number="3">(<span class="dt">eigenvalues =</span> idx<span class="op">$</span>x)</a></code></pre></div>
<pre><code>## [1]  3.219e+01  2.854e-01 -2.801e-15 -4.881e-15</code></pre>
<div class="sourceCode" id="cb1148"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1148-1" data-line-number="1">(<span class="dt">eigenvectors =</span>  e<span class="op">$</span>vectors[idx<span class="op">$</span>ix,]) </a></code></pre></div>
<pre><code>##         [,1]      [,2]      [,3]    [,4]
## [1,] -0.2087 -0.006532  0.850126 -0.6859
## [2,] -0.3862 -0.586611 -0.373448  0.4384
## [3,]  0.7074 -0.769453  0.002199 -0.3476
## [4,]  0.5540  0.252562 -0.371237  0.4653</code></pre>
<p>Finally, to plot, let us compute the projections of the datasets (see Figure <a href="machinelearning1.html#fig:ldareduction1">9.56</a>).</p>

<div class="sourceCode" id="cb1150"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1150-1" data-line-number="1">coeff =<span class="st"> </span>eigenvectors</a>
<a class="sourceLine" id="cb1150-2" data-line-number="2">LD1 =<span class="st"> </span>coeff[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1150-3" data-line-number="3"><span class="st">      </span>coeff[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Width&quot;</span>)]  <span class="op">+</span></a>
<a class="sourceLine" id="cb1150-4" data-line-number="4"><span class="st">      </span>coeff[<span class="dv">3</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1150-5" data-line-number="5"><span class="st">      </span>coeff[<span class="dv">4</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Width&quot;</span>)]</a>
<a class="sourceLine" id="cb1150-6" data-line-number="6">LD2 =<span class="st"> </span>coeff[<span class="dv">1</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1150-7" data-line-number="7"><span class="st">      </span>coeff[<span class="dv">2</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Width&quot;</span>)]  <span class="op">+</span></a>
<a class="sourceLine" id="cb1150-8" data-line-number="8"><span class="st">      </span>coeff[<span class="dv">3</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1150-9" data-line-number="9"><span class="st">      </span>coeff[<span class="dv">4</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Width&quot;</span>)]</a></code></pre></div>
<div class="sourceCode" id="cb1151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1151-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(LD1), <span class="dt">ylim=</span><span class="kw">range</span>(LD2),</a>
<a class="sourceLine" id="cb1151-2" data-line-number="2">     <span class="dt">ylab=</span><span class="st">&quot;LD2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;LD1&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Linear Discriminant Analysis&quot;</span>)</a>
<a class="sourceLine" id="cb1151-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1151-4" data-line-number="4">col =<span class="st"> </span><span class="kw">rep</span>(<span class="st">&quot;&quot;</span>, <span class="kw">length</span>(LD1))</a>
<a class="sourceLine" id="cb1151-5" data-line-number="5">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;setosa&quot;</span>) ] =<span class="st"> &quot;red&quot;</span></a>
<a class="sourceLine" id="cb1151-6" data-line-number="6">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;versicolor&quot;</span>) ] =<span class="st"> &quot;blue&quot;</span></a>
<a class="sourceLine" id="cb1151-7" data-line-number="7">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;virginica&quot;</span>) ] =<span class="st"> &quot;green&quot;</span></a>
<a class="sourceLine" id="cb1151-8" data-line-number="8"><span class="kw">points</span>(LD1,LD2, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span>col)</a>
<a class="sourceLine" id="cb1151-9" data-line-number="9"><span class="kw">legend</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="dv">-5</span>,   <span class="kw">c</span>( <span class="st">&quot;Setosa&quot;</span>, <span class="st">&quot;Versicolor&quot;</span>, <span class="st">&quot;Virginica&quot;</span> ),</a>
<a class="sourceLine" id="cb1151-10" data-line-number="10">     <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>), </a>
<a class="sourceLine" id="cb1151-11" data-line-number="11">     <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>,   <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ldareduction1"></span>
<img src="DS_files/figure-html/ldareduction1-1.png" alt="Linear Discriminant Analysis" width="70%" />
<p class="caption">
Figure 9.56: Linear Discriminant Analysis
</p>
</div>

<p><strong>Sixth</strong>, to validate, we use <strong>lda(.)</strong> from library <strong>MASS</strong>:</p>

<div class="sourceCode" id="cb1152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1152-1" data-line-number="1"><span class="kw">library</span>(MASS)</a>
<a class="sourceLine" id="cb1152-2" data-line-number="2">(<span class="dt">lda.model =</span> <span class="kw">lda</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>iris, <span class="dt">prior=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)<span class="op">/</span><span class="dv">3</span>, </a>
<a class="sourceLine" id="cb1152-3" data-line-number="3">                 <span class="dt">method=</span><span class="st">&quot;moment&quot;</span>)) </a></code></pre></div>
<pre><code>## Call:
## lda(Species ~ ., data = iris, prior = c(1, 1, 1)/3, method = &quot;moment&quot;)
## 
## Prior probabilities of groups:
##     setosa versicolor  virginica 
##     0.3333     0.3333     0.3333 
## 
## Group means:
##            Sepal.Length Sepal.Width Petal.Length Petal.Width
## setosa            5.006       3.428        1.462       0.246
## versicolor        5.936       2.770        4.260       1.326
## virginica         6.588       2.974        5.552       2.026
## 
## Coefficients of linear discriminants:
##                  LD1     LD2
## Sepal.Length  0.8294  0.0241
## Sepal.Width   1.5345  2.1645
## Petal.Length -2.2012 -0.9319
## Petal.Width  -2.8105  2.8392
## 
## Proportion of trace:
##    LD1    LD2 
## 0.9912 0.0088</code></pre>

<p>The result gives us the <strong>Coefficients of the linear discriminants</strong> and the <strong>LD1</strong> and <strong>LD2</strong>. The calculations of <strong>LD1</strong> and <strong>LD2</strong> are based on the <strong>Coefficients</strong>:</p>

<div class="sourceCode" id="cb1154"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1154-1" data-line-number="1">coeff =<span class="st"> </span>lda.model<span class="op">$</span>scaling</a>
<a class="sourceLine" id="cb1154-2" data-line-number="2">LD1 =<span class="st"> </span>coeff[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1154-3" data-line-number="3"><span class="st">      </span>coeff[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Width&quot;</span>)]  <span class="op">+</span></a>
<a class="sourceLine" id="cb1154-4" data-line-number="4"><span class="st">      </span>coeff[<span class="dv">3</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1154-5" data-line-number="5"><span class="st">      </span>coeff[<span class="dv">4</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Width&quot;</span>)]</a>
<a class="sourceLine" id="cb1154-6" data-line-number="6">LD2 =<span class="st"> </span>coeff[<span class="dv">1</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1154-7" data-line-number="7"><span class="st">      </span>coeff[<span class="dv">2</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Width&quot;</span>)]  <span class="op">+</span></a>
<a class="sourceLine" id="cb1154-8" data-line-number="8"><span class="st">      </span>coeff[<span class="dv">3</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1154-9" data-line-number="9"><span class="st">      </span>coeff[<span class="dv">4</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Width&quot;</span>)]</a></code></pre></div>

<p>We can then use the result to plot a 2D representation of our dataset (see Figure <a href="machinelearning1.html#fig:ldareduction2">9.57</a>):</p>

<div class="sourceCode" id="cb1155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1155-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(LD1), <span class="dt">ylim=</span><span class="kw">range</span>(LD2),</a>
<a class="sourceLine" id="cb1155-2" data-line-number="2">     <span class="dt">ylab=</span><span class="st">&quot;LD2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;LD1&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Linear Discriminant Analysis&quot;</span>)</a>
<a class="sourceLine" id="cb1155-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1155-4" data-line-number="4">col =<span class="st"> </span><span class="kw">rep</span>(<span class="st">&quot;&quot;</span>, <span class="kw">length</span>(LD1))</a>
<a class="sourceLine" id="cb1155-5" data-line-number="5">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;setosa&quot;</span>) ] =<span class="st"> &quot;red&quot;</span></a>
<a class="sourceLine" id="cb1155-6" data-line-number="6">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;versicolor&quot;</span>) ] =<span class="st"> &quot;blue&quot;</span></a>
<a class="sourceLine" id="cb1155-7" data-line-number="7">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;virginica&quot;</span>) ] =<span class="st"> &quot;green&quot;</span></a>
<a class="sourceLine" id="cb1155-8" data-line-number="8"><span class="kw">points</span>(LD1,LD2, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span>col)</a>
<a class="sourceLine" id="cb1155-9" data-line-number="9"><span class="kw">legend</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">9</span>,   <span class="kw">c</span>( <span class="st">&quot;Setosa&quot;</span>, <span class="st">&quot;Versicolor&quot;</span>, <span class="st">&quot;Virginica&quot;</span> ),</a>
<a class="sourceLine" id="cb1155-10" data-line-number="10">     <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>), </a>
<a class="sourceLine" id="cb1155-11" data-line-number="11">     <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>,   <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ldareduction2"></span>
<img src="DS_files/figure-html/ldareduction2-1.png" alt="Linear Discriminant Analysis" width="70%" />
<p class="caption">
Figure 9.57: Linear Discriminant Analysis
</p>
</div>

<p>Note that <strong>LDA</strong>, as shown in the figure, is also used as a basic <strong>classifier</strong>.</p>
<p>For non-linear discriminants, we leave readers to investigate <strong>Quadratic Discriminant Analysis (QDA)</strong>.</p>
</div>
<div id="feature-construction" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.5</span> Feature Construction <a href="machinelearning1.html#feature-construction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Feature Construction</strong> is a method to create or filter features. It helps to discuss this in the context of pattern detection and pattern recognition, especially if working with images, texts, sounds, and signals. In this literature, we may also regard this <strong>Feature Engineering</strong> component as <strong>Feature Extraction by Construction</strong>.</p>
<p>To illustrate, let us use the MNIST database made available by Y. LeCun, C. Cortes, and C. Burges, which contains over 60,000 grayscale images of handwritten digits. The database comes with four files with the following contents: a training set of images, a training set of labels, a test set of images, and a test set of labels.</p>
<p>In our case, we use the training set to exemplify.</p>
<p><strong>First</strong>, we read the file that contains the digit labels.</p>

<div class="sourceCode" id="cb1156"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1156-1" data-line-number="1">minst_file =<span class="st"> </span><span class="kw">paste0</span>(dir, <span class="st">&quot;train-labels-idx1-ubyte&quot;</span>)</a>
<a class="sourceLine" id="cb1156-2" data-line-number="2">minst.handler =<span class="st"> </span><span class="kw">file</span>( minst_file, <span class="st">&quot;rb&quot;</span>)</a>
<a class="sourceLine" id="cb1156-3" data-line-number="3">magic.number   =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="kw">integer</span>(), <span class="dt">n=</span><span class="dv">1</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1156-4" data-line-number="4">image.count    =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="kw">integer</span>(), <span class="dt">n=</span><span class="dv">1</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1156-5" data-line-number="5">image.labels =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="kw">integer</span>(), <span class="dt">size=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1156-6" data-line-number="6">                       <span class="dt">n=</span> image.count, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1156-7" data-line-number="7"><span class="kw">close</span>(minst.handler)</a>
<a class="sourceLine" id="cb1156-8" data-line-number="8"><span class="kw">c</span>(<span class="st">&quot;Magic Number&quot;</span>  =<span class="st"> </span>magic.number,  <span class="st">&quot;Number of Images&quot;</span>    =<span class="st"> </span>image.count)</a></code></pre></div>
<pre><code>##     Magic Number Number of Images 
##             2049            60000</code></pre>

<p>We display the first 20 digit labels:</p>
<div class="sourceCode" id="cb1158"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1158-1" data-line-number="1">image.labels[<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>]</a></code></pre></div>
<pre><code>##  [1] 5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9</code></pre>
<p>Because the digit images are collected randomly, let us find the first occurrence of digits 0 through 9 in ascending order.</p>
<div class="sourceCode" id="cb1160"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1160-1" data-line-number="1">digits =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">9</span>)</a>
<a class="sourceLine" id="cb1160-2" data-line-number="2">( <span class="dt">indices.of.1st.occurrences =</span> <span class="kw">match</span>(digits, image.labels) )</a></code></pre></div>
<pre><code>##  [1]  2  4  6  8  3  1 14 16 18  5</code></pre>
<p><strong>Second</strong>, the number of digit images and the constant size of the images are encoded into the first few bytes of the MNIST database. Therefore, we need to extract the information as so:</p>

<div class="sourceCode" id="cb1162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1162-1" data-line-number="1">minst_file =<span class="st"> </span><span class="kw">paste0</span>(dir, <span class="st">&quot;train-images-idx3-ubyte&quot;</span>)</a>
<a class="sourceLine" id="cb1162-2" data-line-number="2">minst.handler =<span class="st"> </span><span class="kw">file</span>( minst_file, <span class="st">&quot;rb&quot;</span>)</a>
<a class="sourceLine" id="cb1162-3" data-line-number="3"><span class="co"># Get metadata ( number of images, rows, columns)</span></a>
<a class="sourceLine" id="cb1162-4" data-line-number="4">magic.number =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="st">&#39;integer&#39;</span>, <span class="dt">n =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1162-5" data-line-number="5">                       <span class="dt">size =</span> <span class="dv">4</span>, <span class="dt">endian =</span> <span class="st">&#39;big&#39;</span>)</a>
<a class="sourceLine" id="cb1162-6" data-line-number="6">image.count   =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="st">&#39;integer&#39;</span>, <span class="dt">n =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1162-7" data-line-number="7">                        <span class="dt">size=</span><span class="dv">4</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1162-8" data-line-number="8">image.rows    =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="st">&#39;integer&#39;</span>, <span class="dt">n =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1162-9" data-line-number="9">                        <span class="dt">size=</span><span class="dv">4</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1162-10" data-line-number="10">image.columns =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="st">&#39;integer&#39;</span>, <span class="dt">n =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1162-11" data-line-number="11">                        <span class="dt">size=</span><span class="dv">4</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1162-12" data-line-number="12"><span class="kw">c</span>(  <span class="st">&quot;Number of Images&quot;</span>  =<span class="st"> </span>image.count, </a>
<a class="sourceLine" id="cb1162-13" data-line-number="13">    <span class="st">&quot;Number of Rows&quot;</span>    =<span class="st"> </span>image.rows,</a>
<a class="sourceLine" id="cb1162-14" data-line-number="14">    <span class="st">&quot;Number of Columns&quot;</span> =<span class="st"> </span>image.columns)</a></code></pre></div>
<pre><code>##  Number of Images    Number of Rows Number of Columns 
##             60000                28                28</code></pre>

<p><strong>Third</strong>, for our purpose, we read only the first 25 28x28 digit images and filter the first occurrence of digits 0 through 9 into a list structure:</p>

<div class="sourceCode" id="cb1164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1164-1" data-line-number="1">images =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1164-2" data-line-number="2"><span class="cf">for</span> (idx <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">25</span>) {</a>
<a class="sourceLine" id="cb1164-3" data-line-number="3">  img.hex.vector =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="dt">what =</span> <span class="st">&quot;raw&quot;</span>, <span class="dt">n =</span> <span class="dv">784</span> , </a>
<a class="sourceLine" id="cb1164-4" data-line-number="4">                           <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1164-5" data-line-number="5">  <span class="cf">if</span> (idx <span class="op">%in%</span><span class="st"> </span>indices.of<span class="fl">.1</span>st.occurrences) {</a>
<a class="sourceLine" id="cb1164-6" data-line-number="6">    img.dec.vector =<span class="st"> </span><span class="kw">as.integer</span>(img.hex.vector )</a>
<a class="sourceLine" id="cb1164-7" data-line-number="7">    images[[idx]] =<span class="st"> </span><span class="kw">matrix</span>(img.dec.vector, <span class="dt">nrow=</span><span class="dv">28</span>, <span class="dt">ncol=</span><span class="dv">28</span>)</a>
<a class="sourceLine" id="cb1164-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb1164-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb1164-10" data-line-number="10"><span class="kw">close</span>(minst.handler)</a></code></pre></div>

<p>Now, herein lies the construction of features. We intentionally show 25 iterations, each reading 784 bytes and storing them into a 784-length vector. The entire 784 bytes of data represent one image of a digit. In other words, each byte is a feature. The representation of a digit depends upon the value of each byte. Therefore, it becomes more apparent to store the vector in a 28x28 matrix for visualization. Each cell in a matrix represents a grayscale code between 0-255 (bytesize) that is mapped to one pixel. However, for classification, a vector of features is more convenient for computation. Certain image perturbations (such as thinning, swelling, thickening, and fractures) of each digit are then evaluated for feature matching (for whatever intended purpose).</p>
<p><strong>Fourth</strong>, for visualization, we use a 3rd-party R package called <strong>imager</strong> and use its built-in function <strong>as.cimg(.)</strong> to convert the image list into <strong>cimage</strong> format so that we can plot the actual image (see Figure <a href="machinelearning1.html#fig:digits">9.58</a>). Note that each image in the figure represents 784 features.</p>

<div class="sourceCode" id="cb1165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1165-1" data-line-number="1"><span class="kw">library</span>(imager)</a>
<a class="sourceLine" id="cb1165-2" data-line-number="2">digits =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb1165-3" data-line-number="3"><span class="cf">for</span> (n <span class="cf">in</span> digits.index) {</a>
<a class="sourceLine" id="cb1165-4" data-line-number="4"> digits =<span class="st"> </span><span class="kw">rbind</span>(digits, <span class="kw">rep</span>(<span class="dv">255</span>, <span class="dv">28</span>), images[[n]])</a>
<a class="sourceLine" id="cb1165-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1165-6" data-line-number="6"><span class="kw">plot</span>(<span class="kw">as.cimg</span>(digits), <span class="dt">axes=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:digits"></span>
<img src="mnist_digits.png" alt="MNIST digits" width="80%" />
<p class="caption">
Figure 9.58: MNIST digits
</p>
</div>

<p>Other types of datasets, such as the one used by <strong>Natural Language Processing (NLP)</strong>, use <strong>word embeddings</strong> generated by tools such as <strong>word2vec</strong> using <strong>CBOW</strong> and <strong>Skip-gram</strong>. We defer this topic towards the end around <strong>Attention</strong> and <strong>Deep Learning</strong>.</p>
<p><strong>Gesture and Speech Recognition</strong> are two other areas that demand different representations and interpretations of their datasets and corresponding features. For example, <strong>Speech Recognition</strong> uses <strong>spectrogram</strong>.</p>
<p>Datasets that do not fall under the category of images, sound, signals, or texts have simpler features. For example, the dataset <strong>mtcars</strong> deals with <strong>tuples</strong> that are transactional and uses the following simpler features (e.g., cyl, disp, hp, drt, wt, asec, and so on ):</p>
<div class="sourceCode" id="cb1166"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1166-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1166-2" data-line-number="2"><span class="kw">attributes</span>(mtcars)<span class="op">$</span>names</a></code></pre></div>
<pre><code>##  [1] &quot;mpg&quot;  &quot;cyl&quot;  &quot;disp&quot; &quot;hp&quot;   &quot;drat&quot; &quot;wt&quot;   &quot;qsec&quot; &quot;vs&quot;   &quot;am&quot;  
## [10] &quot;gear&quot; &quot;carb&quot;</code></pre>
<p>As we have shown, <strong>Feature Construction</strong> demands some expert knowledge in a specific domain. We need to understand the structure and mechanics involved in translating and converting inputs to their corresponding interpretable representation for a specific domain to construct features. In the next section, we expand the knowledge of <strong>Feature Engineering</strong> from the concept of construction to the concept of selection.</p>
</div>
<div id="featureselection" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.6</span> Feature Selection<a href="machinelearning1.html#featureselection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Feature Selection</strong> is a method of selecting salient features based on the strength of relevance. We favor relevantly strong features over those that are relevantly weak. </p>
<p>There are three common categories:</p>
<p><strong>Filter-based:</strong></p>
<p>This category selects features based on chosen scoring and ranking system. Scoring (or rating) and ranking may come in many shapes and flavors. Data is given a score and ordered in rank, after which a recommendation is carried out. We favor top-ranked features over low-ranks in the case of <strong>Filter-based</strong> feature selection.</p>
<p>The computation of the result is based upon which result is required. Therefore, we can use a combination of the following measures to compare the outcome. Some measures are already discussed in the <strong>Distance Metrics</strong> section. Others are discussed in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>) under the <strong>Information Theory</strong> Section.</p>

<table>
<tbody>
<tr class="odd">
<td align="left">Precision</td>
<td align="left">Support</td>
<td align="left">Distance</td>
</tr>
<tr class="even">
<td align="left">Recall</td>
<td align="left">Error Rate</td>
<td align="left">Loss vs Gain</td>
</tr>
<tr class="odd">
<td align="left">Sensitivity</td>
<td align="left">Accuracy</td>
<td align="left">Entropy (Mutual Information)</td>
</tr>
<tr class="even">
<td align="left">Specificity</td>
<td align="left">Similarity</td>
<td align="left">Correlation (Collinearity)</td>
</tr>
</tbody>
</table>

<p>For selection, one common comparison of features is using <strong>Correlation</strong> which we cover under <strong>linear regression analysis</strong> in Chapter <strong>6</strong> (<strong>Statistical Computation</strong>). We also extend the concept of <strong>Correlation or Collinearity</strong> under the <strong>Exploratory Data Analysis</strong> section. Additionally, we have a long discussion on <strong>ANOVA</strong>, <strong>Chi-Square</strong>, and <strong>RMSE</strong> in Chapter <strong>6</strong>. More importantly, it helps to understand <strong>Statistical Interaction</strong> under the <strong>Linear Regression Modeling</strong> section in Chapter <strong>6</strong>. The idea is to evaluate the <strong>additive and interactive</strong> strengths of features when combined, along with the interaction of <strong>dummy or indicator variables</strong>.</p>
<p>Here, we use <strong>AUC on ROC</strong> as one of the many tools that can be used as a scoring mechanism for <strong>Filter-based Feature Selection</strong>. We use an actual <strong>ML</strong> training set based on using one of the five folds (5 randomly generated samples from the <strong>BreastCancer</strong> dataset from <strong>mlbench</strong> library). </p>

<div class="sourceCode" id="cb1168"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1168-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb1168-2" data-line-number="2"><span class="kw">library</span>(mlbench)</a>
<a class="sourceLine" id="cb1168-3" data-line-number="3"><span class="kw">data</span>(BreastCancer)</a>
<a class="sourceLine" id="cb1168-4" data-line-number="4">BC =<span class="st"> </span><span class="kw">na.omit</span>( BreastCancer )</a>
<a class="sourceLine" id="cb1168-5" data-line-number="5"><span class="co"># 5-fold (creates 5 groups)</span></a>
<a class="sourceLine" id="cb1168-6" data-line-number="6">fold.indices =<span class="st"> </span><span class="kw">createFolds</span>(BC<span class="op">$</span>Class, <span class="dt">k =</span> <span class="dv">5</span>, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1168-7" data-line-number="7"><span class="co"># choose the first fold for our test group.</span></a>
<a class="sourceLine" id="cb1168-8" data-line-number="8">test =<span class="st"> </span>BC[fold.indices<span class="op">$</span>Fold1,]</a>
<a class="sourceLine" id="cb1168-9" data-line-number="9"><span class="co"># choose the other folds for training group.</span></a>
<a class="sourceLine" id="cb1168-10" data-line-number="10">train =<span class="st"> </span>BC[<span class="op">-</span>fold.indices<span class="op">$</span>Fold1,]</a>
<a class="sourceLine" id="cb1168-11" data-line-number="11"><span class="co"># preserve test label for comparison later</span></a>
<a class="sourceLine" id="cb1168-12" data-line-number="12">test.class =<span class="st"> </span>test<span class="op">$</span>Class </a>
<a class="sourceLine" id="cb1168-13" data-line-number="13">test<span class="op">$</span>Class =<span class="st"> </span><span class="ot">NULL</span></a></code></pre></div>

<p>We have the following Training set attributes:</p>
<div class="sourceCode" id="cb1169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1169-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1169-2" data-line-number="2"><span class="kw">attributes</span>(train)<span class="op">$</span>names</a></code></pre></div>
<pre><code>##  [1] &quot;Id&quot;              &quot;Cl.thickness&quot;    &quot;Cell.size&quot;      
##  [4] &quot;Cell.shape&quot;      &quot;Marg.adhesion&quot;   &quot;Epith.c.size&quot;   
##  [7] &quot;Bare.nuclei&quot;     &quot;Bl.cromatin&quot;     &quot;Normal.nucleoli&quot;
## [10] &quot;Mitoses&quot;         &quot;Class&quot;</code></pre>
<p>We also have the Test set attributes (omitting the Label or Class):</p>
<div class="sourceCode" id="cb1171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1171-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1171-2" data-line-number="2"><span class="kw">attributes</span>(test)<span class="op">$</span>names</a></code></pre></div>
<pre><code>##  [1] &quot;Id&quot;              &quot;Cl.thickness&quot;    &quot;Cell.size&quot;      
##  [4] &quot;Cell.shape&quot;      &quot;Marg.adhesion&quot;   &quot;Epith.c.size&quot;   
##  [7] &quot;Bare.nuclei&quot;     &quot;Bl.cromatin&quot;     &quot;Normal.nucleoli&quot;
## [10] &quot;Mitoses&quot;</code></pre>
<p>Then we use a <strong>randomForest</strong> algorithm to model the fit with the <strong>multinomial</strong> dataset (in this case, we are using only the <strong>binomial</strong> dataset).</p>
<div class="sourceCode" id="cb1173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1173-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb1173-2" data-line-number="2"><span class="co"># Random Forest Model</span></a>
<a class="sourceLine" id="cb1173-3" data-line-number="3">model.rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(Class <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">importance=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1173-4" data-line-number="4">                         <span class="dt">ntree=</span><span class="dv">3</span>)</a></code></pre></div>
<p>Finally, we make a prediction and generate the <strong>AUC</strong> score using the <strong>performance(.)</strong> function from the <strong>ROCR</strong> library. For the TEST set, we use the following R code:</p>

<div class="sourceCode" id="cb1174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1174-1" data-line-number="1"><span class="kw">library</span>(ROCR)</a>
<a class="sourceLine" id="cb1174-2" data-line-number="2"><span class="co"># Calculate AUC for test set</span></a>
<a class="sourceLine" id="cb1174-3" data-line-number="3">test.pred  =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(model.rf, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>,<span class="dt">newdata =</span> test)[,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb1174-4" data-line-number="4">test.pred  =<span class="st"> </span><span class="kw">prediction</span>(test.pred, test.class)</a>
<a class="sourceLine" id="cb1174-5" data-line-number="5">(<span class="dt">test.auc  =</span> <span class="kw">performance</span>(test.pred, <span class="dt">measure =</span> <span class="st">&quot;auc&quot;</span>)<span class="op">@</span>y.values[[<span class="dv">1</span>]] )</a></code></pre></div>
<pre><code>## [1] 0.9631</code></pre>

<p>We calculate the performance of prediction (in the context of predicting the class) by comparing its <strong>Sensitivity</strong> and <strong>Specificity</strong> corresponding to their <strong>true-positive rates</strong> and <strong>false-positive rates</strong>. See Figure <a href="machinelearning1.html#fig:rocrf1">9.59</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rocrf1"></span>
<img src="DS_files/figure-html/rocrf1-1.png" alt="Prediction Performance" width="70%" />
<p class="caption">
Figure 9.59: Prediction Performance
</p>
</div>

<p>We also calculate the accuracy of the prediction. See Figure <a href="machinelearning1.html#fig:rocrf2">9.60</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rocrf2"></span>
<img src="DS_files/figure-html/rocrf2-1.png" alt="Prediction Performance" width="70%" />
<p class="caption">
Figure 9.60: Prediction Performance
</p>
</div>
<p>The <strong>AUC</strong> score and accuracy show that using the training set for the <strong>Random Forest</strong> algorithm makes a strong prediction. Here, our training and test sets contain relevant features. One has to manually mix and match the different combinations of the features to rank predictions and select the combination with higher <strong>AUC</strong> and <strong>Accuracy</strong> scores than other combinations.</p>
<p>In a previous section, we introduce the concept of <strong>AUC on ROC</strong> with details on comparing <strong>AUC</strong> of each feature (or their corresponding predictions). It helps to review the section on how we did a simple feature selection using <strong>AUC on ROC</strong>.</p>
<p>Lastly, we use dataset <strong>iris</strong> as an example to illustrate <strong>Feature Selection</strong>. Here, we use <strong>Random Forest</strong> to model our classification and use a few <strong>measures of importance</strong> such as <strong>MSE</strong> and <strong>Gini Index</strong>.</p>

<div class="sourceCode" id="cb1176"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1176-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1176-2" data-line-number="2"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb1176-3" data-line-number="3"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb1176-4" data-line-number="4"><span class="kw">data</span>(iris)</a>
<a class="sourceLine" id="cb1176-5" data-line-number="5">model.rf =<span class="st"> </span><span class="kw">randomForest</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> iris, <span class="dt">importance =</span> <span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb1176-6" data-line-number="6">pred.rf =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(model.rf)</a>
<a class="sourceLine" id="cb1176-7" data-line-number="7"><span class="kw">confusionMatrix</span>(pred.rf, iris<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         50          0         0
##   versicolor      0         47         3
##   virginica       0          3        47
## 
## Overall Statistics
##                                         
##                Accuracy : 0.96          
##                  95% CI : (0.915, 0.985)
##     No Information Rate : 0.333         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.94          
##                                         
##  Mcnemar&#39;s Test P-Value : NA            
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                  1.000             0.940            0.940
## Specificity                  1.000             0.970            0.970
## Pos Pred Value               1.000             0.940            0.940
## Neg Pred Value               1.000             0.970            0.970
## Prevalence                   0.333             0.333            0.333
## Detection Rate               0.333             0.313            0.313
## Detection Prevalence         0.333             0.333            0.333
## Balanced Accuracy            1.000             0.955            0.955</code></pre>

<p>To extract importance of features from <strong>random Forest</strong>, we use a function called <strong>varImpPlot(.)</strong> and rank features based on <strong>Mean Decrease Accuracy</strong> and <strong>Mean Decrease Gini</strong>. See Figure <a href="machinelearning1.html#fig:rocrf3">9.61</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rocrf3"></span>
<img src="DS_files/figure-html/rocrf3-1.png" alt="Variable Importance" width="70%" />
<p class="caption">
Figure 9.61: Variable Importance
</p>
</div>
<p><strong>Mean Decrease in Accuracy (MDA)</strong>, also called <strong>Percent increase in MSE</strong>, as measured by <strong>Mean Squared Error (MSE)</strong>, ranks features based on how worst if variable is present. For example, <strong>Petal.width</strong> is important (or significant), <strong>Sepal.Width</strong> is worst in importance.    </p>
<p>Equivalently, <strong>Mean Decrease in Gini (MDG)</strong> ranks features based on how worst if variable is not present. For example, <strong>Petal.width</strong> is important, <strong>Sepal.Width</strong> is worst.  </p>
<p>The <strong>importance(.)</strong> function shows the importance of features. </p>

<div class="sourceCode" id="cb1178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1178-1" data-line-number="1">importance.model =<span class="st"> </span><span class="kw">importance</span>(model.rf)</a>
<a class="sourceLine" id="cb1178-2" data-line-number="2"><span class="kw">colnames</span>(importance.model) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;setosa&quot;</span>, <span class="st">&quot;versicolor&quot;</span>, <span class="st">&quot;virginica&quot;</span>, </a>
<a class="sourceLine" id="cb1178-3" data-line-number="3">                               <span class="st">&quot;MDAccuracy&quot;</span>, <span class="st">&quot;MDGini&quot;</span>)</a>
<a class="sourceLine" id="cb1178-4" data-line-number="4">importance.model</a></code></pre></div>
<pre><code>##              setosa versicolor virginica MDAccuracy MDGini
## Sepal.Length  6.169      8.087     7.862      10.78  9.557
## Sepal.Width   4.619      1.322     4.584       5.16  2.299
## Petal.Length 20.785     32.482    27.886      32.89 42.613
## Petal.Width  23.520     32.978    30.729      33.69 44.785</code></pre>

<p>The <strong>varUsed(.)</strong> function shows how frequently each variable is used. Notice below an imbalance of feature usage.</p>

<div class="sourceCode" id="cb1180"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1180-1" data-line-number="1"><span class="kw">varUsed</span>(model.rf)</a></code></pre></div>
<pre><code>## [1]  786  567 1255 1222</code></pre>

<p>Both <strong>MDA</strong> and <strong>MDG</strong> are farther discussed under <strong>Decision Trees</strong> subsection in <strong>Regression</strong> and <strong>Classification</strong> sections respectively.</p>
<p><strong>Wrapper-based:</strong></p>
<p>This category starts with a subset of features evaluated against a model based on performance measures. In Chapter <strong>6</strong> (<strong>Statistical Computation</strong>), we have discussed <strong>Aikike Information Criterion (AIC)</strong> and <strong>Bayesian Information Criterion (BIC)</strong> in great detail. We encourage readers to review the algorithm in that Chapter and carefully review the <strong>forward, backward, and exhaustive steps and bidirectional steps</strong>. While the topic focuses on <strong>model selection</strong>, the underlying concept and method altogether apply to <strong>feature selection</strong> in that the best model chosen is the result of mixing and matching different combinations of features based on the <strong>AIC</strong> score.</p>
<p>Below is a summarized outcome of <strong>AIC-based</strong> scoring. We start with a formula that includes the following features: disp, hp, drat, wt, qsec. Then, after performing a <strong>bidirectional step</strong> for our <strong>Feature Selection</strong>, the final formula lists a reduced number of features, namely drat, wt, qsec.</p>

<div class="sourceCode" id="cb1182"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1182-1" data-line-number="1">initial.model =<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st">  </span>disp <span class="op">+</span><span class="st"> </span>hp <span class="op">+</span><span class="st"> </span>drat <span class="op">+</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>qsec , <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb1182-2" data-line-number="2">(<span class="dt">selected.model =</span> <span class="kw">step</span>(initial.model, <span class="dt">direction =</span> <span class="st">&quot;both&quot;</span>, <span class="dt">k=</span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb1182-3" data-line-number="3">                       <span class="dt">trace =</span> <span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ drat + wt + qsec, data = mtcars)
## 
## Coefficients:
## (Intercept)         drat           wt         qsec  
##      11.394        1.656       -4.398        0.946</code></pre>

<p>Given our formula’s initial number of features, we start with a high <strong>AIC</strong> score of 65.4663, After the <strong>bidirectional AIC-based selection</strong>, the above formula with a reduced list of features gives us the lowest <strong>AIC</strong> score of 63.8911. See below:</p>
<div class="sourceCode" id="cb1184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1184-1" data-line-number="1">selected.model<span class="op">$</span>anova<span class="op">$</span>AIC</a></code></pre></div>
<pre><code>## [1] 65.47 64.21 63.89</code></pre>
<p><strong>Embedded-based:</strong></p>
<p>The feature selection in this category is part of model fitting. There is no need to subset features or to score features. Features are instead rewarded or penalized. That involves tuning <strong>coefficients</strong>, which gives weights to features - we call this <strong>Regularization</strong>. A detailed discussion of <strong>Regularization</strong> is discussed in a few sections ahead when we cover <strong>General Modeling</strong> in which we introduce <strong>Ridge</strong>, <strong>Lasso</strong>, and <strong>Elasticnet</strong> Regularization. The idea is that features are weighted and dropped from the list of relevant features as their weights reach zero.</p>
<p>The discussion of <strong>Embedded-based</strong> feature selection is covered primarily in Chapter <strong>13</strong> (<strong>Computational Deep Learning II</strong>).</p>
</div>
<div id="feature-transformation" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.7</span> Feature Transformation <a href="machinelearning1.html#feature-transformation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Feature Transformation</strong> is a method to improve the interpretability of features by transforming features into a somewhat more interpretable <strong>synthetic</strong> form. In this literature, we regard this <strong>Feature Engineering</strong> component as <strong>Feature Extraction by Transformation</strong>.</p>
<p>Among many other transformations, we give about six transformations that may help us to improve interpretability. In fact, a few of them are derived from <strong>Econometrics</strong> dealing with <strong>Pooled Models</strong> and <strong>Panel Models</strong>, including <strong>homogeneity</strong> and <strong>heterogeneity</strong> of observations which we introduced in Chapter <strong>6</strong> (<strong>Statistical Computation</strong>). We wrap the <strong>models</strong> around the context of <strong>Feature Transformation</strong> in a sense because certain raw data can be merged cross-sectionally or cross-sequentially and arrived at a newly formed <strong>synthetic</strong> set of features. <strong>Synthesized</strong> features may undergo different modeling altogether. That brings us to models such as <strong>Pooled OLS model</strong>, <strong>Fixed Effect Model (FEM)</strong>, and <strong>Random Effect Model (REM)</strong>, which we introduce under the <strong>Time-Series Forecasting</strong> section in Chapter <strong>11</strong> (<strong>Computational Learning III</strong>).</p>
<p><strong>One Hot Encoding:</strong></p>
<p>This transformation allows the creation of a new separate feature for each category in a feature expressed in the form of a one-hot vector. We have covered <strong>One Hot encoding</strong> in detail in the <strong>Exploratory Data Analysis</strong> section.</p>
<p><strong>Bucketize Feature Columns:</strong></p>
<p>This transformation allows assigning feature values into corresponding buckets. That is almost equivalent to <strong>binning</strong>; however, instead of <strong>discretizing</strong> numerical values into <strong>discrete</strong> values, we put values (whether numerical or discrete) into bounded buckets - or a set of ranges. For example, one good feature to extract from a date feature and then bucketize is the year. Our example below shows that essential events are categorized into two buckets based on dates representing a decade between 2001-2010 and another decade between 2011-2020 (note that the events are fictitious except for the COVID-19 pandemic). Here, a new feature called <strong>Decade</strong> is added.</p>

<table>
<caption><span id="tab:bucketize">Table 9.35: </span>Bucketize Feature (Yearly)</caption>
<tbody>
<tr class="odd">
<td align="left">Important Event</td>
<td align="left">Date</td>
<td align="left">Decade</td>
</tr>
<tr class="even">
<td align="left">Discovery of Time Machine</td>
<td align="left">01-Jan-2009</td>
<td align="left">2001-2010</td>
</tr>
<tr class="odd">
<td align="left">First Encounter of the 4th kind</td>
<td align="left">23-Feb-2010</td>
<td align="left">2001-2010</td>
</tr>
<tr class="even">
<td align="left">Covid Pandemic</td>
<td align="left">11-Mar-2020</td>
<td align="left">2011-2020</td>
</tr>
</tbody>
</table>

<p><strong>Crossed Feature Columns:</strong></p>
<p>This transformation allows multiple features to be combined into one feature. Each feature crosses the other to form one new feature. That is different from <strong>correlation</strong> in which we drop duplicates. That is also different from <strong>interactive</strong> of multiple features as discussed in <strong>Statistical Computation</strong> in which we multiply the weights of two features.</p>
<p>An example of a <strong>crossed feature</strong> is when combining <strong>longitude</strong> and <strong>latitude</strong> to develop a new feature called <strong>global position</strong>. The value of the new feature can be represented by hashing the longitude and latitude together as an example, and then perhaps implement <strong>bucketize feature transformation</strong> to further process the feature into buckets - such as into <strong>regions, zones, or hemispheres</strong>.</p>
<p><strong>Cross-Sectional vs Longitudinal Columns:</strong> </p>
<p>Both <strong>Cross-Sectional Feature Columns (CSFC)</strong> and <strong>Longitudinal Feature Columns (LFC)</strong>, also called <strong>Panel Feature Columns (PFC)</strong>, do extract data from time-series data. <strong>CSFC</strong> differs from <strong>LFC</strong> in that <strong>CSFC</strong> analyzes subjects of varying categories at only one moment (a snapshot), whereas <strong>LFC</strong> analyzes subjects in the same category across the varying period. See Figure <a href="machinelearning1.html#fig:crosssectional">9.62</a>. For cross-sectional analysis, the data of interest contains subjects under age brackets 20, 30, 40, 50, and 60 in 2016. Moreover, for longitudinal analysis, the data of interest contains subjects under Cohorts D across the years 2015, 2016, 2017, and 2018.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:crosssectional"></span>
<img src="cross_sectional.png" alt="Cross-Sectional, Longitudinal, Cross-Sequential" width="90%" />
<p class="caption">
Figure 9.62: Cross-Sectional, Longitudinal, Cross-Sequential
</p>
</div>
<p><strong>Cross-Sequential Feature Columns:</strong> </p>
<p><strong>Cross-Sequential Features</strong> combines both <strong>Cross-Sectional</strong> and <strong>Longitudinal</strong> features in that, for example, subjects under age 50 across Cohorts A through D is being analyzed across the year 2015, 2016, 2017, 2018. We may also perform the same analysis for subjects under age 40 and so on.</p>
<p><strong>Cross-Sectional Windowing:</strong></p>
<p>Now, in terms of transformation, the tables in Figure <a href="machinelearning1.html#fig:crosssectional">9.62</a> are assumed to be coming from time-series data. The transformation of <strong>CSFC</strong> from time-series data into another <strong>synthetic</strong> form relies on <strong>Windowing</strong>. As an example, in Figure <a href="machinelearning1.html#fig:windowing">9.63</a>, if we are to account for seasonality on monthly sales, perhaps we can stretch an overlapping 4-month window that covers Holiday sales and see if sales are higher than non-Holiday sales.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:windowing"></span>
<img src="windowing.png" alt="Windowing (Cross-Sectional)" width="100%" />
<p class="caption">
Figure 9.63: Windowing (Cross-Sectional)
</p>
</div>
<p><strong>Pooled Data Model vs Panel Data Model:</strong>  </p>
<p><strong>Pooled Model</strong> and <strong>Panel Model</strong> are terms that can be interchanged with <strong>Cross-Sectional Model</strong> and <strong>Longitudinal Model</strong>. In concept, <strong>Pooled model</strong> refers to sampling different subjects of the same population that are independently collected at different time intervals and pooled together, whereas <strong>Panel model</strong> refers to sampling the same set of subjects collected at different times intervals.</p>
<p>One reason for analyzing the panel dataset is to determine resiliency and sustainability.</p>
</div>
<div id="model-specification-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.8</span> Model Specification <a href="machinelearning1.html#model-specification-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Chapter <strong>6</strong> (<strong>Statistical Computation</strong>), we introduce the concept of <strong>Statistical Model Specification</strong>, or <strong>Model Specification</strong> in general. In specifying a model, we need to be able to describe and define the relationships of observations and features and what they represent. Not having a correct specification may result in an imbalance of <strong>bias</strong> and <strong>variance</strong>.</p>
<p>In fitting a poorly specified model, we may consequently obtain inaccurate results. We can say that inaccurate results can either be <strong>underspecified</strong> or <strong>overspecified</strong>. To mitigate such situations, part of the goal of <strong>feature engineering</strong> as discussed in the previous section and which we discuss next in subsequent sections around <strong>model building</strong> is to understand how different <strong>machine learning</strong> algorithms introduce regularization as a way to measure the importance of observations and features. The existence of a feature that may be deemed not necessary or relevant may create a misrepresentation of our model. Similarly, a missing feature may create <strong>data shift</strong> or <strong>data perturbation</strong> - observations that are not accounted for yet necessary or relevant.</p>
<p>In the context of <strong>anomalies</strong> and <strong>outliers</strong>, it helps to identify, distinguish and describe the existence of such observations - such rare phenomena - and specify their requirements, representations, and relationships.</p>
</div>
</div>
<div id="general-modeling" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.7</span> General Modeling<a href="machinelearning1.html#general-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section introduces a few important <strong>operations</strong> behind <strong>Computational learning</strong>; in particular, we cover the <strong>Learning</strong>, <strong>Testing</strong>, and <strong>Validation</strong> operations.</p>
<div id="training-learning" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.1</span> Training (Learning)<a href="machinelearning1.html#training-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Training a model starts with a dataset - we call this fitting the model to data. The <strong>model</strong> can be a <strong>linear model</strong> such that we <strong>fit</strong> a <strong>line</strong> through the data points. In <strong>Machine Learning (ML)</strong>, it is common to split a dataset into subsets and use a portion of the split for training while the rest are held out for validation and test. The portion of the split for training is called <strong>training set</strong>. We use the <strong>training set</strong> to <strong>train or fit a model</strong>. An example of <strong>fitting a model</strong> is illustrated in <strong>Ordinary Least Square (OLS)</strong>. Geometrically, we randomly place a line through a random set of data points and adjust the line until it meets the minimum computed Least-Squares - a measure for the <strong>goodness of fit</strong>. The <strong>fitted line</strong> is a geometric representation of an <strong>ML model</strong> in the form of the following equation - this accounts for <strong>noise</strong> denoted by the symbol epsilon <span class="math inline">\(\epsilon\)</span>, which also stands for error: </p>
<p><span class="math display">\[\begin{align}
h_{\theta}(X) = \hat{f}(x) = \sum_{i=1}^n \theta x_i + \epsilon,
\end{align}\]</span></p>
<p>where <strong>h(.)</strong> stands for <strong>hypothetical value</strong> - the <span class="math inline">\(\mathbf{yhat}\ (\hat{y})\)</span> and the <span class="math inline">\(\theta\)</span> symbol stands for <strong>coefficient</strong> - the <strong>weight</strong> multiplied to features.</p>
<p>Without <strong>noise</strong>, the data points align perfectly well such that a line can go through all of them. Such a case can then be expressed as a <strong>linear expression</strong> like so (in its most simple linear form):</p>
<p><span class="math display">\[\begin{align}
y = mx + b
\end{align}\]</span></p>
<p>However, an <strong>ML model</strong> does not just consist of a <strong>linear expression</strong>. The <strong>linear expression</strong> above comes with two model parameters: <strong>m</strong> for slope and <strong>b</strong> for intercept. We also treat these parameters as <strong>weights</strong> or <strong>coefficients</strong> - in some cases, it is denoted by the symbol <strong>theta</strong> <span class="math inline">\(\theta\)</span>. When we say <strong>fit the line</strong> or <strong>fit the model</strong> or <strong>train the model</strong>, we mean to use the <strong>linear expression</strong> while adjusting the <strong>weights</strong>. For every adjustment of the weights, we measure and take note of all the distances of the data points to the fitted line - such distance is called <strong>residual</strong>. The one adjustment that renders the least square distance effectively determines the final <strong>ML model</strong>. For example:</p>
<p><span class="math display">\[\begin{align}
\epsilon = y - \hat{y}\ \ \rightarrow \ \ \ MSE = \frac{1}{n}\sum \epsilon^2
\end{align}\]</span></p>
<p>The final <strong>ML model</strong> in terms of <strong>linear regression</strong> comes with the following sufficient <strong>ML</strong> artifacts (other implementation of models come with additional supplementary artifacts, e.g. for <strong>ML model provenance</strong>):</p>
<ul>
<li>The linear equation, e.g. <span class="math inline">\(\hat{y} = \beta_0 + \beta_1 x\)</span></li>
<li>The coefficient values, e.g. <span class="math inline">\(\beta_0 = 30, \beta_1 = 50\)</span></li>
<li>The training set: <span class="math inline">\(\{x, y\}_{i=1}^n\)</span></li>
<li>The features: x</li>
<li>The fitted values: (<span class="math inline">\(\hat{y}\)</span>)</li>
<li>The residual: <span class="math inline">\(\epsilon\)</span></li>
</ul>
<p>Note that by running the following R code below, a linear model in R using the function <strong>lm(.)</strong> captures the following content (preserved in a <strong>list</strong> structure): coefficients, residuals, effects, rank, fitted.values, qr, df.residual, xlevels, call, formula, terms, and others.</p>

<div class="sourceCode" id="cb1186"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1186-1" data-line-number="1"><span class="co"># Using mtcars dataset to illustrate linear modeling</span></a>
<a class="sourceLine" id="cb1186-2" data-line-number="2">my.model =<span class="st"> </span><span class="kw">lm</span>(<span class="dt">formula =</span> mpg <span class="op">~</span><span class="st"> </span>cyl <span class="op">+</span><span class="st"> </span>am, <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb1186-3" data-line-number="3"><span class="kw">saveRDS</span>(my.model, <span class="st">&quot;./my_model.rds&quot;</span>)   <span class="co"># save our model</span></a>
<a class="sourceLine" id="cb1186-4" data-line-number="4">my.model =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;./my_model.rds&quot;</span>)  <span class="co"># retrieve our model</span></a>
<a class="sourceLine" id="cb1186-5" data-line-number="5"><span class="kw">names</span>(my.model)</a></code></pre></div>
<pre><code>##  [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;       &quot;rank&quot;         
##  [5] &quot;fitted.values&quot; &quot;assign&quot;        &quot;qr&quot;            &quot;df.residual&quot;  
##  [9] &quot;xlevels&quot;       &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;</code></pre>

<p>Our definition of a <strong>model</strong> should be all-encompassing to include all necessary ingredients to reconstruct our model. The list of artifacts above exemplifies that. However, Chapter <strong>14</strong> (<strong>Distributed Computation</strong>) covers <strong>Open Standards</strong>, which describe standard model specifications that guide how models can be portable and sharable. At the very least, a model should meet the minimum required content for inferencing.</p>
</div>
<div id="validation-tuning" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.2</span> Validation (Tuning) <a href="machinelearning1.html#validation-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>fitting a model</strong>, it is not unusual to encounter <strong>overfitting</strong>. That means the model is too tailored to fit the <strong>training set</strong> perfectly well but loses generality; thus, it cannot be used for new <strong>unseen</strong> data. If this happens, additional adjustment is needed to the model to minimize the overfit. This adjustment is called <strong>tuning</strong> the model.</p>
<p><strong>Tuning</strong> is essentially needed for optimization. Primarily, we tune <strong>hyperparameters</strong>. A combination of <strong>cross-validation</strong> and calibration of hyperparameters helps improve the performance of our models. In two separate sections a little later ahead, we discuss more of <strong>cross-validation</strong> and <strong>tuning</strong> by using <strong>regularization</strong> techniques.</p>
<p>Now, one way to overcome overfitting is to use <strong>cross-validation</strong>. The idea is to split the dataset into <strong>k</strong> sets (see Figure <a href="machinelearning1.html#fig:crossvalidation">9.64</a>). We then perform model fitting by <strong>holding out</strong> the first set and using the rest of the sets for training. After which, we perform another model fitting by <strong>holding out</strong> the second set and using the rest of the sets. We continue to do so iteratively until completing several iterations, e.g., five iterations.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:crossvalidation"></span>
<img src="crossvalidation.png" alt="5-Fold Cross-Validation" width="60%" />
<p class="caption">
Figure 9.64: 5-Fold Cross-Validation
</p>
</div>
<p>After the last iteration, we then analyze the result. We choose the model that renders the best result. In this case, for <strong>linear regression</strong>, we choose one with least error using <span class="math inline">\(\mathbf{\text{RMSE}, \text{R}^2,}\text{ and }\mathbf{\text{MAE}}\)</span>.</p>
</div>
<div id="testing-assessing" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.3</span> Testing (Assessing) <a href="machinelearning1.html#testing-assessing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After a model is trained, our next goal is to assess the performance (e.g., predictive power) of the model. Here, we treat the <strong>holdout set</strong> as a <strong>testing set</strong>, which contains a list of predictive variables only - the features. Unlike the <strong>training set</strong>, we do not account for the <strong>response</strong> variable in the <strong>testing set</strong>.</p>
<p>Ideally, the values in the <strong>response</strong> variables in the <strong>testing set</strong> are preserved as <strong>ground truth</strong> for later evaluation. For example, we have covered <strong>Auc on ROC</strong> under the <strong>Feature Selection</strong> section as a method we can use to compare the predicted result of our test over the actual values.</p>
<p>In terms of the proportionality of our <strong>testing set</strong> over <strong>training set</strong>, it may be safe to assume that there is no specific hard rule in the manner in which we split our dataset, but it seems sensical to assume (in a general sense, but not always) that smaller datasets may not have to be split into many folds. A simple 2-fold split helps when the first fold becomes the training set and the second fold becomes a <strong>holdout set</strong>. The proportion may be arbitrary depending on necessity, e.g., <strong>training set</strong> may accommodate 75% of the dataset, and 25% goes into a <strong>holdout set</strong>.</p>
<p>The question now comes when we use the <strong>holdout set</strong>. If the set is used to assess our model’s performance (e.g., predictive power) and shows high results (e.g., high accuracy or low error), it may not necessarily signify a good thing. <strong>Overfitting</strong> is a good example. To ensure that accuracy holds, we perform multiple tests using <strong>cross-validation</strong>.</p>
<p>Below is an example of splitting the dataset <strong>BreastCancer</strong> in 5-fold using <strong>createFolds(.)</strong> function.</p>

<div class="sourceCode" id="cb1188"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1188-1" data-line-number="1"><span class="kw">require</span>(caret)</a>
<a class="sourceLine" id="cb1188-2" data-line-number="2"><span class="kw">require</span>(mlbench)</a>
<a class="sourceLine" id="cb1188-3" data-line-number="3">n=<span class="dv">80</span></a>
<a class="sourceLine" id="cb1188-4" data-line-number="4">X =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span> n)</a>
<a class="sourceLine" id="cb1188-5" data-line-number="5"><span class="co"># note that the output is the location</span></a>
<a class="sourceLine" id="cb1188-6" data-line-number="6"><span class="co"># (index of corresponding data points)</span></a>
<a class="sourceLine" id="cb1188-7" data-line-number="7">(<span class="dt">fold.indices =</span> <span class="kw">createFolds</span>(X, <span class="dt">k =</span> <span class="dv">5</span>, <span class="dt">returnTrain=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## $Fold1
##  [1]  2  8 11 14 22 24 25 40 44 45 51 59 62 65 76 78
## 
## $Fold2
##  [1]  4  7  9 10 21 26 29 37 46 49 52 60 67 70 72 75
## 
## $Fold3
##  [1]  1  3 15 16 23 28 33 38 41 48 56 57 61 63 64 79
## 
## $Fold4
##  [1]  5  6 13 18 30 31 35 36 47 53 55 58 68 69 77 80
## 
## $Fold5
##  [1] 12 17 19 20 27 32 34 39 42 43 50 54 66 71 73 74</code></pre>

<p>Given the 5-folds, we can use the 1st fold for our test set and the rest of the folds for our training set like so:</p>
<div class="sourceCode" id="cb1190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1190-1" data-line-number="1"><span class="co"># choose the first fold for our holdout set.</span></a>
<a class="sourceLine" id="cb1190-2" data-line-number="2">holdout.set =<span class="st"> </span>X[fold.indices<span class="op">$</span>Fold1] </a>
<a class="sourceLine" id="cb1190-3" data-line-number="3"><span class="co"># choose the other folds for our training set</span></a>
<a class="sourceLine" id="cb1190-4" data-line-number="4">training.set =<span class="st"> </span>X[<span class="op">-</span>fold.indices<span class="op">$</span>Fold1]</a>
<a class="sourceLine" id="cb1190-5" data-line-number="5"><span class="kw">c</span>(<span class="st">&quot;HoldOut&quot;</span>=<span class="kw">length</span>(holdout.set), <span class="st">&quot;Training&quot;</span>=<span class="kw">length</span>(training.set))</a></code></pre></div>
<pre><code>##  HoldOut Training 
##       16       64</code></pre>
</div>
<div id="cross-validation-cv" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.4</span> Cross-Validation (CV)  <a href="machinelearning1.html#cross-validation-cv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us further extend the concept of <strong>Cross-Validation (CV)</strong>. There are a few other reasons why we need <strong>CV</strong> of which we mention the following three reasons:</p>
<ul>
<li><strong>Model Selection and Tuning</strong></li>
<li><strong>Hyperparameter Selection and Tuning</strong></li>
<li><strong>Algorithm Selection and Tuning</strong></li>
</ul>
<p>Here, we discuss two <strong>CV</strong> techniques. The first <strong>CV</strong> technique is called the <strong>K-fold Cross-Validation</strong>. We split the dataset into k-folds. See Figure <a href="machinelearning1.html#fig:crossvalidation">9.64</a> which shows a 5-fold split. </p>
<p>There is no hard rule when choosing <strong>k</strong> for splitting the dataset. However, care must be considered to avoid <strong>data leakage</strong> - a topic introduced in the <strong>EDA</strong> section. It is easy to <strong>contaminate</strong> our dataset, especially when using <strong>cross-validation</strong>. This happens if the algorithms and hyperparameters are not well-tuned.</p>
<p>To tune our algorithm and hyperparameter and select the best combination, it is ideal for performing a <strong>repeated K-fold CV</strong>. For example, Figure <a href="machinelearning1.html#fig:modelselection">9.65</a> shows a 1-iteration evaluation of five different attempts to tune a model, e.g., given some set of tunable parameters. In practice, we can use multiple iterations across several different configurations (e.g., one combination of a chosen model and hyperparameters).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:modelselection"></span>
<img src="modelselection.png" alt="Repeated 5-Fold Cross-Validation (Model Selection)" width="60%" />
<p class="caption">
Figure 9.65: Repeated 5-Fold Cross-Validation (Model Selection)
</p>
</div>
<p>On the other hand, we are not limited to only one algorithm for evaluation. Figure <a href="machinelearning1.html#fig:algoselection">9.66</a> illustrates an example of evaluating the performance of different algorithms using a <strong>repeated K-fold CV</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:algoselection"></span>
<img src="algoselection.png" alt="Repeated 5-Fold Cross-Validation (Algorithm Selection)" width="60%" />
<p class="caption">
Figure 9.66: Repeated 5-Fold Cross-Validation (Algorithm Selection)
</p>
</div>
<p>The second <strong>CV</strong> technique is called the <strong>Leave One Out Cross Validation (LOOCV)</strong>. It is also referred to as the <strong>Jackknife</strong> approach. The idea is simply to <strong>hold out</strong> a data point and use the rest of the dataset for training. Recall in <strong>linear regression</strong> how we geometrically fit a line through data points. Such a line is fitted by adjusting its slope and intercept. To derive a good fit, we use the <strong>Jackknife</strong> approach in which we perform the following steps:  </p>
<ol style="list-style-type: decimal">
<li>Using the entire dataset, we compute the initial values of the slope and intercept.</li>
<li>We then hold out one data point and use the rest of the dataset to compute the slope and intercept values.</li>
<li>We repeat step 2 by holding out the next data point until all the data points have taken turns being held out.</li>
<li>Compute the average of all the computed slopes and intercepts.</li>
</ol>
<p>Such an approach aims to balance <strong>bias</strong> and <strong>variance</strong>.</p>
</div>
<div id="bias-and-variance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.5</span> Bias and Variance <a href="machinelearning1.html#bias-and-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us use a popular diagram used in discussing <strong>bias</strong> and <strong>variance</strong>. See Figure <a href="machinelearning1.html#fig:biasvariance">9.67</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:biasvariance"></span>
<img src="biasvariance.png" alt="Variance and Bias" width="60%" />
<p class="caption">
Figure 9.67: Variance and Bias
</p>
</div>
<p>In <strong>fitting a model</strong>, we rely on measuring the <strong>error</strong> of our fit against some true value. A breakdown (or decomposition) of the <strong>error</strong> is essential in understanding how we may deal with <strong>underfitting</strong> or <strong>overfitting</strong> a model. To compute for the <strong>total error</strong> (e.g., <strong>MSE</strong>), we use the following decomposition:</p>
<p><span class="math display">\[\begin{align}
\text{Total Error}(x) = Bias^2 +  Variance + \text{Irreducible Error}
\end{align}\]</span></p>
<p><strong>Bias</strong> and <strong>Variance</strong> are derived from the following equations:</p>
<p><span class="math display">\[\begin{align}
Bias^2 = \left[\mathbb{E}[\hat{f}(x)] - f(x) \right]^2 
\ \ \ \ \ \ \ \ \ \ \ \ \ \
Variance = \mathbb{E}\left[ \left(\hat{f}(x) - \mathbb{E}[\hat{f}(x)]\right)^2\right] 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\underbrace{\hat{y} = \hat{f}(x)}_{
\begin{array}{l}
\text{Estimated Response} \\
\text{Learned from X}
\end{array}
}
\ \ \ \ \ \ \ \ \ \
\underbrace{y = {f}(x) }_{\text{Actual Response}}
\ \ \ \ \ \ \ \ \ \ \
\underbrace{\text{Irreducible Error} = \sigma^2_e}_{Noise}
\]</span></p>
<p>Let us design a simple dataset and create a 3-fold dataset to illustrate <strong>bias</strong> and <strong>variance</strong>.</p>

<div class="sourceCode" id="cb1192"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1192-1" data-line-number="1"><span class="kw">require</span>(caret)</a>
<a class="sourceLine" id="cb1192-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2019</span>)</a>
<a class="sourceLine" id="cb1192-3" data-line-number="3">n =<span class="st"> </span><span class="dv">10</span>; k =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb1192-4" data-line-number="4">f &lt;-<span class="st"> </span><span class="cf">function</span>(X) { <span class="kw">sqrt</span>(X) }</a>
<a class="sourceLine" id="cb1192-5" data-line-number="5">X =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span> n <span class="op">*</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1192-6" data-line-number="6">E =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n <span class="op">*</span><span class="st"> </span>k, <span class="dv">0</span>, <span class="fl">0.1</span>)      <span class="co"># Irreducible Error</span></a>
<a class="sourceLine" id="cb1192-7" data-line-number="7">Y =<span class="st"> </span><span class="kw">f</span>(X)                  <span class="co"># Actual Values without Noise</span></a>
<a class="sourceLine" id="cb1192-8" data-line-number="8">Y.observed =<span class="st"> </span><span class="kw">f</span>(X) <span class="op">+</span><span class="st"> </span>E     <span class="co"># Observed Values with Noise</span></a>
<a class="sourceLine" id="cb1192-9" data-line-number="9">fold.indices =<span class="st"> </span><span class="kw">createFolds</span>(X, <span class="dt">k =</span> k, <span class="dt">returnTrain=</span><span class="ot">FALSE</span>)</a></code></pre></div>

<p>Given the 3 folds, we have the plot in Figure <a href="machinelearning1.html#fig:biasfold">9.68</a>:</p>

<div class="sourceCode" id="cb1193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1193-1" data-line-number="1"><span class="kw">require</span>(ModelMetrics)</a>
<a class="sourceLine" id="cb1193-2" data-line-number="2">Y.bias =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, k)</a>
<a class="sourceLine" id="cb1193-3" data-line-number="3">Y.var  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, k)</a>
<a class="sourceLine" id="cb1193-4" data-line-number="4">Y.mse  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, k)</a>
<a class="sourceLine" id="cb1193-5" data-line-number="5"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="fl">1.3</span>), </a>
<a class="sourceLine" id="cb1193-6" data-line-number="6">      <span class="dt">xlab=</span><span class="st">&quot;Predictor&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Response&quot;</span>, </a>
<a class="sourceLine" id="cb1193-7" data-line-number="7">     <span class="dt">main=</span><span class="st">&quot;Cross Validation&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1193-8" data-line-number="8"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1193-9" data-line-number="9"><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {</a>
<a class="sourceLine" id="cb1193-10" data-line-number="10">  indices =<span class="st"> </span>fold.indices[[j]] </a>
<a class="sourceLine" id="cb1193-11" data-line-number="11">  <span class="co"># choose the j-fold for our holdout set.</span></a>
<a class="sourceLine" id="cb1193-12" data-line-number="12">  X.holdout.set =<span class="st"> </span>X[indices]; Y.holdout.set =<span class="st"> </span>Y.observed[indices]</a>
<a class="sourceLine" id="cb1193-13" data-line-number="13">  <span class="co"># choose the other folds for our training set</span></a>
<a class="sourceLine" id="cb1193-14" data-line-number="14">  X.training.set =<span class="st"> </span>X[<span class="op">-</span>indices]; Y.training.set =<span class="st"> </span>Y.observed[<span class="op">-</span>indices]</a>
<a class="sourceLine" id="cb1193-15" data-line-number="15">  <span class="co"># fit a linear model</span></a>
<a class="sourceLine" id="cb1193-16" data-line-number="16">  Y.model =<span class="st"> </span><span class="kw">lm</span>(Y.training.set <span class="op">~</span><span class="st"> </span>X.training.set) </a>
<a class="sourceLine" id="cb1193-17" data-line-number="17">  Y.predicted =<span class="st"> </span><span class="kw">predict.lm</span>(Y.model, </a>
<a class="sourceLine" id="cb1193-18" data-line-number="18">                <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">X.training.set  =</span> X.holdout.set))</a>
<a class="sourceLine" id="cb1193-19" data-line-number="19">  Y.bias[j]   =<span class="st"> </span><span class="kw">mean</span>( ( <span class="kw">mean</span>(Y.predicted)  <span class="op">-</span><span class="st"> </span>Y.holdout.set )<span class="op">^</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb1193-20" data-line-number="20">  Y.var[j]    =<span class="st"> </span><span class="kw">sum</span>( (Y.predicted <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(Y.predicted))<span class="op">^</span><span class="dv">2</span> ) <span class="op">*</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>(n<span class="dv">-1</span>) </a>
<a class="sourceLine" id="cb1193-21" data-line-number="21">  Y.mse[j]    =<span class="st"> </span><span class="kw">mean</span>((Y.predicted <span class="op">-</span><span class="st"> </span>Y.holdout.set)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1193-22" data-line-number="22">  <span class="kw">points</span>(X.holdout.set, Y.holdout.set, <span class="dt">col=</span>j)</a>
<a class="sourceLine" id="cb1193-23" data-line-number="23">  <span class="kw">lines</span>(X.training.set, Y.model<span class="op">$</span>fitted.values, <span class="dt">col=</span>j)</a>
<a class="sourceLine" id="cb1193-24" data-line-number="24">}</a>
<a class="sourceLine" id="cb1193-25" data-line-number="25"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, </a>
<a class="sourceLine" id="cb1193-26" data-line-number="26">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;Train 1&quot;</span>, <span class="st">&quot;Train 2&quot;</span>, <span class="st">&quot;Train 3&quot;</span>),</a>
<a class="sourceLine" id="cb1193-27" data-line-number="27">    <span class="dt">col=</span><span class="kw">seq</span>(<span class="dv">1</span>,k), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">1</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:biasfold"></span>
<img src="DS_files/figure-html/biasfold-1.png" alt="Cross Validation" width="70%" />
<p class="caption">
Figure 9.68: Cross Validation
</p>
</div>

<p>There are situations in which we find that no matter how good we fit our model, we cannot reduce the error. This so-called <strong>irreducible error</strong> could indicate a dataset with a certain amount of noise - some perturbation that could be the norm in a system. Perhaps we may acknowledge that additional data processing, adjustment, or transformation may be required before fitting a model - all that with one thing in mind. That is to do our best to improve our model’s <strong>goodness of fit</strong> by balancing <strong>bias</strong> and <strong>variance</strong> - a trade-off. That is only as long as we do not fall into <strong>underfitting</strong> or <strong>overfitting</strong> conditions.</p>

<div class="sourceCode" id="cb1194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1194-1" data-line-number="1">(<span class="dt">measure =</span> <span class="kw">data.frame</span>( <span class="dt">Train =</span> <span class="kw">seq</span>(<span class="dv">1</span>,k), </a>
<a class="sourceLine" id="cb1194-2" data-line-number="2">    <span class="dt">bias =</span> Y.bias, <span class="dt">variance =</span> Y.var, <span class="dt">mse =</span> Y.mse))</a></code></pre></div>
<pre><code>##   Train    bias variance     mse
## 1     1 0.05536  0.07679 0.02174
## 2     2 0.08065  0.08024 0.01423
## 3     3 0.10337  0.05376 0.01895</code></pre>

<p>When it comes to <strong>losing generality</strong>, there is a correlation between <strong>generalizability</strong> and <strong>variance</strong>. If the dataset deviates or varies a lot, we may still be able to come up with a model that can predict. Otherwise, perhaps we can split the dataset into groups and use different modeling algorithms for each group such that when we merge the models into an aggregated model - an ensemble model, it will be able to predict.</p>
<p>In a <strong>linear regression</strong>, if we start to fit a line in such a manner that the line starts to curve or adjust towards every data point (basically introducing a higher degree of freedom), then it can be guaranteed that our model will just not fit any new unseen data. The model loses <strong>generalizability</strong>. See Figure <a href="machinelearning1.html#fig:goodnessoffit">9.69</a>.</p>

<div class="sourceCode" id="cb1196"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1196-1" data-line-number="1">fit =<span class="st"> </span><span class="cf">function</span>(x) { x<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb1196-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1196-3" data-line-number="3">N =<span class="st"> </span><span class="dv">40</span></a>
<a class="sourceLine" id="cb1196-4" data-line-number="4">x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span>N)</a>
<a class="sourceLine" id="cb1196-5" data-line-number="5">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="fl">0.1</span>) <span class="co"># Noise</span></a>
<a class="sourceLine" id="cb1196-6" data-line-number="6">y =<span class="st"> </span><span class="kw">fit</span>(x)                     <span class="co"># f(x) &lt;- Actual Response </span></a>
<a class="sourceLine" id="cb1196-7" data-line-number="7">y.hat =<span class="st"> </span>y <span class="op">+</span><span class="st"> </span>e</a>
<a class="sourceLine" id="cb1196-8" data-line-number="8"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb1196-9" data-line-number="9">      <span class="dt">xlab=</span><span class="st">&quot;Predictor&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Response&quot;</span>, </a>
<a class="sourceLine" id="cb1196-10" data-line-number="10">     <span class="dt">main=</span><span class="st">&quot;Goodness of Fit&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1196-11" data-line-number="11"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1196-12" data-line-number="12"><span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1196-13" data-line-number="13"><span class="kw">lines</span>(x, y, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1196-14" data-line-number="14"><span class="kw">lines</span>(<span class="kw">lowess</span>(x, y.hat, <span class="dt">f=</span><span class="fl">0.02</span>), <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb1196-15" data-line-number="15"><span class="kw">points</span>(x, y.hat, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">pch=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb1196-16" data-line-number="16"><span class="co">### Legend</span></a>
<a class="sourceLine" id="cb1196-17" data-line-number="17"><span class="kw">legend</span>(<span class="op">-</span><span class="fl">0.2</span>, <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1196-18" data-line-number="18">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;Fitted Line (Optimal)&quot;</span>, <span class="st">&quot;Underfit (High Bias)&quot;</span>,</a>
<a class="sourceLine" id="cb1196-19" data-line-number="19">              <span class="st">&quot;Overfit (High Variance)&quot;</span>),</a>
<a class="sourceLine" id="cb1196-20" data-line-number="20">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>),  <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:goodnessoffit"></span>
<img src="DS_files/figure-html/goodnessoffit-1.png" alt="Goodness of Fit" width="70%" />
<p class="caption">
Figure 9.69: Goodness of Fit
</p>
</div>

<p>Similarly, in a case where a <strong>decision tree</strong> has too many branches or as the tree depth increases, there is a tendency to overfit since every leaf tends toward every data point. Once again, this indicates that the tree model will not fit any new (unseen) set of data points.</p>
<p>We continue to discuss <strong>Overfitting</strong> in subsequent chapters. We will introduce hyperparameters and network layers starting with Chapter <strong>12</strong> (<strong>Computational Deep Learning I</strong>). The more we begin to tailor the network against many layers where each layer filters toward every data point, the more we overfit.</p>
</div>
<div id="loss-and-cost-functions" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.6</span> Loss and Cost Functions  <a href="machinelearning1.html#loss-and-cost-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When we calculate our estimates, intuitively, we quantify the error in our estimates and adjust our operations as necessary. Additionally, we define functions that allow us to measure the difference of our estimates based on error, adjust as necessary, and numerically determine how much of an error we can tolerate.</p>
<p><strong>Loss (Error) function</strong> </p>
<p><strong>Loss function</strong> measures the difference between our estimated and actual values. The larger the difference, the larger the error. We use an <strong>error</strong> function to compute our error. <strong>Squared Error Loss</strong> (e.g. <strong>SSE</strong>, <strong>MSE</strong>, <strong>MAE</strong>) and <strong>Huber Loss</strong> are examples of <strong>Loss functions</strong> for regression. A general formula for a <strong>Loss function</strong> in regression is expressed as such:</p>
<p><span class="math display">\[\begin{align}
\underbrace{Lik(\theta) = \sum(y - \hat{y})^2}_{\text{squared error loss}}
\end{align}\]</span></p>
<p>Note that <strong>classification problems</strong> introduce other forms of measures for Loss functions based on <strong>Information Theory</strong> and <strong>Bayesian Theory</strong>. Examples of measures are the <strong>Gini Index</strong>, <strong>Cross-Entropy</strong>, and <strong>Information Gain</strong>, which are used for tree classifications, <strong>Logit Loss</strong> for binary classification, and <strong>Hinge Loss</strong> for SVM. We cover this in the <strong>Classification</strong> section later. Below are examples of <strong>Hinge Loss</strong> function and <strong>Logit Loss</strong> function respectively:</p>
<p><span class="math display">\[\begin{align}
\underbrace{Lik(\theta) = max\left\{0, 1 - f(x)\right\}}_{\text{hinge loss}}\ \ \ \ \ \ \ \ \ 
\underbrace{Lik(\theta) = max\left\{1 + exp(- f(x))\right\}}_{\text{log loss}}
\end{align}\]</span></p>
<p><strong>Objective (Cost) function</strong> </p>
<p><strong>Objective function</strong> measures the Cost of fitting a model to data. Intuitively, our goal is to reduce the <strong>Cost</strong>. In this respect, it is understandable that we determine our tolerance level - how much <strong>Cost</strong> is acceptable. <strong>Cost</strong> is a function of the <strong>loss</strong> that we incur and the complexity of our model to fit. If our model is too complex, there is a chance to overfit. As shown in the previous section, we can use <strong>Lasso</strong> or <strong>Ridge</strong> regularization for linear regression to balance complexity. A general formula for the objective function is expressed as such: </p>
<p><span class="math display">\[\begin{align}
J(\theta) = \theta^{\text{*}} = \text{arg}\ \underset{\theta}{\text{min}}\ Lik(\theta)  
\ \ \ \ \ \ \ \ \ \ \ \ \ \ 
J(\theta)_R = \theta^{\text{*}}_R = \underbrace{ \text{arg}\ \underset{\theta}{\text{min}}
\left\{Lik(\theta) + \lambda(\theta)\right\}  }_{\text{with regularization}}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is a regularization function</p>
<p><strong>Stopping Criterion function</strong> </p>
<p>We use an <strong>objective</strong> function to find the minimum error. However, we might wonder why we have to find a minimum error when we can go straight to zero error, meaning we just get the estimated value equal to the actual value. Indeed, that would be ideal if we knew the actual value. Unfortunately, in some cases, we do not have the actual value. All we have are data points, and the most basic method we can use is to average the data points and estimate the actual value based on the average. However, we know that average will not always give accurate results when our data points are inherently skewed. In this sense, we need better methods to find a minimum error that is optimal enough to describe the accuracy of our estimate. That said, either we keep looking for that optimal minimum error, or at some point, we have to <strong>stop</strong>. Furthermore, this is where we need to determine the exit criterion to <strong>stop</strong>? We use a stopping criterion function to determine our stopping point - which we also term our <strong>tolerance level</strong>.</p>
<p>Note that the three general functions ( loss, cost, stopping criterion) are commonly used and emphasized in <strong>Machine Learning</strong>. Most algorithms are readily available in many languages and are implemented in different ways. Here, we use a library called <strong>caret</strong> to illustrate two functions, namely <strong>trainControl(.)</strong> and <strong>train(.)</strong>. The former is used to control <strong>cross-validation</strong> training. The latter is used to train a model by injecting a selection of algorithms and corresponding hyperparameter settings.</p>
<p>The <strong>trainControl(.)</strong> function allows us to set parameters that control how we train our model. Such parameters include the validation method, number of splits to a dataset, and others.</p>
<div class="sourceCode" id="cb1197"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1197-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb1197-2" data-line-number="2">tr.control =<span class="st"> </span>caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,  <span class="dt">number =</span> <span class="dv">5</span>)</a></code></pre></div>
<p>We then plug the control parameters to <strong>train(.)</strong>. The function allows us to set the <strong>algorithm</strong> to use for the training such as <strong>general linear model (glm)</strong>, <strong>gradient boosting machine (gbm)</strong>, <strong>regression forest (rf)</strong>, etc.</p>
<p>The <strong>tr.tuneGrid</strong> is a configuration of <strong>regularization choices</strong> and a selection of <strong>lambdas</strong> as discussed in previous chapter. We then use <strong>train(.)</strong> to train our model with <strong>centering</strong> and <strong>scaling</strong> for pre-processing. Notice the use of <strong>expand.grid(.)</strong>. That allows multiple hyperparameters to produce different unique combinations. In our case, we use alpha and lambda. We then train the model and determine which combination renders the optimal fit.</p>

<div class="sourceCode" id="cb1198"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1198-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a></code></pre></div>
<pre><code>## Loaded glmnet 3.0-2</code></pre>
<div class="sourceCode" id="cb1200"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1200-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1200-2" data-line-number="2">predict =<span class="st"> </span>stats<span class="op">::</span>predict</a>
<a class="sourceLine" id="cb1200-3" data-line-number="3">tr.tune =<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">alpha =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">1</span>), <span class="dt">lambda =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1200-4" data-line-number="4">(<span class="dt">tr.model =</span> caret<span class="op">::</span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>hp <span class="op">+</span><span class="st"> </span>cyl, <span class="dt">data=</span>mtcars, </a>
<a class="sourceLine" id="cb1200-5" data-line-number="5">          <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;scale&quot;</span>, <span class="st">&quot;center&quot;</span> ),</a>
<a class="sourceLine" id="cb1200-6" data-line-number="6">          <span class="dt">method=</span><span class="st">&quot;glmnet&quot;</span>, <span class="dt">tuneGrid =</span> tr.tune, <span class="dt">trControl =</span> tr.control))</a></code></pre></div>
<pre><code>## glmnet 
## 
## 32 samples
##  3 predictor
## 
## Pre-processing: scaled (3), centered (3) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 25, 27, 25, 27, 24 
## Resampling results across tuning parameters:
## 
##   alpha  lambda  RMSE   Rsquared  MAE  
##   0.0    0.0     2.599  0.8681    2.164
##   0.0    0.5     2.600  0.8682    2.165
##   0.0    1.0     2.614  0.8692    2.168
##   0.5    0.0     2.619  0.8650    2.192
##   0.5    0.5     2.625  0.8659    2.198
##   0.5    1.0     2.670  0.8662    2.213
##   1.0    0.0     2.621  0.8647    2.195
##   1.0    0.5     2.670  0.8618    2.236
##   1.0    1.0     2.808  0.8580    2.326
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 0 and lambda = 0.</code></pre>

<p>The output shows an optimal model based on the lowest <strong>RMSE</strong> (e.g., the choice of performance metric) using ridge regression (<strong>alpha=0.0</strong>) and <strong>lambda=0.0</strong>. We can then use the model for prediction.</p>
<div class="sourceCode" id="cb1202"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1202-1" data-line-number="1">tr.predict =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(tr.model, <span class="dt">newx =</span> x)</a></code></pre></div>
</div>
<div id="global-and-local-minima" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.7</span> Global and Local Minima  <a href="machinelearning1.html#global-and-local-minima" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, in the context of a <strong>Loss function</strong>, when we deal with data, we need to perform the estimation in most cases. We try our best to predict an estimated value of our data close to an actual value. Because we are estimating, our result will come with some inaccuracies or errors. Intuitively, especially in cases where we cannot afford errors, we need to minimize these errors. Errors can be costly and so when we say minimize error, we may also be minimizing cost.</p>
<p>We will be covering <strong>Error functions</strong> and <strong>Loss functions</strong> in later chapters. However, for now, let us use Figure <a href="machinelearning1.html#fig:localminima">9.70</a> to talk about the basic concepts of Global and Local Minima. The local minima is at <span class="math inline">\(x = 1\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:localminima"></span>
<img src="localminima.png" alt="Global and Local Minima" width="60%" />
<p class="caption">
Figure 9.70: Global and Local Minima
</p>
</div>
<p>In training a good model, we notice that the result of our <strong>Loss function</strong> for every iteration during training begins to follow a convex curve when charted. We hope to find the minimum point somewhere along the convex curve - that is our minimum error. However, there may be cases when the curve does not necessarily form a single convex curve. It may form multiple convex shapes or saddle areas, each with multiple minimum points - we call these points our local minima. Our hope in this situation is to find the global minima - the smallest minima. </p>
<p>We shall see how we minimize <strong>Loss functions</strong> and avoid local minima when we discuss <strong>Neural Network</strong>, which involves using gradient descent and all other optimization strategies.</p>
</div>
<div id="regularization" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.8</span> Regularization<a href="machinelearning1.html#regularization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Regularization</strong> is mostly used as an added <strong>tuning</strong> technique during <strong>learning</strong>. The idea is to calibrate hyperparameters and thus tune modeling algorithms. In doing so, we effectively optimize our models and reduce model complexity. It can, therefore, also effectively disqualify features by penalizing coefficients (a.l.a embedded-based feature selection in essence). </p>
<p>Recall that <strong>Regularization</strong> is a way to reward or penalize. In linear regression, we use regularization to reward or penalize coefficients through the <strong>Loss functions</strong>, either diminishing or amplifying the effect of the <strong>Loss</strong>. Our goal is to guide coefficients, so the model fits even more optimal among data points. Note that <strong>OLS</strong> fits a model by adjusting two <strong>model parameters</strong>, also called <strong>coefficients</strong>, namely <strong>slope</strong> and <strong>interface</strong>. We add weight to the two coefficients by introducing a tunable <strong>regularization hyperparameter</strong>, namely the <strong>lambda</strong> <span class="math inline">\(\lambda\)</span>. We then use an updated <strong>Loss</strong> function that accommodates the hyperparameter to specifically minimize error, e.g., <strong>MSE</strong> (mean square error); hence, calibration is about minimizing our objective (or loss) function.</p>
<p>Let us review three <strong>regularization</strong> techniques in this section, namely <strong>ridge</strong>, <strong>lasso</strong>, and <strong>elastic net</strong>. First, let us mention a couple of <strong>loss</strong> functions we use during <strong>cross-validation</strong>.</p>
<ul>
<li><strong>L1-norm vs L2-norm</strong>  </li>
</ul>
<p>The <strong>least absolute deviation (LAD)</strong> is referred to as <strong>L1-norm</strong> and is expressed mathematically as: </p>
<p><span class="math display">\[\begin{align}
LS_{L1-norm} = \sum_{i=1} |y_{i} - f(x_{i})|
\end{align}\]</span></p>
<p>The <strong>least squared error (LSE)</strong> is referred to as <strong>L2-norm</strong> and is expressed mathematically as: </p>
<p><span class="math display">\[\begin{align}
LS_{L2-norm} = \sum_{i=1} (y_{i} - f(x_{i}))^2
\end{align}\]</span></p>
<p>The difference in the formula is between using absolute difference versus using squared difference.</p>
<ul>
<li><strong>L1-loss vs L2-loss</strong>  </li>
</ul>
<p>The <strong>mean absolute error (MAE)</strong> is referred to as L1-loss and is expressed mathematically as: </p>
<p><span class="math display">\[\begin{align}
MAE_{L1} = \frac{LS_{L1-norm}}{N}
\end{align}\]</span></p>
<p>The <strong>mean squared error (MSE)</strong> is referred to as L2-loss and is expressed mathematically as:  </p>
<p><span class="math display">\[\begin{align}
MSE_{L2} = \frac{LS_{L2-norm}}{N}
\end{align}\]</span></p>
<p>As for <strong>Regularization</strong>, we focus on using <strong>lambda</strong> as a tuning hyperparameter. The first to cover is <strong>Ridge</strong> regularization.</p>
<p><strong>Ridge Regularization (alpha=0)</strong> </p>
<p>With <strong>Ridge regularization</strong>, we have an updated formula for coefficients <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\hat{\beta}_{ridge} = 
\underbrace{(X^{T}X + \lambda I)^{-1}X^{T}y}_{\text{Regularized OLS thru Ridge}}
\ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ \ \underbrace{(X^{T}X)^{-1}X^{T}y}_{OLS}
\end{align}\]</span></p>
<p>Our least square objective <span class="math inline">\(LS_{(objective)}\)</span> is expressed as:</p>
<p><span class="math display">\[\begin{align}
LS_{(objective)} = | y - X\beta|^2 = \sum_{i=1}^n (y_{i} - x_{i}\beta)^2,
\end{align}\]</span></p>
<p>We modify the function to accommodate the <strong>Ridge regularization</strong> like so:</p>
<p><span class="math display">\[\begin{align}
RSS_{(\beta_{ridge})} = |y - X\beta|^2 + \lambda \beta^2 = 
\underbrace{\sum_{i=1}^n (y_{i} - x_{i}\beta)^2}_{\text{RSS}} + 
\underbrace{\lambda \sum_{i=1}^n \beta_i^2}_{\text{Penalty Term}}
\end{align}\]</span></p>
<p>Notice the addition of <strong>lambda</strong> denoted as <span class="math inline">\(\lambda I\)</span>. This tuning hyperparameter allows shrinking the coefficients, <span class="math inline">\(\beta s\)</span>, making Ridge coefficients lesser than Least-Square coefficients.</p>
<p>Our objective function for <strong>Ridge regression</strong> is therefore expressed as such:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta}_{ridge} = \text{arg}\ \underset{\beta}{\mathrm{min}}\ RSS_{(\beta_{ridge})}
\end{align}\]</span></p>
<p>To illustrate, let us use the <strong>glmnet(.)</strong> function to generate a <strong>General Linear Regression model</strong>. We provide <strong>alpha</strong> <span class="math inline">\((\alpha)\)</span> equal to 0 to indicate we are using <strong>ridge regression</strong> (see Figure <a href="machinelearning1.html#fig:ridgereg1">9.71</a>).</p>

<div class="sourceCode" id="cb1203"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1203-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a>
<a class="sourceLine" id="cb1203-2" data-line-number="2">x =<span class="st">  </span><span class="kw">data.matrix</span>( mtcars[,<span class="op">!</span>(<span class="kw">names</span>(mtcars) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>))] )</a>
<a class="sourceLine" id="cb1203-3" data-line-number="3">y =<span class="st">  </span>mtcars[, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)]</a>
<a class="sourceLine" id="cb1203-4" data-line-number="4">ridge.model =<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1203-5" data-line-number="5"><span class="kw">plot</span>(ridge.model, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>,  <span class="dt">label=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1203-6" data-line-number="6"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">legend=</span><span class="kw">colnames</span>(x), <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1203-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ridgereg1"></span>
<img src="DS_files/figure-html/ridgereg1-1.png" alt="Ridge Regularization" width="70%" />
<p class="caption">
Figure 9.71: Ridge Regularization
</p>
</div>

<p>Figure <a href="machinelearning1.html#fig:ridgereg1">9.71</a> shows a plot of <strong>coefficients</strong>, each corresponding to a feature variable in the <strong>mtcars</strong> dataset. Each coefficient converges toward zero from left to right as <strong>log lambda</strong> increases, virtually eliminating the features. If we settle the <strong>log lambda</strong> at 4, we can see that <strong>am</strong>, <strong>drat</strong>, <strong>vs</strong>, <strong>gear</strong>, <strong>disp</strong>, <strong>cyl</strong>, <strong>wt</strong>, and <strong>qsec</strong> features all have coefficients that are non-zero. On the other hand, <strong>qsec</strong> is close to being zero. In other words, it seems only eight out of ten are important features just based on observing the plot.</p>
<p>Let us perform <strong>cross-validation</strong> using <strong>cv.glmnet(.)</strong> function to confirm our assessment of the eight coefficients. Here, we split the <strong>mtcars</strong> dataset into 5 sets (see Figure <a href="machinelearning1.html#fig:ridgereg2">9.72</a>).</p>

<div class="sourceCode" id="cb1204"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1204-1" data-line-number="1">cv.ridge =<span class="st"> </span><span class="kw">cv.glmnet</span>(x, y, <span class="dt">nfolds=</span><span class="dv">5</span>, <span class="dt">type=</span><span class="st">&quot;mse&quot;</span>, <span class="dt">alpha=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1204-2" data-line-number="2"><span class="kw">plot</span>(cv.ridge)</a>
<a class="sourceLine" id="cb1204-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ridgereg2"></span>
<img src="DS_files/figure-html/ridgereg2-1.png" alt="Ridge Regularization (CV)" width="70%" />
<p class="caption">
Figure 9.72: Ridge Regularization (CV)
</p>
</div>

<p>In Figure <a href="machinelearning1.html#fig:ridgereg2">9.72</a>, there are two vertical dotted lines. The first dotted line represents the minimum lambda, and the second dotted line represents <strong>one standard error</strong> (<strong>1se</strong>) from the minimum lambda. It also shows that the range between the minimum lambda and the lambda is one standard error away, covering a minimum <strong>MSE</strong>, making any lambdas within the range candidates for regularization. </p>

<div class="sourceCode" id="cb1205"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1205-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;Minimum Lambda&quot;</span>=<span class="st"> </span>(<span class="dt">lambda.min =</span> cv.ridge<span class="op">$</span>lambda.min),</a>
<a class="sourceLine" id="cb1205-2" data-line-number="2">  <span class="st">&quot;1se Lambda&quot;</span> =<span class="st"> </span>(<span class="dt">lambda.1se =</span> cv.ridge<span class="op">$</span>lambda<span class="fl">.1</span>se))</a></code></pre></div>
<pre><code>## Minimum Lambda     1se Lambda 
##          2.747          9.206</code></pre>

<p>Note that as we use <strong>cross-validation</strong>, we need to compute the <strong>MSE</strong> between <strong>training sets</strong>. The equivalent specific <strong>lambdas</strong> for <strong>min</strong> and <strong>1se</strong> are computed using the following:</p>

<div class="sourceCode" id="cb1207"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1207-1" data-line-number="1">idx.min =<span class="st"> </span><span class="kw">which</span>(cv.ridge<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>lambda.min)</a>
<a class="sourceLine" id="cb1207-2" data-line-number="2">idx<span class="fl">.1</span>se =<span class="st"> </span><span class="kw">which</span>(cv.ridge<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>lambda<span class="fl">.1</span>se) </a>
<a class="sourceLine" id="cb1207-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;MSE@lambda.min&quot;</span> =<span class="st"> </span>cv.ridge<span class="op">$</span>cvm[idx.min],</a>
<a class="sourceLine" id="cb1207-4" data-line-number="4">  <span class="st">&quot;MSE@lambda.1se&quot;</span> =<span class="st"> </span>cv.ridge<span class="op">$</span>cvm[idx<span class="fl">.1</span>se],</a>
<a class="sourceLine" id="cb1207-5" data-line-number="5">  <span class="st">&quot;1se&quot;</span> =<span class="st"> </span>cv.ridge<span class="op">$</span>cvsd[idx.min],</a>
<a class="sourceLine" id="cb1207-6" data-line-number="6">  <span class="st">&quot;MSE(our 1se)&quot;</span>  =<span class="st"> </span>cv.ridge<span class="op">$</span>cvm[idx.min] <span class="op">+</span><span class="st"> </span>cv.ridge<span class="op">$</span>cvsd[idx.min]</a>
<a class="sourceLine" id="cb1207-7" data-line-number="7">  )</a></code></pre></div>
<pre><code>## MSE@lambda.min MSE@lambda.1se            1se   MSE(our 1se) 
##          6.881          7.965          1.228          8.109</code></pre>

<p>If we choose <strong>lambda.1se</strong> (9.2061), then we have the following coefficients for the features:</p>

<div class="sourceCode" id="cb1209"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1209-1" data-line-number="1"><span class="kw">coef</span>(cv.ridge, <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>)</a></code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                    1
## (Intercept) 19.85948
## cyl         -0.36149
## disp        -0.00524
## hp          -0.00962
## drat         0.98359
## wt          -0.86947
## qsec         0.15136
## vs           0.87153
## am           1.17967
## gear         0.49498
## carb        -0.36970</code></pre>

<p>Equivalently, we can arbitrarily plug the value of <strong>lambda.1se</strong> and run <strong>regression</strong> like so:</p>
<div class="sourceCode" id="cb1211"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1211-1" data-line-number="1">ridge.model =<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha=</span><span class="dv">0</span>, <span class="dt">lambda=</span>lambda<span class="fl">.1</span>se)</a>
<a class="sourceLine" id="cb1211-2" data-line-number="2"><span class="kw">coef</span>(ridge.model)</a></code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                    s0
## (Intercept) 19.862051
## cyl         -0.361608
## disp        -0.005244
## hp          -0.009626
## drat         0.984155
## wt          -0.869465
## qsec         0.151322
## vs           0.870986
## am           1.179181
## gear         0.494695
## carb        -0.369557</code></pre>
<p>Afterwhich, we can then perform prediction using the ridge model:</p>
<div class="sourceCode" id="cb1213"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1213-1" data-line-number="1">ridge.predict =<span class="st"> </span><span class="kw">predict.glmnet</span>(ridge.model, <span class="dt">newx =</span> x)</a></code></pre></div>
<p>As one can see, based on our <strong>cross-validation</strong>, it seems that our <strong>cross-validation</strong> does show that all ten coefficients are used as long as our lambda between the range <strong>lambda.min</strong> and <strong>lambda.1se</strong>, which fall under a minimum <strong>MSE</strong>. It does not force the <strong>coefficients</strong> to become zero. Therefore, we still see that all ten coefficients are kept. We will show <strong>LASSO</strong> regularization next, which forces <strong>coefficients</strong> to become zero.</p>
<p>A couple of notes to be aware of:</p>
<ul>
<li><p>Increase in ridge lambda decreases variance but increases bias.</p></li>
<li><p>Ridge avoids overfitting by penalizing coefficients. It means it shrinks beta coefficients reducing MSE and predicted error.</p></li>
</ul>
<p><strong>Lasso Regularization (alpha=1)</strong> </p>
<p><strong>LASSO</strong> stands for <strong>Least Absolute Selection and Shrinkage Operation</strong>. This <strong>regularization</strong> technique also decreases variance but increases bias. However, unlike <strong>Ridge regularization</strong>, it forces coefficients to become zero. Because of that, coefficients that are not significant tend toward zero and are eliminated, so in a way, this removes unwanted input variables.</p>
<p>Similarly, with <strong>Lasso regularization</strong>, we have an updated formula for coefficients <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\hat{\beta}_{lasso} = 
\underbrace{(X^{T}X + \lambda I)^{-1}X^{T}y}_{\text{Regularized OLS thru Lasso}}
\ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ \ \underbrace{(X^{T}X)^{-1}X^{T}y}_{OLS}
\end{align}\]</span></p>
<p>Our least square objective <span class="math inline">\(LS_{(objective)}\)</span> is expressed as:</p>
<p><span class="math display">\[\begin{align}
LS_{objective} = | y - X\beta|^2 = \sum_{i=1}^n (y_{i} - x_{i}\beta)^2, 
\end{align}\]</span></p>
<p>We modify the function to accommodate the <strong>ridge regularization</strong> like so:</p>
<p><span class="math display">\[\begin{align}
RSS_{(\beta_{lasso})} = |y - X\beta|^2 + \lambda |\beta| = 
\underbrace{\sum_{i=1}^n (y_{i} - x_{i}\beta)^2}_{\text{RSS}} + 
\underbrace{\lambda \sum_{i=1}^n |\beta_i|}_{\text{Penalty Term}},  
\end{align}\]</span></p>
<p>Our objective function for <strong>Ridge regression</strong> is therefore expressed as such:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta}_{lasso} = \text{arg}\ \underset{\beta}{\mathrm{min}}\ RSS_{(\beta_{lasso})}.
\end{align}\]</span></p>
<p>To illustrate, we provide <strong>alpha</strong> <span class="math inline">\((\alpha)\)</span> equal to 1 to indicate we are using <strong>LASSO regression</strong> (see Figure <a href="machinelearning1.html#fig:lassoreg1">9.73</a>).</p>

<div class="sourceCode" id="cb1214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1214-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a>
<a class="sourceLine" id="cb1214-2" data-line-number="2">x =<span class="st">  </span><span class="kw">data.matrix</span>( mtcars[,<span class="op">!</span>(<span class="kw">names</span>(mtcars) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>))] )</a>
<a class="sourceLine" id="cb1214-3" data-line-number="3">y =<span class="st">  </span>mtcars[, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)]</a>
<a class="sourceLine" id="cb1214-4" data-line-number="4">lasso.model =<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1214-5" data-line-number="5"><span class="kw">plot</span>(lasso.model, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>,  <span class="dt">label=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1214-6" data-line-number="6"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">legend=</span><span class="kw">colnames</span>(x), <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1214-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lassoreg1"></span>
<img src="DS_files/figure-html/lassoreg1-1.png" alt="Lasso Regularization" width="70%" />
<p class="caption">
Figure 9.73: Lasso Regularization
</p>
</div>

<p>Figure <a href="machinelearning1.html#fig:lassoreg1">9.73</a> shows a plot of <strong>coefficients</strong>, each corresponding to a feature variable in the <strong>mtcars</strong> dataset. From left to right, each coefficient converges toward zero as <strong>log lambda</strong> increases, virtually eliminating the feature. Unlike <strong>ridge</strong>, we can see in the plot that given a <strong>log lambda</strong> equal to zero, the number of coefficients (see top labels) is equal to three. Just by observation, the three features corresponding to the coefficients are <strong>wt</strong>, <strong>cyl</strong>, and <strong>hp</strong>.</p>
<p>Let us perform <strong>cross-validation</strong> using <strong>cv.glmnet(.)</strong> function to confirm our assessment of the three coefficients. Here, as before, we split the <strong>mtcars</strong> dataset into 5 sets (see Figure <a href="machinelearning1.html#fig:lassoreg2">9.74</a>).</p>
<div class="sourceCode" id="cb1215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1215-1" data-line-number="1">cv.lasso =<span class="st"> </span><span class="kw">cv.glmnet</span>(x, y, <span class="dt">nfolds=</span><span class="dv">5</span>, <span class="dt">type=</span><span class="st">&quot;mse&quot;</span>, <span class="dt">alpha=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1215-2" data-line-number="2"><span class="kw">plot</span>(cv.lasso)</a>
<a class="sourceLine" id="cb1215-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lassoreg2"></span>
<img src="DS_files/figure-html/lassoreg2-1.png" alt="Lasso Regularization (CV)" width="70%" />
<p class="caption">
Figure 9.74: Lasso Regularization (CV)
</p>
</div>
<p>In Figure <a href="machinelearning1.html#fig:lassoreg2">9.74</a>, we see the range between the minimum lambda and the lambda that is one standard error away, which covers a minimum <strong>MSE</strong>, making any lambdas within the range candidates for regularization.</p>
<div class="sourceCode" id="cb1216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1216-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;Minimum Lambda&quot;</span>=<span class="st"> </span>(<span class="dt">lambda.min =</span> cv.lasso<span class="op">$</span>lambda.min),</a>
<a class="sourceLine" id="cb1216-2" data-line-number="2">  <span class="st">&quot;1se Lambda&quot;</span> =<span class="st"> </span>(<span class="dt">lambda.1se =</span> cv.lasso<span class="op">$</span>lambda<span class="fl">.1</span>se))</a></code></pre></div>
<pre><code>## Minimum Lambda     1se Lambda 
##         0.1246         1.1617</code></pre>
<p>It can be observed that the vertical dotted line for the <strong>lambda.1se</strong> points towards a lesser number of coefficients.</p>
<p>Note that as we are using <strong>cross-validation</strong>, we need to compute the <strong>MSE</strong> between <strong>training sets</strong>. The equivalent specific <strong>lambdas</strong> for <strong>min</strong> and <strong>1se</strong> are computed using the following:
</p>
<div class="sourceCode" id="cb1218"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1218-1" data-line-number="1">idx.min =<span class="st"> </span><span class="kw">which</span>(cv.lasso<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>lambda.min)</a>
<a class="sourceLine" id="cb1218-2" data-line-number="2">idx<span class="fl">.1</span>se =<span class="st"> </span><span class="kw">which</span>(cv.lasso<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>lambda<span class="fl">.1</span>se) </a>
<a class="sourceLine" id="cb1218-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;MSE@lambda.min&quot;</span> =<span class="st"> </span>cv.lasso<span class="op">$</span>cvm[idx.min],</a>
<a class="sourceLine" id="cb1218-4" data-line-number="4">  <span class="st">&quot;MSE@lambda.1se&quot;</span> =<span class="st"> </span>cv.lasso<span class="op">$</span>cvm[idx<span class="fl">.1</span>se],</a>
<a class="sourceLine" id="cb1218-5" data-line-number="5">  <span class="st">&quot;1se&quot;</span> =<span class="st"> </span>cv.lasso<span class="op">$</span>cvsd[idx.min],</a>
<a class="sourceLine" id="cb1218-6" data-line-number="6">  <span class="st">&quot;MSE(our 1se)&quot;</span>  =<span class="st"> </span>cv.lasso<span class="op">$</span>cvm[idx.min] <span class="op">+</span><span class="st"> </span>cv.lasso<span class="op">$</span>cvsd[idx.min]</a>
<a class="sourceLine" id="cb1218-7" data-line-number="7">  )</a></code></pre></div>
<pre><code>## MSE@lambda.min MSE@lambda.1se            1se   MSE(our 1se) 
##         8.7300         9.5551         0.8342         9.5642</code></pre>

<p>If we choose <strong>lambda.1se</strong> (1.1617), then we have the following coefficients for the features:</p>
<div class="sourceCode" id="cb1220"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1220-1" data-line-number="1"><span class="kw">coef</span>(cv.lasso, <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>)</a></code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                     1
## (Intercept) 34.758210
## cyl         -0.860295
## disp         .       
## hp          -0.008835
## drat         .       
## wt          -2.501672
## qsec         .       
## vs           .       
## am           .       
## gear         .       
## carb         .</code></pre>
<p>Equivalently, we can arbitrarily plug the value of <strong>lambda.1se</strong> and run <strong>regression</strong> like so:</p>
<div class="sourceCode" id="cb1222"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1222-1" data-line-number="1">lasso.model =<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha=</span><span class="dv">1</span>, <span class="dt">lambda=</span>lambda<span class="fl">.1</span>se)</a>
<a class="sourceLine" id="cb1222-2" data-line-number="2"><span class="kw">coef</span>(lasso.model)</a></code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                    s0
## (Intercept) 34.757287
## cyl         -0.859728
## disp         .       
## hp          -0.008845
## drat         .       
## wt          -2.502011
## qsec         .       
## vs           .       
## am           .       
## gear         .       
## carb         .</code></pre>
<p>Notice that we only see three features listed, namely <strong>cyl</strong>, <strong>hp</strong>, and <strong>wt</strong>.</p>
<p>We can then perform prediction using the lasso model:</p>
<div class="sourceCode" id="cb1224"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1224-1" data-line-number="1">lasso.predict =<span class="st"> </span><span class="kw">predict.glmnet</span>(lasso.model, <span class="dt">newx =</span> x)</a></code></pre></div>
<p><strong>Elastic Net Regularization (alpha=0.5)</strong> </p>
<p>We leave readers to investigate <strong>Elastic Net</strong>. This technique meets in between both <strong>Ridge</strong> and <strong>LASSO</strong> regularization. It accepts <strong>alpha</strong> equal to 0.5, e.g. (see Figure <a href="machinelearning1.html#fig:elasticnet1">9.75</a>).</p>
<div class="sourceCode" id="cb1225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1225-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a>
<a class="sourceLine" id="cb1225-2" data-line-number="2">x =<span class="st">  </span><span class="kw">data.matrix</span>( mtcars[,<span class="op">!</span>(<span class="kw">names</span>(mtcars) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>))] )</a>
<a class="sourceLine" id="cb1225-3" data-line-number="3">y =<span class="st">  </span>mtcars[, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)]</a>
<a class="sourceLine" id="cb1225-4" data-line-number="4">elastic.model =<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb1225-5" data-line-number="5"><span class="kw">plot</span>(elastic.model, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>,  <span class="dt">label=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1225-6" data-line-number="6"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">legend=</span><span class="kw">colnames</span>(x), <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1225-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:elasticnet1"></span>
<img src="DS_files/figure-html/elasticnet1-1.png" alt="Elastic Net Regularization" width="70%" />
<p class="caption">
Figure 9.75: Elastic Net Regularization
</p>
</div>
<p>And for <strong>cross-validation</strong> of <strong>Elastic Net</strong>, see Figure <a href="machinelearning1.html#fig:elasticnet2">9.76</a>.</p>
<div class="sourceCode" id="cb1226"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1226-1" data-line-number="1">cv.elastic =<span class="st"> </span><span class="kw">cv.glmnet</span>(x, y, <span class="dt">nfolds=</span><span class="dv">5</span>, <span class="dt">type=</span><span class="st">&quot;mse&quot;</span>, <span class="dt">alpha=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb1226-2" data-line-number="2"><span class="kw">plot</span>(cv.elastic)</a>
<a class="sourceLine" id="cb1226-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:elasticnet2"></span>
<img src="DS_files/figure-html/elasticnet2-1.png" alt="Elastic Net Regularization (CV)" width="70%" />
<p class="caption">
Figure 9.76: Elastic Net Regularization (CV)
</p>
</div>
<p>We also leave readers to investigate <strong>Coordinate Descent</strong> for <strong>Lasso</strong>, which may be helpful for multivariate multinomial predictors.</p>
</div>
</div>
<div id="supervised-vs.unsupervised-learning" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.8</span> Supervised vs. Unsupervised Learning  <a href="machinelearning1.html#supervised-vs.unsupervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In <strong>Computational Learning</strong>, there are two common types of <strong>learning</strong> discussed in other literature, namely <strong>supervised versus unsupervised learning</strong>. Our simple definition of <strong>supervised learning</strong> falls under the premise that a model can be trained if labels exist in a dataset being used for training and that the dataset is well labeled. However, if the dataset is not equipped with labels, then learning becomes <strong>unsupervised</strong>. In the following chapter, we cover <strong>Clustering</strong> as an <strong>unsupervised learning</strong> category.</p>
<p>This section relies on two <strong>supervised learning</strong> categories when dealing with <strong>predictions</strong>. If our dataset consists of a <strong>continuous</strong> response variable, we use <strong>regression</strong> to train the model. On the other hand, if our response variable is categorical, we use <strong>classification</strong> to train the model.</p>
<p>Also, if <strong>labels</strong> are dichotomous (binary) in form, we can use <strong>logistic regression</strong> for classification. There are pieces of literature that cover dichotomizing continuous labels or even performing <strong>quantile regression</strong>; however, this is outside the scope of our discussions, and thus we leave readers to investigate the subject further.</p>
<p>In terms of predictor variables (features), some ML algorithms may prefer features to be either all continuous or all categorical but not mixed. If our dataset is mixed with both continuous and categorical features, it may be best to perform <strong>transformation</strong> to have a dataset with only all categorical or all continuous features.</p>
<p>The choice of <strong>transformation</strong> depends upon necessary considerations. Some features may correlate well with the response variable in their transformed version. Some <strong>categorical</strong> features may have very high cardinality - many distinct values. The use of <strong>One Hot Encoding</strong> may not be practical as a <strong>transformation</strong> method for some features as this creates a high number of <strong>dummy variables</strong>. A good approach is perhaps to use <strong>stratification</strong> by grouping categories into <strong>strata</strong>. Using <strong>weight of evidence (WoE)</strong> and <strong>Information Value (IV)</strong>, we may be able to group categories into their corresponding <strong>WoE</strong> or <strong>IV</strong> values in continuous form.</p>
<p>On the other hand, some <strong>continuous</strong> features can be <strong>discretized</strong> into <strong>bins</strong> if the goal is to have a dataset with only <strong>categorical</strong> features.</p>
</div>
<div id="summary-6" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.9</span> Summary<a href="machinelearning1.html#summary-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Having gone through <strong>Exploratory Data Analysis</strong>, let us now move on to discuss <strong>supervised learning</strong>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="machinelearning2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
