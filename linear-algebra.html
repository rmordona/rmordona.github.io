<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Numerical Linear Algebra I | The Power and Art of Approximation</title>
  <meta name="description" content="Inspired by the vast amount of knowledge across a wide span of fields, this book covers a compendium of both analytical and numerical techniques, conflated into a common idea to showcase the fundamental requirements of Data Science and Machine Learning (ML) Engineering. Our common theme across the book is intuition, contemplating more on fundamental operations than mathematical rigor. This book is written for those who are new to Data Science and have developed some proclivity towards this field but may not know where to begin. The hope is that we can introduce some fundamental aspects of Data Science in a more progressive and possibly structured manner. Depending on interest, this book tries to avoid being specific to a target audience. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher in a specific domain, or, for that matter, an undergraduate student just trying to get into this field. As a starting point and as a supplemental reference for anyone (professional or not alike) wanting to pursue Data Science in conjunction with his or her domain, it is essential to take a refresh of mathematical concepts first which we encourage readers to take this first step. For that reason, we cover a list of mathematical concepts that are no doubt valuable to get us to Machine Learning concepts eventually. Only a certain elementary and introductory portion of each field of mathematics are covered while we put emphasis only on the relevant and essential areas. But while that is the case, admittedly, the first half (or the first volume) of this book talks about Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. This is founded upon the idea that most of what we do in Data Science is expressed in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide on the basis of close approximation in many situations. And it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they fundamentally depend upon. The second half of the book covers ML methods such as Linear Regression, Regression and Classification Trees, Random Forest, XGBoost, SVM, and many others. It covers clustering such as KNN, Hierarchical clustering, and DBSCAN. Finally, it covers Deep Neural Networks such as CNN, RNN (LSTM/GRU), ResNet, and Transformers." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Numerical Linear Algebra I | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Inspired by the vast amount of knowledge across a wide span of fields, this book covers a compendium of both analytical and numerical techniques, conflated into a common idea to showcase the fundamental requirements of Data Science and Machine Learning (ML) Engineering. Our common theme across the book is intuition, contemplating more on fundamental operations than mathematical rigor. This book is written for those who are new to Data Science and have developed some proclivity towards this field but may not know where to begin. The hope is that we can introduce some fundamental aspects of Data Science in a more progressive and possibly structured manner. Depending on interest, this book tries to avoid being specific to a target audience. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher in a specific domain, or, for that matter, an undergraduate student just trying to get into this field. As a starting point and as a supplemental reference for anyone (professional or not alike) wanting to pursue Data Science in conjunction with his or her domain, it is essential to take a refresh of mathematical concepts first which we encourage readers to take this first step. For that reason, we cover a list of mathematical concepts that are no doubt valuable to get us to Machine Learning concepts eventually. Only a certain elementary and introductory portion of each field of mathematics are covered while we put emphasis only on the relevant and essential areas. But while that is the case, admittedly, the first half (or the first volume) of this book talks about Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. This is founded upon the idea that most of what we do in Data Science is expressed in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide on the basis of close approximation in many situations. And it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they fundamentally depend upon. The second half of the book covers ML methods such as Linear Regression, Regression and Classification Trees, Random Forest, XGBoost, SVM, and many others. It covers clustering such as KNN, Hierarchical clustering, and DBSCAN. Finally, it covers Deep Neural Networks such as CNN, RNN (LSTM/GRU), ResNet, and Transformers." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Numerical Linear Algebra I | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Inspired by the vast amount of knowledge across a wide span of fields, this book covers a compendium of both analytical and numerical techniques, conflated into a common idea to showcase the fundamental requirements of Data Science and Machine Learning (ML) Engineering. Our common theme across the book is intuition, contemplating more on fundamental operations than mathematical rigor. This book is written for those who are new to Data Science and have developed some proclivity towards this field but may not know where to begin. The hope is that we can introduce some fundamental aspects of Data Science in a more progressive and possibly structured manner. Depending on interest, this book tries to avoid being specific to a target audience. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher in a specific domain, or, for that matter, an undergraduate student just trying to get into this field. As a starting point and as a supplemental reference for anyone (professional or not alike) wanting to pursue Data Science in conjunction with his or her domain, it is essential to take a refresh of mathematical concepts first which we encourage readers to take this first step. For that reason, we cover a list of mathematical concepts that are no doubt valuable to get us to Machine Learning concepts eventually. Only a certain elementary and introductory portion of each field of mathematics are covered while we put emphasis only on the relevant and essential areas. But while that is the case, admittedly, the first half (or the first volume) of this book talks about Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. This is founded upon the idea that most of what we do in Data Science is expressed in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide on the basis of close approximation in many situations. And it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they fundamentally depend upon. The second half of the book covers ML methods such as Linear Regression, Regression and Classification Trees, Random Forest, XGBoost, SVM, and many others. It covers clustering such as KNN, Hierarchical clustering, and DBSCAN. Finally, it covers Deep Neural Networks such as CNN, RNN (LSTM/GRU), ResNet, and Transformers." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2022-02-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="numerical-methods.html"/>
<link rel="next" href="numerical-linear-algebra.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#acknowledgment-and-motivations"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i><b>0.2</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.3" data-path="mathematical-notation.html"><a href="mathematical-notation.html#notation"><i class="fa fa-check"></i><b>0.3</b> Notation</a></li>
<li class="chapter" data-level="0.4" data-path="mathematical-notation.html"><a href="mathematical-notation.html#number-system"><i class="fa fa-check"></i><b>0.4</b> Number System</a></li>
<li class="chapter" data-level="0.5" data-path="mathematical-notation.html"><a href="mathematical-notation.html#implementation"><i class="fa fa-check"></i><b>0.5</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numerical-methods.html"><a href="numerical-methods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="numerical-methods.html"><a href="numerical-methods.html#closed-form-equation"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="numerical-methods.html"><a href="numerical-methods.html#analytical-and-numerical-solutions"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="numerical-methods.html"><a href="numerical-methods.html#significant-figures"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="numerical-methods.html"><a href="numerical-methods.html#accuracy"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="numerical-methods.html"><a href="numerical-methods.html#precision"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="numerical-methods.html"><a href="numerical-methods.html#stability-and-sensitivity"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="numerical-methods.html"><a href="numerical-methods.html#stiffness-and-implicitness"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="numerical-methods.html"><a href="numerical-methods.html#conditioning-and-posedness"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-algebra.html"><a href="linear-algebra.html#system-of-linear-equations"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="linear-algebra.html"><a href="linear-algebra.html#scalar-vector-and-matrix-tensor"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="linear-algebra.html"><a href="linear-algebra.html#transposition-and-multiplication"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="linear-algebra.html"><a href="linear-algebra.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="linear-algebra.html"><a href="linear-algebra.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="linear-algebra.html"><a href="linear-algebra.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="linear-algebra.html"><a href="linear-algebra.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linear-algebra.html"><a href="linear-algebra.html#magnitude-direction-unit-vectors"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="linear-algebra.html"><a href="linear-algebra.html#linear-combination-and-independence"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="linear-algebra.html"><a href="linear-algebra.html#space-span-and-basis"><i class="fa fa-check"></i><b>2.6</b> Space, Span and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="linear-algebra.html"><a href="linear-algebra.html#determinants"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="linear-algebra.html"><a href="linear-algebra.html#minors-cofactors-and-adjugate-forms"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="linear-algebra.html"><a href="linear-algebra.html#inverse-form-and-row-echelon-form"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="linear-algebra.html"><a href="linear-algebra.html#linear-transformations"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="linear-algebra.html"><a href="linear-algebra.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="linear-algebra.html"><a href="linear-algebra.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="linear-algebra.html"><a href="linear-algebra.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="linear-algebra.html"><a href="linear-algebra.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="linear-algebra.html"><a href="linear-algebra.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="linear-algebra.html"><a href="linear-algebra.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="linear-algebra.html"><a href="linear-algebra.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="linear-algebra.html"><a href="linear-algebra.html#rank-and-nullity"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="linear-algebra.html"><a href="linear-algebra.html#singularity-and-triviality"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="linear-algebra.html"><a href="linear-algebra.html#orthogonality-and-orthonormality"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="linear-algebra.html"><a href="linear-algebra.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="linear-algebra.html"><a href="linear-algebra.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.15</b> Eigen (Spectral) Decomposition</a></li>
<li class="chapter" data-level="2.16" data-path="linear-algebra.html"><a href="linear-algebra.html#diagonalizability-of-a-matrix"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="linear-algebra.html"><a href="linear-algebra.html#trace-of-a-square-matrix"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="linear-algebra.html"><a href="linear-algebra.html#algebraic-and-geometric-multiplicity"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="linear-algebra.html"><a href="linear-algebra.html#types-of-matrices"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-factorization"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="linear-algebra.html"><a href="linear-algebra.html#eigen-spectral-decomposition-1"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="linear-algebra.html"><a href="linear-algebra.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Gauss-Jordan Elimination)</a></li>
<li class="chapter" data-level="2.20.3" data-path="linear-algebra.html"><a href="linear-algebra.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="linear-algebra.html"><a href="linear-algebra.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="linear-algebra.html"><a href="linear-algebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="linear-algebra.html"><a href="linear-algebra.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="linear-algebra.html"><a href="linear-algebra.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="linear-algebra.html"><a href="linear-algebra.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="linear-algebra.html"><a href="linear-algebra.html#software-libraries"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="linear-algebra.html"><a href="linear-algebra.html#summary"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#iteration-and-convergence"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#approximating-eigenvalues-and-eigenvectors-by-iteration-a-v-lambda-v"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and Eigenvectors by Iteration (<span class="math inline">\(A v = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#approximating-root-and-fixed-point-by-iteration"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#approximating-solutions-to-systems-of-equations-by-iteration-ax-b"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Equations by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#polynomial_regression"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#higher_degree_polynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#approximating-polynomial-functions-by-series-expansion"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#polynomial_interpolation"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#monomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Monomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#polynomial_smoothing"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#scatterplot-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Scatterplot Smoothing </a></li>
<li class="chapter" data-level="3.8.3" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.3</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#polynomial-optimization"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#simplex_method"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#dual_simplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#primal_dual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#summary-1"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numerical-calculus.html"><a href="numerical-calculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="numerical-calculus.html"><a href="numerical-calculus.html#calculus"><i class="fa fa-check"></i><b>4.1</b> Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numerical-calculus.html"><a href="numerical-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="numerical-calculus.html"><a href="numerical-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="numerical-calculus.html"><a href="numerical-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="numerical-calculus.html"><a href="numerical-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="numerical-calculus.html"><a href="numerical-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="numerical-calculus.html"><a href="numerical-calculus.html#approximation-by-numerical-integration"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="numerical-calculus.html"><a href="numerical-calculus.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="numerical-calculus.html"><a href="numerical-calculus.html#composite-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="numerical-calculus.html"><a href="numerical-calculus.html#adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.3</b> Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.4" data-path="numerical-calculus.html"><a href="numerical-calculus.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.4</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.5" data-path="numerical-calculus.html"><a href="numerical-calculus.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.5</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="numerical-calculus.html"><a href="numerical-calculus.html#approximation-by-numerical-differentiation"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="numerical-calculus.html"><a href="numerical-calculus.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="numerical-calculus.html"><a href="numerical-calculus.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="numerical-calculus.html"><a href="numerical-calculus.html#approximation-using-ordinary-differential-equations"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="numerical-calculus.html"><a href="numerical-calculus.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="numerical-calculus.html"><a href="numerical-calculus.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="numerical-calculus.html"><a href="numerical-calculus.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="numerical-calculus.html"><a href="numerical-calculus.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="numerical-calculus.html"><a href="numerical-calculus.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="numerical-calculus.html"><a href="numerical-calculus.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="numerical-calculus.html"><a href="numerical-calculus.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="numerical-calculus.html"><a href="numerical-calculus.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="numerical-calculus.html"><a href="numerical-calculus.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="numerical-calculus.html"><a href="numerical-calculus.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="numerical-calculus.html"><a href="numerical-calculus.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="numerical-calculus.html"><a href="numerical-calculus.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="numerical-calculus.html"><a href="numerical-calculus.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="numerical-calculus.html"><a href="numerical-calculus.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="numerical-calculus.html"><a href="numerical-calculus.html#approximation-using-functional-differential-equations"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="numerical-calculus.html"><a href="numerical-calculus.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="numerical-calculus.html"><a href="numerical-calculus.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="numerical-calculus.html"><a href="numerical-calculus.html#approximation-using-partial-differential-equations"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="numerical-calculus.html"><a href="numerical-calculus.html#the-poisson-equation"><i class="fa fa-check"></i><b>4.6.1</b> The Poisson Equation </a></li>
<li class="chapter" data-level="4.6.2" data-path="numerical-calculus.html"><a href="numerical-calculus.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="numerical-calculus.html"><a href="numerical-calculus.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="numerical-calculus.html"><a href="numerical-calculus.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.4</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.5" data-path="numerical-calculus.html"><a href="numerical-calculus.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.6" data-path="numerical-calculus.html"><a href="numerical-calculus.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.6</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="numerical-calculus.html"><a href="numerical-calculus.html#approximation-using-fourier-series-and-transform"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a></li>
<li class="chapter" data-level="4.8" data-path="numerical-calculus.html"><a href="numerical-calculus.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.8</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.9" data-path="numerical-calculus.html"><a href="numerical-calculus.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.9</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.10" data-path="numerical-calculus.html"><a href="numerical-calculus.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.10</b> Fast Fourier Transform (FFT)  </a></li>
<li class="chapter" data-level="4.11" data-path="numerical-calculus.html"><a href="numerical-calculus.html#summary-2"><i class="fa fa-check"></i><b>4.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="numerical-probability.html"><a href="numerical-probability.html"><i class="fa fa-check"></i><b>5</b> Numerical Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="numerical-probability.html"><a href="numerical-probability.html#approximation-based-on-random-chances"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="numerical-probability.html"><a href="numerical-probability.html#distribution"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="numerical-probability.html"><a href="numerical-probability.html#mass-and-density"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="numerical-probability.html"><a href="numerical-probability.html#probability"><i class="fa fa-check"></i><b>5.4</b> Probability </a></li>
<li class="chapter" data-level="5.5" data-path="numerical-probability.html"><a href="numerical-probability.html#probability-density-function-pdf"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="numerical-probability.html"><a href="numerical-probability.html#probability-mass-function-pmf"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="numerical-probability.html"><a href="numerical-probability.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="numerical-probability.html"><a href="numerical-probability.html#special-functions"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="numerical-probability.html"><a href="numerical-probability.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="numerical-probability.html"><a href="numerical-probability.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="numerical-probability.html"><a href="numerical-probability.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="numerical-probability.html"><a href="numerical-probability.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="numerical-probability.html"><a href="numerical-probability.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="numerical-probability.html"><a href="numerical-probability.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function</a></li>
<li class="chapter" data-level="5.8.7" data-path="numerical-probability.html"><a href="numerical-probability.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="numerical-probability.html"><a href="numerical-probability.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="numerical-probability.html"><a href="numerical-probability.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="numerical-probability.html"><a href="numerical-probability.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="numerical-probability.html"><a href="numerical-probability.html#distribution_types"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="numerical-probability.html"><a href="numerical-probability.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="numerical-probability.html"><a href="numerical-probability.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="numerical-probability.html"><a href="numerical-probability.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="numerical-probability.html"><a href="numerical-probability.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="numerical-probability.html"><a href="numerical-probability.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="numerical-probability.html"><a href="numerical-probability.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="numerical-probability.html"><a href="numerical-probability.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="numerical-probability.html"><a href="numerical-probability.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="numerical-probability.html"><a href="numerical-probability.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="numerical-probability.html"><a href="numerical-probability.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="numerical-probability.html"><a href="numerical-probability.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="numerical-probability.html"><a href="numerical-probability.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="numerical-probability.html"><a href="numerical-probability.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="numerical-probability.html"><a href="numerical-probability.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="numerical-probability.html"><a href="numerical-probability.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="numerical-probability.html"><a href="numerical-probability.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="numerical-probability.html"><a href="numerical-probability.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="numerical-probability.html"><a href="numerical-probability.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="numerical-probability.html"><a href="numerical-probability.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="numerical-probability.html"><a href="numerical-probability.html#wishart_distribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="numerical-probability.html"><a href="numerical-probability.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="numerical-probability.html"><a href="numerical-probability.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="numerical-probability.html"><a href="numerical-probability.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="numerical-probability.html"><a href="numerical-probability.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="numerical-probability.html"><a href="numerical-probability.html#summary-3"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="statistics.html"><a href="statistics.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="statistics.html"><a href="statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="statistics.html"><a href="statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="statistics.html"><a href="statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="statistics.html"><a href="statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="statistics.html"><a href="statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="statistics.html"><a href="statistics.html#inferential-statistics"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="statistics.html"><a href="statistics.html#the-significance-of-difference"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="statistics.html"><a href="statistics.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="statistics.html"><a href="statistics.html#t-test"><i class="fa fa-check"></i><b>6.3.2</b> T-Test </a></li>
<li class="chapter" data-level="6.3.3" data-path="statistics.html"><a href="statistics.html#z-test"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test </a></li>
<li class="chapter" data-level="6.3.4" data-path="statistics.html"><a href="statistics.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="statistics.html"><a href="statistics.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="statistics.html"><a href="statistics.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="statistics.html"><a href="statistics.html#chi-squared-test"><i class="fa fa-check"></i><b>6.3.7</b> Chi-squared Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="statistics.html"><a href="statistics.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="statistics.html"><a href="statistics.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="statistics.html"><a href="statistics.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="statistics.html"><a href="statistics.html#post-hoc-analysis"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="statistics.html"><a href="statistics.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="statistics.html"><a href="statistics.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="statistics.html"><a href="statistics.html#multiple-comparison-tests"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="statistics.html"><a href="statistics.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="statistics.html"><a href="statistics.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="statistics.html"><a href="statistics.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="statistics.html"><a href="statistics.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="statistics.html"><a href="statistics.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="statistics.html"><a href="statistics.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="statistics.html"><a href="statistics.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="statistics.html"><a href="statistics.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="statistics.html"><a href="statistics.html#statistical-modeling"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="statistics.html"><a href="statistics.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="statistics.html"><a href="statistics.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="statistics.html"><a href="statistics.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="statistics.html"><a href="statistics.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="statistics.html"><a href="statistics.html#regression-analysis"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="statistics.html"><a href="statistics.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="statistics.html"><a href="statistics.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="statistics.html"><a href="statistics.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="statistics.html"><a href="statistics.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="statistics.html"><a href="statistics.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="statistics.html"><a href="statistics.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="statistics.html"><a href="statistics.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="statistics.html"><a href="statistics.html#the-significance-of-regression"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="statistics.html"><a href="statistics.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="statistics.html"><a href="statistics.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="statistics.html"><a href="statistics.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="statistics.html"><a href="statistics.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="statistics.html"><a href="statistics.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="statistics.html"><a href="statistics.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="statistics.html"><a href="statistics.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="statistics.html"><a href="statistics.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="statistics.html"><a href="statistics.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="statistics.html"><a href="statistics.html#inference-for-regression"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="statistics.html"><a href="statistics.html#goodness-of-fit"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit </a></li>
<li class="chapter" data-level="6.9.2" data-path="statistics.html"><a href="statistics.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.2</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="statistics.html"><a href="statistics.html#summary-4"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian.html"><a href="bayesian.html#probability-1"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian.html"><a href="bayesian.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian.html"><a href="bayesian.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian.html"><a href="bayesian.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian.html"><a href="bayesian.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="bayesian.html"><a href="bayesian.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian.html"><a href="bayesian.html#probability-rules"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian.html"><a href="bayesian.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian.html"><a href="bayesian.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian.html"><a href="bayesian.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="bayesian.html"><a href="bayesian.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="bayesian.html"><a href="bayesian.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="bayesian.html"><a href="bayesian.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="bayesian.html"><a href="bayesian.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="bayesian.html"><a href="bayesian.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="bayesian.html"><a href="bayesian.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian.html"><a href="bayesian.html#bayes-theorem"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian.html"><a href="bayesian.html#naive-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naive Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian.html"><a href="bayesian.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="bayesian.html"><a href="bayesian.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian.html"><a href="bayesian.html#conjugacy"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian.html"><a href="bayesian.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian.html"><a href="bayesian.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian.html"><a href="bayesian.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="bayesian.html"><a href="bayesian.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="bayesian.html"><a href="bayesian.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="bayesian.html"><a href="bayesian.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="bayesian.html"><a href="bayesian.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="bayesian.html"><a href="bayesian.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="bayesian.html"><a href="bayesian.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="bayesian.html"><a href="bayesian.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="bayesian.html"><a href="bayesian.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="bayesian.html"><a href="bayesian.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian.html"><a href="bayesian.html#information-theory"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian.html"><a href="bayesian.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian.html"><a href="bayesian.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian.html"><a href="bayesian.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian.html"><a href="bayesian.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian.html"><a href="bayesian.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian.html"><a href="bayesian.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian.html"><a href="bayesian.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bayesian.html"><a href="bayesian.html#bayesian_inference"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="bayesian.html"><a href="bayesian.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="bayesian.html"><a href="bayesian.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="bayesian.html"><a href="bayesian.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="bayesian.html"><a href="bayesian.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="bayesian.html"><a href="bayesian.html#bayesian-models"><i class="fa fa-check"></i><b>7.7</b> Bayesian Models </a><ul>
<li class="chapter" data-level="7.7.1" data-path="bayesian.html"><a href="bayesian.html#belief-propagation"><i class="fa fa-check"></i><b>7.7.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="7.7.2" data-path="bayesian.html"><a href="bayesian.html#expectation-propagation"><i class="fa fa-check"></i><b>7.7.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="7.7.3" data-path="bayesian.html"><a href="bayesian.html#markov-chain"><i class="fa fa-check"></i><b>7.7.3</b> Markov Chain </a></li>
<li class="chapter" data-level="7.7.4" data-path="bayesian.html"><a href="bayesian.html#hidden-markov-model"><i class="fa fa-check"></i><b>7.7.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="7.7.5" data-path="bayesian.html"><a href="bayesian.html#dynamic-system-model"><i class="fa fa-check"></i><b>7.7.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="7.7.6" data-path="bayesian.html"><a href="bayesian.html#bayes-filter"><i class="fa fa-check"></i><b>7.7.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="7.7.7" data-path="bayesian.html"><a href="bayesian.html#kalman-filter"><i class="fa fa-check"></i><b>7.7.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="7.7.8" data-path="bayesian.html"><a href="bayesian.html#extended-kalman-filter"><i class="fa fa-check"></i><b>7.7.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="7.7.9" data-path="bayesian.html"><a href="bayesian.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>7.7.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="7.7.10" data-path="bayesian.html"><a href="bayesian.html#particle-filter"><i class="fa fa-check"></i><b>7.7.10</b> Particle Filter </a></li>
<li class="chapter" data-level="7.7.11" data-path="bayesian.html"><a href="bayesian.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>7.7.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="bayesian.html"><a href="bayesian.html#simulation-and-sampling"><i class="fa fa-check"></i><b>7.8</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="7.8.1" data-path="bayesian.html"><a href="bayesian.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>7.8.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="7.8.2" data-path="bayesian.html"><a href="bayesian.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>7.8.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="7.8.3" data-path="bayesian.html"><a href="bayesian.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>7.8.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="7.8.4" data-path="bayesian.html"><a href="bayesian.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>7.8.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="7.8.5" data-path="bayesian.html"><a href="bayesian.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>7.8.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="7.8.6" data-path="bayesian.html"><a href="bayesian.html#gibbs-sampling"><i class="fa fa-check"></i><b>7.8.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="7.8.7" data-path="bayesian.html"><a href="bayesian.html#importance-sampling"><i class="fa fa-check"></i><b>7.8.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="7.8.8" data-path="bayesian.html"><a href="bayesian.html#rejection-sampling"><i class="fa fa-check"></i><b>7.8.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="7.8.9" data-path="bayesian.html"><a href="bayesian.html#jags-modeling"><i class="fa fa-check"></i><b>7.8.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="bayesian.html"><a href="bayesian.html#bayesian-analysis"><i class="fa fa-check"></i><b>7.9</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="7.9.1" data-path="bayesian.html"><a href="bayesian.html#autocorrelation"><i class="fa fa-check"></i><b>7.9.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="7.9.2" data-path="bayesian.html"><a href="bayesian.html#predictive-probability"><i class="fa fa-check"></i><b>7.9.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="7.9.3" data-path="bayesian.html"><a href="bayesian.html#posterior-interval"><i class="fa fa-check"></i><b>7.9.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="7.9.4" data-path="bayesian.html"><a href="bayesian.html#bayes-factor"><i class="fa fa-check"></i><b>7.9.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="bayesian.html"><a href="bayesian.html#summary-5"><i class="fa fa-check"></i><b>7.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>8</b> Computational Learning I</a><ul>
<li class="chapter" data-level="8.1" data-path="machinelearning1.html"><a href="machinelearning1.html#observation-and-measurement"><i class="fa fa-check"></i><b>8.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="8.1.1" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-measurements"><i class="fa fa-check"></i><b>8.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="8.1.2" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>8.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="8.1.3" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>8.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="8.1.4" data-path="machinelearning1.html"><a href="machinelearning1.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>8.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="machinelearning1.html"><a href="machinelearning1.html#input-data"><i class="fa fa-check"></i><b>8.2</b> Input Data</a><ul>
<li class="chapter" data-level="8.2.1" data-path="machinelearning1.html"><a href="machinelearning1.html#structured-data"><i class="fa fa-check"></i><b>8.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="8.2.2" data-path="machinelearning1.html"><a href="machinelearning1.html#non-structured-data"><i class="fa fa-check"></i><b>8.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="8.2.3" data-path="machinelearning1.html"><a href="machinelearning1.html#statistical-data"><i class="fa fa-check"></i><b>8.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="8.2.4" data-path="machinelearning1.html"><a href="machinelearning1.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>8.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="8.2.5" data-path="machinelearning1.html"><a href="machinelearning1.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>8.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="8.2.6" data-path="machinelearning1.html"><a href="machinelearning1.html#data-lake"><i class="fa fa-check"></i><b>8.2.6</b> Data lake</a></li>
<li class="chapter" data-level="8.2.7" data-path="machinelearning1.html"><a href="machinelearning1.html#natural-language-nl"><i class="fa fa-check"></i><b>8.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="8.2.8" data-path="machinelearning1.html"><a href="machinelearning1.html#multimedia-md"><i class="fa fa-check"></i><b>8.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="machinelearning1.html"><a href="machinelearning1.html#primitive-methods"><i class="fa fa-check"></i><b>8.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="8.3.1" data-path="machinelearning1.html"><a href="machinelearning1.html#weighting"><i class="fa fa-check"></i><b>8.3.1</b> Weighting</a></li>
<li class="chapter" data-level="8.3.2" data-path="machinelearning1.html"><a href="machinelearning1.html#smoothing"><i class="fa fa-check"></i><b>8.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="8.3.3" data-path="machinelearning1.html"><a href="machinelearning1.html#normalizing"><i class="fa fa-check"></i><b>8.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="8.3.4" data-path="machinelearning1.html"><a href="machinelearning1.html#standardizing"><i class="fa fa-check"></i><b>8.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="8.3.5" data-path="machinelearning1.html"><a href="machinelearning1.html#centering"><i class="fa fa-check"></i><b>8.3.5</b> Centering </a></li>
<li class="chapter" data-level="8.3.6" data-path="machinelearning1.html"><a href="machinelearning1.html#scaling-1"><i class="fa fa-check"></i><b>8.3.6</b> Scaling </a></li>
<li class="chapter" data-level="8.3.7" data-path="machinelearning1.html"><a href="machinelearning1.html#transforming"><i class="fa fa-check"></i><b>8.3.7</b> Transforming</a></li>
<li class="chapter" data-level="8.3.8" data-path="machinelearning1.html"><a href="machinelearning1.html#clipping"><i class="fa fa-check"></i><b>8.3.8</b> Clipping </a></li>
<li class="chapter" data-level="8.3.9" data-path="machinelearning1.html"><a href="machinelearning1.html#regularizing"><i class="fa fa-check"></i><b>8.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="machinelearning1.html"><a href="machinelearning1.html#distance-metrics"><i class="fa fa-check"></i><b>8.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="8.4.1" data-path="machinelearning1.html"><a href="machinelearning1.html#cosine-similarity"><i class="fa fa-check"></i><b>8.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="8.4.2" data-path="machinelearning1.html"><a href="machinelearning1.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>8.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="8.4.3" data-path="machinelearning1.html"><a href="machinelearning1.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>8.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="8.4.4" data-path="machinelearning1.html"><a href="machinelearning1.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>8.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="8.4.5" data-path="machinelearning1.html"><a href="machinelearning1.html#hamming-distance"><i class="fa fa-check"></i><b>8.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="8.4.6" data-path="machinelearning1.html"><a href="machinelearning1.html#mahalanobis-distance"><i class="fa fa-check"></i><b>8.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="8.4.7" data-path="machinelearning1.html"><a href="machinelearning1.html#precision-and-accuracy"><i class="fa fa-check"></i><b>8.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="8.4.8" data-path="machinelearning1.html"><a href="machinelearning1.html#auc-on-roc"><i class="fa fa-check"></i><b>8.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="machinelearning1.html"><a href="machinelearning1.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>8.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="8.5.1" data-path="machinelearning1.html"><a href="machinelearning1.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>8.5.1</b> Data Cleaning (Wrangling)</a></li>
<li class="chapter" data-level="8.5.2" data-path="machinelearning1.html"><a href="machinelearning1.html#association"><i class="fa fa-check"></i><b>8.5.2</b> Association</a></li>
<li class="chapter" data-level="8.5.3" data-path="machinelearning1.html"><a href="machinelearning1.html#pattern-discovery"><i class="fa fa-check"></i><b>8.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="8.5.4" data-path="machinelearning1.html"><a href="machinelearning1.html#null-invariance"><i class="fa fa-check"></i><b>8.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="8.5.5" data-path="machinelearning1.html"><a href="machinelearning1.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>8.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="8.5.6" data-path="machinelearning1.html"><a href="machinelearning1.html#covariance"><i class="fa fa-check"></i><b>8.5.6</b> Covariance </a></li>
<li class="chapter" data-level="8.5.7" data-path="machinelearning1.html"><a href="machinelearning1.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>8.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="8.5.8" data-path="machinelearning1.html"><a href="machinelearning1.html#dominating-factors"><i class="fa fa-check"></i><b>8.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="8.5.9" data-path="machinelearning1.html"><a href="machinelearning1.html#missingness-and-imputation"><i class="fa fa-check"></i><b>8.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="8.5.10" data-path="machinelearning1.html"><a href="machinelearning1.html#confounding-variable"><i class="fa fa-check"></i><b>8.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="8.5.11" data-path="machinelearning1.html"><a href="machinelearning1.html#data-leakage"><i class="fa fa-check"></i><b>8.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="8.5.12" data-path="machinelearning1.html"><a href="machinelearning1.html#one-hot-encoding"><i class="fa fa-check"></i><b>8.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="8.5.13" data-path="machinelearning1.html"><a href="machinelearning1.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>8.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="8.5.14" data-path="machinelearning1.html"><a href="machinelearning1.html#discretization"><i class="fa fa-check"></i><b>8.5.14</b> Discretization </a></li>
<li class="chapter" data-level="8.5.15" data-path="machinelearning1.html"><a href="machinelearning1.html#stratification"><i class="fa fa-check"></i><b>8.5.15</b> Stratification </a></li>
<li class="chapter" data-level="8.5.16" data-path="machinelearning1.html"><a href="machinelearning1.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>8.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="8.5.17" data-path="machinelearning1.html"><a href="machinelearning1.html#embedding"><i class="fa fa-check"></i><b>8.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="machinelearning1.html"><a href="machinelearning1.html#feature_engineering"><i class="fa fa-check"></i><b>8.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="8.6.1" data-path="machinelearning1.html"><a href="machinelearning1.html#machine-learning-features"><i class="fa fa-check"></i><b>8.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="8.6.2" data-path="machinelearning1.html"><a href="machinelearning1.html#dimensionality-reduction"><i class="fa fa-check"></i><b>8.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="8.6.3" data-path="machinelearning1.html"><a href="machinelearning1.html#principal-component-analysis"><i class="fa fa-check"></i><b>8.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="8.6.4" data-path="machinelearning1.html"><a href="machinelearning1.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>8.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="8.6.5" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-construction"><i class="fa fa-check"></i><b>8.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="8.6.6" data-path="machinelearning1.html"><a href="machinelearning1.html#feature_selection"><i class="fa fa-check"></i><b>8.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="8.6.7" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-transformation"><i class="fa fa-check"></i><b>8.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="8.6.8" data-path="machinelearning1.html"><a href="machinelearning1.html#model-specification-1"><i class="fa fa-check"></i><b>8.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="machinelearning1.html"><a href="machinelearning1.html#general-modeling"><i class="fa fa-check"></i><b>8.7</b> General Modeling</a><ul>
<li class="chapter" data-level="8.7.1" data-path="machinelearning1.html"><a href="machinelearning1.html#training-learning"><i class="fa fa-check"></i><b>8.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="8.7.2" data-path="machinelearning1.html"><a href="machinelearning1.html#validation-tuning"><i class="fa fa-check"></i><b>8.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="8.7.3" data-path="machinelearning1.html"><a href="machinelearning1.html#testing-assessing"><i class="fa fa-check"></i><b>8.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="8.7.4" data-path="machinelearning1.html"><a href="machinelearning1.html#cross-validation-cv"><i class="fa fa-check"></i><b>8.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="8.7.5" data-path="machinelearning1.html"><a href="machinelearning1.html#bias-and-variance"><i class="fa fa-check"></i><b>8.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="8.7.6" data-path="machinelearning1.html"><a href="machinelearning1.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>8.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="8.7.7" data-path="machinelearning1.html"><a href="machinelearning1.html#global-and-local-minima"><i class="fa fa-check"></i><b>8.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="8.7.8" data-path="machinelearning1.html"><a href="machinelearning1.html#regularization"><i class="fa fa-check"></i><b>8.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="machinelearning1.html"><a href="machinelearning1.html#supervised-vs-unsupervised-learning"><i class="fa fa-check"></i><b>8.8</b> Supervised vs Unsupervised Learning  </a></li>
<li class="chapter" data-level="8.9" data-path="machinelearning1.html"><a href="machinelearning1.html#regression"><i class="fa fa-check"></i><b>8.9</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="8.9.1" data-path="machinelearning1.html"><a href="machinelearning1.html#regression-trees"><i class="fa fa-check"></i><b>8.9.1</b> Regression Trees </a></li>
<li class="chapter" data-level="8.9.2" data-path="machinelearning1.html"><a href="machinelearning1.html#ensemble-methods"><i class="fa fa-check"></i><b>8.9.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="8.9.3" data-path="machinelearning1.html"><a href="machinelearning1.html#random-forest"><i class="fa fa-check"></i><b>8.9.3</b> Random Forest </a></li>
<li class="chapter" data-level="8.9.4" data-path="machinelearning1.html"><a href="machinelearning1.html#Adaoost"><i class="fa fa-check"></i><b>8.9.4</b> AdaBoost</a></li>
<li class="chapter" data-level="8.9.5" data-path="machinelearning1.html"><a href="machinelearning1.html#gradient-boost"><i class="fa fa-check"></i><b>8.9.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="8.9.6" data-path="machinelearning1.html"><a href="machinelearning1.html#xgboost"><i class="fa fa-check"></i><b>8.9.6</b> XGBoost </a></li>
<li class="chapter" data-level="8.9.7" data-path="machinelearning1.html"><a href="machinelearning1.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>8.9.7</b> Generalized Linear Modeling (GLM)  </a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="machinelearning1.html"><a href="machinelearning1.html#summary-6"><i class="fa fa-check"></i><b>8.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machinelearning2.html"><a href="machinelearning2.html"><i class="fa fa-check"></i><b>9</b> Computational Learning II</a><ul>
<li class="chapter" data-level="9.1" data-path="machinelearning2.html"><a href="machinelearning2.html#binary-classification-supervised"><i class="fa fa-check"></i><b>9.1</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="machinelearning2.html"><a href="machinelearning2.html#logistic_regression"><i class="fa fa-check"></i><b>9.1.1</b> Logistic Regression</a></li>
<li class="chapter" data-level="9.1.2" data-path="machinelearning2.html"><a href="machinelearning2.html#poisson"><i class="fa fa-check"></i><b>9.1.2</b> Poisson Regression</a></li>
<li class="chapter" data-level="9.1.3" data-path="machinelearning2.html"><a href="machinelearning2.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>9.1.3</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="9.1.4" data-path="machinelearning2.html"><a href="machinelearning2.html#kernel-svm-smo"><i class="fa fa-check"></i><b>9.1.4</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="9.1.5" data-path="machinelearning2.html"><a href="machinelearning2.html#sdca-based-svm"><i class="fa fa-check"></i><b>9.1.5</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="machinelearning2.html"><a href="machinelearning2.html#multi-class-classification-supervised"><i class="fa fa-check"></i><b>9.2</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="9.2.1" data-path="machinelearning2.html"><a href="machinelearning2.html#bayesian-classification"><i class="fa fa-check"></i><b>9.2.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="9.2.2" data-path="machinelearning2.html"><a href="machinelearning2.html#classification-trees"><i class="fa fa-check"></i><b>9.2.2</b> Classification Trees </a></li>
<li class="chapter" data-level="9.2.3" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods-1"><i class="fa fa-check"></i><b>9.2.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="9.2.4" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest-1"><i class="fa fa-check"></i><b>9.2.4</b> Random Forest </a></li>
<li class="chapter" data-level="9.2.5" data-path="machinelearning2.html"><a href="machinelearning2.html#AdaBoost"><i class="fa fa-check"></i><b>9.2.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="9.2.6" data-path="machinelearning2.html"><a href="machinelearning2.html#logitboost-j-classes"><i class="fa fa-check"></i><b>9.2.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="9.2.7" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost-1"><i class="fa fa-check"></i><b>9.2.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="9.2.8" data-path="machinelearning2.html"><a href="machinelearning2.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>9.2.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machinelearning2.html"><a href="machinelearning2.html#clustering-unsupervised"><i class="fa fa-check"></i><b>9.3</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="9.3.1" data-path="machinelearning2.html"><a href="machinelearning2.html#k-means-clustering"><i class="fa fa-check"></i><b>9.3.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="9.3.2" data-path="machinelearning2.html"><a href="machinelearning2.html#hierarchical-clustering"><i class="fa fa-check"></i><b>9.3.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="9.3.3" data-path="machinelearning2.html"><a href="machinelearning2.html#dbscan-clustering"><i class="fa fa-check"></i><b>9.3.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="9.3.4" data-path="machinelearning2.html"><a href="machinelearning2.html#quality-of-clustering"><i class="fa fa-check"></i><b>9.3.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="machinelearning2.html"><a href="machinelearning2.html#meta-learning"><i class="fa fa-check"></i><b>9.4</b> Meta-Learning </a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="machinelearning3.html"><a href="machinelearning3.html"><i class="fa fa-check"></i><b>10</b> Computational Learning III</a><ul>
<li class="chapter" data-level="10.1" data-path="machinelearning3.html"><a href="machinelearning3.html#natural-language-processing-nlp"><i class="fa fa-check"></i><b>10.1</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="10.1.1" data-path="machinelearning3.html"><a href="machinelearning3.html#pre-processing-texts"><i class="fa fa-check"></i><b>10.1.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="10.1.2" data-path="machinelearning3.html"><a href="machinelearning3.html#ranking-and-scoring"><i class="fa fa-check"></i><b>10.1.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="10.1.3" data-path="machinelearning3.html"><a href="machinelearning3.html#document-similarity"><i class="fa fa-check"></i><b>10.1.3</b> Document Similarity </a></li>
<li class="chapter" data-level="10.1.4" data-path="machinelearning3.html"><a href="machinelearning3.html#linguistic-analysis"><i class="fa fa-check"></i><b>10.1.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="10.1.5" data-path="machinelearning3.html"><a href="machinelearning3.html#lexical-analysis"><i class="fa fa-check"></i><b>10.1.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="10.1.6" data-path="machinelearning3.html"><a href="machinelearning3.html#semantic-analysis"><i class="fa fa-check"></i><b>10.1.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="10.1.7" data-path="machinelearning3.html"><a href="machinelearning3.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>10.1.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="machinelearning3.html"><a href="machinelearning3.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>10.1.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-forecasting"><i class="fa fa-check"></i><b>10.2</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="10.2.1" data-path="machinelearning3.html"><a href="machinelearning3.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>10.2.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-models"><i class="fa fa-check"></i><b>10.2.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="10.2.3" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>10.2.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="10.2.4" data-path="machinelearning3.html"><a href="machinelearning3.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>10.2.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="10.2.5" data-path="machinelearning3.html"><a href="machinelearning3.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>10.2.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="10.2.6" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-decomposition"><i class="fa fa-check"></i><b>10.2.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="10.2.7" data-path="machinelearning3.html"><a href="machinelearning3.html#stl-with-aicbic"><i class="fa fa-check"></i><b>10.2.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="10.2.8" data-path="machinelearning3.html"><a href="machinelearning3.html#multivariate-time-series"><i class="fa fa-check"></i><b>10.2.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="10.2.9" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-considerations"><i class="fa fa-check"></i><b>10.2.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="machinelearning3.html"><a href="machinelearning3.html#recommender-systems"><i class="fa fa-check"></i><b>10.3</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="deeplearning1.html"><a href="deeplearning1.html"><i class="fa fa-check"></i><b>11</b> Deep Computational Learning I</a><ul>
<li class="chapter" data-level="11.1" data-path="deeplearning1.html"><a href="deeplearning1.html#simple-perceptron"><i class="fa fa-check"></i><b>11.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="11.2" data-path="deeplearning1.html"><a href="deeplearning1.html#adaptive-linear-neuron-adaline"><i class="fa fa-check"></i><b>11.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="11.3" data-path="deeplearning1.html"><a href="deeplearning1.html#multi-layer-perceptron-mlp"><i class="fa fa-check"></i><b>11.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed"><i class="fa fa-check"></i><b>11.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="11.3.2" data-path="deeplearning1.html"><a href="deeplearning1.html#backward-feed"><i class="fa fa-check"></i><b>11.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="11.3.3" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation"><i class="fa fa-check"></i><b>11.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="11.3.4" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-example"><i class="fa fa-check"></i><b>11.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="11.3.5" data-path="deeplearning1.html"><a href="deeplearning1.html#activation-function"><i class="fa fa-check"></i><b>11.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="11.3.6" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-implementation"><i class="fa fa-check"></i><b>11.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="11.3.7" data-path="deeplearning1.html"><a href="deeplearning1.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>11.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="deeplearning1.html"><a href="deeplearning1.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>11.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="11.3.9" data-path="deeplearning1.html"><a href="deeplearning1.html#dead-relu"><i class="fa fa-check"></i><b>11.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="11.3.10" data-path="deeplearning1.html"><a href="deeplearning1.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>11.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="11.3.11" data-path="deeplearning1.html"><a href="deeplearning1.html#parameter-initialization"><i class="fa fa-check"></i><b>11.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="11.3.12" data-path="deeplearning1.html"><a href="deeplearning1.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>11.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="11.3.13" data-path="deeplearning1.html"><a href="deeplearning1.html#batch-normalization"><i class="fa fa-check"></i><b>11.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="11.3.14" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization"><i class="fa fa-check"></i><b>11.3.14</b> Optimization </a></li>
<li class="chapter" data-level="11.3.15" data-path="deeplearning1.html"><a href="deeplearning1.html#interpretability"><i class="fa fa-check"></i><b>11.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="deeplearning1.html"><a href="deeplearning1.html#convolutional-neural-network-cnn"><i class="fa fa-check"></i><b>11.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="11.4.1" data-path="deeplearning1.html"><a href="deeplearning1.html#computer-graphics"><i class="fa fa-check"></i><b>11.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="11.4.2" data-path="deeplearning1.html"><a href="deeplearning1.html#convolution"><i class="fa fa-check"></i><b>11.4.2</b> Convolution </a></li>
<li class="chapter" data-level="11.4.3" data-path="deeplearning1.html"><a href="deeplearning1.html#stride-and-padding"><i class="fa fa-check"></i><b>11.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="11.4.4" data-path="deeplearning1.html"><a href="deeplearning1.html#kernels-and-filters"><i class="fa fa-check"></i><b>11.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="11.4.5" data-path="deeplearning1.html"><a href="deeplearning1.html#dilation"><i class="fa fa-check"></i><b>11.4.5</b> Dilation </a></li>
<li class="chapter" data-level="11.4.6" data-path="deeplearning1.html"><a href="deeplearning1.html#pooling"><i class="fa fa-check"></i><b>11.4.6</b> Pooling </a></li>
<li class="chapter" data-level="11.4.7" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-architectures"><i class="fa fa-check"></i><b>11.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="11.4.8" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed-1"><i class="fa fa-check"></i><b>11.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="11.4.9" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation-1"><i class="fa fa-check"></i><b>11.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="11.4.10" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization-1"><i class="fa fa-check"></i><b>11.4.10</b> Optimization</a></li>
<li class="chapter" data-level="11.4.11" data-path="deeplearning1.html"><a href="deeplearning1.html#normalization"><i class="fa fa-check"></i><b>11.4.11</b> Normalization</a></li>
<li class="chapter" data-level="11.4.12" data-path="deeplearning1.html"><a href="deeplearning1.html#step-decay"><i class="fa fa-check"></i><b>11.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="11.4.13" data-path="deeplearning1.html"><a href="deeplearning1.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>11.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="11.4.14" data-path="deeplearning1.html"><a href="deeplearning1.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>11.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="11.4.15" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-implementation"><i class="fa fa-check"></i><b>11.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="11.4.16" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-application"><i class="fa fa-check"></i><b>11.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="11.4.17" data-path="deeplearning1.html"><a href="deeplearning1.html#summary-7"><i class="fa fa-check"></i><b>11.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deeplearning2.html"><a href="deeplearning2.html"><i class="fa fa-check"></i><b>12</b> Deep Computational Learning II</a><ul>
<li class="chapter" data-level="12.1" data-path="deeplearning2.html"><a href="deeplearning2.html#residual-network-resnet"><i class="fa fa-check"></i><b>12.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="12.2" data-path="deeplearning2.html"><a href="deeplearning2.html#recurrent-neural-network-rnn"><i class="fa fa-check"></i><b>12.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="12.2.1" data-path="deeplearning2.html"><a href="deeplearning2.html#vanilla-rnn"><i class="fa fa-check"></i><b>12.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="12.2.2" data-path="deeplearning2.html"><a href="deeplearning2.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>12.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="12.2.3" data-path="deeplearning2.html"><a href="deeplearning2.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>12.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-rnn"><i class="fa fa-check"></i><b>12.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="12.4" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-bidirectional-rnn"><i class="fa fa-check"></i><b>12.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="12.5" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-neural-network-tnn"><i class="fa fa-check"></i><b>12.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="12.5.1" data-path="deeplearning2.html"><a href="deeplearning2.html#attention"><i class="fa fa-check"></i><b>12.5.1</b> Attention </a></li>
<li class="chapter" data-level="12.5.2" data-path="deeplearning2.html"><a href="deeplearning2.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>12.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="12.5.3" data-path="deeplearning2.html"><a href="deeplearning2.html#multi-head-attention"><i class="fa fa-check"></i><b>12.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="12.5.4" data-path="deeplearning2.html"><a href="deeplearning2.html#word-embedding"><i class="fa fa-check"></i><b>12.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="12.5.5" data-path="deeplearning2.html"><a href="deeplearning2.html#positional-embedding"><i class="fa fa-check"></i><b>12.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="12.5.6" data-path="deeplearning2.html"><a href="deeplearning2.html#sequence-alignment"><i class="fa fa-check"></i><b>12.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="12.5.7" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-architectures"><i class="fa fa-check"></i><b>12.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="deeplearning2.html"><a href="deeplearning2.html#applications-using-tnn-and-rnn"><i class="fa fa-check"></i><b>12.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="12.6.1" data-path="deeplearning2.html"><a href="deeplearning2.html#speech-recognition"><i class="fa fa-check"></i><b>12.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="12.6.2" data-path="deeplearning2.html"><a href="deeplearning2.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>12.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="12.6.3" data-path="deeplearning2.html"><a href="deeplearning2.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>12.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="12.6.4" data-path="deeplearning2.html"><a href="deeplearning2.html#model-evaluation"><i class="fa fa-check"></i><b>12.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="deeplearning2.html"><a href="deeplearning2.html#generative-adversarial-network-gan"><i class="fa fa-check"></i><b>12.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="12.8" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-reinforcement-network-dqn"><i class="fa fa-check"></i><b>12.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="12.9" data-path="deeplearning2.html"><a href="deeplearning2.html#summary-8"><i class="fa fa-check"></i><b>12.9</b> Summary</a></li>
<li class="chapter" data-level="12.10" data-path="deeplearning2.html"><a href="deeplearning2.html#general-summary"><i class="fa fa-check"></i><b>12.10</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>13</b> Appendix</a><ul>
<li class="chapter" data-level="13.1" data-path="appendix.html"><a href="appendix.html#appendix-a"><i class="fa fa-check"></i><b>13.1</b> Appendix A</a><ul>
<li class="chapter" data-level="13.1.1" data-path="appendix.html"><a href="appendix.html#trigonometry"><i class="fa fa-check"></i><b>13.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="13.1.2" data-path="appendix.html"><a href="appendix.html#logarithms"><i class="fa fa-check"></i><b>13.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="13.1.3" data-path="appendix.html"><a href="appendix.html#category-theory"><i class="fa fa-check"></i><b>13.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="appendix.html"><a href="appendix.html#appendix-b"><i class="fa fa-check"></i><b>13.2</b> Appendix B</a><ul>
<li class="chapter" data-level="13.2.1" data-path="appendix.html"><a href="appendix.html#on-random-chances"><i class="fa fa-check"></i><b>13.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="13.2.2" data-path="appendix.html"><a href="appendix.html#on-replacements"><i class="fa fa-check"></i><b>13.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="13.2.3" data-path="appendix.html"><a href="appendix.html#on-factorials"><i class="fa fa-check"></i><b>13.2.3</b> On Factorials</a></li>
<li class="chapter" data-level="13.2.4" data-path="appendix.html"><a href="appendix.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>13.2.4</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="13.2.5" data-path="appendix.html"><a href="appendix.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>13.2.5</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="13.2.6" data-path="appendix.html"><a href="appendix.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>13.2.6</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="13.2.7" data-path="appendix.html"><a href="appendix.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>13.2.7</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="13.2.8" data-path="appendix.html"><a href="appendix.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>13.2.8</b> On Mutual Exclusivity</a></li>
<li class="chapter" data-level="13.2.9" data-path="appendix.html"><a href="appendix.html#on-expectation-and-variance"><i class="fa fa-check"></i><b>13.2.9</b> On Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="appendix.html"><a href="appendix.html#appendix-d"><i class="fa fa-check"></i><b>13.3</b> Appendix D</a><ul>
<li class="chapter" data-level="13.3.1" data-path="appendix.html"><a href="appendix.html#lubridate-library"><i class="fa fa-check"></i><b>13.3.1</b> Lubridate Library</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="appendix.html"><a href="appendix.html#appendix-c"><i class="fa fa-check"></i><b>13.4</b> Appendix C</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear_algebra" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Numerical Linear Algebra I</h1>
<p>Linear Algebra is unquestionably a precursory requirement needed to advance one’s knowledge in Data Science. This becomes apparent throughout the book as we course through the many solutions discussed, leading all the way to the advanced topics in Deep Learning. Note that there is so much to learn, yet too little space to fit in one chapter. Nonetheless, it still helps to discuss some of the known classic fundamental concepts of Linear Algebra. That becomes our goal in this chapter which is to collate formulas, rules, and techniques that have become quite ubiquitous in many forums relevant to Data Science. We then support our discussions with a little touch of intuition in the form of diagrams and implementation. Also, note that, while it may appear pedagogical, as terse the subjects as they are, take them as referential hints only in this chapter and in subsequent chapters; thus it is still preferable to take the full comprehensive courses and text materials - complete with logical and practical exercises for students. Though, at this point, it is essential to know that our theme in this chapter and in all the following chapters is to build on end an intuitive narrative around how we are solving problems and truly seeing through the power and art of the strategies employed by way of approximation.</p>
<p>In this chapter, before we jump to <strong>Indirect Methods</strong> in the latter part of the chapter, we first concentrate on <strong>Direct Methods</strong> in the context of Linear Algebra . Here, we reference the works of Press W.H et al <span class="citation">(<a href="references.html#ref-ref215w">2007</a>)</span> and Edwards H. et al <span class="citation">(<a href="references.html#ref-ref207c">2018</a>)</span>, along with other additional references for consistency.</p>
<div id="system-of-linear-equations" class="section level2">
<h2><span class="header-section-number">2.1</span> System of Linear Equations</h2>
<p>In this section, we start by describing a few concepts around <strong>linear equation</strong>, <strong>system of linear of equations</strong>, and <strong>solutions to a system</strong>.</p>
<p><strong>Linear Equation</strong></p>
<p>A <strong>linear equation</strong> is a mathematical equation that geometrically describes a line as shown in Figure . In the figure, we are using a 2D cartesian coordinate system.</p>
<div class="figure" style="text-align: center"><span id="fig:solution"></span>
<img src="solution.png" alt="System of Linear Equations" width="50%" />
<p class="caption">
Figure 2.1: System of Linear Equations
</p>
</div>
<p>Mathematically and generally, a <strong>linear equation</strong> is represented by the following simple formula, where <strong>m</strong> is the slope and <strong>b</strong> is the y-intercept:</p>
<p><span class="math display">\[\begin{align}
y = mx + b \label{eqn:one}
\end{align}\]</span></p>
<p>The slope <strong>m</strong> is computed using the following formula:</p>
<p><span class="math display">\[
m = \frac{\Delta{y}}{\Delta{x}} = \frac{\text{change of y}}{\text{change of x}} = \frac{y_2 - y_1}{ x_2 - x_1}
\]</span></p>
<p>Note that the <strong>“change of y”</strong> is represented by a vertical distance between two points in reference to the y-axis. Similarly, the <strong>“change of x”</strong> is represented by a horizontal distance between the two points in reference to the x-axis.</p>
<p>The <strong>m</strong> represents the <strong>rate of change</strong>.</p>
<p><strong>System of Linear of Equations</strong></p>
<p>A <strong>system of linear equations</strong> is a set (a collection) of linear equations, involving the same set of variables (e.g. x and y variables).</p>
<p>In Figure , there are two linear equations in the system:</p>
<p><span class="math display">\[\begin{align*}
y {}&amp;= -05.x + 4\\
y &amp;= x
\end{align*}\]</span></p>
<p>The first linear equation is computed as:</p>
<p><span class="math display">\[
y = mx + b = \frac{-2-4}{4-0} \times x + 4 =  \frac{-2}{4}x + 4 = -0.5x + 4
\]</span>
The second linear equation is computed as:</p>
<p><span class="math display">\[
y = mx + b = \frac{4-1}{4-1} \times x + 0 =  \frac{1}{1}x + 0 = x + 0
\]</span></p>
<p><strong>Solutions to a System</strong></p>
<p>A <strong>solution</strong>, in the context of linear systems, is that value we put to a variable to make the system true. Variables have unknown values and so it is natural to <strong>solve for the unknowns</strong>. In the example above where we have a system of linear equations involving 3 common variables, <span class="math inline">\(x_1,\ x_2,\ x_3\)</span>, the <strong>solution</strong> we seek is to know what are the values of those 3 common variables that apply to the three equations.</p>
<p>In Figure , the two lines intersect at a <strong>point</strong>. That <strong>point</strong> is the <strong>solution</strong> to the system of linear equations below:</p>
<p><span class="math display">\[\begin{align*}
y {}&amp;= -0.5x + 4\\
y &amp;= x
\end{align*}\]</span></p>
<p>where x = 2.67 and y = 2.67.</p>
<p>If a system has no <strong>solution</strong> however, then the system is called an <strong>inconsistent</strong> system. A simple example is when two equations (two lines) do not intersect. On the other hand, if a system has at least one solution, then the system is called a <strong>consistent</strong> system.</p>
<p>A good example of a system that always has a solution is a <strong>homogeneous system</strong> in which the right-hand side is zero. Here is another example of a <strong>homogenous system</strong> involving <strong>three unknowns (or variables)</strong>, <span class="math inline">\(x_1,\ x_2,\ x_3\)</span>:</p>
<p><span class="math display">\[
\left(\begin{array}{lll}  
3x_1 + 3x_2 + 3x_3 {}&amp;= 0 \\
2x_1 + 3x_2 + 4x_3 &amp;= 0 \\
1x_1 + 5x_2 + 5x_3 &amp;= 0
 \end{array}\right)
\]</span></p>
<p>Now, if the number of equations in a system is less than the number of unknowns, then the system is called <strong>under-determined system</strong>. If there are more unknowns than the number of equations, then the system is called <strong>over-determined system</strong>.</p>
<p>The sample system below is <strong>under-determined</strong> because it has only two equations but three unknowns.</p>
<p><span class="math display">\[
\left(\begin{array}{lll}  
3x_1 + 3x_2 + 3x_3 {}&amp;= 0 \\
2x_1 + 3x_2 + 4x_3 &amp;= 0 \\
 \end{array}\right)
\]</span>
<strong>Solutions</strong> to <strong>under-determined</strong> systems are further explained in <strong>Rank and Nullity</strong> section.</p>
<p>We continue to cover system of linear equations and how to solve other linear equation problems when we cover the <strong>matrix factorization</strong> in this chapter, e.g. <strong>LU factorization</strong> by <strong>Gaussian Elimination</strong>.</p>
<p>For now, the next section introduces structures widely covered in linear algebra: <strong>scalar, vector, matrix</strong>.</p>
</div>
<div id="scalar-vector-and-matrix-tensor" class="section level2">
<h2><span class="header-section-number">2.2</span> Scalar, Vector, and Matrix, Tensor</h2>
<p>In <strong>Linear Algebra</strong>, it helps to know the data structures used. There are four common structures to be familiar, namely <strong>Scalar</strong>, <strong>Vector</strong>, <strong>Matrix</strong>, and <strong>Tensor</strong>.</p>
<p><strong>Scalar</strong> is represented by a variable that holds a single number. </p>
<p><span class="math display">\[x_i = 2.54\]</span>
<strong>Vector</strong> is represented by a variable that holds a collection (or array) of elements (e.g. real numbers, complex numbers, etc.). </p>
<p>Given index (i), e.g. i = 1,2,..,n</p>
<p><span class="math display">\[
\mathbf{\vec{V_{n}}} = \left[\begin{array}{l} x_{i=1} \\ x_{i=2} \\ \vdots \\ x_{i=n} \end{array}\right],
\ \ \ \ \
\mathbf{\vec{V_1}} = \left[\begin{array}{r} 1 \\ 4 \\ -3 \\ 5 \end{array}\right],
\ \ \ \ \
\mathbf{\vec{V_2}} = \left[\begin{array}{c} 2 + 3i \\ ln(2) \\ 3.0 + 4.0i \\ sin({\frac{\pi}{2}}) \end{array}\right],
\ \ \ \ \
\mathbf{\vec{V_3}} = \left[\begin{array}{ccccc} 6.251 \\ 20.3e^{10} \\ \frac{1}{2} \\ \sqrt{2} \end{array}\right]
\]</span>
A vector can be scaled by multiplying it with a scalar number.</p>
<p><span class="math display">\[
\mathbf{\vec{V_{n}}} = a\left[\begin{array}{ccccc} x_{i=1} \\ x_{i=2} \\ \vdots \\ x_{i=n} \end{array}\right]
=
\left[\begin{array}{ccccc} ax_{1} \\ ax_{2} \\ ax_{3} \\ ax_{4} \end{array}\right],
\ \ \ \ \ \ \
\mathbf{\vec{V_1}} = 2\left[\begin{array}{ccccc} 1 \\ 4 \\ 3 \\ 5 \end{array}\right]
=
\left[\begin{array}{ccccc} 2 \\ 8 \\ 6 \\ 10 \end{array}\right]
\]</span></p>
<p><strong>Matrix</strong> is represented by a variable that holds a 2-dimensional array of numbers. </p>
<p>Given row index (i), e.g. i = 1,2,..,n, and column index (j), j = 1,2,..m:</p>
<p><span class="math display">\[
\mathbf{A}_{m \times n} = \left[\begin{array}{ccccc} 
x_{i=1,j=1} &amp; x_{1,2} &amp; \dots &amp; x_{1,n} \\ 
x_{2,1} &amp; x_{2,2} &amp; \dots &amp; x_{2,n} \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
x_{m,1} &amp; x_{m,2} &amp; \dots &amp; x_{i=m,j=n} 
\end{array}\right]_{m \times n}
\]</span></p>
<p>A matrix can be scaled by multiplying it with a scalar number.</p>
<p><span class="math display">\[
\mathbf{A} = a\left[\begin{array}{ccccc} 
x_{1,1} &amp; x_{1,2} &amp; x_{1,3} \\ 
x_{2,1} &amp; x_{2,2} &amp; x_{2,3} \\ 
x_{3,1} &amp; x_{3,2} &amp; x_{3,3} 
\end{array}\right]
=
\left[\begin{array}{ccccc} 
ax_{1,1} &amp; ax_{1,2} &amp; ax_{1,3} \\ 
ax_{2,1} &amp; ax_{2,2} &amp; ax_{2,3} \\ 
ax_{3,1} &amp; ax_{3,2} &amp; ax_{3,3} 
\end{array}\right]
\]</span></p>
<p><span class="math display">\[
\mathbf{A} = 2\left[\begin{array}{ccccc} 
1 &amp; 2 &amp; 3 \\ 
4 &amp; 5 &amp; 6 \\ 
7 &amp; 8 &amp; 9 
\end{array}\right]
=
\left[\begin{array}{rrr} 
2 &amp; 4 &amp; 6 \\ 
8 &amp; 10 &amp; 12 \\ 
14 &amp; 16 &amp; 18 
\end{array}\right]
\]</span></p>
<p>The last structure is the <strong>Tensor</strong>. <strong>Tensor</strong> is considered to be a generalized form of <strong>Matrix</strong>. </p>
<p>A <strong>vector</strong> is considered a <strong>first-order tensor</strong> in 1D (one-dimension) space.</p>
<p>A <strong>matrix</strong> is considered a <strong>second-order tensor</strong> in 2D (two-dimension) space</p>
<p>Beyond a second-order tensor, it is natural to begin to cover <strong>hyper-cubes</strong> and <strong>hyper-planes</strong> under a higher N-dimension or <strong>N-order tensor</strong>.</p>
<p>In the N-dimension space, N&gt;2, we begin to consider talking about <strong>dynamic interaction</strong>, <strong>field theory</strong>, and so on.</p>
<p><strong>Revisiting system of linear equations</strong></p>
<p>Having a vector structure and a matrix structure now allow us to represent any <strong>system of linear equations</strong> and <strong>solutions to the system</strong> in matrix equation:</p>
<p><span class="math display">\[\begin{align}
A\mathbf{\vec{x}} = \mathbf{\vec{b}} \label{eqn:axb_eq}
\end{align}\]</span></p>
<p>The <strong>variable x</strong> is a vector of <strong>unknown variables</strong> - the <strong>solution</strong> or <strong>solution space</strong>. In other words, this vector is what we are trying to solve.</p>
<p>The <strong>variable A</strong> is a matrix of <strong>coefficients</strong> - the <strong>transformation</strong> matrix. More about <strong>transformations</strong> are covered in the section about <strong>linear transformation</strong>.</p>
<p>The <strong>variable b</strong> is a vector of <strong>constants</strong> - the <strong>result</strong> of the Matrix-vector multiplication.</p>
<p>This topic becomes more apparent when we cover the section about <strong>Rank</strong> and <strong>Nullity</strong>.</p>
<p>A few properties of scalar and matrix multiplication is shown:</p>
<p><span class="math display">\[
(x + y)A = xA + yA\ \ \ \ \ \ \ \ \ \ x(A + B) = xA + xB
\]</span></p>
</div>
<div id="transposition-and-multiplication" class="section level2">
<h2><span class="header-section-number">2.3</span> Transposition and Multiplication</h2>
<div id="transposition" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Transposition</h3>
<p>Vectors are implicitly column vectors. To convert vectors to row vectors, we use <strong>Transposition</strong> . It is another way of switching the vector from column-wise to row-wise form. We can also transpose row vectors back to column vectors.</p>
<p>See below:</p>
<p><span class="math display">\[
\vec{V} (column\ vector)= \left[\begin{array}{ccccc} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{array}\right]
\rightarrow
\ \ \ \ \ 
\vec{V}^T (transposed\ row\ vector) = \left[\begin{array}{ccccc} x_{1} &amp; x_{2} &amp; \cdots &amp; x_{n} \end{array}\right]
\]</span></p>
<p>To transpose a matrix, we flip the matrix over its diagonal.</p>
<div class="figure" style="text-align: center"><span id="fig:transpose"></span>
<img src="transpose.png" alt="Transpose" width="50%" />
<p class="caption">
Figure 2.2: Transpose
</p>
</div>
<p>As can be seen, the small white square at the lower left side gets flipped over to the top right. Similarly, the matrix below gets flipped over where for example the entry <span class="math inline">\(x_{4,1}\)</span> at the lower left corner gets flipped to the top right corner, and <span class="math inline">\(x_{1,3}\)</span> gets flipped to the lower left corner, so does the other non-diagonal entries.</p>
<p><span class="math display">\[
\mathbf{A} = \left[\begin{array}{ccccc} 
x_{1,1} &amp; x_{1,2} &amp; x_{1,3} &amp; x_{1,4} \\ 
x_{2,1} &amp; x_{2,2} &amp; x_{2,3} &amp; x_{2,4} \\ 
x_{3,1} &amp; x_{3,2} &amp; x_{3,3} &amp; x_{3,4} \\
x_{4,1} &amp; x_{4,2} &amp; x_{4,3} &amp; x_{4,4} 
\end{array}\right]
\rightarrow
\mathbf{A}^T = \left[\begin{array}{ccccc} 
x_{1,1} &amp; x_{2,1} &amp; x_{3,1} &amp; x_{4,1} \\ 
x_{1,2} &amp; x_{2,2} &amp; x_{3,2} &amp; x_{4,2} \\ 
x_{1,3} &amp; x_{2,3} &amp; x_{3,3} &amp; x_{4,3} \\
x_{1,4} &amp; x_{2,4} &amp; x_{3,4} &amp; x_{4,4} 
\end{array}\right]
\]</span>
Example:</p>
<p>Transposing a 4x4 matrix:</p>
<p><span class="math display">\[
\mathbf{A} = \left[\begin{array}{rrrr} 
1 &amp; 2 &amp; 3 &amp; 4 \\ 
5 &amp; 6 &amp; 7 &amp; 8 \\ 
9 &amp; 10 &amp; 11 &amp; 12\\
13 &amp; 14 &amp; 15 &amp; 16
\end{array}\right]
\rightarrow
\mathbf{A}^T = \left[\begin{array}{rrrr} 
1 &amp; 5 &amp; 9 &amp; 13\\ 
2 &amp; 6 &amp; 10 &amp; 14\\ 
3 &amp; 7 &amp; 11 &amp; 15\\
4 &amp; 8 &amp; 12 &amp; 16
\end{array}\right]
\]</span>
Transposing a 3x4 matrix:</p>
<p><span class="math display">\[
\mathbf{A} = \left[\begin{array}{rrrr} 
1 &amp; 2 &amp; 3 &amp; 4 \\ 
5 &amp; 6 &amp; 7 &amp; 8 \\ 
9 &amp; 10 &amp; 11 &amp; 12 
\end{array}\right]
\rightarrow
\mathbf{A}^T = \left[\begin{array}{rrrr} 
1 &amp; 5 &amp; 9 \\ 
2 &amp; 6 &amp; 10 \\ 
3 &amp; 7 &amp; 11 \\
4 &amp; 8 &amp; 12
\end{array}\right]
\]</span></p>
<p>A few properties to be aware.</p>
<ul>
<li>Transposition reverses everything in the parenthesis like so:</li>
</ul>
<p><span class="math display">\[
(a^TAb)^T = aA^Tb^T
\]</span></p>
<ul>
<li>Transposition of a symmetric matrix yields an identity matrix:</li>
</ul>
<p><span class="math display">\[
A^TA = I
\]</span></p>
<ul>
<li>Other properties of transposition:</li>
</ul>
<p><span class="math display">\[
\begin{array}{lll}
(AB)^T = B^TA^T &amp; &amp; (A + B)^T = A^T + B^T \\
(xA)^T = xA^T &amp; &amp; A^T(x)A + A^T(y)A = A^T(x+y)A \\
xA^T + xB^T = x(A+B)^T &amp; &amp; xA^T + yA^T = (x+y)A^T\\
A^Txy + A^Tzw = A^T(xy + zw)
\end{array}
\]</span></p>
</div>
<div id="dot-product" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Dot Product</h3>
<p>We use the following formula to compute for the <strong>dot product</strong> of two arbitrary vectors :</p>
<p><span class="math display">\[
\vec{a} \cdotp \vec{b} = \|a\| \times \|b\| \times cos(\theta) 
\]</span></p>
<p>From Figure , given vector <span class="math inline">\(\mathbf{\vec{a}}\)</span>, <span class="math inline">\(\left[\begin{array}{c} 1 \\ 3 \end{array}\right]\)</span>, and vector <span class="math inline">\(\mathbf{\vec{b}}\)</span>, <span class="math inline">\(\left[\begin{array}{c} 4 \\ 1 \end{array}\right]\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\vec{a} \cdotp \vec{b} {} &amp;= \|a\| \times \|b\| \times cos(\theta)  \\
&amp;= \sqrt{1^2 + 3^2} \times \sqrt{4^2 + 1^2} \times cos(\theta)\\
&amp;= \sqrt{10} \times \sqrt{17} \times cos(57.53^{\circ} \times (\pi/180)) \\
&amp;= 7
\end{align*}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:dotproduct"></span>
<img src="dot_product.png" alt="Dot Product" width="40%" />
<p class="caption">
Figure 2.3: Dot Product
</p>
</div>
<p>The formula below is another method to get the <strong>dot product</strong> of two vectors. We simply multiply the corresponding entries of vectors and then we add all the resulting products.</p>
<p><span class="math display">\[\begin{align}
\vec{a} \cdotp \vec{b} = \sum_{i=1}^n \left( a_i \times b_i \right) \label{eqn:dot-product}
\end{align}\]</span></p>
<p>Using the same vectors from Figure , we get the same <strong>dot product</strong> result:</p>
<p><span class="math display">\[
\vec{a} \cdotp \vec{b} = a_{1} \times b_{1} + a_{2} \times b_{2} = 1 \times 3 + 4 \times 1 = 7
\]</span></p>
<p>Generally, given vector <span class="math inline">\(\mathbf{\vec{a}}\)</span> and vector <span class="math inline">\(\mathbf{\vec{b}}\)</span>:</p>
<p><span class="math display">\[
 \mathbf{\vec{a}} = \left[\begin{array}{c} a_{1} \\ a_{2} \\ \vdots \\ a_{n} \end{array}\right],
 \mathbf{\vec{b}} = \left[\begin{array}{c} b_{1} \\ b_{2} \\ \vdots \\ b_{n} \end{array}\right]
\]</span></p>
<p>Note that we use the transposed row vector to multiply with a column vector.</p>
<p><span class="math display">\[
 \mathbf{\vec{a}}^T \cdotp  \mathbf{\vec{b}} 
 = \left[\begin{array}{ccccc} a_{1} &amp; a_{2} &amp; \cdots &amp; b_{n} \end{array}\right] \cdotp 
\left[\begin{array}{c} b_{1} \\ b_{2} \\ \vdots \\ b_{n} \end{array}\right]
\rightarrow
\ \ \ \ \ a_{1}b_{1} + a_{2}b_{2} + ... a_{n}b_{n}
\]</span></p>
<p>Example:</p>
<p><span class="math display">\[
 \mathbf{\vec{a}}^T \cdotp  \mathbf{\vec{b}} \ \ \ =\ \ \ \left[\begin{array}{ccc} 1 &amp; 2 &amp;  3 \end{array}\right] \cdotp 
\left[\begin{array}{c} 4 \\ 5 \\ 6 \end{array}\right]
\ \ \ =
\ \ \ \ \ (1)(4) + (2)(5) + (3)(6)\ \ \ =\ \ \ 32
\]</span></p>
<p>We get a scalar value of 32.</p>
<p>If we switch and use the column vector first to multiply to the transposed row vector, we get a 3x3 matrix instead of a scalar result.</p>
<p><span class="math display">\[
 \mathbf{\vec{b}} \cdotp  \mathbf{\vec{a}}^T
\ \ \ =\ \ \ \ \left[\begin{array}{ccc} 4 \\ 5 \\ 6 \end{array}\right] \cdotp \left[\begin{array}{ccc} 1 &amp; 2 &amp;  3 \end{array}\right] 
\ \ \ =
\left[\begin{array}{rrrr} 
4 &amp; 8 &amp; 12 \\ 
5 &amp; 10 &amp; 15 \\ 
6 &amp; 12 &amp; 18
\end{array}\right]
\]</span></p>
<p>If we expand the <span class="math inline">\(\mathbf{\vec{b}}\)</span> and <span class="math inline">\(\mathbf{\vec{a}}\)</span> vectors into 3x3 matrices, we get the following <strong>dot product</strong> based on matrix multiplication:</p>
<p><span class="math display">\[
\mathbf{\vec{b}} \cdotp  \mathbf{\vec{a}}^T
\ \ \ \ =
\left[\begin{array}{ccccc} 
4 &amp; 0 &amp; 0 \\ 
5 &amp; 0 &amp; 0 \\ 
6 &amp; 0 &amp; 0
\end{array}\right] \cdotp
\left[\begin{array}{ccccc} 
1 &amp; 2 &amp; 3 \\ 
0 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 0
\end{array}\right]
\ \ \ =
\left[\begin{array}{rrrr} 
4 &amp; 8 &amp; 12 \\ 
5 &amp; 10 &amp; 15 \\ 
6 &amp; 12 &amp; 18
\end{array}\right]
\]</span></p>
<p>To perform matrix multiplication given matrix A and B:</p>
<p><span class="math display">\[
\mathbf{A} = \left[\begin{array}{ccccc} 
a_1 &amp; a_2 &amp; a_3 \\ 
a_4 &amp; a_5 &amp; a_6 \\
a_7 &amp; a_8 &amp; a_9 \\
\end{array}\right],\ \ \ \ 
\mathbf{B} = \left[\begin{array}{ccccc} 
b_1 &amp; b_2 &amp; b_3 \\ 
b_4 &amp; b_5 &amp; b_6 \\
b_7 &amp; b_8 &amp; b_9 \\
\end{array}\right]
\]</span>
first, we take the individual rows of A, and the individual columns of B and perform the following computation:</p>
<p><span class="math display">\[
\mathbf{A_{1st\ row}} = \left[\begin{array}{ccccc}  a_1 &amp; a_2 &amp; a_3\end{array}\right],\ \ \ 
\mathbf{B_{1st\ col}} = \left[\begin{array}{ccccc}  b_1 \\ b_4 \\ b_7 \end{array}\right]
\rightarrow
(a_1 \times b_1 + a_2 \times b_4 + a_3 \times b_7)
\]</span></p>
<p><span class="math display">\[
\mathbf{A_{1st\ row}} = \left[\begin{array}{ccccc}  a_1 &amp; a_2 &amp; a_3\end{array}\right],\ \ \ 
\mathbf{B_{2nd\ col}} = \left[\begin{array}{ccccc}  b_2 \\ b_5 \\ b_8 \end{array}\right]
\rightarrow
(a_1 \times b_2 + a_2 \times b_5 + a_3 \times b_8)
\]</span></p>
<p><span class="math display">\[
\mathbf{A_{1st\ row}} = \left[\begin{array}{ccccc}  a_1 &amp; a_2 &amp; a_3\end{array}\right],\ \ \ 
\mathbf{B_{3rd\ col}} = \left[\begin{array}{ccccc}  b_3 \\ b_6 \\ b_9 \end{array}\right]
\rightarrow
(a_1 \times b_3 + a_2 \times b_6 + a_3 \times b_9)
\]</span></p>
<p>We then repeat the process for the 2nd row of A and the 3rd row of A. The resulting <strong>dot product</strong> matrix is:</p>
<p><span class="math display">\[
\mathbf{A \cdotp B} = \left[\begin{array}{ccccc} 
(a_1b_1 + a_2 b_4 + a_3b_7) 
&amp; (a_1b_2 + a_2b_5 + a_3b_8)
&amp; (a_1b_3 + a_2b_6 + a_3b_9)\\
(a_4b_1 + a_5b_4 + a_6b_7) 
&amp; (a_4b_2 + a_5b_5 + a_6 b_8)
&amp; (a_4b_3 + a_5b_6 + a_6 b_9)\\
(a_7b_1 + a_8 b_4 + a_9 b_7) 
&amp; (a_7b_2 + a_8 b_5 + a_9 b_8)
&amp; (a_7b_3 + a_8 b_6 + a_9 b_9)\\
\end{array}\right]
\]</span></p>
<p>In the case above, we get the following computation:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{B \cdotp A^T} {}&amp; = \left[\begin{array}{ccccc} 
(4 \times 1 + 0 \times 0 + 0 \times 0) 
&amp; (4 \times 2 + 0 \times 0 + 0 \times 0)
&amp; (4 \times 3 + 0 \times 0 + 0 \times 0)\\
(5 \times 1 + 0 \times 0 + 0 \times 0) 
&amp; (5 \times 2 + 0 \times 0 + 0 \times 0)
&amp; (5 \times 3 + 0 \times 0 + 0 \times 0)\\
(6 \times 1 + 0 \times 0 + 0 \times 0) 
&amp; (6 \times 2 + 0 \times 0 + 0 \times 0)
&amp; (6 \times 3 + 0 \times 0 + 0 \times 0)\\
\end{array}\right]
\\
\\
\mathbf{B \cdotp A^T} &amp; =
\left[\begin{array}{ccccc} 
4 &amp; 8 &amp; 12 \\ 
5 &amp; 10 &amp; 15 \\ 
6 &amp; 12 &amp; 18
\end{array}\right]
\end{align*}\]</span></p>
</div>
<div id="hadamard-product" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Hadamard Product</h3>
<p> <strong>Hadamard product</strong> is an element-wise multiplication of two arbitrary matrix. See below:</p>
<p><span class="math display">\[
\left[\begin{array}{ccccc} 
a &amp; b\\ 
c &amp; d
\end{array}\right] \circ
\left[\begin{array}{ccccc} 
e &amp; f\\ 
g &amp; h
\end{array}\right] = 
\left[\begin{array}{ccccc} 
ae &amp; bf\\ 
cd &amp; dh
\end{array}\right] 
\]</span></p>
</div>
<div id="kronecker-product" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Kronecker Product</h3>
<p> <strong>Kronecker product</strong> is an element-to-matrix multiplication of two arbitrary matrix to obtain a block matrix. See below:</p>
<p><span class="math display">\[
\left[\begin{array}{ccccc} 
a &amp; b\\ 
c &amp; d
\end{array}\right] \otimes
\left[\begin{array}{ccccc} 
e &amp; f\\ 
g &amp; h
\end{array}\right] = 
\left[\begin{array}{cc}
a
\left[\begin{array}{ccccc} 
e &amp; f\\ 
g &amp; h
\end{array}\right] 
b
\left[\begin{array}{ccccc} 
e &amp; f\\ 
g &amp; h
\end{array}\right] 
\\
c
\left[\begin{array}{ccccc} 
e &amp; f\\ 
g &amp; h
\end{array}\right] 
d
\left[\begin{array}{ccccc} 
e &amp; f\\ 
g &amp; h
\end{array}\right] 
\end{array}
\right] =
\left[
\begin{array}{cccc}
ae &amp; af &amp; be &amp; bf \\
ag &amp; ah &amp; bg &amp; bh \\
ce &amp; cf &amp; de &amp; df \\
cg &amp; ch &amp; dg &amp; dh \\
\end{array}
\right] 
\]</span></p>
</div>
</div>
<div id="magnitude-direction-unit-vectors" class="section level2">
<h2><span class="header-section-number">2.4</span> Magnitude, Direction, Unit Vectors</h2>
<p>Vectors can be represented geometrically by using a 2-D or a 3-D Cartesian coordinate system.</p>
<p><strong>Direction</strong> </p>
<p>The direction of a vector is where the “arrow” points. It is measured using the angle of a vector in a counter-clockwise direction.</p>
<p>Figure  shows 4 vectors, {U,V,W,X}, in a 2D coordinate system. Vector U shows an angle that stretches from the angle 0 at the x-axis all the way to the vector U in the 1st quadrant of the Cartesian plane. Similarly, vector V shows an angle that stretches from the angle 0 all the way to the vector V in the 2nd quadrant of the Cartesian plane.</p>
<div class="figure" style="text-align: center"><span id="fig:vectors2d"></span>
<img src="vectors_2d.png" alt="Vectors in 2D plane" width="60%" />
<p class="caption">
Figure 2.4: Vectors in 2D plane
</p>
</div>
<p>Figure  shows a vector, {U}, in a 3D coordinate system. The direction of vector U shows a counter-clockwise rotation of the vector and can only be reflected based on the projected vectors on the 3 2d planes (X-Y plane, Y-Z plane, Z-X plane). There are three projections of vector U:</p>
<ul>
<li>projection of U to the X-Y plane. The angle to the projection is the direction in that plane.</li>
<li>projection of U to the Y-Z plane. The angle to the projection is the direction in that plane.</li>
<li>projection of U to the Z-X plane. The angle to the projection is the direction in that plane.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:vectors3d"></span>
<img src="vectors_3d.png" alt="A vector in 3D plane" width="60%" />
<p class="caption">
Figure 2.5: A vector in 3D plane
</p>
</div>
<p><strong>Using the point coordinates of the unit vector</strong></p>
<p>Another way to determine the direction of a vector is to normalize the vector into its unit vector and use its point coordinates.</p>
<p><strong>Magnitude:</strong> </p>
<p>The magnitude of a vector is its length. In euclidean space, given an n-dimension vector, <span class="math inline">\(\mathbf{\vec{v}}\)</span> = {<span class="math inline">\(x_1, x_2, ..., x_n\)</span>}, the magnitude is computed based on the following formula:</p>
<p><span class="math display">\[
magnitude(v) = \|v\| = \sqrt{x_1^2 + x_2^2 + ... + x_n^2 }
\]</span></p>
<p>Therefore, vector U in Figure  has the following magnitude:</p>
<p><span class="math display">\[
magnitude(U) = \|U\| = \sqrt{3^2 + 4^2 + 5^2 } = 50
\]</span></p>
<p><strong>Unit Vectors</strong> </p>
<p>Unit vectors are vectors that have magnitude of 1. Figure  shows two unit vectors, <span class="math inline">\(\mathbf{\vec{i}}\)</span> and <span class="math inline">\(\mathbf{\vec{j}}\)</span> in 2D coordinate system and three unit vectors, <span class="math inline">\(\mathbf{\vec{i}}\)</span>, <span class="math inline">\(\mathbf{\vec{j}}\)</span>, and <span class="math inline">\(\mathbf{\vec{k}}\)</span> in 3D coordinate system.</p>
<div class="figure" style="text-align: center"><span id="fig:unitvector"></span>
<img src="unit_vectors.png" alt="Unit Vectors" width="80%" />
<p class="caption">
Figure 2.6: Unit Vectors
</p>
</div>
<p>The magnitude of the unit vector <span class="math inline">\(\mathbf{\vec{i}}\)</span>, <span class="math inline">\(\left[\begin{array}{cc} 1 \\ 0\end{array}\right]\)</span>, in the 2D coordinate system, is computed as such:</p>
<p><span class="math display">\[
Magnitude\ of\ i = ||i|| =  \sqrt{1^2 + 0^2 } = 1
\]</span></p>
<p>The magnitude of the unit vector <span class="math inline">\(\mathbf{\vec{i}}\)</span>, <span class="math inline">\(\left[\begin{array}{ccc} 1 \\ 0 \\ 0 \end{array}\right]\)</span>, in the 3D coordinate system, is computed as such:</p>
<p><span class="math display">\[
Magnitude\ of\ i = ||i|| =  \sqrt{1^2 + 0^2  + 0^2} = 1
\]</span></p>
<p>To compute for the unit vector, <span class="math inline">\(\mathbf{\vec{u}}\)</span>, of a vector, <span class="math inline">\(\mathbf{\vec{v}}\)</span>, we use the following formula:</p>
<p><span class="math display">\[
u = \frac{v}{||v||} 
\]</span>
Therefore, given a vector, <span class="math inline">\(\mathbf{\vec{v}}\)</span> = <span class="math inline">\(\left[\begin{array}{ccc} 4.0 \\ 2.5 \end{array}\right]\)</span>, to derive the unit vector, we first compute for the magnitude of <span class="math inline">\(\mathbf{\vec{v}}\)</span>:</p>
<p><span class="math display">\[
|v|| =  \sqrt{4.0^2 + 2.5^2 } = 4.716991
\]</span>
Now, let’s get the <strong>unit vector</strong>:</p>
<p><span class="math display">\[
u = \frac{v}{||v||} = \frac{1}{||v||} * v = \frac{1}{4.716991} *  \left[\begin{array}{ccc}  4.0 \\ 2.5  \end{array}\right] = \left[\begin{array}{ccc}  \frac{4.0}{4.716991} \\ \frac{2.5}{4.716991}  \end{array}\right]  =  \left[\begin{array}{ccc}  0.85 \\ 0.53 \end{array}\right]
\]</span></p>
</div>
<div id="linear-combination-and-independence" class="section level2">
<h2><span class="header-section-number">2.5</span> Linear Combination and Independence</h2>
<p>We explain linear combination and linear independence using vectors <span class="citation">(Edwards C. H. et al <a href="references.html#ref-ref207c">2018</a>)</span>.</p>
<p><strong>Linear Combination:</strong> </p>
<p>A linear combination is the sum of the product of scalar coefficients and their respective vectors so that given a set of scalar coefficients <span class="math inline">\(\{c_1, c_2, c_3,..., c_{n}\}\)</span> and the corresponding vectors <span class="math inline">\(\{v_1, v_2, v_3,..., v_{n}\}\)</span>, we get the following formula:</p>
<p><span class="math display">\[
c_{1}v_{1} + c_{2}v_{2} + c_{3}v_{3} + ... +  c_{n}v_{n} = \mathbb{R}^n
\]</span></p>
<p>Here, coefficients may represent any useful values that can be interpreted as scale factors for the vectors or their weights.</p>
<div class="figure" style="text-align: center">
<img src="linear_comb.png" alt="Linear Combination" width="60%" />
<p class="caption">
(#fig:linear_comb)Linear Combination
</p>
</div>
<p>For example, vector <strong>V1</strong> in Figure  is a new vector derived from a linear combination of vectors <strong>U</strong> and <strong>W</strong>, <strong>5(j) + 2(i)</strong>, producing the following entries, <span class="math inline">\(\left[\begin{array}{ccccc} 2 \\ 5 \end{array}\right]\)</span>. The result is based on the following computation:</p>
<p><span class="math display">\[
U + W = 5(j) + 2(i) =
5\left[\begin{array}{ccccc} 0 \\ 1 \end{array}\right] + 2\left[\begin{array}{ccccc} 1 \\ 0 \end{array}\right] = \left[\begin{array}{ccccc} 0 \\ 5 \end{array}\right] + \left[\begin{array}{ccccc} 2 \\ 0 \end{array}\right] = \left[\begin{array}{ccccc} 2 \\ 5 \end{array}\right]
\]</span></p>
<p>Here, j (j-hat) and i (i-hat) are unit vectors. Using those two unit vectors, we can scale them by multiplying them with a scalar value so that 5(j) means that we are scaling the j unit vector by 5, e.g. <span class="math inline">\(5\left[\begin{array}{ccccc} 0 \\ 1 \end{array}\right] = \left[\begin{array}{ccccc} 0 \\ 5 \end{array}\right]\)</span>. In the Cartesian plane, 5(j) vector stretches from the x-y coordinate (0,0) to (0,5).</p>
<p>For another example, vector <strong>V2</strong> in Figure  is a linear combination of 4(j) + 3(i) resulting in <span class="math inline">\(\left[\begin{array}{ccccc} 3 \\ 4 \end{array}\right]\)</span> which is based on the following computation:</p>
<p><span class="math display">\[
4(j) + 3(i) =
4\left[\begin{array}{ccccc} 0 \\ 1 \end{array}\right] + 3\left[\begin{array}{ccccc} 1 \\ 0 \end{array}\right] = \left[\begin{array}{ccccc} 0 \\ 4 \end{array}\right] + \left[\begin{array}{ccccc} 3 \\ 0 \end{array}\right] = \left[\begin{array}{ccccc} 3 \\ 4 \end{array}\right]
\]</span></p>
<p><strong>Linear Independence:</strong> </p>
<p>Using Figure , we have three unit vectors, <span class="math inline">\(\mathbf{\vec{i}}\)</span>, <span class="math inline">\(\mathbf{\vec{j}}\)</span>, and <span class="math inline">\(\mathbf{\vec{k}}\)</span>. Unit vector <strong>i</strong> rests along the x-axis. Unit vector <span class="math inline">\(\mathbf{\vec{j}}\)</span> rests along the y-axis. And unit vector <span class="math inline">\(\mathbf{\vec{k}}\)</span> is rotated counter-clockwise from x-axis to about 53.13° so that if we scale <span class="math inline">\(\mathbf{\vec{k}}\)</span> by 5, we get vector <span class="math inline">\(\mathbf{\vec{W}}\)</span>, <span class="math inline">\(\left[\begin{array}{ccccc} 4 \\ 3 \end{array}\right]\)</span>.</p>
<p>Vector <strong>W</strong> is derived based on the computation below:</p>
<p><span class="math display">\[
5 \times k = 5\left[\begin{array}{ccccc} sin(53.13^{\circ} )  \\ cos(53.13^{\circ} )  \end{array}\right] = 
\left[\begin{array}{ccccc} 4 \\ 3 \end{array}\right]
\]</span></p>
<div class="figure" style="text-align: center">
<img src="linear_dependence.png" alt="Linear Dependence" width="60%" />
<p class="caption">
(#fig:linear_dependence)Linear Dependence
</p>
</div>
<p>If we add the two vectors, <strong>U</strong> + <strong>W</strong>, we get the following linear combination, <strong>V1</strong> = <span class="math inline">\(\left[\begin{array}{l} 4 \\ 5 \end{array}\right]\)</span>:</p>
<p><span class="math display">\[
U + W = 2(j) + 5(k) =
2\left[\begin{array}{l} 0 \\ 1 \end{array}\right] + 5\left[\begin{array}{l} 0.8 \\ 0.6 \end{array}\right] = \left[\begin{array}{l} 0 \\ 2 \end{array}\right] + \left[\begin{array}{l} 4 \\ 3 \end{array}\right] = \left[\begin{array}{l} 4 \\ 5 \end{array}\right]
\]</span>
On the other hand, if we add the two vectors, <strong>U</strong> + <strong>V</strong>, we get the following linear combination, <strong>V2</strong> = <span class="math inline">\(\left[\begin{array}{ccccc} 5 \\ 2 \end{array}\right]\)</span>:</p>
<p><span class="math display">\[
U + V = 2(j) + 5(i) =
2\left[\begin{array}{ccccc} 0 \\ 1 \end{array}\right] + 5\left[\begin{array}{ccccc} 1 \\ 0 \end{array}\right] = \left[\begin{array}{ccccc} 0 \\ 2 \end{array}\right] + \left[\begin{array}{ccccc} 5 \\ 0 \end{array}\right] = \left[\begin{array}{ccccc} 5 \\ 2 \end{array}\right]
\]</span></p>
<p>It can be said that for a vector to be a <strong>linear combination</strong>, it needs two other vectors. Here, vector <strong>V1</strong> is expressed as a linear combination of vectors <strong>U</strong> and <strong>W</strong>. Any vector expressed as a linear combination is considered to be a <strong>linearly dependent</strong> vector. And any vector that is used for the summation to arrive at a linear combination is a <strong>linearly independent</strong> vector. Vector <strong>V1</strong>, as a <strong>linear combination vector</strong>, is linearly dependent on vectors <strong>U</strong> and <strong>W</strong>; whereas vectors <strong>U</strong> and <strong>W</strong> are linearly independent vectors used for the linear combination.</p>
<p>Similarly, vector <strong>V2</strong>, as a <strong>linear combination vector</strong>, is linearly dependent on vectors <strong>U</strong> and <strong>V</strong>; whereas vectors <strong>U</strong> and <strong>V</strong> are linearly independent vectors used for the linear combination.</p>
<p>Now, let us pause for a moment and ask: is vector <strong>V2</strong> a linear combination of vectors <strong>U</strong> and <strong>W</strong>? The answer is ‘No’. Vector <strong>V2</strong> is not a linearly dependent vector based on <strong>U</strong> and <strong>W</strong>. In fact, if we use the unit vectors <span class="math inline">\(\mathbf{\vec{j}}\)</span> and <span class="math inline">\(\mathbf{\vec{k}}\)</span> as our basis for choosing a set of linearly independent vectors, no matter how much we scale (multiply scalars) to the unit vectors, we will not be able to get a linear combination that is vector <strong>V2</strong>. The vector <strong>V2</strong> is completely unreachable using <span class="math inline">\(\mathbf{\vec{j}}\)</span> and <span class="math inline">\(\mathbf{\vec{k}}\)</span> vectors in any linear combination.</p>
<p><span class="math display">\[
c1 \times j + c2 \times i \rightarrow &lt;any\ linearly\ dependent\ vector&gt;
\]</span></p>
<p>On the other hand, <strong>V1</strong> is actually reachable by using unit vectors <span class="math inline">\(\mathbf{\vec{j}}\)</span> and <span class="math inline">\(\mathbf{\vec{i}}\)</span>. If we scale <span class="math inline">\(\mathbf{\vec{j}}\)</span> by 5, and scale <span class="math inline">\(\mathbf{\vec{i}}\)</span> by 4, we will get <strong>V1</strong>.</p>
<p><span class="math display">\[
v1 = 5(j) + 4(i) = 
5\left[\begin{array}{ccccc} 0 \\ 1 \end{array}\right] + 4\left[\begin{array}{ccccc} 1 \\ 0 \end{array}\right] = \left[\begin{array}{ccccc} 0 \\ 5 \end{array}\right] + \left[\begin{array}{ccccc} 4 \\ 0 \end{array}\right] = \left[\begin{array}{ccccc} 4 \\ 5 \end{array}\right]
\]</span></p>
<p>Given all that, it can be said that <strong>V2</strong> is not in the span of unit vectors <strong>j</strong> and <strong>k</strong>. But that <strong>V1</strong> and <strong>V2</strong> are both in the span of unit vectors <span class="math inline">\(\mathbf{\vec{j}}\)</span> and <span class="math inline">\(\mathbf{\vec{i}}\)</span>.</p>
<p>Now, if we however closely compare the linear combination, <strong>V1</strong>, using two different unit vectors, we get two different entries for the vector.</p>
<p>Using unit vectors <span class="math inline">\(\mathbf{\vec{j}}\)</span> and <span class="math inline">\(\mathbf{\vec{k}}\)</span> as our basis:</p>
<p><span class="math display">\[
v1 = 2(j) + 5(k) = \left[\begin{array}{l} 4 \\ 5 \end{array}\right]
\]</span>
Using unit vectors <span class="math inline">\(\mathbf{\vec{j}}\)</span> and <span class="math inline">\(\mathbf{\vec{i}}\)</span> as our basis:</p>
<p><span class="math display">\[
v1 = 5(j) + 4(i) = \left[\begin{array}{c} 4 \\ 5 \end{array}\right]
\]</span></p>
<p>One additional concept to note: To determine if a set of vectors, <span class="math inline">\(\{v_1, v_2, v_3, ..., v_n\}\)</span>, are <strong>linearly dependent</strong>, if there are scalars <span class="math inline">\(\{c_1, c_2, c_3, ..., c_n\}\)</span>, with at least one being non-zero, then we can define the following formula:</p>
<p><span class="math display">\[
 c_{1}v_{1} + c_{2}v_{2} + c_{3}v_{3} + ... +  c_{n}v_{n} =  0
\]</span>
Suppose one of the vectors is a <strong>linear combination</strong> - <strong>linearly dependent</strong> - of the others, arbitrarily using v1 as an example, then using the above formula, we can extract <span class="math inline">\(c_{1}v_{1}\)</span> such that:</p>
<p><span class="math display">\[
c_{2}v_{2} + c_{3}v_{3} + ... +  c_{n}v_{n} =  -c_{1}v_{1}
\]</span></p>
<p>For example, given a system of 3 linear equations:</p>
<p><span class="math display">\[
1x + 3y + 6z= 0\\
2x + 7y + 3z= 0\\
0x + 1y - 9z = 0
\]</span>
We form the following <strong>augmented</strong> matrix :
<span class="math display">\[
A = 
x\left[\begin{array}{r} 1 \\ 2 \\ 0   \end{array}\right] + 
y\left[\begin{array}{r} 3 \\ 7 \\ 1 \end{array}\right] + 
z\left[\begin{array}{r} 6 \\ 3 \\ -9 \end{array}\right]
=
\left[\begin{array}{rrr|r} 
1 &amp; 3 &amp; 6 &amp; 0\\ 2 &amp; 7 &amp; 3 &amp; 0 \\ 0 &amp; 1 &amp; -9 &amp; 0
\end{array}\right]
\]</span></p>
<p>In the later section, we talk about <strong>how to convert</strong> a matrix to its <strong>row echelon</strong> form. We also talk about <strong>pivot</strong> columns, and <strong>free variables</strong>. But for now, let us show the <strong>row echelon</strong> form of matrix A.</p>
<p>Here, we let a <strong>free variable</strong>, <strong>z</strong>, take any non-zero arbitrary value, say <strong>z</strong> = <strong>1</strong> (or for now, let us use <strong>z</strong> = <strong>a</strong> where <span class="math inline">\(z \ne 0\)</span>):</p>
<p><span class="math display">\[
\left[\begin{array}{rrr|r} 
\color{red}{1} &amp; 0 &amp; 33 &amp; 0\\ 0 &amp; \color{red}{1}  &amp; -9 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0
\end{array}\right]
\rightarrow
\left[\begin{array}{r|r} x - 33\color{blue}{a} &amp; 0\\ y + 9\color{blue}{a} &amp; 0\\ z  &amp; \color{blue}{a} \end{array}\right] 
\rightarrow
\left[\begin{array}{r} x  \\ y \\ z  \end{array}\right] =
\left[\begin{array}{r} -33\color{blue}{a} \\ 9\color{blue}{a} \\ \color{blue}{a}  \end{array}\right]
\]</span></p>
<p>In other words,</p>
<p><span class="math display">\[
 c_{1}v_{1} + c_{2}v_{2} + c_{3}v_{3} 
 \rightarrow
 x \times v_1 + y \times v_2 + z \times v_3
 \rightarrow
x = -33a, y = 9a, z=a
\]</span></p>
</div>
<div id="space-span-and-basis" class="section level2">
<h2><span class="header-section-number">2.6</span> Space, Span and Basis</h2>
<p><strong>Basis</strong> </p>
<p>In linear systems of equations, a <strong>basis</strong> is a set of all <strong>linearly independent vectors</strong> - the <strong>basis vectors</strong> - that are <strong>in the system</strong> <span class="citation">(Edwards C. H. et al <a href="references.html#ref-ref207c">2018</a>)</span>.</p>
<p>Everytime we talk about linear combination, we need <strong>basis vectors</strong> . And we also know that basis vectors are linearly independent vectors. A matrix <strong>B</strong> is called a <strong>basis</strong> if its elements are all basis vectors of which none of those vectors depend on other independent vectors in the <strong>basis</strong> to become linearly dependent. It also means, none of the vectors is a multiple of the others.</p>
<p><span class="math display">\[
B= \{b_1, b_2, ..., b_n\}
\]</span></p>
<p><strong>Space</strong> </p>
<p>There is probably nothing interesting about combining two linearly independent vectors, say vectors <span class="math inline">\(\mathbf{\vec{i}}\)</span> and <span class="math inline">\(\mathbf{\vec{j}}\)</span> to get just one dependent vector (one linear combination). But what if we are interested to know all the possible linear combinations - all the possible dependent vectors we can derive out of any (or all possible) scales of those two linearly independent vectors.</p>
<p>For example, if we use two unit vectors <span class="math inline">\(\mathbf{\vec{j}}\)</span> and <span class="math inline">\(\mathbf{\vec{i}}\)</span> as our <strong>basis vectors</strong>, and scale each one, we may get some arbitrary linear combination of vectors such as below:</p>
<ul>
<li>scaling <span class="math inline">\(\mathbf{\vec{i}}\)</span> and <span class="math inline">\(\mathbf{\vec{j}}\)</span> by 2 gives us: 2(<span class="math inline">\(\mathbf{\vec{i}}\)</span> + <span class="math inline">\(\mathbf{\vec{j}}\)</span>) = [2,2]</li>
<li>scaling <span class="math inline">\(\mathbf{\vec{i}}\)</span> and <span class="math inline">\(\mathbf{\vec{j}}\)</span> by 3 gives us: 3(<span class="math inline">\(\mathbf{\vec{i}}\)</span> + <span class="math inline">\(\mathbf{\vec{j}}\)</span>) = [3,3]</li>
<li>scaling <span class="math inline">\(\mathbf{\vec{i}}\)</span> by 2 and <span class="math inline">\(\mathbf{\vec{j}}\)</span> by 3 gives us: 2(<span class="math inline">\(\mathbf{\vec{i}}\)</span>) + 3(<span class="math inline">\(\mathbf{\vec{j}}\)</span>) = [2,3]</li>
<li>scaling <span class="math inline">\(\mathbf{\vec{i}}\)</span> by 3 and <span class="math inline">\(\mathbf{\vec{j}}\)</span> by 2 gives us: 3(<span class="math inline">\(\mathbf{\vec{i}}\)</span>) + 2(<span class="math inline">\(\mathbf{\vec{j}}\)</span>) = [3,2]</li>
<li>scaling <span class="math inline">\(\mathbf{\vec{i}}\)</span> by 20 and <span class="math inline">\(\mathbf{\vec{j}}\)</span> by 50 gives us: 20(<span class="math inline">\(\mathbf{\vec{i}}\)</span>) + 50(<span class="math inline">\(\mathbf{\vec{j}}\)</span>) = [20,50]</li>
<li>scaling <span class="math inline">\(\mathbf{\vec{i}}\)</span> by -20 and <span class="math inline">\(\mathbf{\vec{j}}\)</span> by -50 gives us: -20(<span class="math inline">\(\mathbf{\vec{i}}\)</span>) + -50(<span class="math inline">\(\mathbf{\vec{j}}\)</span>) = [-20,-50]</li>
</ul>
<p>We can go on and on and we may end up collecting a set of these linear combinations. That <strong>set of linear combinations</strong> forms what we call a <strong>vector space, V</strong> which forms a <strong>subspace</strong>  in the <span class="math inline">\(\mathbb{R}^2\)</span> <strong>space</strong>.</p>
<p>Recall that the system of equations is denoted by the following:</p>
<p><span class="math display">\[
A\mathbf{\vec{x}} = \mathbf{\vec{b}}
\]</span></p>
<p>We describe <strong>vector</strong> <span class="math inline">\(\mathbf{\vec{x}}\)</span> as the <strong>solution space</strong> . The intent is not just to solve for only one vector that is <strong>x</strong> and say that we have solved the equation. The intent is to assume that vector <strong>x</strong> can hold a set of all possible <strong>solutions</strong> - that set is called the <strong>solution space for the system</strong>.</p>
<p>So in other words, if the system is such that we have:</p>
<p><span class="math display">\[
A\mathbf{\vec{x}} \neq 0 \ \ \ \ \ \ where\ \mathbf{\vec{b}} \neq 0
\]</span></p>
<p>what we have done is to constraint or limit our system of linear equation such that vector <strong>x</strong> holds <strong>only</strong> a set of <strong>solutions</strong> for which <span class="math inline">\(\mathbf{\vec{b}} \ne 0\)</span>. That <strong>set of solutions</strong> for which <span class="math inline">\(\mathbf{\vec{b}} \ne 0\)</span> is our <strong>solution space</strong> for which the system is <span class="math inline">\(A\mathbf{\vec{x}} \neq 0\)</span>.</p>
<p>Therefore, if <span class="math inline">\(\mathbf{\vec{x}}\)</span> is a solution to the system, <span class="math inline">\(A\mathbf{\vec{x}} \neq 0\)</span>, it also means that the corresponding vector <span class="math inline">\(\mathbf{\vec{b}}\)</span> is a <strong>linear combination</strong> of the <strong>solumn space</strong> , <span class="math inline">\(C(A)\)</span>.</p>
<p>On the other hand, if the system is such that we have:</p>
<p><span class="math display">\[
A\mathbf{\vec{x}} = 0 \ \ \ \ \ \ where\ \mathbf{\vec{b}} = 0
\]</span></p>
<p>what we have done is to constraint or limit our system of linear equation such that vector <strong>x</strong> holds <strong>only</strong> a set of <strong>solutions</strong> for which <span class="math inline">\(b = 0\)</span>. That <strong>set of solutions</strong> for which <span class="math inline">\(\mathbf{\vec{b}} = 0\)</span> is our <strong>solution space</strong> for which the system is <span class="math inline">\(A\mathbf{\vec{x}} = 0\)</span>.</p>
<p>Lastly, the <strong>row space</strong>  of a matrix is the column space of its transpose - <span class="math inline">\(C(A^T)\)</span>. And the <strong>left null-space</strong>  of a matrix is the null space of its transpose - <span class="math inline">\(N(A^T)\)</span>.</p>
<p>Therefore, the following subspace dimensions are like so:</p>
<ul>
<li>A <strong>Vector Space</strong> is a set of all linear combinations in a system, denoted by V(A).</li>
<li>A <strong>Column Space</strong> is a set of all linearly independent vectors of a system’s matrix, denoted as C(A). See rank and nullity in later section.</li>
<li>A <strong>Null-space</strong> is a set of all linearly dependent vectors of a system’s matrix, denoted as N(A) or Nul(A) or Ker(A). See rank and nullity in later section.</li>
<li>A <strong>Solution Space</strong> is a set of all possible solutions of a system. The <strong>solution space</strong> in a system for which <span class="math inline">\(A\mathbf{\vec{x}} \neq 0\)</span> is different from the <strong>solution space</strong> in a system for which <span class="math inline">\(A\mathbf{\vec{x}} = 0\)</span>.</li>
<li>a <strong>Row Space</strong> is a <strong>Column space</strong> of a system’s matrix transpose, denoted by <span class="math inline">\(C(A^T)\)</span>.</li>
<li>a <strong>Left Null-space</strong> is a <strong>Null-space</strong>  of a system’s matrix transpose, denoted by <span class="math inline">\(N(A^T)\)</span>.</li>
</ul>
<p><strong>Span</strong> </p>
<p>A <strong>Vector Space</strong> is limited only by the number of possible linear combinations and will grow in space as long as the unit vectors have enough unique coefficients to scale them. To <strong>grow</strong> the vector space is to <strong>span</strong> the <strong>basis</strong>. In another way to put it, the amount of space covered in <strong>V</strong> is the <strong>span</strong> of the <strong>basis B</strong> and can be denoted as:</p>
<p><span class="math display">\[
span\{B\} = V 
\]</span>
The longer version is:</p>
<p><span class="math display">\[
span\{b_1, b_2, ..., b_m\} = \{v_1, v_2, v_3,  ..., v_n\}
\]</span>
where <strong>{<span class="math inline">\(b_1,b_2,..., b_m\)</span>}</strong> are all <strong>basis vectors</strong> of <strong>basis B</strong> and <strong>{<span class="math inline">\(v_1, v_2, v_3, ..., v_n\)</span>}</strong> are all the linear combinations of those basis vectors, forming a <strong>vector space V</strong> which is spanned by the <strong>basis B</strong>.</p>
<p>We therefore can say that <strong>Span</strong> is the set of linear combination of a vector space, V.</p>
<p>For example:</p>
<p><span class="math display">\[
span\ \{ i\} \rightarrow \text{span of basis vector i  in 1D space}
\]</span></p>
<p>For example:</p>
<p><span class="math display">\[
span\ \{ i, j\} \rightarrow \text{span of basis vectors i and j in 2D space}
\]</span></p>
<p>For example:</p>
<p><span class="math display">\[
span\ \{ i, j, k\} \rightarrow \text{span of basis vectors i and j and k  in 3D space}
\]</span></p>
<p>It is important to not miss the point that each element in the notation <strong>span {</strong> <span class="math inline">\(b_1, b_2, ...,b_m\)</span> <strong>}</strong> is a <strong>basis vector</strong> - in other words, each element is a linearly independent vector that can be paired (matched) with other linearly independent vectors to form any linear combination in the subspace. In other words, given a span <strong>S</strong>, with {<span class="math inline">\(b_1, b_2, ..., b_m\)</span>}, all the vectors are basis vectors if they’re all linearly independent.</p>
<p>It may also be important to not miss the point that basis vectors can best be represented by unit vectors because unit vectors are easier to point out as being scalable.</p>
<p>Consider Figure , there are two unit vectors <strong>{</strong> <span class="math inline">\(\mathbf{\vec{i}}, \mathbf{\vec{j}}\)</span> <strong>}</strong> used as basis vectors. In the example figure, the <strong>span</strong> of those two basis vectors, written as <strong>span {</strong> <span class="math inline">\(\mathbf{\vec{i}}, \mathbf{\vec{j}}\)</span><strong>}</strong>, covers a finite set of 16 unique linear combinations - based on <span class="math inline">\(c_1 * i + c_2 * j\)</span>, using different combination of coefficients (scales) forming a subspace <strong>V</strong> in <span class="math inline">\(\mathbb{R}^2\)</span> space whose vectors <strong>{</strong> <span class="math inline">\(v_1, v_2, ..., v_{16}\)</span> <strong>}</strong> in the subspace are linearly dependent:</p>
<p><span class="math display">\[
span\ \{ i, j\} \rightarrow c_1 \times i + c_2 \times j \rightarrow 
\{ v_1, v_2, v_3, ...,  v_{16}\} \in  \mathbb{R}^2
\]</span>
Figure  shows that the basis vectors, unit vectors <strong>{</strong> <span class="math inline">\(\mathbf{\vec{i}}, \mathbf{\vec{j}}\)</span> <strong>}</strong>, can be scaled by multiplying each one with any possible scalar value (any possible coefficients) and therefore the resulting possibility of any linear combination can span across the entire plane in the 2D coordinate system.</p>
<div class="figure" style="text-align: center"><span id="fig:span2d2"></span>
<img src="span_2d_2.png" alt="Span 2D" width="80%" />
<p class="caption">
Figure 2.7: Span 2D
</p>
</div>
<p>Now, compare that with Figure . The basis vectors, unit vectors <strong>{</strong> <span class="math inline">\(\mathbf{\vec{i}}, \mathbf{\vec{j}}\)</span> <strong>}</strong>, are rotated such that they are now squashed together into a 1 dimension space, <span class="math inline">\(\mathbb{R}^1\)</span>, both fitting along a diagonal line (instead of a 2D plane). Because <span class="math inline">\(\mathbf{\vec{j}}\)</span> = [1,1] and <span class="math inline">\(\mathbf{\vec{i}}\)</span> = [1,1], no matter how much we scale any one or both of those basis vectors, the scaled version of the vectors will always fall on the same diagonal line.</p>
<p>In this condition of the figure, the <strong>span</strong> of those two unit vectors, written as <strong>span {</strong> <span class="math inline">\(\mathbf{\vec{i}}, \mathbf{\vec{j}}\)</span><strong>}</strong>, yields a set of 10 unique linear combinations - based on <span class="math inline">\(c_1 \times i + c_2 \times j\)</span>, forming a subspace <strong>V</strong> in <span class="math inline">\(\mathbb{R}^1\)</span> space whose vectors <strong>{</strong> <span class="math inline">\(v_1, v_2, ..., v_{10}\)</span> <strong>}</strong> in the subspace are linearly dependent.</p>
<p>Notice that there is a rogue vector <strong>U</strong> not belonging to the subspace. This is because no matter how we scale the basis vectors, the span never reaches whatever subspace the vector <strong>U</strong> belongs. In this situation, it can be assumed that the vector <strong>U</strong> must be a separate independent vector, having its own span and can therefore scale on its own to form its own subspace in <span class="math inline">\(\mathbb{R}^1\)</span> space.</p>
<p>Vector <strong>U</strong> can also partner with any of the unit vectors <span class="math inline">\(\mathbf{\vec{i}}\)</span> or <span class="math inline">\(\mathbf{\vec{j}}\)</span> to form a new span and thus a new subspace in <span class="math inline">\(\mathbb{R}^2\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:span2d1"></span>
<img src="span_2d_1.png" alt="Span 2D" width="80%" />
<p class="caption">
Figure 2.8: Span 2D
</p>
</div>
</div>
<div id="determinants" class="section level2">
<h2><span class="header-section-number">2.7</span> Determinants </h2>
<p>Let us use vectors first to explain about determinants. To get an insight about a determinant, we may need to draw 2 vectors in a 2D Cartesian plane, each vector reflected at the tip of the other to form a geometric area (or region) similar to Figure .</p>
<div class="figure" style="text-align: center"><span id="fig:determinant"></span>
<img src="determinant.png" alt="Determinant" width="80%" />
<p class="caption">
Figure 2.9: Determinant
</p>
</div>
<p>In Figure , there are 6 sample linear combinations with corresponding determinants. A quick view of the figure may show a relationship between the determinant and the area of the closed region formed between two basis vectors. That being so, a determinant is a scaling factor - <span class="math inline">\(\mathbb{R}^2\)</span> for 2D plane - of a given unit area (e.g. the unit area formed by the unit vectors <span class="math inline">\(\mathbf{\vec{i}}\)</span> and <span class="math inline">\(\mathbf{\vec{j}}\)</span>).</p>
<p>We start with a unit area by computing for the magnitude of unit vectors <span class="math inline">\(\mathbf{\vec{i}}\)</span> and <span class="math inline">\(\mathbf{\vec{j}}\)</span>:</p>
<p><span class="math display">\[
Area_{\{i\ and\ j\}} = |\mathbf{\vec{i}}| \times |\mathbf{\vec{j}}|  = \sqrt{1^2 + 0^1} \times \sqrt{0^2 + 1^1} = 1 * 1 = 1
\]</span></p>
<p>In the first coordinate system, we scaled the magnitudes of the unit vectors using (3,1). Then we compute for the area.</p>
<p><span class="math display">\[
Area_{\{i\ and\ j\}} = 3|\mathbf{\vec{i}}| \times 1|\mathbf{\vec{j}}|  = 3(1) \times 1(1) =  3
\]</span></p>
<p>In the second coordinate system, we scaled the magnitudes of the unit vectors using (3,2). Then we compute for the area.</p>
<p><span class="math display">\[
Area_{\{i\ and\ j\}} = 3|\mathbf{\vec{i}}| \times 2|\mathbf{\vec{j}}|  = 3(1) \times 2(1) = 6
\]</span></p>
<p>In the fourth coordinate system, we rotated unit vector <span class="math inline">\(\mathbf{\vec{j}}\)</span> by 45 degrees and that gives a different coordinate, <span class="math inline">\(\left[\begin{array}{c} 0.707 \\ 0.707 \end{array}\right]\)</span>. It also gives us a parallelogram shape instead of a rectangular shape. Nonetheless, we will still compute for the area as if the shape is rectangular.</p>
<p>Now for the area of the parallelogram, first, we need to compute for the magnitude of the projected version of the unit vector <span class="math inline">\(\mathbf{\vec{j}}\)</span> using: <span class="math inline">\(|\mathbf{\vec{j}}| * sin(45^{\circ})\)</span>. See Figure .</p>
<div class="figure" style="text-align: center"><span id="fig:adjacent"></span>
<img src="adjacent.png" alt="Compute for Adjacent" width="20%" />
<p class="caption">
Figure 2.10: Compute for Adjacent
</p>
</div>
<p><span class="math display">\[
a = |\mathbf{\vec{j}}| \times sin(45^{\circ}) = (1) (0.707) = 0.707
\]</span>
Then we compute for the area.</p>
<p><span class="math display">\[
Area_{\{\mathbf{\vec{i}}\ and\ \mathbf{\vec{j}}\}} = 3|\mathbf{\vec{i}}| \times 2a  = 3(1) \times 2(0.707) = 4.242
\]</span></p>
<p>To validate the result, let us use a different way to compute for the area. This time, we will compute the determinant using <strong>Leibnitz</strong> formula.</p>
<p><span class="math display">\[\begin{align*}
Given: 3 \times \mathbf{\vec{i}} {}&amp; = 3\left[\begin{array}{cc} 1   \\ 0  \end{array}\right] = \left[\begin{array}{cc} 3   \\ 0  \end{array}\right]\\
Given: 2 \times \mathbf{\vec{j}} &amp; = 2\left[\begin{array}{cc} 0.707   \\ 0.707  \end{array}\right] = \left[\begin{array}{cc} 1.414   \\ 1.414  \end{array}\right]
\end{align*}\]</span></p>
<p>We get the matrix form:</p>
<p><span class="math display">\[
A = \left[\begin{array}{cc} 3 &amp; 1.414 \\ 0 &amp; 1.414 \end{array}\right]
\]</span></p>
<p>Now computing for the determinant using <strong>Leibniz</strong> formula, we get:</p>
<p><span class="math display">\[
det(A) = 3 \times 1.414 - 0 \times 1.414 = 3 \times 1.414 = 4.242
\]</span></p>
<p>The computed area of the parallelogram ends up being equal to the computed determinant using <strong>Leibniz</strong> formula.</p>
<p>In 2-d space, if two vectors span a line (transformed into lower dimension, e.g. 1D), then it is said that the two vectors are linearly dependent on each other and thus the determinant is zero. The area turns into a line, or even into a point.</p>
<p>In 3-d space, if three vectors span a plane (transformed into lower dimension, e.g. 2D), then it is said that the three vectors are linearly dependent on one another and thus the determinant is zero. The volume turns into an area, or even into a line, or even into a point.</p>
<p>Now, if determinant is negative, it means that the space is flipped over. See Figure </p>
<div class="figure" style="text-align: center"><span id="fig:flipped"></span>
<img src="flipped.png" alt="Flipped over" width="80%" />
<p class="caption">
Figure 2.11: Flipped over
</p>
</div>
<p><strong>Methods in computing for determinants:</strong></p>
<p>Let us recall a few formulas that help to compute for determinants:</p>
<p><strong>Leibniz Formula</strong> </p>
<p>Given the following <strong>2x2 matrix</strong> ( or two vectors ):
<span class="math display">\[
A = \left[\begin{array}{cc} a &amp; b \\ c &amp; d \end{array}\right]_{2 \times 2}
\]</span></p>
<p>we get the following <strong>Leibniz</strong> formula:</p>
<p><span class="math display">\[\begin{align}
det(A) = |A| = a \times d - b \times c \label{eqn:Leibniz}
\end{align}\]</span></p>
<p>Notice that the determinant of a matrix uses the following notation <span class="math inline">\(|A|\)</span>.</p>
<p><strong>Laplace Formula</strong> </p>
<p>Given the following <strong>3x3 matrix</strong> ( or three vectors ):
<span class="math display">\[
A = \left[\begin{array}{ccccc} a &amp; b &amp; c\\ d &amp; e &amp; f \\g &amp; h &amp; i \end{array}\right]_{3 \times 3}
\]</span></p>
<p>the matrix gets expanded into:</p>
<p><span class="math display">\[
det(A) = |A| = 
a\left|\begin{array}{ccccc} e &amp; f \\ h &amp; i \end{array}\right| - 
b\left|\begin{array}{ccccc} d &amp; f \\ g &amp; i \end{array}\right| + 
c\left|\begin{array}{ccccc} d &amp; e \\ g &amp; h \end{array}\right| 
\]</span></p>
<p>so then, we get the following <strong>Laplace</strong> expansion formula</p>
<p><span class="math display">\[\begin{align}
det(A) = |A| = a \times (e \times i - f \times h) - b \times (d \times i - f \times g) 
+ c (d \times h - e \times g) \label{eqn:laplace}
\end{align}\]</span></p>
<p>The formula is recursive in that it can be applied into any nxn matrix; though computation gets inefficient with higher dimensions. As an example, for <strong>4x4 matrix</strong>, see below:</p>
<p><span class="math display">\[
A = \left[\begin{array}{ccccc} a &amp; b &amp; c &amp; d\\ e &amp; f &amp; g &amp; h \\i &amp; j &amp; k &amp; l \\ m &amp; n &amp; o &amp; p \end{array}\right]_{4 \times 4} 
\]</span></p>
<p>Using <strong>Laplace</strong> formula, we get the expanded version:</p>
<p><span class="math display">\[
det(A) = |A| = 
a \left|\begin{array}{ccccc} f &amp; g &amp; h\\ j &amp; k &amp; l \\n &amp; o &amp; p \end{array}\right| -
b \left|\begin{array}{ccccc} e &amp; g &amp; h\\ i &amp; k &amp; l \\m &amp; o &amp; p \end{array}\right| +
c \left|\begin{array}{ccccc} e &amp; f &amp; h\\ i &amp; j &amp; l \\m &amp; n &amp; p \end{array}\right| -
d \left|\begin{array}{ccccc} e &amp; f &amp; g\\ i &amp; j &amp; k \\m &amp; n &amp; o \end{array}\right| 
\]</span></p>
<p>then compute for the determinant of each of the 3x3 matrix using the <strong>Laplace</strong> expansion formula again.</p>
<p><strong>Sarrus Rule (Basketweave method)</strong>  </p>
<p>Sarrus Rule is another way to computer for 3x3 matrices though may not necessarily be reliable for higher dimensions. We compute by subtracting the sum of the products of the backward diagonal line from the sum of the product of the forward diagonal lines.</p>
<div class="figure" style="text-align: center"><span id="fig:Sarrus"></span>
<img src="sarrus.png" alt="Sarrus Rule" width="30%" />
<p class="caption">
Figure 2.12: Sarrus Rule
</p>
</div>
<p>Given a 3x3 matrix (A):</p>
<p><span class="math display">\[
det(A) = |A| = \left|\begin{array}{ccc|cc}  a &amp; b &amp; c  &amp; a &amp; b \\ d &amp; e &amp; f &amp; d &amp; e\\g &amp; h &amp; i &amp; g &amp; h\end{array}\right|
\]</span></p>
<p>We get:</p>
<p><span class="math display">\[
det(A) = |A| = ( a \times e \times i + b \times f \times g + c \times d \times h ) -
 ( c \times e \times g + a \times f \times h + b \times d \times i)
\]</span></p>
<p><strong>Gaussian and Gauss-Jordan Elimination</strong> </p>
<p>Determinants, especially for matrices with higher dimensions (e.g. &gt;3) and non-square matrices, can be calculated based on converting matrices into their row echelon form using a method called Gaussian elimination. The method is introduced in the next section in the context of deriving the row echelon form of a matrix. Moreover, Gaussian and Gauss-Jordan elimination is discussed later in this chapter in the context of matrix factorization.</p>
<p><strong>Use of Determinants</strong></p>
<p>One of the usefulness of determinants is in computing for system of equations using <strong>Cramer’s Rule</strong>. </p>
<p>Given the following systems of equations:</p>
<p><span class="math display">\[
3x + 2y = 4 \\
9x + 4y = 12
\]</span></p>
<p>Express the system of equations in matrix form:</p>
<p><span class="math display">\[
A = \left[\begin{array}{rr|r} C_{x1} &amp; C_{y1} &amp; C_{b1} \\ C_{x2} &amp; C_{y2} &amp; C_{b2} \end{array}\right]
=
\left[\begin{array}{rr|r} 3 &amp; 2 &amp; 4 \\ 9 &amp; 5 &amp; 12 \end{array}\right]
\]</span></p>
<p>Then, compute for the determinants using <strong>Leibniz</strong> formula:</p>
<p><span class="math display">\[\begin{align*}
det(A) {} &amp; = |A|  
= \left|\begin{array}{rr} C_{x1} &amp; C_{y1}  \\ C_{x2} &amp; C_{y2} \end{array}\right|
= \left|\begin{array}{rr} 3 &amp; 2  \\ 9 &amp; 5  \end{array}\right|
= 3 \times 5 - 2 \times 9 = -3 \\
det(A_x) &amp; = |A_x|  
= \left|\begin{array}{rr} C_{b1} &amp; C_{y1}  \\ C_{b2} &amp; C_{y2} \end{array}\right|
= \left|\begin{array}{rr} 4 &amp; 2  \\ 12 &amp; 5  \end{array}\right|
= 4 \times 5 - 2 \times 12 = -4 \\
det(A_y) &amp; = |A_y|  
= \left|\begin{array}{rr} C_{x1} &amp; C_{b1}  \\ C_{x2} &amp; C_{b2} \end{array}\right|
= \left|\begin{array}{rr} 3 &amp; 4  \\ 9 &amp; 12  \end{array}\right|
= 3 \times 12 - 4 \times 9 = 0 \\
\end{align*}\]</span></p>
<p>Now, solve for x and y:</p>
<p><span class="math display">\[
x = \frac{det(A_x)}{det(A)} = \frac{|Ax|}{|A|} = \frac{-4}{-3} = \frac{4}{3} 
\\
y = \frac{det(A_y)}{det(A)} = \frac{|Ay|}{|A|} = \frac{0}{-3} = 0
\]</span></p>
<p>Note that there are other methods more efficient in computing for systems of equations than <strong>Cramer’s Rule</strong>. The chapter about numerical analysis focuses on optimized methods.</p>
<p>It is also noteworthy to mention that a system of equation does not have a solution if the det(A) = 0 and <span class="math inline">\(det(A_x) \ne 0\)</span>. That is, the <span class="math inline">\(x = \frac{|A_x|}{|A|}\)</span> cannot be solved because the denominator is <span class="math inline">\(|A| = 0\)</span></p>
<p>For example, the following system of equations cannot be solved:</p>
<p><span class="math display">\[
x + 2y = 6 \\
3x + 6y = 4
\]</span></p>
<p>That is because the determinant of the matrix is zero:</p>
<p><span class="math display">\[
det(A)  = |A|  
= \left|\begin{array}{rr} C_{x1} &amp; C_{y1}  \\ C_{x2} &amp; C_{y2} \end{array}\right|
= \left|\begin{array}{rr} 1 &amp; 2  \\ 3 &amp; 6  \end{array}\right|
= 1 \times 6 - 2 \times 3 = 0 \\
\]</span></p>
<p>Figure  shows that vector Y is a multiple - 2X that - of vector X. Both vectors point to the same direction and thus show a determinant of zero - geometrically, it has no area as it follows a line. It can also be interpreted that since Y and X can both be multiples of each other, x has <strong>infinite solution</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:nosolution"></span>
<img src="no_solution.png" alt="Zero Determinant" width="40%" />
<p class="caption">
Figure 2.13: Zero Determinant
</p>
</div>
</div>
<div id="minors-cofactors-and-adjugate-forms" class="section level2">
<h2><span class="header-section-number">2.8</span> Minors, Cofactors, and Adjugate Forms</h2>
<p>In previous section, we introduced two methods that can help to compute for determinants of matrices. Now, let us introduce minors, cofactors, and adjugates in this chapter.</p>
<p><strong>Minor Form</strong> </p>
<p>Consider a 3x3 matrix:</p>
<p><span class="math display">\[
A = \left[\begin{array}{ccc} a &amp; b &amp; c\\ d &amp; e &amp; f \\ g &amp; h &amp; i \end{array}\right]_{3 \times 3}
\]</span></p>
<p>The <strong>A</strong> matrix can be formed into an <strong>M</strong> matrix - the minor form of the matrix:</p>
<p><span class="math display">\[
M = \left[\begin{array}{ccc} 
M_{1,1} &amp; M_{1,2} &amp; M_{1,3} \\ 
M_{2,1} &amp; M_{2,2} &amp; M_{2,3} \\ 
M_{3,1} &amp; M_{3,2} &amp; M_{3,3} \\ 
\end{array}\right|_{3 \times 3}
= 
\begin{pmatrix}
\left|\begin{array}{rrr}
 e &amp; f \\ h &amp; i
\end{array}\right|_{2 \times 2} &amp;  
\left|\begin{array}{rrr}
d &amp;  f \\ g &amp;  i
\end{array}\right|_{2 \times 2} &amp;
\left|\begin{array}{rrr}
d &amp; e \\g &amp; h 
\end{array}\right|_{2 \times 2} 
\\
\\
\left|\begin{array}{rrr}
b &amp; c \\ h &amp; i
\end{array}\right|_{2 \times 2} &amp;  
\left|\begin{array}{rrr}
a &amp; c \\ g  &amp; i
\end{array}\right|_{2 \times 2} &amp;
\left|\begin{array}{rrr}
a &amp; b \\ g &amp; h 
\end{array}\right|_{2 \times 2} 
\\
\\
\left|\begin{array}{rrr}
b &amp; c \\ e &amp; f 
\end{array}\right|_{2 \times 2} &amp;  
\left|\begin{array}{rrr}
a &amp; c \\ d &amp; f 
\end{array}\right|_{2 \times 2} &amp;
\left|\begin{array}{rrr}
a &amp; b \\ d &amp; e  
\end{array}\right|_{2 \times 2} 
\end{pmatrix}_{3 \times 3}
\]</span>
Each of the elements in the <strong>M</strong> matrix is called a <strong>minor</strong> - <span class="math inline">\(M_{i,j}\)</span> - which is <strong>the determinant of the corresponding sub-matrix</strong>. Basically, we remove the i-th row and j-th column of the <strong>A</strong> matrix and the remaining rows and columns form the sub-matrix.</p>
<p>For example, the <strong>first minor</strong> <span class="math inline">\(M_{1,1}\)</span> yields the following:</p>
<p><span class="math display">\[
M_{1,1} = 
\left|\begin{array}{rrr}
\color{red}{\square} &amp; \color{red}{\square} &amp; \color{red}{\square} \\
\color{red}{\square} &amp; e &amp; f \\
\color{red}{\square} &amp; h &amp; i
\end{array}\right|
=
\left|\begin{array}{rrr}
 e &amp; f \\ h &amp; i
\end{array}\right|_{2 \times 2}
= det(\text{lower right submatrix}) = e \times i - f \times h
\]</span></p>
<p>Another example, the <strong>minor</strong> <span class="math inline">\(M_{2,3}\)</span> yields the following:</p>
<p><span class="math display">\[
M_{1,1} = 
\left|\begin{array}{rrr}
a &amp; b &amp; \color{red}{\square} \\
\color{red}{\square} &amp; \color{red}{\square} &amp; \color{red}{\square} \\
g &amp; h &amp; \color{red}{\square}
\end{array}\right| 
=
\left|\begin{array}{rrr}
 a &amp; b \\ g &amp; h
\end{array}\right|_{2 \times 2}
= det(\text{lower right submatrix}) = a \times h - b \times g
\]</span></p>
<p>The matrix can be expanded into its minor form:</p>
<p><span class="math display">\[
M = \begin{pmatrix}
\left|\begin{array}{rrr}
\color{red}{\square} &amp; \color{red}{\square} &amp; \color{red}{\square} \\
\color{red}{\square} &amp; e &amp; f \\
\color{red}{\square} &amp; h &amp; i
\end{array}\right| &amp;  
\left|\begin{array}{rrr}
\color{red}{\square} &amp; \color{red}{\square} &amp; \color{red}{\square} \\
d &amp; \color{red}{\square} &amp; f \\
g &amp; \color{red}{\square} &amp; i
\end{array}\right| &amp;
\left|\begin{array}{rrr}
\color{red}{\square} &amp; \color{red}{\square} &amp; \color{red}{\square} \\
d &amp; e &amp; \color{red}{\square} \\
g &amp; h &amp; \color{red}{\square}
\end{array}\right| 
\\
\\
\left|\begin{array}{rrr}
\color{red}{\square} &amp; b &amp; c \\
\color{red}{\square} &amp; \color{red}{\square} &amp; \color{red}{\square} \\
\color{red}{\square} &amp; h &amp; i
\end{array}\right| &amp;  
\left|\begin{array}{rrr}
a &amp; \color{red}{\square} &amp; c\\
\color{red}{\square} &amp; \color{red}{\square} &amp; \color{red}{\square} \\
g  &amp; \color{red}{\square} &amp; i
\end{array}\right| &amp;
\left|\begin{array}{rrr}
a &amp; b &amp; \color{red}{\square} \\
\color{red}{\square} &amp; \color{red}{\square} &amp; \color{red}{\square} \\
g &amp; h &amp; \color{red}{\square}
\end{array}\right| 
\\
\\
\left|\begin{array}{rrr}
\color{red}{\square} &amp; b &amp; c \\
\color{red}{\square} &amp; e &amp; f \\
\color{red}{\square} &amp; \color{red}{\square} &amp; \color{red}{\square} 
\end{array}\right| &amp;  
\left|\begin{array}{rrr}
a &amp; \color{red}{\square} &amp; c \\
d &amp; \color{red}{\square} &amp; f \\
\color{red}{\square} &amp; \color{red}{\square} &amp; \color{red}{\square} 
\end{array}\right| &amp;
\left|\begin{array}{rrr}
a &amp; b &amp; \color{red}{\square} \\
d &amp; e &amp; \color{red}{\square} \\
\color{red}{\square} &amp; \color{red}{\square} &amp; \color{red}{\square} 
\end{array}\right|
\end{pmatrix}_{3 \times 3}
\]</span></p>
<p><strong>Cofactor Form</strong> </p>
<p>To convert the <strong>M</strong> matrix into the cofactor form - the <strong>C</strong> matrix - we multiply the <strong>minor</strong> elements of the <strong>M</strong> matrix by <span class="math inline">\((-1)^{i + j}\)</span>.</p>
<p>For example, the cofactor <span class="math inline">\(C_{1,1}\)</span> is derived based on <span class="math inline">\((-1)^{1+1}M_{1,1}\)</span>.</p>
<p>The general cofactor form of a matrix is shown below:</p>
<p><span class="math display">\[
C  = \left[\begin{array}{ccccc} 
(-1)^{1+1}M_{1,1} &amp; (-1)^{1+2}M_{1,2} &amp; \dots &amp; (-1)^{1+n}M_{1,n} \\ 
(-1)^{2+1}M_{2,1} &amp; (-1)^{2+2}M_{2,2} &amp; \dots &amp; (-1)^{2+n}M_{2,n} \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
(-1)^{n+1}M_{n,1} &amp; (-1)^{n+2}M_{n,2} &amp; \dots &amp; (-1)^{n+n}M_{n,n} 
\end{array}\right]_{n \times n}
\]</span></p>
<p>That yields the cofactor form of a 3x3 <strong>M</strong> matrix:</p>
<p><span class="math display">\[
C = \left[\begin{array}{ccccc} 
+M_{1,1} &amp; -M_{1,2} &amp; &amp; +M_{1,3} \\ 
-M_{2,1} &amp; +M_{2,2} &amp; &amp; -M_{2,3} \\ 
+M_{3,1} &amp; -M_{3,2} &amp; &amp; +M_{3,3} \\ 
\end{array}\right]_{3 \times 3}
=
\left[\begin{array}{ccccc} 
C_{1,1} &amp; C_{1,2} &amp; &amp; C_{1,3} \\ 
C_{2,1} &amp; C_{2,2} &amp; &amp; C_{2,3} \\ 
C_{3,1} &amp; C_{3,2} &amp; &amp; C_{3,3} \\ 
\end{array}\right]_{3 \times 3}
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
C_{1,1} = + M_{1,1}\ \ \ \ and \ \ \ \ \ C_{1,2} = - M_{1,2}  
\]</span></p>
<p>Therefore, the <strong>cofactor form</strong> of matrix <strong>A</strong> is the following:</p>
<p><span class="math display">\[
A = \left[\begin{array}{ccc} a &amp; b &amp; c\\ d &amp; e &amp; f \\ g &amp; h &amp; i \end{array}\right]_{3 \times 3}
\rightarrow
C = 
\left[\begin{array}{ccccc} 
+(e \times i - f \times h) &amp; - (d \times i - f \times g) &amp; +(d \times h - e \times g) \\
-(b \times i - c \times h) &amp; + (a \times i - c \times g)  &amp; - (a \times h - b \times g) \\
+(b \times f - c \times e) &amp; - (a \times f - c \times d) &amp; +(a \times e - b \times d) 
\end{array}\right]_{3 \times 3}
\]</span></p>
<p><strong>Adjugate (Adjoint) Form</strong> </p>
<p>To convert the <strong>C</strong> matrix to the adjugate form of <strong>A</strong> matrix - Adj(A) - we just tranpose the <strong>C</strong> matrix.</p>
<p><span class="math display">\[\begin{align}
adj(A) = C^T \label{eqn:adjugateform}
\end{align}\]</span></p>
<p>By <strong>transposing the cofactor</strong> form of a 3x3 matrix, we get the following <strong>adjugate</strong> form:</p>
<p><span class="math display">\[
adj(A) = C^T = 
\left[\begin{array}{ccccc} 
C_{1,1} &amp; C_{1,2} &amp; &amp; C_{1,3} \\ 
C_{2,1} &amp; C_{2,2} &amp; &amp; C_{2,3} \\ 
C_{3,1} &amp; C_{3,2} &amp; &amp; C_{3,3} \\ 
\end{array}\right]_{3 \times 3}^T
=
\left[\begin{array}{ccccc} 
C_{1,1} &amp; C_{2,1} &amp; &amp; C_{3,1} \\ 
C_{1,2} &amp; C_{2,2} &amp; &amp; C_{3,2} \\ 
C_{1,3} &amp; C_{2,3} &amp; &amp; C_{3,3} \\ 
\end{array}\right]_{3 \times 3}
\]</span></p>
</div>
<div id="inverse-form-and-row-echelon-form" class="section level2">
<h2><span class="header-section-number">2.9</span> Inverse Form and Row Echelon Form</h2>
<p><strong>Converting a matrix to its inverse form</strong> </p>
<p>The inverse of a matrix is expressed as such:</p>
<p><span class="math display">\[
A \rightarrow A^{-1}
\]</span></p>
<p>To compute for the inverse of a matrix, we <strong>multiply</strong> the <strong>reciprocal of the determinant</strong> of matrix <strong>A</strong> by its <strong>adjugate</strong>, yielding the following formula:</p>
<p><span class="math display">\[\begin{align}
A^{-1} = \frac{1}{det(A)} \times adj(A) \label{eqn:invmatrix}
\end{align}\]</span></p>
<p>Previously, we have shown how to calculate for the determinant and adjugate of a matrix. Let us use a simple example:</p>
<p>Given the following 3x3 matrix:</p>
<p><span class="math display">\[
A = \left[\begin{array}{ccc} 2 &amp; 1 &amp; 3\\ 1 &amp; 2 &amp; 0 \\ 1 &amp; 2 &amp; 3 \end{array}\right]_{3 \times 3}
\]</span>
First, derive for the adjugate of the matrix:</p>
<p><span class="math display">\[
adj(A) = \left[\begin{array}{rrr} 6 &amp; 3 &amp; -6\\ -3 &amp; 3 &amp; 3 \\ 0 &amp; -3 &amp; 3 \end{array}\right]_{3 \times 3}
\]</span></p>
<p>Then, using <strong>Laplace</strong> formula, we compute for the determinant:</p>
<p><span class="math display">\[
det(A) = |A| = + 2(2 \times 3 - 0 \times 2) - 1(1 \times 3 - 0 \times 1) + 3(1 \times 2 - 2 \times 1) = 9 
\]</span></p>
<p>And then, lastly, for the <strong>inverse</strong> matrix:</p>
<p><span class="math display">\[
A^{-1} = \frac{1}{det(A)} \times adj(A) = \frac{1}{9} \left[\begin{array}{rrr} 6 &amp; 3 &amp; -6\\ -3 &amp; 3 &amp; 3 \\ 0 &amp; -3 &amp; 3 \end{array}\right] = 
\left[\begin{array}{rrr} 2/3 &amp; 1/3 &amp; -2/3\\ -1/3 &amp; 1/3 &amp; 1/3 \\ 0 &amp; -1/3 &amp; 1/3 \end{array}\right]
\]</span></p>
<p>It is important to highlight that computing for the inverse of a matrix can be very expensive. We cover other more efficient methods of computing for the inverse of a matrix later in the chapter.</p>
<p><strong>Converting a matrix to its echelon form</strong> </p>
<p>A matrix has two row echelon forms:</p>
<ul>
<li>Row Echelon Form (REF) - using <strong>Gaussian Elimination</strong>  </li>
<li><strong>Reduced</strong> Row Echelon Form (RREF) - using <strong>Gauss-Jordan Elimination</strong>  </li>
</ul>
<p>Review Figure  to visualize the difference.</p>
<div class="figure" style="text-align: center"><span id="fig:echelon"></span>
<img src="echelon.png" alt="Row Echelon" width="60%" />
<p class="caption">
Figure 2.14: Row Echelon
</p>
</div>
<p>There are four properties to consider for a matrix to be in row echelon form:</p>
<ul>
<li>The first non-zero element in each row is a 1. This is the leading entry called <strong>pivot</strong>.</li>
<li>The leading entry of each row should be to the right of the leading entry of previous rows.</li>
<li>Rows with zero elements should be below rows with non-zero elements.</li>
<li>A column with non-zero entry but without a leading entry holds a <strong>free variable</strong>.</li>
</ul>
<p>An example of the fourth property is shown in Figure  where the third column has no <strong>pivot</strong>; and thus that column is represented by a <strong>free variable</strong> .</p>
<p>Additionally, there are three other properties to consider for a matrix to be in <strong>reduced</strong> row echelon form:</p>
<ul>
<li>The form follows the three rules of row echelon form.</li>
<li>Additionally, the leading entry (or <strong>pivot</strong>) is the only non-zero entry in its column.</li>
</ul>
<p>E.g. Given the following system of equations:</p>
<p><span class="math display">\[\begin{align*}
1x + 3y + 6z= 0\\
2x + 7y + 3z= 0\\
0x + 1y - 9z = 0
\end{align*}\]</span></p>
<p>We get the <strong>reduced echelon form</strong> with a <strong>free variable</strong>, <strong>z</strong>, in which we can assign any non-zero arbitrary value, e.g. z = a:</p>
<p><span class="math display">\[
\left[\begin{array}{rrr|r} 
\color{red}{1} &amp; 0 &amp; 33 &amp; 0\\ 0 &amp; \color{red}{1}  &amp; -9 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0
\end{array}\right]
\rightarrow
\left[\begin{array}{r|r} x - 33\color{blue}{a} &amp; 0\\ y + 9\color{blue}{a} &amp; 0\\ z  &amp; \color{blue}{a} \end{array}\right] 
\rightarrow
\left[\begin{array}{r} x  \\ y \\ z  \end{array}\right] =
\left[\begin{array}{r} -33\color{blue}{a} \\ 9\color{blue}{a} \\ \color{blue}{a}  \end{array}\right]
\]</span></p>
<p>To get to the echelon forms, there are three primary matrix operations we can use:</p>
<ul>
<li>Multiply a row by a scalar constant - equivalent to scaling a row</li>
<li>Switch or interchange two rows</li>
<li>Add two rows</li>
</ul>
<p>Let us go through those operations to solve for the echelon form of the following matrix:</p>
<p><span class="math display">\[
A=\left[\begin{array}{rrrr} 
8 &amp; 2 &amp; 4 \\
9 &amp; 2 &amp; 2 \\
-3 &amp; 1 &amp; 6 \\
4 &amp; 3 &amp;  2
\end{array}\right]_{4\times3}
\]</span></p>
<p>To begin - and as note - just for convenience, for every operation performed on the matrix, the resulting row will be appended with an asterisk (*):</p>
<p><span class="math display">\[
A=\left[
\begin{array} {c}1/8R_1
\\--------\\
\begin{array}{rrrr} 
\color{red}{8} &amp; \color{red}{2} &amp; \color{red}{4} \\
9 &amp; 2 &amp; 2 \\
-3 &amp; 1 &amp; 6 \\
4 &amp; 3 &amp;  2
\end{array}
\end{array}
\right] 
\rightarrow
\left[
\begin{array} {c}-9R_1 + R_2
\\--------\\
\begin{array}{rrrr} 
*\color{red}{1} &amp; \color{red}{1/4} &amp; \color{red}{1/2} \\
\color{blue}{9} &amp; \color{blue}{2} &amp; \color{blue}{2} \\
-3 &amp; 1 &amp; 6 \\
4 &amp; 3 &amp;  2
\end{array}
\end{array}
\right] 
\rightarrow
\left[
\begin{array} {c}3R_1 + R_3
\\--------\\
\begin{array}{rrrr} 
\color{red}{1} &amp; \color{red}{1/4} &amp; \color{red}{1/2} \\
*0 &amp; -1/4 &amp; -5/2 \\
\color{blue}{-3} &amp; \color{blue}{1} &amp; \color{blue}{6} \\
4 &amp; 3 &amp;  2
\end{array}
\end{array}
\right] 
\]</span></p>
<p>To recap, three operations were performed:</p>
<ul>
<li>1/8R_1 : Multiply Row 1 by 1/8 to arrive at a new Row 1.</li>
<li>-9R_1 + R_2 : Multiply Row 1 by -9 then add result to Row 2.</li>
<li>3R_1 + R_3 : Multiply Row 1 by 3 then add result to Row 3.</li>
</ul>
<p>Now, to continue:</p>
<p><span class="math display">\[
A=\left[
\begin{array} {c}-4R_1 + R_4
\\--------\\
\begin{array}{rrrr} 
\color{red}{1} &amp; \color{red}{1/4} &amp; \color{red}{1/2} \\
0 &amp; -1/4 &amp; -5/2 \\
*0 &amp; 7/4 &amp;  15/2 \\
\color{blue}{4} &amp; \color{blue}{3} &amp; \color{blue}{2} 
\end{array}
\end{array}
\right] 
\rightarrow
\left[
\begin{array} {c}-4R_2
\\--------\\
\begin{array}{rrrr} 
1 &amp; 1/4 &amp; 1/2 \\
\color{red}{0} &amp; \color{red}{-1/4} &amp; \color{red}{-5/2} \\
0 &amp; 7/4 &amp;  15/2 \\
*0 &amp; 2 &amp; 0
\end{array}
\end{array}
\right] 
\rightarrow
\left[
\begin{array} {c}-7/4R_2 + R_3
\\--------\\
\begin{array}{rrrr} 
1 &amp; 1/4 &amp; 1/2 \\
*\color{red}{0} &amp; \color{red}{1} &amp; \color{red}{10} \\
\color{blue}{0} &amp; \color{blue}{7/4} &amp; \color{blue}{15/2} \\
0 &amp; 2 &amp; 0
\end{array}
\end{array}
\right] 
\]</span></p>
<p>Next, we perform another three operations:</p>
<ul>
<li>-4R_1 + R_4 : Multiply Row 1 by -4 then add result to Row 4.</li>
<li>-4R_2 : Multiply Row 2 by -4 to arrive at a new Row 2.</li>
<li>-7/4R_2 + R_3 : Multiply Row 2 by -7/4 then add result to Row 3.</li>
</ul>
<p>To continue:</p>
<p><span class="math display">\[
A=\left[
\begin{array} {c}-2R_2 + R_4
\\--------\\
\begin{array}{rrrr} 
1 &amp; 1/4 &amp; 1/2 \\
\color{red}{0} &amp; \color{red}{1} &amp; \color{red}{10} \\
*0 &amp; 0 &amp; -10\\
\color{blue}{0} &amp; \color{blue}{2} &amp; \color{blue}{0} 
\end{array}
\end{array}
\right] 
\rightarrow
\left[
\begin{array} {c}-1/10R_3
\\--------\\
\begin{array}{rrrr} 
1 &amp; 1/4 &amp; 1/2 \\
0 &amp; 1 &amp; 10\\
\color{red}{0} &amp; \color{red}{0} &amp; \color{red}{-10} \\
*0 &amp; 0 &amp; -20
\end{array}
\end{array}
\right] 
\rightarrow
\left[
\begin{array} {c}20R_3 + R_4
\\--------\\
\begin{array}{rrrr} 
1 &amp; 1/4 &amp; 1/2 \\
0 &amp; 1 &amp; 10\\
*\color{red}{0} &amp; \color{red}{0} &amp; \color{red}{1} \\
\color{blue}{0} &amp; \color{blue}{0} &amp; \color{blue}{-20} 
\end{array}
\end{array}
\right] 
\]</span></p>
<p>Next, we perform another three operations:</p>
<ul>
<li>-2R_2 + R_4 : Multiply Row 2 by -2 then add result to Row 4.</li>
<li>-1/10R_3 : Multiply Row 3 by -1/10 to arrive at a new Row 3.</li>
<li>20R_3 + R_4 : Multiply Row 3 by 20 then add result to Row 4.</li>
</ul>
<p>Finally, we get the <strong>Row Echelon Form (REF)</strong>.</p>
<p><span class="math display">\[
echelon(A) =
\left[
\begin{array} {c} \text{Echelon Form}
\\--------\\
\begin{array}{rrrr} 
\color{red}{1} &amp; 1/4 &amp; 1/2 \\
0 &amp; \color{red}{1} &amp; 10\\
0 &amp; 0 &amp; \color{red}{1}\\
*0 &amp; 0 &amp; 0
\end{array}
\end{array}
\right] 
\]</span></p>
<p>To compute for the <strong>reduced</strong> echelon form, we continue further:</p>
<p><span class="math display">\[
A=\left[
\begin{array} {c}-10R_3 + R_2
\\--------\\
\begin{array}{rrrr} 
1 &amp; 1/4 &amp; 1/2 \\
\color{blue}{0} &amp; \color{blue}{1} &amp; \color{blue}{10} \\
\color{red}{0} &amp; \color{red}{0} &amp; \color{red}{1} \\
*0 &amp; 0 &amp; 0
\end{array}
\end{array}
\right] 
\rightarrow
\left[
\begin{array} {c} -1/2R_3 + R_1
\\--------\\
\begin{array}{rrrr} 
\color{blue}{1} &amp; \color{blue}{1/4} &amp; \color{blue}{1/2} \\
*0 &amp; 1 &amp; 0\\
\color{red}{0} &amp; \color{red}{0} &amp; \color{red}{1} \\
0 &amp; 0 &amp; 0
\end{array}
\end{array}
\right] 
\rightarrow
\left[
\begin{array} {c} -1/4R_2 + R_1
\\--------\\
\begin{array}{rrrr} 
*\color{blue}{1} &amp; \color{blue}{1/4} &amp; \color{blue}{0} \\
\color{red}{0} &amp; \color{red}{1} &amp; \color{red}{0} \\
0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0
\end{array}
\end{array}
\right] 
\]</span></p>
<p>We performed three operations:</p>
<ul>
<li>-10R_3 + R_2 : Multiply Row 3 by -10 then add result to Row 2.</li>
<li>-1/2R_3 + R_1 : Multiply Row 3 by -1/2 then add result to Row 1.</li>
<li>-1/4R_2 + R_1 : Multiply Row 2 by -1/4 to add result to Row 1.</li>
</ul>
<p>Finally, we get the <strong>Reduced Row Echelon Form (RREF)</strong> form:</p>
<p><span class="math display">\[
reduced\ echelon(A) =
\left[
\begin{array} {c} \text{Reduced Echelon}
\\--------\\
\begin{array}{rrrr} 
*\color{red}{1} &amp; 0 &amp; 0\\
0 &amp; \color{red}{1} &amp; 0\\
0 &amp; 0 &amp; \color{red}{1}\\
0 &amp; 0 &amp; 0
\end{array}
\end{array}
\right] 
\]</span></p>
<p>Apparently, just seeing a diagonal matrix with 1s does not give any meaning; however, to transform a matrix equation using <strong>Gauss-Jordan Elimination</strong> along with the vector y is a way to solve the system of equations for x:</p>
<p><span class="math display">\[
Ax = y
\]</span></p>
<p>Let us implement <strong>Row Echelon</strong> and <strong>Reduced Row Echelon</strong> in R code to solve a system of linear equation (this naive implementation uses simple partial pivoting - more of this topic later in the chapter) :</p>

<p><span class="math display">\[
\left(\begin{array}{lll}  
3x_1 + 3x_2 + 3x_3 = 6 \\
2x_1 + 4x_2 + 5x_3 = 5 \\
1x_1 + 5x_2 + 5x_3 = 6 
 \end{array}\right) \rightarrow
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array}
\right]
\left[ \begin{array}{rrr} x_1 \\ x_2 \\ x_3 \end{array} \right] =
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right] 
\]</span>
</p>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1">ref &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">    n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">    m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb1-4" data-line-number="4">    j =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-5" data-line-number="5">    i =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-6" data-line-number="6">    tol =<span class="st"> </span><span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb1-7" data-line-number="7">    <span class="cf">while</span> (j <span class="op">&lt;=</span><span class="st"> </span>m <span class="op">&amp;&amp;</span><span class="st"> </span>i <span class="op">&lt;=</span><span class="st"> </span>n) {</a>
<a class="sourceLine" id="cb1-8" data-line-number="8">        <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Row &quot;</span>, j))</a>
<a class="sourceLine" id="cb1-9" data-line-number="9">        max_row =<span class="st"> </span><span class="kw">which</span>( <span class="kw">abs</span>(A[,i]) <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(A[,i])) )[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">        <span class="cf">if</span> (j <span class="op">&lt;</span><span class="st"> </span>max_row ) { </a>
<a class="sourceLine" id="cb1-11" data-line-number="11">            <span class="cf">if</span> (<span class="kw">abs</span>( A[max_row,i] ) <span class="op">&gt;</span><span class="st"> </span>tol) {</a>
<a class="sourceLine" id="cb1-12" data-line-number="12">                <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;swap row &quot;</span>, j, <span class="st">&quot; and row &quot;</span>, max_row))</a>
<a class="sourceLine" id="cb1-13" data-line-number="13">                A[<span class="kw">c</span>(j,max_row),] =<span class="st"> </span>A[<span class="kw">c</span>(max_row,j),]</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">            }</a>
<a class="sourceLine" id="cb1-15" data-line-number="15">        } </a>
<a class="sourceLine" id="cb1-16" data-line-number="16">        <span class="cf">if</span> (<span class="kw">abs</span>( A[j,i] ) <span class="op">&lt;</span><span class="st"> </span>tol) { i =<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>; <span class="cf">next</span> }</a>
<a class="sourceLine" id="cb1-17" data-line-number="17">        <span class="cf">if</span> (A[j,i] <span class="op">!=</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1-18" data-line-number="18">            <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;scale: multiply row &quot;</span>, j, <span class="st">&quot; by &quot;</span>, <span class="dv">1</span><span class="op">/</span>A[j,i] ))</a>
<a class="sourceLine" id="cb1-19" data-line-number="19">            scale_to_<span class="dv">1</span> =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>A[j,i]</a>
<a class="sourceLine" id="cb1-20" data-line-number="20">            A[j,i<span class="op">:</span>n] =<span class="st"> </span>A[j,i<span class="op">:</span>n] <span class="op">*</span><span class="st"> </span>scale_to_<span class="dv">1</span></a>
<a class="sourceLine" id="cb1-21" data-line-number="21">        }</a>
<a class="sourceLine" id="cb1-22" data-line-number="22">        k =<span class="st"> </span>j</a>
<a class="sourceLine" id="cb1-23" data-line-number="23">        <span class="cf">while</span> (k <span class="op">&lt;</span><span class="st"> </span>m) {</a>
<a class="sourceLine" id="cb1-24" data-line-number="24">            r =<span class="st"> </span>A[k <span class="op">+</span><span class="st"> </span><span class="dv">1</span>, i]</a>
<a class="sourceLine" id="cb1-25" data-line-number="25">            <span class="cf">if</span> (<span class="kw">abs</span>(r) <span class="op">&gt;</span><span class="st"> </span>tol ) {</a>
<a class="sourceLine" id="cb1-26" data-line-number="26">                <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;multiply row &quot;</span>, j, <span class="st">&quot; by &quot;</span>, r, </a>
<a class="sourceLine" id="cb1-27" data-line-number="27">                            <span class="st">&quot; then subtract from row &quot;</span>, k <span class="op">+</span><span class="st"> </span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1-28" data-line-number="28">                A[k<span class="op">+</span><span class="dv">1</span>,i<span class="op">:</span>n] =<span class="st"> </span>A[k<span class="op">+</span><span class="dv">1</span>,i<span class="op">:</span>n]  <span class="op">-</span><span class="st"> </span>r <span class="op">*</span><span class="st"> </span>A[j,i<span class="op">:</span>n] </a>
<a class="sourceLine" id="cb1-29" data-line-number="29">            }</a>
<a class="sourceLine" id="cb1-30" data-line-number="30">            k =<span class="st"> </span>k <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-31" data-line-number="31">        }</a>
<a class="sourceLine" id="cb1-32" data-line-number="32">        j =<span class="st"> </span>j <span class="op">+</span><span class="st"> </span><span class="dv">1</span>; i =<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-33" data-line-number="33">    }</a>
<a class="sourceLine" id="cb1-34" data-line-number="34">    A</a>
<a class="sourceLine" id="cb1-35" data-line-number="35">}</a>
<a class="sourceLine" id="cb1-36" data-line-number="36">rref &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb1-37" data-line-number="37">    n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb1-38" data-line-number="38">    m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb1-39" data-line-number="39">    j =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-40" data-line-number="40">    i =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-41" data-line-number="41">    tol =<span class="st"> </span><span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb1-42" data-line-number="42">    <span class="cf">while</span> (j <span class="op">&lt;=</span><span class="st"> </span>m <span class="op">&amp;&amp;</span><span class="st"> </span>i <span class="op">&lt;=</span><span class="st"> </span>n) {</a>
<a class="sourceLine" id="cb1-43" data-line-number="43">        <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Row &quot;</span>, j))</a>
<a class="sourceLine" id="cb1-44" data-line-number="44">        max_row =<span class="st"> </span><span class="kw">which</span>( <span class="kw">abs</span>(A[,i]) <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(A[,i])) )[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1-45" data-line-number="45">        <span class="cf">if</span> (j <span class="op">&lt;</span><span class="st"> </span>max_row ) { </a>
<a class="sourceLine" id="cb1-46" data-line-number="46">            <span class="cf">if</span> (<span class="kw">abs</span>( A[max_row,i] ) <span class="op">&gt;</span><span class="st"> </span>tol) {</a>
<a class="sourceLine" id="cb1-47" data-line-number="47">                <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;swap row &quot;</span>, j, <span class="st">&quot; and row &quot;</span>, max_row))</a>
<a class="sourceLine" id="cb1-48" data-line-number="48">                A[<span class="kw">c</span>(j,max_row),] =<span class="st"> </span>A[<span class="kw">c</span>(max_row,j),]</a>
<a class="sourceLine" id="cb1-49" data-line-number="49">            }</a>
<a class="sourceLine" id="cb1-50" data-line-number="50">        } </a>
<a class="sourceLine" id="cb1-51" data-line-number="51">        <span class="cf">if</span> (<span class="kw">abs</span>( A[j,i] ) <span class="op">&lt;</span><span class="st"> </span>tol) { i =<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>; <span class="cf">next</span> }</a>
<a class="sourceLine" id="cb1-52" data-line-number="52">        <span class="cf">if</span> (A[j,i] <span class="op">!=</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1-53" data-line-number="53">            <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;scale: multiply row &quot;</span>, j, <span class="st">&quot; by &quot;</span>, <span class="dv">1</span><span class="op">/</span>A[j,i] ))</a>
<a class="sourceLine" id="cb1-54" data-line-number="54">            scale_to_<span class="dv">1</span> =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>A[j,i]</a>
<a class="sourceLine" id="cb1-55" data-line-number="55">            A[j,i<span class="op">:</span>n] =<span class="st"> </span>A[j,i<span class="op">:</span>n] <span class="op">*</span><span class="st"> </span>scale_to_<span class="dv">1</span></a>
<a class="sourceLine" id="cb1-56" data-line-number="56">        }</a>
<a class="sourceLine" id="cb1-57" data-line-number="57">        k =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-58" data-line-number="58">        <span class="cf">while</span> (k <span class="op">&lt;=</span><span class="st"> </span>m) {</a>
<a class="sourceLine" id="cb1-59" data-line-number="59">            r =<span class="st"> </span>A[k, i]</a>
<a class="sourceLine" id="cb1-60" data-line-number="60">            <span class="cf">if</span> ( k<span class="op">!=</span><span class="st"> </span>j <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">abs</span>(r) <span class="op">&gt;</span><span class="st"> </span>tol) {</a>
<a class="sourceLine" id="cb1-61" data-line-number="61">                <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;multiply row &quot;</span>, j, <span class="st">&quot; by &quot;</span>, r, </a>
<a class="sourceLine" id="cb1-62" data-line-number="62">                            <span class="st">&quot; then subtract from row &quot;</span>, k))</a>
<a class="sourceLine" id="cb1-63" data-line-number="63">                A[k,i<span class="op">:</span>n] =<span class="st"> </span>A[k,i<span class="op">:</span>n]  <span class="op">-</span><span class="st"> </span>r <span class="op">*</span><span class="st"> </span>A[j,i<span class="op">:</span>n] </a>
<a class="sourceLine" id="cb1-64" data-line-number="64">            }</a>
<a class="sourceLine" id="cb1-65" data-line-number="65">            k =<span class="st"> </span>k <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-66" data-line-number="66">        }</a>
<a class="sourceLine" id="cb1-67" data-line-number="67">        j =<span class="st"> </span>j <span class="op">+</span><span class="st"> </span><span class="dv">1</span>; i =<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-68" data-line-number="68">    }</a>
<a class="sourceLine" id="cb1-69" data-line-number="69">    A</a>
<a class="sourceLine" id="cb1-70" data-line-number="70">}</a>
<a class="sourceLine" id="cb1-71" data-line-number="71">backward_sub &lt;-<span class="st"> </span><span class="cf">function</span>(A, b) {</a>
<a class="sourceLine" id="cb1-72" data-line-number="72">    pivots =<span class="st"> </span><span class="kw">min</span>(<span class="kw">ncol</span>(A), <span class="kw">nrow</span>(A))</a>
<a class="sourceLine" id="cb1-73" data-line-number="73">    <span class="cf">for</span> (i <span class="cf">in</span> pivots<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1-74" data-line-number="74">        b[i] =<span class="st"> </span>b[i] <span class="op">/</span><span class="st"> </span>A[i,i]</a>
<a class="sourceLine" id="cb1-75" data-line-number="75">        <span class="cf">for</span> (j <span class="cf">in</span> (i<span class="dv">-1</span>)<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1-76" data-line-number="76">            <span class="cf">if</span> (j <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) <span class="cf">break</span></a>
<a class="sourceLine" id="cb1-77" data-line-number="77">            b[j] =<span class="st"> </span>b[j] <span class="op">-</span><span class="st"> </span>A[j,i] <span class="op">*</span><span class="st">  </span>b[i]</a>
<a class="sourceLine" id="cb1-78" data-line-number="78">        }</a>
<a class="sourceLine" id="cb1-79" data-line-number="79">    }</a>
<a class="sourceLine" id="cb1-80" data-line-number="80">    b</a>
<a class="sourceLine" id="cb1-81" data-line-number="81">}</a>
<a class="sourceLine" id="cb1-82" data-line-number="82">forward_sub &lt;-<span class="st"> </span><span class="cf">function</span>(A, b) {</a>
<a class="sourceLine" id="cb1-83" data-line-number="83">    pivots =<span class="st"> </span><span class="kw">min</span>(<span class="kw">ncol</span>(A), <span class="kw">nrow</span>(A))</a>
<a class="sourceLine" id="cb1-84" data-line-number="84">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>pivots) {</a>
<a class="sourceLine" id="cb1-85" data-line-number="85">        b[i] =<span class="st"> </span>b[i] <span class="op">/</span><span class="st"> </span>A[i,i]</a>
<a class="sourceLine" id="cb1-86" data-line-number="86">        <span class="cf">for</span> (j <span class="cf">in</span> (i<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>pivots) {</a>
<a class="sourceLine" id="cb1-87" data-line-number="87">            <span class="cf">if</span> (j <span class="op">&gt;</span><span class="st"> </span>pivots) <span class="cf">break</span></a>
<a class="sourceLine" id="cb1-88" data-line-number="88">            b[j] =<span class="st"> </span>b[j] <span class="op">-</span><span class="st"> </span>A[j,i] <span class="op">*</span><span class="st">  </span>b[i]</a>
<a class="sourceLine" id="cb1-89" data-line-number="89">        }</a>
<a class="sourceLine" id="cb1-90" data-line-number="90">    }</a>
<a class="sourceLine" id="cb1-91" data-line-number="91">    b</a>
<a class="sourceLine" id="cb1-92" data-line-number="92">}</a></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">(<span class="dt">A =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">6</span>,   <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">5</span>,   <span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">6</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    3    3    3    6
## [2,]    2    4    5    5
## [3,]    1    5    5    6</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">(<span class="dt">R =</span> <span class="kw">rref</span>(A))</a></code></pre></div>
<pre><code>## [1] &quot;Row  1&quot;
## [1] &quot;scale: multiply row  1  by  0.333333333333333&quot;
## [1] &quot;multiply row  1  by  2  then subtract from row  2&quot;
## [1] &quot;multiply row  1  by  1  then subtract from row  3&quot;
## [1] &quot;Row  2&quot;
## [1] &quot;swap row  2  and row  3&quot;
## [1] &quot;scale: multiply row  2  by  0.25&quot;
## [1] &quot;multiply row  2  by  1  then subtract from row  1&quot;
## [1] &quot;multiply row  2  by  2  then subtract from row  3&quot;
## [1] &quot;Row  3&quot;
## [1] &quot;multiply row  3  by  1  then subtract from row  2&quot;</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    1    0    0    1
## [2,]    0    1    0    2
## [3,]    0    0    1   -1</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="co"># If solving linear systems for &#39;x&#39; and ref() is used, </span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="co"># then use backward substitution, assuming matrix</span></a>
<a class="sourceLine" id="cb7-3" data-line-number="3"><span class="co"># is augmented (with a &#39;b&#39; column)</span></a>
<a class="sourceLine" id="cb7-4" data-line-number="4"><span class="co"># b = R[, ncol(R)]</span></a>
<a class="sourceLine" id="cb7-5" data-line-number="5"><span class="co"># (x = backward_sub(R, b))</span></a></code></pre></div>

<p>Here, we have solved for x:</p>
<p><span class="math display">\[
x_1 = 1,\ \ \ \ \ x_2 = 2,\ \ \ \ \ x_3 = -1
\]</span></p>
</div>
<div id="linear-transformations" class="section level2">
<h2><span class="header-section-number">2.10</span> Linear Transformations</h2>
<p>Let us use 2D coordinate system to explain linear transformation geometrically. We will use a vector to sample performing transformation. Below is a notation expressing the case where a 2x1 input vector (<span class="math inline">\(\mathbb{R}^2\)</span>) goes through transformation, yielding a 2x1 output vector (<span class="math inline">\(\mathbb{R}^2\)</span>).</p>
<p><span class="math display">\[
T: \mathbb{R}^2 \rightarrow \mathbb{R}^2
\]</span></p>
<p>Given a unit vector:</p>
<p><span class="math display">\[
\mathbf{\vec{u}} = \left[\begin{array}{ccc} 1 \\ 1  \end{array}\right]_{2 \times 1}
\]</span></p>
<p>let us start by using a 2x2 identity matrix as a transformation matrix to transform the unit vector:</p>
<p><span class="math display">\[
I = \left[\begin{array}{ccc} 1 &amp; 0  \\ 0  &amp; 1  \end{array}\right]_{2 \times 2}
\]</span></p>
<p>We use matrix multiplication for the transformation, yielding a <strong>dot product</strong>.</p>
<p><span class="math display">\[
I \cdotp u = \left[\begin{array}{ccc} 1 &amp; 0  \\ 0  &amp; 1  \end{array}\right]_{\{transformer\}} \cdotp \left[\begin{array}{ccc} 1 \\ 1 \end{array}\right]_{\{input\}} = 
\left[\begin{array}{ccc} (1 \times 1) + ( 0 \times 1)  \\ (0 \times 1) + ( 1 \times 1)  \end{array}\right] =\left[\begin{array}{ccc} 1 \\ 1 \end{array}\right]_{\{output\}}
\]</span>
Here, the result of the transformation is a vector of same dimension, <span class="math inline">\(\mathbb{R}^2\)</span>.</p>
<div id="scaling" class="section level3">
<h3><span class="header-section-number">2.10.1</span> Scaling </h3>
<p>Let us try another example but this time, we scale the vector by 9 using a different 2x2 matrix as our <strong>transformer</strong>:</p>
<p><span class="math display">\[
I \cdotp u = \left[\begin{array}{ccc} 9 &amp; 0  \\ 0  &amp; 9  \end{array}\right]_{\{transformer\}} \cdotp \left[\begin{array}{ccc} 1 \\ 1 \end{array}\right]_{\{input\}} = 
\left[\begin{array}{ccc} (9\times 1) + ( 0 \times 1)  \\ (0 \times 1) + ( 9 \times 1)  \end{array}\right] 
= \left[\begin{array}{ccc} 9 \\ 9 \end{array}\right]_{\{output\}}
\]</span></p>
<p>Here, the vector output is scaled to 9. In fact, we can also use a scalar value to scale it:</p>
<p><span class="math display">\[
\left[\begin{array}{ccc} 9 &amp; 0  \\ 0  &amp; 9  \end{array}\right] \cdotp \left[\begin{array}{ccc} 1 \\ 1 \end{array}\right]_{\{input\}} = 9_{\{scalar\}} \times
\left[\begin{array}{ccc} 1  \\ 1 \end{array}\right] 
= \left[\begin{array}{ccc} 9 \\ 9 \end{array}\right]_{\{output\}}
\]</span></p>
<p>Notice that the input vector, <span class="math inline">\(\left[\begin{array}{ccc} 1 \\ 1 \end{array}\right]\)</span> can be transformed using a matrix or a scalar value.</p>
<p><span class="math display">\[
\left[\begin{array}{ccc} 9 &amp; 0  \\ 0  &amp; 9  \end{array}\right] \cdotp \left[\begin{array}{ccc} 1 \\ 1 \end{array}\right]_{\{input\}} = 9_{\{scalar\}} \times
\left[\begin{array}{ccc} 1  \\ 1 \end{array}\right]_{\{input\}}
\]</span></p>
<p>The equation above can be expressed as an equation in its more general form:</p>
<p><span class="math display">\[
A \cdotp v = \lambda \times v
\]</span></p>
<p>Later in this chapter, we cover <strong>eigenvectors</strong> and <strong>eigenvalues</strong>.</p>
</div>
<div id="transvection-shearing" class="section level3">
<h3><span class="header-section-number">2.10.2</span> Transvection (Shearing)  </h3>
<p>Transvection (or shearing) is a transformation that displaces vectors by scaling only a portion of the entries in the vector. For example, an <span class="math inline">\(\mathbb{R}^2\)</span> vector has the following coordinate entries, <span class="math inline">\(\left[\begin{array}{rrr} x \\ y \end{array}\right]\)</span>. By scaling only the y-coordinate entry, e.g. <span class="math inline">\(\left[\begin{array}{rrr} x \\ cy \end{array}\right]\)</span>, this is effectively displacing the vector’s y-coordinate.</p>
<p>As an example, transform matrix <strong>A</strong> using a transvection transformation matrix by multiplying only x-coordinate entry of vector <strong>a</strong> by 1/3, effectively displacing the vector horizontally:</p>
<p><span class="math display">\[
T_{\{shear\}} \cdotp A = \left[\begin{array}{rrr} 1 &amp; 1/3 \\ 0  &amp; 1  \end{array}\right]_{\{transform\}} \cdotp \left[\begin{array}{ccc} 4 &amp; 0 \\ 0 &amp; 3 \end{array}\right]_{\{input\}} 
= \left[\begin{array}{rrr} 4 &amp; 1 \\ 0 &amp; 3 \end{array}\right]_{\{output\}}
\]</span></p>
<p>Here, the top side of the rectangular figure formed by vectors <strong>a</strong> and <strong>b</strong> is shifted horizontally by 1 grid to the right. The vector <strong>a</strong> is the one being displaced from <span class="math inline">\(\left[\begin{array}{rrr} 0 \\ 3 \end{array}\right]\)</span> to <span class="math inline">\(\left[\begin{array}{rrr} 1 \\ 3 \end{array}\right]\)</span>. Note that this displacement is not equivalent to rotating the vector.</p>
<div class="figure" style="text-align: center"><span id="fig:shear"></span>
<img src="shear.png" alt="Transvection" width="60%" />
<p class="caption">
Figure 2.15: Transvection
</p>
</div>
</div>
<div id="rotation" class="section level3">
<h3><span class="header-section-number">2.10.3</span> Rotation </h3>
<p>Figure  illustrates two vectors being rotated counter-clockwise at <span class="math inline">\(90^{\circ}\)</span>, equivalent to <span class="math inline">\(\pi / 2\)</span> radians. The two vectors are outlined in matrix-form:</p>
<p><span class="math display">\[
A = \left[\begin{array}{rr} 
1 &amp; 4\\
3 &amp; 1
\end{array}\right]
\]</span></p>
<p>We then perform rotation transformation:</p>
<p><span class="math display">\[
T_{\{rotation\}}: \mathbb{R}^2 \rightarrow \mathbb{R}^2
\]</span>
Here, we use a rotation transformation matrix:</p>
<p><span class="math display">\[
R_{\theta} = \left[\begin{array}{rr} 
cos(\theta) &amp; -sin(\theta) \\
sin(\theta)  &amp; cos(\theta) 
\end{array}\right]
=
\left[\begin{array}{rr} 
cos(\pi/2) &amp; -sin(\pi/2) \\
sin(\pi/2)  &amp; cos(\pi/2) 
\end{array}\right]
=
\left[\begin{array}{rr} 
0 &amp; -1 \\
1 &amp; 0
\end{array}\right]_{\{transformer\}}
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:rotate"></span>
<img src="rotate.png" alt="Rotation" width="60%" />
<p class="caption">
Figure 2.16: Rotation
</p>
</div>
<p>Now, let’s use the rotation matrix as a transformer:</p>
<p><span class="math display">\[
R_{\theta} \cdotp A 
= \left[\begin{array}{rr} 
0 &amp; -1 \\
1 &amp; 0
\end{array}\right]_{\{transformer\}} \cdotp
\left[\begin{array}{rr} 
1 &amp; 4\\
3 &amp; 1
\end{array}\right]_{\{input\}}
=
\left[\begin{array}{rr} 
-3 &amp; -1\\
1 &amp; 4
\end{array}\right]_{\{output\}}
\]</span></p>
</div>
<div id="reflection" class="section level3">
<h3><span class="header-section-number">2.10.4</span> Reflection </h3>
<p>Figure  illustrates how vector <strong>a</strong> gets <strong>reflected over</strong> the line (vector <strong>c</strong>), forming vector <strong>b</strong> at the opposite side of the line.</p>
<p>Using a reflection transformation matrix, we get the output vector as shown in . In the example, the entries of the vector are switched.</p>
<p><span class="math display">\[
T_{\{reflect\}} \cdotp A = \left[\begin{array}{rrr} 0 &amp; 1 \\ 1  &amp; 0  \end{array}\right]_{\{transformer\}} \cdotp \left[\begin{array}{ccc} 1 \\ 3 \end{array}\right]_{\{input\}} 
= \left[\begin{array}{rrr} 3 \\ 1 \end{array}\right]_{\{output\}}
\]</span></p>
<p>In fact, the following transformation matrices can be used to transform the input (original) vector either for reflection purpose or for rotation purpose:</p>
<p><span class="math display">\[\begin{align*}
\left[\begin{array}{rrr} 0 &amp; -1 \\ -1  &amp; 0  \end{array}\right]_{(3)} \rightarrow
\left[\begin{array}{rrr} -3 \\ -1  \end{array}\right]
,\ \ \ \ \ \ 
\left[\begin{array}{rrr} 1 &amp; 0 \\ 0  &amp; -1  \end{array}\right]_{(5)} \rightarrow
\left[\begin{array}{rrr} 1 \\ -3  \end{array}\right]
\\
\left[\begin{array}{rrr} -1 &amp; 0 \\ 0  &amp; -1  \end{array}\right]_{(4)} \rightarrow
\left[\begin{array}{rrr} -1 \\ -3  \end{array}\right]
,\ \ \ \ \ \ 
\left[\begin{array}{rrr} -1 &amp; 0 \\ 0  &amp; 1  \end{array}\right]_{(1)} \rightarrow
\left[\begin{array}{rrr} -1 \\ 3  \end{array}\right]
\end{align*}\]</span></p>
<p>Figure  shows more results of reflection/rotation:</p>
<div class="figure" style="text-align: center"><span id="fig:morereflect"></span>
<img src="more_reflect.png" alt="Reflect and Rotation" width="60%" />
<p class="caption">
Figure 2.17: Reflect and Rotation
</p>
</div>
</div>
<div id="projection" class="section level3">
<h3><span class="header-section-number">2.10.5</span> Projection </h3>
<p>Figure  also illustrates how vector <strong>a</strong> gets <strong>projected into</strong> the subspace <strong>V</strong>, forming vector <strong>b</strong> along the line.</p>
<div class="figure" style="text-align: center"><span id="fig:reflect"></span>
<img src="reflect_project.png" alt="Reflect and Project" width="60%" />
<p class="caption">
Figure 2.18: Reflect and Project
</p>
</div>
<p>The projection can be expressed using the following formulas, e.g. projecting vector <strong>a</strong> to subspace <strong>V</strong>, yielding vector <strong>b</strong>:</p>
<p>Computing for the scale factor - C:</p>
<p><span class="math display">\[
C = \frac{V \cdotp a}{\|V\| \times \|V\|} = \frac{V \cdotp a}{\|V\|^2}
\]</span>
This C scales the projection, e.g. <span class="math inline">\(C \times V\)</span>. Then we have:</p>
<p><span class="math display">\[
b = proj_{(V)}a = C \times V = \frac{V \cdotp a}{\|V\|^2} \times V 
\]</span>
The computation is as follows:</p>
<p><span class="math display">\[
b = \text{projection of a to V} = 
\frac
{
\left[\begin{array}{rrr} 4 \\ 4 \end{array}\right] \cdotp
\left[\begin{array}{rrr} 1 \\ 3 \end{array}\right]
}
{\left\|\begin{array}{rrr} 4 \\ 4 \end{array}\right\|^2} \times
\left[\begin{array}{rrr} 4 \\ 4 \end{array}\right] 
= \frac{16}{32} \times \left[\begin{array}{rrr} 4 \\ 4 \end{array}\right]
= 0.5 \times \left[\begin{array}{rrr} 4 \\ 4 \end{array}\right]
= \left[\begin{array}{rrr} 2 \\ 2 \end{array}\right]
\]</span></p>
</div>
<div id="translation" class="section level3">
<h3><span class="header-section-number">2.10.6</span> Translation </h3>
<p>Figure  shows 4 vectors, forming a square, and is represented by the below matrix:</p>
<p><span class="math display">\[\begin{align*}
\ \ \ {} &amp;\left(\begin{array}{rrrr} 
a &amp; b &amp; c &amp; d
\end{array}\right) \\
\left[\begin{array}{c} 
x \\ y
\end{array}\right]
\ \ \ &amp;\left[\begin{array}{cccc} 
1 &amp; 4 &amp; 1 &amp; 4 \\
4 &amp; 4 &amp; 1 &amp; 1
\end{array}\right]
\rightarrow
1 + \left[\begin{array}{cccc} 
1 &amp; 4 &amp; 1 &amp; 4 \\
4 &amp; 4 &amp; 1 &amp; 1
\end{array}\right]
\rightarrow
\left[\begin{array}{cccc} 
2 &amp; 5 &amp; 2 &amp; 5 \\
5 &amp; 5 &amp; 2 &amp; 2
\end{array}\right]
\end{align*}\]</span></p>
<p>Translation is a transformation that pushes or moves a figure to a new position in the coordinate system by displacing the vectors forming the figure. In Figure , we add a scalar value (1) to the matrix (of column vectors), each of the 4 vectors extending by 1, in effect, geometrically pushing each side of the the square by 1 grid up and 1 grid to the right.</p>
<div class="figure" style="text-align: center"><span id="fig:translate"></span>
<img src="translate.png" alt="Translation" width="60%" />
<p class="caption">
Figure 2.19: Translation
</p>
</div>
</div>
<div id="dilation-and-composition" class="section level3">
<h3><span class="header-section-number">2.10.7</span> Dilation and Composition  </h3>
<p>Figure  shows 4 vectors, forming a square figure which is represented by the below matrix:</p>
<p><span class="math display">\[\begin{align*}
\ \ \ {} &amp;\left(\begin{array}{rrrr} 
a &amp; b &amp; c &amp; d
\end{array}\right) \\
\left[\begin{array}{c} 
x \\ y
\end{array}\right]
\ \ \ &amp;\left[\begin{array}{rrrr} 
2 &amp; 3 &amp; 2 &amp; 3 \\
3 &amp; 3 &amp; 2 &amp; 2
\end{array}\right]
\rightarrow
3 \times \left[\begin{array}{rrrr} 
2 &amp; 3 &amp; 2 &amp; 3 \\
3 &amp; 3 &amp; 2 &amp; 2
\end{array}\right]
\rightarrow
\left[\begin{array}{cccc} 
6 &amp; 9 &amp; 6 &amp; 9 \\
9 &amp; 9 &amp; 6 &amp; 6
\end{array}\right]
\end{align*}\]</span></p>
<p>Here, dilation is a transformation that expands or contracts the size of a figure geometrically. One way is to multiply the matrix by a scalar value. In the Figure , the small square gets larger on all sides by a scalar value of 1 grid.</p>
<div class="figure" style="text-align: center"><span id="fig:dilate"></span>
<img src="dilate.png" alt="Dilation" width="60%" />
<p class="caption">
Figure 2.20: Dilation
</p>
</div>
<p>Notice in the Figure  that the coordinates of the large square does not match the coordinates of the column vectors <strong>a</strong>, <strong>b</strong>, <strong>c</strong>, and <strong>d</strong> of the newly transformed matrix after <strong>dilation</strong>. That is because, we have basically applied another transformation to the matrix by adding -5 like so:</p>
<p><span class="math display">\[\begin{align*}
\ \ \ {} &amp;\left(\begin{array}{rrrr} 
a &amp; b &amp; c &amp; d
\end{array}\right) \\
-5 + \left[\begin{array}{cccc} 
6 &amp; 9 &amp; 6 &amp; 9 \\
9 &amp; 9 &amp; 6 &amp; 6
\end{array}\right]
\rightarrow
&amp;\left[\begin{array}{cccc} 
1 &amp; 4 &amp; 1 &amp; 4 \\
4 &amp; 4 &amp; 1 &amp; 1
\end{array}\right]
\end{align*}\]</span></p>
<p>Performing <strong>multiple transformations</strong> against a vector or matrix is called <strong>composition</strong>. In the example above, we have applied dilation to the square figure formed by the 4 vectors, and then we applied translation to move the large square to a new location where it surrounds the smaller square.</p>
<p>So if we apply a <strong>composition</strong> transformation by dilating the input matrix first and then translating it, the output matrix will look like so (<strong>operation</strong> is applied <strong>from right to left</strong>):</p>
<p><span class="math display">\[\begin{align*}
\ \ \ {} &amp;\left(\begin{array}{rrrr} 
a &amp; b &amp; c &amp; d
\end{array}\right) \\
-5_{(translate)} + 
3_{(dilate)} \times \left[\begin{array}{rrrr} 
2 &amp; 3 &amp; 2 &amp; 3 \\
3 &amp; 3 &amp; 2 &amp; 2
\end{array}\right]
\rightarrow
-5 + \left[\begin{array}{cccc} 
6 &amp; 9 &amp; 6 &amp; 9 \\
9 &amp; 9 &amp; 6 &amp; 6
\end{array}\right]
\rightarrow
&amp;\left[\begin{array}{cccc} 
1 &amp; 4 &amp; 1 &amp; 4 \\
4 &amp; 4 &amp; 1 &amp; 1
\end{array}\right]
\end{align*}\]</span></p>
</div>
</div>
<div id="rank-and-nullity" class="section level2">
<h2><span class="header-section-number">2.11</span> Rank and Nullity  </h2>
<p>First of all, <strong>Rank</strong> complements <strong>Nullity</strong>. That statement can be expressed this way:</p>
<p><span class="math display">\[
ncol(A) = rank + nullity = dim(C(A)) + dim(N(A)),\ \ \ \  \text{where ncol = no of columns}
\]</span></p>
<p>The following difference holds between the two properties:</p>
<p><strong>Rank</strong> is represented as <strong>rank(Matrix) = C(Matrix)</strong></p>
<ul>
<li>it is the number of dimensions in the column space</li>
<li>represents the number of linearly independent vectors</li>
<li>The number of <strong>pivot</strong> column vectors in <strong>RREF</strong></li>
</ul>
<p><strong>Nullity</strong> is represented as <strong>N(Matrix)</strong></p>
<ul>
<li>it is the number of dimensions in the null space</li>
<li>represents the number of linearly dependent vectors</li>
<li>The number of <strong>non-pivot</strong> column vectors in <strong>RREF</strong>, equivalent to the number of <strong>free variables</strong>.</li>
</ul>
<p>The <strong>rank</strong> of a matrix is denoted by the number of <strong>column vectors</strong> (or row vectors) in the matrix that are linearly independent - meaning, that those vectors can form linear combinations. The rank of a matrix is based on either the column space or row space. Either way, suppose the number of row vectors in a 3x3 matrix is 3, all linearly independent vectors. In that case, the <strong>row rank</strong> of the matrix is full-rank. If a matrix has 3 column vectors with 1 linearly dependent vector and 2 linearly independent vectors, then we say that the <strong>column rank</strong> of the matrix is 2. A full-rank matrix is when all the column vectors are linearly independent. Therefore, in a 3x3 matrix, the matrix is full-rank if all three column vectors are linearly independent.</p>
<p>The <strong>nullity</strong> (or <strong>dimension of the null space</strong>) of a matrix on the other hand is denoted by the number of column vectors (or row vectors) in the matrix that are linearly dependent - meaning, that those vectors are formed by linear combinations of other existing column vectors that are linearly independent. A 3x3 full-rank matrix has nullity equal to zero. A rank-2 3x3 matrix has a nullity equal to one.</p>
<p>We learned about <strong>free variables</strong> when we covered the topic about <strong>row echelon</strong>. Specially if a system is <strong>under-determined</strong>, we may find <strong>free variables</strong> indicating that solutions may exist but may <strong>not be unique</strong>.</p>
<p>An <strong>augmented</strong> matrix in its <strong>echelon</strong> form explains this better,</p>

<p><span class="math display">\[
\left[\begin{array}{rrrrr|r} 
\color{red}{1} &amp; 2 &amp; 0 &amp; 0 &amp; 9 &amp; 0\\
0 &amp; 0 &amp; \color{red}{1}  &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; \color{red}{1}  &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
\end{array}\right]_{(RREF)} 
\left[\begin{array}{r} x_1\\ x_2 \\ x_3 \\ x_4 \\ x_5\end{array}\right]_{x} .
\]</span>
</p>
<p>The solutions for each of the <strong>x</strong> variables in the system is then represented as:</p>

<p><span class="math display">\[
\begin{cases}
x_1 + 2x_2 + 9x_5 = 0 \\
x_3 + x_5 = 0 \\
x_4 + 2x_5 = 0
\end{cases}
\text{simplified into}
\begin{cases}
x_1 = -2r -9s \\
x_2 = r\\
x_3 = -s\\
x_4 = -2s \\
x_5 = s 
\end{cases}
\rightarrow
x = 
r\left[\begin{array}{r} -2\\ 1 \\ 0 \\ 0 \\ 0\end{array}\right]_{r} +
s\left[\begin{array}{r} -9\\ 0 \\ -1 \\ -2 \\ 1\end{array}\right]_{s} 
\]</span>
</p>
<p>where <strong>r</strong> and <strong>s</strong> are <strong>free non-zero</strong> variables.</p>
<ul>
<li>There are three <strong>pivots</strong> in the <strong>column space</strong> - <span class="math inline">\(rank: dim(C(A)) = 3\)</span>.</li>
<li>There are two <strong>free variables</strong> - <strong>non-pivot</strong> - in the <strong>null-space</strong> - <span class="math inline">\(nullity: dim(N(A)) = 2\)</span>.</li>
<li>There are five column vectors in the matrix.
<span class="math display">\[
ncol(A) = rank + nullity = dim(C(A)) + dim(N(A)) = 3 + 2 = 5
\]</span></li>
</ul>
<p>And if we are to transpose the matrix and evaluate the <strong>column space</strong> and <strong>null space</strong>, we observe the following:</p>
<ul>
<li>There are three <strong>pivots</strong> in the <strong>row space</strong> - transposed echelon form.</li>
<li>There is 1 <strong>free variable</strong> in the <strong>left null-space</strong> - transposed echelon form.</li>
</ul>
<p>Now, if rank(A) &lt; ncol(A) or dim(N(A)) &gt; 0, it means that the matrix is <strong>rank-deficient</strong> and is, thus, not <strong>full-rank</strong>.</p>
<p>We discuss more of this topic about <strong>rank and nullity</strong> later in this chapter under <strong>Jordan Decomposition</strong>.</p>
</div>
<div id="singularity-and-triviality" class="section level2">
<h2><span class="header-section-number">2.12</span> Singularity and Triviality  </h2>
<p><strong>Singular</strong></p>
<p>A matrix is said to be singular if the determinant is zero. And if determinant is zero (e.g. area is zero), then matrix is non-invertible. There is nothing to invert.</p>
<p><span class="math display">\[
det(A) = |A| = 0
\]</span></p>
<p>A few examples of matrix that results in a determinant equal to zero:</p>
<p><span class="math display">\[
\left[\begin{array}{rrrr}  
1 &amp; 2 &amp; 3  \\
\color{red}{0} &amp; \color{red}{0} &amp; \color{red}{0}  \\
4 &amp; 5 &amp; 6 
 \end{array}\right],
 \left[\begin{array}{rrrr}  
1 &amp; 2 &amp; 3  \\
4 &amp; 5 &amp; 6  \\
\color{red}{0} &amp; \color{red}{0} &amp; \color{red}{0} 
 \end{array}\right],
 \left[\begin{array}{rrrr}  
\color{red}{0} &amp; 0 &amp; 0  \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1  
 \end{array}\right],
 \left[\begin{array}{rrrr}  
1 &amp; 0 &amp; 0  \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; \color{red}{0}  
 \end{array}\right],
  \left[\begin{array}{rrrr}  
1 &amp; 0 &amp; 0  \\
0 &amp; \color{red}{0} &amp; 0 \\
0 &amp; 0 &amp; 1  
 \end{array}\right]
\]</span>
That also shows that the diagonal of matrix that is not all non-zero is singular. Additionally, see example 6 of Figure  which illustrates two vectors, forming a singular matrix.</p>
<p><span class="math display">\[
\left[\begin{array}{rrrr}  
3(i)\\
2(j)
 \end{array}\right]
 \rightarrow
  \left|\begin{array}{rrrr}  
3 &amp; 2 \\
0 &amp; 0
 \end{array}\right|
 = (3 \times 0) + (2 \times 0) = 0
\]</span></p>
<p><strong>Non-singular</strong></p>
<p>A matrix is non-singular if and only if its determinant is non-zero and is invertible.</p>
<p><span class="math display">\[
det(A) = |A| \ne 0
\]</span></p>
<p>That also shows that the diagonal of matrix that is all non-zero is non-singular. Below is an example of a <strong>non-singular matrix</strong> with the least number of non-zero elements, forming an <strong>identity matrix</strong>.</p>
<p><span class="math display">\[
I = \left[\begin{array}{rrrr}  
1 &amp; 0 &amp; 0  \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1  
 \end{array}\right]
\]</span></p>
<p><strong>Triviality</strong></p>
<p>The term <strong>trivial</strong> (and <strong>non-trivial</strong>) applies to solutions of a system of linear of equations. In particular, a <strong>trivial solution</strong> applies to homogeneous equations where all constants (or coordinates) are zero.</p>
<p>Given the following equation:</p>
<p><span class="math display">\[
\left(\begin{array}{rrr}  
2x_1 + 4x_2 + 6x_3 = 0 \\
1x_1 + 2x_2 + 3x_3 = 0 \\
5x_1 + 3x_2 + 7x_3 = 0
 \end{array}\right)
 \rightarrow
\left[\begin{array}{rrrr}  
2 &amp; 4 &amp; 6  \\
1 &amp; 2 &amp; 3  \\
5 &amp; 3 &amp; 7 
 \end{array}\right]
 \left[\begin{array}{rrr}  
x_1 \\
x_2 \\
x_3
 \end{array}\right] =
  \left[\begin{array}{rrr}  
0 \\
0 \\
0
 \end{array}\right]
\]</span></p>
<p>Here a solution for the given equation is <strong>trivial</strong> if <span class="math inline">\(x=0\)</span>, meaning that <span class="math inline">\(x_1 = 0,\ \ \ x_2 = 0,\ \ \ x_3 = 0\)</span>.</p>
<p>In a null space where <span class="math inline">\(b=0\)</span> for <span class="math inline">\(Ax=b\)</span>:</p>
<p><span class="math display">\[
Ax = 0 \leftarrow \begin{cases} trivial &amp; if\ \sum_i x_i = 0 \\ non-trivial &amp; if\ \sum_i x_i \ne 0  \end{cases}
\]</span></p>
<p>On the other hand, a vector, <span class="math inline">\(x\)</span>, has a <strong>non-trivial</strong> solution if at least one element is non-zero.</p>
</div>
<div id="orthogonality-and-orthonormality" class="section level2">
<h2><span class="header-section-number">2.13</span> Orthogonality and Orthonormality  </h2>
<p>In this section, let us define and describe an <strong>orthogonal matrix</strong>. It may help to note that an <strong>orthogonal matrix</strong> is the same as an <strong>orthonormal matrix</strong>. It becomes more apparent by definition:</p>
<p>An <strong>orthogonal matrix</strong> is an <strong>invertible</strong> matrix whose columns are <strong>unit vectors</strong> and geometrically form a <strong>right angle</strong> (mutually orthogonal) with one another. By virtue of the matrix having <strong>unit vector</strong> columns, it makes it an <strong>orthonormal matrix</strong>.</p>
<p>To describe further, a matrix is <strong>orthogonal</strong> if each of its column vectors results in a <strong>zero dot product</strong> with the other column vectors.</p>
<p>Example, given a matrix:</p>
<p><span class="math display">\[
A  = 
\left[\begin{array}{rr}  
1/\sqrt{2} &amp; \sqrt{2}/2 \\
1/\sqrt{2} &amp; -\sqrt{2}/2 \end{array}\right]
\rightarrow
\left(\begin{array}{r} 
\frac{1}{\sqrt{2}} \times \frac{\sqrt{2}}{2}  + 
\frac{1}{\sqrt{2}} \times -\frac{\sqrt{2}}{2}
\end{array}\right) = 0
\]</span>
Note that <span class="math inline">\(1/\sqrt{2} = \sqrt{2}/2\)</span> - the use of both in the matrix is just to show the effect of the transposition.</p>
<p>Recall a unit vector has a magnitude of 1, <span class="math inline">\(\|u\| = 1\)</span>:</p>
<p><span class="math display">\[
\|u\| = \sqrt{\sum_{i=1}^n(x_i)^2} = \sqrt{x_1^2 + x_2^2+ ... + x_n^2} = 1
\]</span>
Example:</p>
<p><span class="math display">\[\begin{align*}
A {}&amp;= \left[\begin{array}{rr}  
V_{1} &amp; V_{2}\\
---- &amp; ----\\
1/\sqrt{2} &amp; \sqrt{2}/2 \\
1/\sqrt{2} &amp; -\sqrt{2}/2 \end{array}\right]
\\
\\
\|V_{1}\|  &amp;= 
\left[\begin{array}{rr}  
1/\sqrt{2} \\ 1/\sqrt{2} \end{array}\right] =
\sqrt{(1/\sqrt{2})^2 + (1/\sqrt{2})^2} = 1
\\
\\
\|V_{2}\|  &amp;= 
\left[\begin{array}{rr}  
\sqrt{2}/2 \\ \sqrt{2}/2 \end{array}\right] =
\sqrt{(\sqrt{2}/2)^2 + (-\sqrt{2}/2)^2} = 1
\end{align*}\]</span></p>
<p>Additionally, a matrix is <strong>orthonormal</strong> if its <strong>dot product</strong> with its <strong>transposed</strong> version results in an <strong>identity matrix</strong> and if its <strong>inverse</strong> form equals its <strong>transpose</strong> form.</p>
<p><span class="math display">\[\begin{align}
A \cdotp A^T = A^T \cdotp A = I,\ \ \ \ A^{-1} = A^T \label{eqn:orthogonal}
\end{align}\]</span></p>
<p>Example:</p>
<p><span class="math display">\[
A \cdotp A^T = 
\left[\begin{array}{c}
\begin{array}{rr}  
1/\sqrt{2} &amp; \sqrt{2}/2 \\
1/\sqrt{2} &amp; -\sqrt{2}/2 \end{array}
\end{array}\right]_A
\cdotp
\left[\begin{array}{c}
\begin{array}{rr} 
1/\sqrt{2} &amp; 1/\sqrt{2}  \\
\sqrt{2}/2 &amp; -\sqrt{2}/2 \end{array}
\end{array}\right]_{A^T}
\rightarrow
I = 
\left[\begin{array}{c}
\begin{array}{rr}  
1 &amp; 0 \\
0 &amp; 1
\end{array}
\end{array}\right]_{I}
\]</span></p>
<p>In the same manner, we have this:</p>
<p><span class="math display">\[
A^T = A^{-1}
\rightarrow
\left[\begin{array}{c}
\begin{array}{rr}  
1/\sqrt{2} &amp; 1/\sqrt{2} \\
\sqrt{2}/2 &amp; -\sqrt{2}/2 
\end{array}
\end{array}\right]_{A^T}
=
\left[\begin{array}{c}
\begin{array}{rr} 
1/\sqrt{2} &amp; 1/\sqrt{2}  \\
1/\sqrt{2} &amp; -1/\sqrt{2} 
\end{array}
\end{array}\right]_{A^{-1}}
\]</span></p>
<p>Lastly, because the <strong>determinant</strong> of an <strong>identity matrix</strong> is equal to one; therefore, the <strong>determinant</strong> of the <strong>dot product</strong> of its transposed version is equal to <span class="math inline">\(\pm 1\)</span>.</p>
<p>Given that <span class="math inline">\(A^TA = AA^T = I\)</span>, then:</p>
<p><span class="math display">\[
det(A^TA) =  det(I) =   det(A) \cdot det(A^T) = det(A)^2 = \pm 1
\]</span></p>
<p>Note that in other literature, an <strong>orthogonal matrix</strong> has a symbol <span class="math inline">\(Q\)</span>.</p>
<p>For example:</p>
<p><span class="math display">\[\begin{align}
Q^TQ = QQ^T = I \label{eqn:orthonormal}
\end{align}\]</span>
}
In the later part of this chapter, we cover more topics around <strong>orthogonal matrix</strong> under the <strong>QR decomposition</strong> section.</p>
</div>
<div id="eigenvectors-and-eigenvalues" class="section level2">
<h2><span class="header-section-number">2.14</span> Eigenvectors and Eigenvalues  </h2>
<p>One of the most important topics in Linear Algebra is about <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong>. The word <strong>Eigen</strong> is a German term translated to the word <strong>own</strong> or <strong>characteristic</strong> (reference: Wiktionary).</p>
<p>Let us start by introducing the following <strong>Eigen equation</strong>:</p>
<p><span class="math display">\[\begin{align}
A \cdotp \mathbf{\vec{v}} = \lambda \times \mathbf{\vec{v}} \label{eqn:eigen}
\end{align}\]</span></p>
<p>It is easier to understand <strong>Eigenvectors</strong> by understanding <strong>what the formula</strong> is all about.</p>
<p><strong>First</strong>, we start by noting that <strong>the formula</strong> is <strong>about transforming</strong> the vector, <strong>v</strong>.</p>
<p><span class="math display">\[
T:\mathbb{R}^n \rightarrow \mathbb{R}^n
\]</span></p>
<p>To transform the vector, we need a matrix transformer, <strong>A</strong>.</p>
<p>In Figure , we use a rotation matrix as a transformer to rotate two vectors. Recall the following:</p>
<p><span class="math display">\[
R_{\theta} \cdotp A 
= \left[\begin{array}{rr} 
0 &amp; -1 \\
1 &amp; 0
\end{array}\right]_{\{transformer\}} \cdotp
\left[\begin{array}{rr} 
1 &amp; 4\\
3 &amp; 1
\end{array}\right]_{\{input\}}
=
\left[\begin{array}{rr} 
-3 &amp; -1\\
1 &amp; 4
\end{array}\right]_{\{output\}}
\]</span></p>
<p>Here, we use <span class="math inline">\(R_{\theta}\)</span> matrix to transform another matrix, A. In fact, we can use the same <span class="math inline">\(R_{\theta}\)</span> matrix to rotate just the individual column vectors in the matrix, like so:</p>
<p>First vector in the matrix:</p>
<p><span class="math display">\[
A \cdotp v1 = 
R_{\theta} \cdotp v1 
= \left[\begin{array}{rr} 
0 &amp; -1 \\
1 &amp; 0
\end{array}\right]_{\{transformer\}} \cdotp
\left[\begin{array}{rr} 
1 \\
3 
\end{array}\right]_{\{input\ vector\}}
=
\left[\begin{array}{rr} 
-3 \\ 1
\end{array}\right]_{\{output\ vector\}}
\]</span>
Second vector in the matrix:</p>
<p><span class="math display">\[
A \cdotp v2 = 
R_{\theta} \cdotp v2 
= \left[\begin{array}{rr} 
0 &amp; -1 \\
1 &amp; 0
\end{array}\right]_{\{transformer\}} \cdotp
\left[\begin{array}{rr} 
4 \\
1 
\end{array}\right]_{\{input\ vector\}}
=
\left[\begin{array}{rr} 
-4 \\ 1
\end{array}\right]_{\{output\ vector\}}
\]</span></p>
<p>As we can see, the two vectors have been rotated. The new direction of both vectors followed a counter-clockwise rotation.</p>
<p>There is one <strong>important aspect of this whole process</strong> however. And that is, that the <strong>direction</strong> of the vectors are changed.</p>
<p><strong>In contrast</strong>, the one <strong>characteristic</strong> of an <strong>Eigenvector</strong> is that the <strong>direction</strong> does not change when a transformation is applied. And so, the most important highlight to remember is that the only transformation we can perform against an <strong>Eigenvector</strong> is a <strong>scaling</strong> (stretching) transformation - which does not affect the direction but only affects the magnitude. Any <strong>scalar</strong> we use to stretch an <strong>Eigenvector</strong> is called an <strong>Eigenvalue</strong>.</p>
<p>Figure  illustrates a set of vectors. The vectors {<span class="math inline">\(\mathbf{\vec{r}},\mathbf{\vec{s}},\mathbf{\vec{t}}\)</span>} shifted in both location and direction; but the <strong>Eigenvectors</strong> {<span class="math inline">\(\mathbf{\vec{u}},\mathbf{\vec{v}},\mathbf{\vec{w}}\)</span>} shifted in location but not direction.</p>
<div class="figure" style="text-align: center"><span id="fig:eigenvector"></span>
<img src="eigenvector.png" alt="Eigenvector on Shearing" width="80%" />
<p class="caption">
Figure 2.21: Eigenvector on Shearing
</p>
</div>
<p>For any arbitrary matrix, we can find its <strong>Eigenvector</strong> and <strong>Eigenvalue</strong> using Equation ().</p>
<p>The formula explains that the transformation of vector <span class="math inline">\(\mathbf{\vec{v}}\)</span> using matrix A equals the transformation of the same vector <span class="math inline">\(\mathbf{\vec{v}}\)</span> using scalar <span class="math inline">\(\lambda\)</span>.</p>
<p>Example:</p>

<p><span class="math display">\[\begin{align*}
A \cdotp v &amp;= \lambda \times  v \\
&amp; \rightarrow
\left[\begin{array}{rr} 
9 &amp; 0 \\
0 &amp; 9
\end{array}\right]_{\{transformer\}} \cdotp
\left[\begin{array}{rr} 
1 \\
0 
\end{array}\right]_{\{EigenVector\}}
=
9_{\{EigenValue\}} \times \left[\begin{array}{rr} 
1 \\ 0
\end{array}\right]_{\{EigenVector\}}
\end{align*}\]</span>
</p>
<p>Now, to find the <strong>Eigenvector</strong> and <strong>Eigenvalue</strong> of an arbitrary matrix, we perform a simple mathematical transformation to arrive at the following <strong>characteristic equation</strong> by evaluating the determinant:</p>

<p><span class="math display">\[\begin{align}
det(A - \lambda I ) = 0 \label{eqn:determinant1}
\end{align}\]</span>
</p>
<p>derived from:</p>

<p><span class="math display">\[\begin{align}
A \cdotp v = \lambda  \times v\ \ \rightarrow\ \ \ 
A \cdotp v = (\lambda I) \cdotp v\ \ \rightarrow\ \ \ 
(A  - \lambda I) \cdotp v = 0 \label{eqn:determinant2}
\end{align}\]</span>
</p>
<p>where <span class="math inline">\(\lambda\)</span> is intrinsically equivalent in transformation function as <span class="math inline">\(\lambda I\)</span>:</p>

<p><span class="math display">\[
\lambda
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{array} 
\right]_I =
\left[
\begin{array}{rrr}
\lambda &amp; 0 &amp; 0 \\
0 &amp; \lambda &amp; 0 \\
0 &amp; 0 &amp; \lambda
\end{array}
\right]_{\lambda I}
\]</span>
</p>
<p>Now, the idea is to find a <span class="math inline">\(\lambda\)</span> so that when subtracted from the diagonal entries of matrix A, the determinant becomes zero.</p>

<p><span class="math display">\[
A - \lambda I =
\left[
\begin{array}{rrr}
x_{11} &amp; x_{12} &amp; x_{13} \\
x_{21} &amp; x_{22} &amp; x_{23} \\
x_{31} &amp; x_{32} &amp; x_{33} \\
\end{array} 
\right]_A  - 
\lambda
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{array} 
\right]_I = 
\left[
\begin{array}{rrr}
x_{11}- \lambda &amp; x_{12} &amp; x_{13} \\
x_{21} &amp; x_{22}- \lambda &amp; x_{23} \\
x_{31} &amp; x_{32} &amp; x_{33}- \lambda \\
\end{array} 
\right]
\]</span>
</p>
<p>If the determinant is zero, then, as we have already covered in previous discussions, the geometric area is zero and <strong>eigenvector</strong> is the axis of rotation, unchanging in direction, but changing only in scale and therefore cannot be off from the span of the scale.</p>

<p><span class="math display">\[
det(A - \lambda{I}) = |A - \lambda{I}| =  
\left|\begin{array}{rr}
\left[\begin{array}{rr} 
9 &amp; 0 \\
0 &amp; 9
\end{array}\right]
-
9\left[\begin{array}{rr} 
1 &amp; 0 \\
0 &amp; 1
\end{array}\right]
\end{array}\right|
= 0
\]</span>
</p>
<p>Let us use a sample matrix to illustrate how to find the <strong>Eigenvalues</strong> and <strong>Eigenvectors</strong>.</p>
<p><strong>First, compose</strong> the <strong>characteristic polynomial</strong>  of a given matrix by computing for the <strong>determinant</strong>:</p>

<p><span class="math display">\[
A = \left[\begin{array}{rrr}
4 &amp; 1 &amp; 0 \\
1 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; 4
\end{array}\right]
\rightarrow
\left|\begin{array}{rrr}
\left[\begin{array}{rrr}
4 &amp; 1 &amp; 0 \\
1 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; 4
\end{array}\right]
-
\lambda
\left[\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{array}\right]
\end{array}\right|
\rightarrow
\left|\begin{array}{rrr}
4 - \lambda &amp; 1 &amp; 0 \\
1 &amp; 2 - \lambda &amp; 1 \\
0 &amp; 1 &amp; 4 - \lambda
\end{array}\right|
\]</span>
</p>
<p>Below we obtain the <strong>characteristic polynomial</strong> of matrix <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[\begin{align*}
{}&amp;= (4-\lambda)((2-\lambda)(4-\lambda) - 1) - (1(4-\lambda) - 1 \times 0)\\
&amp;= (4-\lambda)(\lambda^2 -6\lambda + 7) - (4-\lambda)\\
&amp;= -\lambda^3 + 10\lambda^2 - 31\lambda + 28 - 4 - \lambda \\
&amp;= -\lambda^3 + 10\lambda^2 - 30\lambda + 24 
\end{align*}\]</span></p>
<p>By a simple definition, the <strong>characteristic polynomial</strong> of a matrix is a polynomial whose <strong>roots</strong> are the <strong>eigenvalues</strong> of the matrix. Therefore, by computing for the <strong>roots</strong>, we get the <strong>Eigenvalues</strong> <span class="math inline">\(\lambda\)</span> for matrix <span class="math inline">\(A\)</span>:</p>
<p><strong>Second, solve for the roots (<span class="math inline">\(\lambda\)</span>) of the polynomial</strong>. That gives us the following <strong>Eigenvalues</strong>:</p>
<p><span class="math display">\[
\lambda_1 = 3 + \sqrt{3},\ \ \ \ \ \lambda_2 = 4,\ \ \  \lambda_3 = 3 - \sqrt{3}
\]</span></p>
<p><strong>Third</strong>, we obtain the <strong>Eigenvectors</strong> based on the discovered <strong>Eigenvalues</strong> above. We use the formula below:</p>
<p><span class="math display">\[\begin{align}
A v = \lambda v\ \ \ \ \rightarrow \ \ \ \ (A - \lambda I)v = 0 \label{eqn:determinant3}
\end{align}\]</span></p>
<p>Substitute each of the <span class="math inline">\(\lambda\)</span>s, starting with <span class="math inline">\(\lambda_1=3 + \sqrt{3}\)</span>:</p>

<p><span class="math display">\[
(A - (3 + \sqrt{3}){I})v =  0\ \ \ \rightarrow\ \ \ \ (A - (3 + \sqrt{3}){I}) \left[\begin{array}{r} x  \\ y \\ z  \end{array}\right] = 0
\]</span>
</p>
<p>That gives us:</p>

<p><span class="math display">\[
\left(\begin{array}{c} 
\left[\begin{array}{rrr} 
4 &amp; 1 &amp; 0\\ 1 &amp; 2 &amp; 1\\ 0 &amp; 1 &amp; 4
\end{array}\right]
- (3 + \sqrt{3})
\left[\begin{array}{rrr} 
1 &amp; 0 &amp; 0\\0 &amp; 1 &amp; 0\\0 &amp; 0 &amp; 1
\end{array}\right] 
\end{array}\right)
\left[\begin{array}{r} x  \\ y \\ z  \end{array}\right] =
\left[\begin{array}{r} 0\\0\\0  \end{array}\right]
\]</span>
</p>
<p>and a simplified version (<strong>reduced row echelon form</strong>):</p>

<p><span class="math display">\[
\left(\begin{array}{rrr} 
1 &amp; -1-\sqrt{3} &amp; 1\\0 &amp; -1 &amp; -1+\sqrt{3}\\0 &amp; 0 &amp; 0
\end{array}\right)
\left[\begin{array}{r} x  \\ y \\ z  \end{array}\right] =
\left[\begin{array}{r} 0\\0\\0  \end{array}\right]
\]</span>
</p>
<p>Here, we let the <strong>free variable</strong>, <strong>z</strong>, take any non-zero arbitrary value, say <strong>z</strong> = <strong>1</strong> (or for now, let us use <strong>z</strong> = <strong>a</strong> where <span class="math inline">\(z \ne 0\)</span>).</p>
<p>We get the <strong>Eigenvector</strong> for <span class="math inline">\(\lambda_1=3 + \sqrt{3}\)</span></p>

<p><span class="math display">\[\begin{align*}
v_1 {}&amp;= \left\{\begin{array}{r} 
x + (-1-\sqrt{3})y + z = 0 \\
-y - (1 + \sqrt{3})z= 0\\ 
z = \color{blue}{a}  
\end{array}\right\}
\rightarrow
\left\{\begin{array}{l} 
x = 2z - z \\ 
y = -(1 - \sqrt{3})z\\ 
z = \color{blue}{a}  \end{array}\right\}
\rightarrow
\left\{\begin{array}{l} x = \color{blue}{a} \\ y = (\sqrt{3} - 1)\color{blue}{a}\\ z = \color{blue}{a} 
\end{array}\right\}
\\
\\
v_1 &amp;=
\left[\begin{array}{l} \color{blue}{a} \\  (\sqrt{3} - 1)\color{blue}{a}\\  \color{blue}{a} 
\end{array}\right]
\rightarrow \text{for } \lambda_1 = 3 + \sqrt{3}
\end{align*}\]</span>
</p>
<p><strong>Next</strong>, substitute the second <span class="math inline">\(\lambda_2=4\)</span>:</p>

<p><span class="math display">\[
(A - 4I)v = 0\ \ \ \rightarrow\ \ \ \ (A - 4I) \left[\begin{array}{r} x  \\ y \\ z  \end{array}\right] = 0
\]</span>
</p>
<p>That gives us:</p>

<p><span class="math display">\[
\left(\begin{array}{c} 
\left[\begin{array}{rrr} 
4 &amp; 1 &amp; 0\\ 1 &amp; 2 &amp; 1\\ 0 &amp; 1 &amp; 4
\end{array}\right]
- 4
\left[\begin{array}{rrr} 
1 &amp; 0 &amp; 0\\0 &amp; 1 &amp; 0\\0 &amp; 0 &amp; 1
\end{array}\right] 
\end{array}\right)
\left[\begin{array}{r} x  \\ y \\ z  \end{array}\right] =
\left[\begin{array}{r} 0\\0\\0  \end{array}\right]
\]</span>
</p>
<p>and a simplified version (<strong>reduced row echelon form</strong>):</p>
<p><span class="math display">\[
\left(\begin{array}{rrr} 
1 &amp; -2 &amp; 1\\0 &amp; 1 &amp; 0\\0 &amp; 0 &amp; 0
\end{array}\right)
\left[\begin{array}{r} x  \\ y \\ z  \end{array}\right] =
\left[\begin{array}{r} 0\\0\\0  \end{array}\right]
\]</span></p>
<p>Here, we let the <strong>free variable</strong>, <strong>z</strong>, take any non-zero arbitrary value, say <strong>z</strong> = <strong>1</strong> (or for now, let us use <strong>z</strong> = <strong>a</strong> where <span class="math inline">\(z \ne 0\)</span>).</p>
<p>We get the <strong>Eigenvector</strong> for <span class="math inline">\(\lambda_2=4\)</span></p>

<p><span class="math display">\[\begin{align*}
v_2 {}&amp;= \left\{\begin{array}{r} 
x -2y + z = 0 \\ 
y = 0\\ 
z = \color{blue}{a}  
\end{array}\right\}
\rightarrow
\left\{\begin{array}{l} x = -z \\ y = 0\\ z = \color{blue}{a}  \end{array}\right\}
\rightarrow
\left\{\begin{array}{l} x = -\color{blue}{a} \\ y = 0\\ z = \color{blue}{a} 
\end{array}\right\}
\\
\\
v_2 &amp;=
\left[\begin{array}{l} -\color{blue}{a} \\  0\\ \color{blue}{a} 
\end{array}\right]
\rightarrow \text{for } \lambda_2 = 4
\end{align*}\]</span>
</p>
<p><strong>Next</strong>, substitute the third <span class="math inline">\(\lambda_3=3 - \sqrt{3}\)</span>:</p>

<p><span class="math display">\[
(A - (3 - \sqrt{3}){I})v =  0\ \ \ \rightarrow\ \ \ \ (A - (3 - \sqrt{3}){I}) \left[\begin{array}{r} x  \\ y \\ z  \end{array}\right] = 0
\]</span>
</p>
<p>That gives us:</p>

<p><span class="math display">\[
\left(\begin{array}{c} 
\left[\begin{array}{rrr} 
4 &amp; 1 &amp; 0\\ 1 &amp; 2 &amp; 1\\ 0 &amp; 1 &amp; 4
\end{array}\right]
- (3 - \sqrt{3})
\left[\begin{array}{rrr}
1 &amp; 0 &amp; 0\\0 &amp; 1 &amp; 0\\0 &amp; 0 &amp; 1
\end{array}\right] 
\end{array}\right)
\left[\begin{array}{r} x  \\ y \\ z  \end{array}\right] =
\left[\begin{array}{r} 0\\0\\0  \end{array}\right]
\]</span>
</p>
<p>and a simplified version (<strong>reduced row echelon form</strong>):</p>

<p><span class="math display">\[
\left(\begin{array}{rrr} 
1+\sqrt{3} &amp; 1 &amp; 0\\0 &amp; 1 &amp; 1+\sqrt{3}\\0 &amp; 0 &amp; 0
\end{array}\right)
\left[\begin{array}{r} x  \\ y \\ z  \end{array}\right] =
\left[\begin{array}{r} 0\\0\\0  \end{array}\right]
\]</span>
</p>
<p>Here, we let the <strong>free variable</strong>, <strong>z</strong>, take any non-zero arbitrary value, say <strong>z</strong> = <strong>1</strong> (or for now, let us use <strong>z</strong> = <strong>a</strong> where <span class="math inline">\(z \ne 0\)</span>).</p>
<p>We get the <strong>Eigenvector</strong> for <span class="math inline">\(\lambda_3=3 - \sqrt{3}\)</span></p>

<p><span class="math display">\[\begin{align*}
v_3 {}&amp;= \left\{\begin{array}{r} 
(1+\sqrt{3})x + y = 0 \\ 
y + (1 + \sqrt{3})z= 0\\ 
z = \color{blue}{a}  \end{array}\right\}
\rightarrow
\left\{\begin{array}{l} 
x = z \\ 
y = -(1 + \sqrt{3})z\\ 
z = \color{blue}{a}  \end{array}\right\}
\rightarrow
\left\{\begin{array}{l} x = \color{blue}{a} \\ y = -(1 + \sqrt{3})\color{blue}{a}\\ z = \color{blue}{a} 
\end{array}\right\}
\\
\\
v_3 &amp;=
\left[\begin{array}{l} \color{blue}{a} \\  (-\sqrt{3} -1)\color{blue}{a}\\  \color{blue}{a} 
\end{array}\right]
\rightarrow \text{for } \lambda_3 = 3 - \sqrt(3)
\end{align*}\]</span>
</p>
<p>The <strong>Eigenvector</strong> to <strong>Eigenvalue</strong> mapping is therefore as follows (let a = 1):</p>

<p><span class="math display">\[
\left(\begin{array}{c}
\text{Matrix of Eigenvectors}\\
-------------\\
\left[\begin{array}{r}v_1\\---\\1 \\ \sqrt{3} -1 \\ 1  \end{array}\right]
\left[\begin{array}{r}v_2\\---\\-1 \\ 0 \\ 1  \end{array}\right]
\left[\begin{array}{r}v_3\\---\\1 \\ -\sqrt{3} -1 \\ 1 \end{array}\right]
\end{array}\right)
\iff
\left[\begin{array}{l}Eigenvalues\\------\\\lambda_1=3 + \sqrt{3}  \\ \lambda_2=4 \\ \lambda_3=3 - \sqrt{3}  \end{array}\right]
\]</span>
</p>
<p>In real situations, often, if not most cases, we do encounter high dimensional matrices of eigenvectors, where n &gt; 3. We may see n &gt; 1000. Without loss of generality, we get the following:</p>

<p><span class="math display">\[
\left(\begin{array}{cccc}
\text{Matrix of Eigenvectors}\\
-------------\\
\begin{array}{ccccc}
v_1 &amp; v_2  &amp; \dots &amp; v_n \\
--- &amp; --- &amp; --- &amp; --- \\
x_{1,1} &amp; x_{1,2}  &amp; \dots &amp; x_{1,n}\\
x_{2,1} &amp; x_{2,2}  &amp; \dots &amp; x_{2,n}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
x_{n,1} &amp; x_{n,2}  &amp; \dots &amp; x_{n,n}\\
\end{array}
\end{array}\right)
\iff
\left(\begin{array}{cccc}
\text{Matrix of Eigenvalues}\\
-------------\\
\begin{array}{ccccc}
\lambda_1 &amp; \lambda_2    &amp; \dots &amp; \lambda_n  \\
--- &amp; --- &amp; --- &amp; --- \\
\lambda_{1,1} &amp; 0  &amp; \dots &amp; 0\\
0 &amp; x_{2,2} &amp; \dots &amp; 0\\
0 &amp; 0  &amp; \dots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
0 &amp; 0  &amp; \dots &amp; \lambda_{n,n}\\
\end{array}
\end{array}\right)
\]</span>
</p>
<p>There are a few pointers to note about <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong>, among many others:</p>
<p>First, we now know that a scalar <span class="math inline">\(\lambda\)</span> is an <strong>Eigenvalue</strong> of a matrix if the following <strong>characteristic equation</strong>  holds true:</p>
<p><span class="math display">\[\begin{align}
det(A - \lambda I) = 0 \label{eqn:chareqn}
\end{align}\]</span></p>
<p>Second, <strong>Eigenvalues</strong> are derived from the <strong>roots</strong> of a matrix’s <strong>characteristic polynomial</strong>. A <strong>characteristic polynomial</strong> can be obtained by using the <strong>characteristic equation</strong> above.</p>
<p>Third, <strong>Eigenvectors</strong> are derived using the following equations provided <strong>Eigenvalues</strong> are determined:</p>
<p><span class="math display">\[\begin{align}
(A - \lambda I)v = 0 \label{eqn:eigenvector}
\end{align}\]</span></p>
<p>Fourth, the <strong>Eigenspace</strong>  is the set of <strong>Eigenvector solutions</strong> for which Equation () holds true. And so therefore, we can also derive the equation for <strong>Eigenspace</strong> of the <strong>characteristic matrix</strong> as such:</p>
<p><span class="math display">\[\begin{align}
E_v = N(A - \lambda I) \label{eqn:eigenspace}
\end{align}\]</span></p>
<p>Lastly, there are algorithms and formulas worth looking into as they are designed to solve the <strong>roots (eigenvalues)</strong> of <strong>characteristic polynomials</strong>, to name a few:</p>
<ul>
<li>Jacobi Formula</li>
<li>Samuelson Formula</li>
<li>Le Verrier or Faddeev-Leverrier Algorithm</li>
<li>Krylov Algorithm</li>
<li>Weber-Voetter</li>
<li>Newton Raphson Algorithm</li>
</ul>
</div>
<div id="eigen-spectral-decomposition" class="section level2">
<h2><span class="header-section-number">2.15</span> Eigen (Spectral) Decomposition</h2>
<p>Previously, what we have shown is an example of decomposing (factorizing) a matrix into smaller components by way of deriving the <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong>.</p>
<p>An important rule to note is that given both <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong>, we should be able to reconstruct the original matrix.</p>
<p>There are reasons why we need to decompose a matrix, and maybe eventually reconstruct back the matrix. One reason is to be able to use the smaller components of a decomposed matrix for computational efficiency. Another reason, particularly for <strong>Eigenvalues</strong>, is to be able to sequence them in decreasing order to find the ones of greater value or greater importance, e.g. we cover PCA (primary component analysis)  in the chapter about machine learning later. Lastly, in situations where we are dealing with very high dimensional matrices, we need to perform matrix decomposition to reduce the dimensionality to a more manageable lower dimension.</p>
<p>Other examples of <strong>matrix decomposition</strong> are:</p>
<ul>
<li>LU decomposition</li>
<li>QR decomposition</li>
<li>Cholesky decomposition</li>
<li>Singular Value Decomposition (SVD)</li>
<li>Jordan decomposition</li>
<li>Schur decomposition</li>
<li>Hessenberg Decomposition</li>
<li>Polar Decomposition</li>
</ul>
<p>We cover them in depth later in the chapter.</p>
<p>Let us perform a reconstruction of a matrix using the following <strong>Eigen Decomposition</strong> equation:</p>
<p><span class="math display">\[\begin{align}
A = P\Lambda P^{-1} \label{eqn:eigendecomp}
\end{align}\]</span></p>
<p>where <span class="math inline">\(P\)</span> is the derived matrix of <strong>Eigenvectors</strong> and <span class="math inline">\(\Lambda = (\lambda \cdotp I)\)</span> is the derived diagonal matrix of <strong>Eigenvalues</strong>:</p>
<p>Let us use the same <strong>matrix A</strong> from previous section as our target matrix for the reconstruction:</p>

<p><span class="math display">\[
A = \left[\begin{array}{rrr}
4 &amp; 1 &amp; 0 \\
1 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; 4
\end{array}\right]
\]</span>
</p>
<p>Given the following <strong>Eigenvectors</strong>, <span class="math inline">\(P\)</span>, and <strong>Eigenvalues</strong>, <span class="math inline">\(\Lambda\)</span>:</p>

<p><span class="math display">\[\begin{align*}
P {}&amp;= \left[\begin{array}{ccc}
1 &amp; -1 &amp; 1\\
\sqrt{3}-1 &amp; 0 &amp; -\sqrt{3} - 1\\
1 &amp; 1 &amp; 1
\end{array}\right]
\\
\\
\lambda  &amp;= \left[\begin{array}{l} 3 + \sqrt{3} \\ 4 \\ 3 - \sqrt{3}  \end{array}\right] 
\rightarrow diagonalized \rightarrow
\Lambda = \left[\begin{array}{rrr} 3 + \sqrt{3} &amp; 0 &amp; 0\\ 0 &amp; 4 &amp; 0 \\ 0 &amp; 0 &amp; 3 - \sqrt{3} \end{array}\right]_\Lambda
\end{align*}\]</span>
</p>
<p>we start <strong>reconstructing</strong>:</p>

<p><span class="math display">\[\begin{align*}
A {}&amp;= P\cdotp \Lambda \cdotp P^{-1} \\
&amp; = \left[\begin{array}{ccc}
1 &amp; -1 &amp; 1\\
\sqrt{3}-1 &amp; 0 &amp; -\sqrt{3} - 1\\
1 &amp; 1 &amp; 1
\end{array}\right]
\cdotp
\left[\begin{array}{rrr} 3 + \sqrt{3} &amp; 0 &amp; 0\\ 0 &amp; 4 &amp; 0 \\ 0 &amp; 0 &amp; 3 - \sqrt{3} \end{array}\right]
\cdotp
\left[\begin{array}{ccc}
\frac{\sqrt{3}+1}{4\sqrt{3}} &amp; \frac{1}{2\sqrt{3}} &amp; \frac{\sqrt{3}+1}{4\sqrt{3}}\\
\frac{-1}{2} &amp; 0 &amp;\frac{1}{2} \\
\frac{(\sqrt{3} - 1}{4\sqrt{3}} &amp; -\frac{1}{2\sqrt{3}} &amp; \frac{(\sqrt{3} - 1}{4\sqrt{3}}
\end{array}\right] 
\\
\\
A &amp;= \left[\begin{array}{rrr}
4 &amp; 1 &amp; 0 \\
1 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; 4
\end{array}\right]
\end{align*}\]</span>
</p>
<p>As an exercise, try to decompose and reconstruct the following matrix:</p>
<p><span class="math display">\[
A = \left[\begin{array}{rrr}
3 &amp; 1 &amp; 0 \\
1 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; 3
\end{array}\right]
\]</span></p>
<p>Hint for the <strong>Eigenvalues</strong>:</p>
<p><span class="math display">\[
\lambda_1 = 4, \lambda_2 =3, \lambda_3 = 1
\]</span>
derived from its <strong>characteristic equation</strong>:</p>
<p><span class="math display">\[
-\lambda^3 + 8\lambda^2 - 19\lambda + 12 = 0
\]</span></p>
<p>where its <strong>characteristic polynomial</strong> is the left-hand side of the equation:</p>
<p><span class="math display">\[
p(\lambda) = -\lambda^3 + 8\lambda^2 - 19\lambda + 12 
\]</span></p>
</div>
<div id="diagonalizability-of-a-matrix" class="section level2">
<h2><span class="header-section-number">2.16</span> Diagonalizability of a Matrix </h2>
<p>While we have used <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong> to reconstruct a matrix, we can use the <strong>Eigenvectors</strong> to diagonalize a matrix.</p>
<p>Let us use the same <strong>matrix A</strong> from previous section as our target matrix for diagonalization:</p>
<p><span class="math display">\[
A = \left[\begin{array}{rrr}
4 &amp; 1 &amp; 0 \\
1 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; 4
\end{array}\right]
\]</span></p>
<p>To perform a diagonalization of matrix A, we use the following equation:</p>
<p><span class="math display">\[\begin{align}
A_D = P^{-1}AP \label{eqn:diagonalizability1}
\end{align}\]</span></p>
<p>So given the following <strong>Eigenvectors</strong>, <span class="math inline">\(P\)</span>:</p>
<p><span class="math display">\[\begin{align*}
P {}&amp;= \left[\begin{array}{ccc}
1 &amp; -1 &amp; 1\\
\sqrt{3}-1 &amp; 0 &amp; -\sqrt{3} - 1\\
1 &amp; 1 &amp; 1
\end{array}\right]
\end{align*}\]</span></p>
<p>we can derive the <strong>diagonal</strong> of a matrix this way:</p>

<p><span class="math display">\[\begin{align*}
A_D {}&amp;= P^{-1}\cdotp A \cdotp P\\
&amp; = 
\left[\begin{array}{ccc}
\frac{\sqrt{3}+1}{4\sqrt{3}} &amp; \frac{1}{2\sqrt{3}} &amp; \frac{\sqrt{3}+1}{4\sqrt{3}}\\
\frac{-1}{2} &amp; 0 &amp;\frac{1}{2} \\
\frac{(\sqrt{3} - 1}{4\sqrt{3}} &amp; -\frac{1}{2\sqrt{3}} &amp; \frac{(\sqrt{3} - 1}{4\sqrt{3}}
\end{array}\right]
\cdotp
\left[\begin{array}{rrr}
4 &amp; 1 &amp; 0 \\
1 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; 4
\end{array}\right]
\cdotp
\left[\begin{array}{ccc}
1 &amp; -1 &amp; 1\\
\sqrt{3}-1 &amp; 0 &amp; -\sqrt{3} - 1\\
1 &amp; 1 &amp; 1
\end{array}\right]
\\
\\
A_D &amp;= \left[\begin{array}{rrr}
3 + \sqrt{3} &amp; . &amp; . \\
. &amp; 4 &amp; . \\
. &amp; . &amp; 3 - \sqrt{3}
\end{array}\right]
\end{align*}\]</span>
</p>
<p>Notice that diagonalizing a matrix, given its <strong>Eigenvectors</strong>, yields a matrix whose diagonal entries are the corresponding <strong>Eigenvalues</strong>.</p>
<p>There may be cases when a matrix is not diagonalizable. This can be determined by looking at the <strong>characteristic polynomial</strong> of the matrix and comparing the <strong>algebraic multiplicity</strong> against its <strong>geometric multiplicity</strong>.</p>
</div>
<div id="trace-of-a-square-matrix" class="section level2">
<h2><span class="header-section-number">2.17</span> Trace of a Square Matrix </h2>
<p>We can use the <strong>Eigenvalues</strong> to trace a matrix. What that means is that we can sum up the main diagonal (the <strong>Eigenvalues</strong>) of a matrix starting from the upper left. It is expressed as:</p>
<p><span class="math display">\[\begin{align}
tr(A) = \sum_{i=1}^n a_{ii} = a_{11} + a_{22} + ... + a_{nn} \label{eqn:trace}
\end{align}\]</span></p>
<p>Let us use the same <strong>matrix A</strong> from previous section as our target matrix for tracing:</p>
<p><span class="math display">\[
A = \left[\begin{array}{rrr}
4 &amp; 1 &amp; 0 \\
1 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; 4
\end{array}\right]
\]</span></p>
<p>To perform a trace of matrix A, we use the following equation:</p>
<p><span class="math display">\[
P(A) = 4 + 2 + 4 = 10
\]</span></p>
<p>Also, it helps to know two properties of <strong>trace</strong> matrix specially when dealing with derivations.</p>
<p>Given three matrices:</p>
<p><span class="math display">\[
X = \left[\begin{array}{cc}1 &amp; 3 \\ 2 &amp; 4 \end{array}\right]
\ \ \ \ \ \ \ \ \ \ \ \ \
Y = \left[\begin{array}{cc}8 &amp; 8 \\ 8 &amp; 8 \end{array}\right]
\ \ \ \ \ \ \ \ \ \ \ \ \
Z = \left[\begin{array}{cc}7 &amp; 7 \\ 7 &amp; 7 \end{array}\right]
\]</span></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="co"># naive implementation</span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2">tr &lt;-<span class="st"> </span><span class="cf">function</span>(m) { <span class="kw">sum</span>(<span class="kw">diag</span>(m)) }</a>
<a class="sourceLine" id="cb8-3" data-line-number="3">x =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>), <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb8-4" data-line-number="4">y =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">8</span>), <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb8-5" data-line-number="5">z =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">7</span>,<span class="dv">7</span>,<span class="dv">7</span>,<span class="dv">7</span>), <span class="dv">2</span>)</a></code></pre></div>
<p><strong>trace</strong> matrix has cyclic permutation property, e..g.:</p>
<p><span class="math display">\[
tr(XYZ) = tr(YZX) = tr(ZYX) = 1120
\]</span></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">c</span>( <span class="kw">tr</span>(x <span class="op">%*%</span><span class="st"> </span>y <span class="op">%*%</span><span class="st"> </span>z), <span class="kw">tr</span>(y <span class="op">%*%</span><span class="st"> </span>z <span class="op">%*%</span><span class="st"> </span>x), <span class="kw">tr</span>(z <span class="op">%*%</span><span class="st"> </span>x <span class="op">%*%</span><span class="st"> </span>y) )</a></code></pre></div>
<pre><code>## [1] 1120 1120 1120</code></pre>
<p><strong>trace</strong> matrix has additive property, e..g.:</p>
<p><span class="math display">\[
tr(X + Y + Z) = tr(X) + tr(Y) + tr(Z) = 1120
\]</span></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">c</span>( <span class="kw">tr</span>(x <span class="op">+</span><span class="st"> </span>y <span class="op">+</span><span class="st"> </span>z), <span class="kw">tr</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">tr</span>(y) <span class="op">+</span><span class="st"> </span><span class="kw">tr</span>(z) )</a></code></pre></div>
<pre><code>## [1] 35 35</code></pre>
</div>
<div id="algebraic-and-geometric-multiplicity" class="section level2">
<h2><span class="header-section-number">2.18</span> Algebraic and Geometric Multiplicity</h2>
<p>The <strong>multiplicity</strong> of a matrix can be determined by solving for the <strong>eigenvectors</strong> and <strong>eigenvalues</strong> of a matrix:</p>
<p>Let us use the following 4x4 matrix:</p>
<p><span class="math display">\[
A = 
\left[\begin{array}{cccc}
1 &amp; 1 &amp; 9 &amp; 0\\
0 &amp; 4 &amp; 3 &amp; 0\\
0 &amp; 3 &amp; 4 &amp; 0 \\
0 &amp; 6 &amp; 1 &amp; 1 \\
\end{array}\right]
\]</span></p>
<p>First, we solve for the <strong>eigenvalues</strong> by evaluating the determinant:</p>
<p><span class="math display">\[
det
\left(
\begin{array}{rrrr}
1-\lambda &amp; 1 &amp; 9 &amp; 0\\
0 &amp; 4-\lambda &amp; 3 &amp; 0\\
0 &amp; 3 &amp;  4-\lambda &amp; 0\\
0 &amp; 6 &amp; 1 &amp; 1-\lambda\\
\end{array}
\right) = 0
\]</span></p>
<p>We get the following <strong>characteristic equation</strong> along with its <strong>minimal polynomials</strong> :</p>
<p><span class="math display">\[\begin{align*}
\lambda^4 - 10\lambda^3 + 24\lambda^2 - 22\lambda + 7 {}&amp;= 0\\
(\lambda-1)(\lambda^3 - 9\lambda^2 + 15\lambda - 7) {}&amp;= 0\\
(\lambda-1)(\lambda-1)(\lambda^2 - 8\lambda + 7) {}&amp;= 0\\
(\lambda-1)(\lambda-1)(\lambda-1)(\lambda-7) {}&amp;= 0\\
(\lambda-1)^3(\lambda-7) &amp;= 0 
\end{align*}\]</span></p>
<p>Here, we get four <strong>eigenvalues</strong>:</p>
<p><span class="math display">\[
\lambda = 1\ \ \ \ \ \lambda = 1\ \ \ \ \ \lambda = 1\ \ \ \ \ \lambda=7
\]</span></p>
<p>This is where <strong>algebraic multiplicity</strong> comes into the picture. The <strong>eigenvalues</strong> for <span class="math inline">\(\lambda\)</span> repeats three times for <span class="math inline">\(\lambda=1\)</span>, and only one time for <span class="math inline">\(\lambda=7\)</span>. Therefore, the <strong>algebraic multiplicity</strong> for <span class="math inline">\(\lambda=1\)</span> is three and the <strong>algebraic multiplicity</strong> for <span class="math inline">\(\lambda=7\)</span> is one. It can be expressed this way:</p>
<p><span class="math display">\[
M_a(\lambda=1) = 3 \ \ \ \ \ \ M_a(\lambda=7) = 1
\]</span></p>
<p>Furthermore, if we inject the <strong>eigenvalues</strong> into Equation (), then we get the following <strong>eigenvectors</strong>:</p>
<p><span class="math display">\[\begin{align*}
for\ \lambda {}&amp;=1
,\ \ \ \  v1 = \left[\begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array}\right]
,\ \ \ \  v2 = \left[\begin{array}{c} 0 \\ 0 \\ 0 \\ 1\end{array}\right] \\
\\
for\ \lambda &amp;=7
,\ \ \ \  v3 = \left[\begin{array}{c} 10 \\ 6 \\ 6 \\ 7 \end{array}\right]
\end{align*}\]</span></p>
<p>This is where <strong>geometric multiplicity</strong>  comes into picture. The <strong>eigenvalue</strong> <span class="math inline">\(\lambda=1\)</span> yields two distinct <strong>eigenvectors</strong> <span class="math inline">\(\mathbf{\vec{v1}}\)</span> and <span class="math inline">\(\mathbf{\vec{v2}}\)</span>, and only one <strong>eigenvector</strong> <span class="math inline">\(\mathbf{\vec{v3}}\)</span> for <strong>eigenvalue</strong> <span class="math inline">\(\lambda=7\)</span> . Therefore, the <strong>geometric multiplicity</strong> for <span class="math inline">\(\lambda=1\)</span> is two and the <strong>geometric multiplicity</strong> for <span class="math inline">\(\lambda=7\)</span> is one. It can be expressed this way:</p>
<p><span class="math display">\[
M_g(\lambda=1) = 2 \ \ \ \ \ \ M_g(\lambda=7) = 1
\]</span>
We can also say that the <strong>geometric multiplicity</strong> of <strong>eigenvectors</strong> is the dimension of the <strong>eigenspace</strong> of their corresponding <strong>eigenvalue</strong> <span class="math inline">\(\lambda\)</span> of matrix A.</p>
<p><span class="math display">\[
M_g(\lambda) = E_a(\lambda)
\]</span></p>
<p>Now, notice the following:</p>
<p><span class="math display">\[
M_a(\lambda=1)=3\ \ \  &gt;\ \ \ M_g(\lambda=1)=2
\]</span>
The <strong>algebraic multiplicity</strong>  is greater than the <strong>geometric multiplicity</strong>. This can also be shown if we review the <strong>minimal polynomial</strong> of the <strong>characteristic polynomial</strong>. What that means is that the matrix is not diagonalizable.</p>
<p>Note that the corresponding <strong>characteristic matrices</strong> of a minimum set of polynomials - the <strong>minimal polynomials</strong>, when multiplied together, will result in a zero matrix.</p>
<p><span class="math display">\[
(\lambda-1)^2(\lambda-7) = 0 \leftarrow \text{\{minimal polynomials\}} 
\]</span></p>
<p>It shows that the <strong>algebraic multiplicity</strong> of the <strong>minimal polynomial</strong> is two, matching the <strong>geometric multiplicity</strong>. If we count the number of <strong>geometric multiplicity</strong> for all <strong>Eigenvectors</strong>, we see three which is lesser than the expected full rank of a matrix which is supposed to be four. So we are missing one eigenvector, therefore. That missing non-zero <strong>eigenvector</strong> is what we call a <strong>generalized eigenvector</strong>.</p>
<p>We show how to find a <strong>generalized eigenvector</strong> later in this chapter under <strong>Jordan decomposition</strong> section.</p>
</div>
<div id="types-of-matrices" class="section level2">
<h2><span class="header-section-number">2.19</span> Types of Matrices</h2>
<p>Let us review a few types of matrices:</p>
<p><strong>Invertible Matrix</strong></p>
<p>A matrix is <strong>Invertible</strong> based on the following few listed properties (which we have covered and discussed), among many others:</p>
<ul>
<li>its determinant is non-zero</li>
<li>its columns are linearly independent</li>
<li>it is non-singular</li>
<li>it is full-rank, and therefore its nullity is zero</li>
</ul>
<p><strong>Positive Definite Matrix</strong> </p>
<p>A matrix is <strong>Positive Definite</strong> based on the following few properties, among many others:</p>
<ul>
<li>it is symmetric and its eigenvalues are all positive</li>
<li>it is symmetric and its pivots, in <strong>row echelon</strong> form, are all positive</li>
<li>if there exists a vector <span class="math inline">\(\vec{v}\)</span> such that <span class="math inline">\(\vec{v}^TA\vec{v} &gt; 0\ where\ \vec{v} \neq 0\)</span>.</li>
</ul>
<p><strong>Identity Matrix</strong> </p>
<p>This is a square matrix with the diagonal entries, from upper left to lower right, being one, and all others being zero. The following notations are used:</p>
<p><span class="math display">\[
\mathbf{1}_n = I_n = \left[\begin{array}{ccccc} 
1_{1,1} &amp; 0_{1,2} &amp; \dots &amp; 0_{1,n} \\ 
0_{2,1} &amp; 1_{2,2} &amp; \dots &amp; 0_{2,n} \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
0_{n,1} &amp; 0_{n,2} &amp; \dots &amp; 1_{n,n} 
\end{array}\right],
\ \ \ \ \ \ \ e.g.\ 10\mathbf{1}_2 = 10I_2 = 
\left[ \begin{array}{cc} 10 &amp; 0 \\ 0 &amp; 10 \end{array} \right]
\]</span></p>
<p><strong>Diagonal and TriDiagonal Matrix</strong> </p>
<p>This is a matrix with the diagonal entries, from upper left to lower right, being non-zero, and all others being zero.</p>
<p>Additionally, the sum of the non-zero diagonal entries is what we call the <strong>trace</strong> of the matrix.</p>
<p><span class="math display">\[
\mathbf{A_{i,j}} = \left[\begin{array}{ccccc} 
x_{1,1} &amp; 0_{1,2} &amp; \dots &amp; 0_{1,j} \\ 
0_{2,1} &amp; x_{2,2} &amp; \dots &amp; 0_{2,j} \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
0_{i,1} &amp; 0_{i,2} &amp; \dots &amp; x_{i,j} 
\end{array}\right]
\mathbf{A_{i,j}} = \left[\begin{array}{ccccc} 
x_{1,1} &amp; x_{1,2} &amp; \dots &amp; 0_{1,j} \\ 
x_{2,1} &amp; x_{2,2} &amp; \dots &amp; 0_{2,j} \\ 
\vdots &amp; \vdots &amp; \ddots &amp; x_{i-1,j}    \\ 
0_{i,1} &amp; 0_{i,2} &amp; x_{i,j-1} &amp; x_{i,j} 
\end{array}\right]
\]</span></p>
<p><strong>Hermitian (Symmetric) Matrix</strong> </p>
<p>This is a matrix that is self-adjoint in which its conjugate is equal to the transpose of its complex conjugate.</p>
<p><span class="math display">\[
\mathbf{A_{i,j}} = \left[\begin{array}{ccccc} 
x_{1,1} &amp; c_{1,2} &amp; b_{1,3} &amp; a_{1,j} \\ 
c_{2,1} &amp; x_{2,2} &amp; c_{2,3} &amp; b_{2,j} \\ 
b_{3,1} &amp; c_{3,2} &amp; \ddots &amp; c_{3,j} \\ 
a_{i,1} &amp; b_{i,2} &amp; c_{i,3} &amp; x_{i,j} 
\end{array}\right]
\]</span></p>
<p><strong>Hessenberg Matrix</strong> </p>
<p>Hessenberg matrix comes as an Upper Hessenberg matrix or Lower Hessenberg matrix.</p>
<ul>
<li>Upper Hessenberg matrix is an upper triangular matrix with an upper superdiagonal and can be expressed as: for all i,j where i &gt; j + 1 then <span class="math inline">\(a_{i,j}\)</span> = 0.</li>
<li>Lower Hessenberg matrix is a lower triangular matrix with a lower superdiagonal and can be expressed as: for all i,j where j &gt; i + 1 then <span class="math inline">\(a_{i,j}\)</span> = 0.</li>
</ul>
<p>Also, a Hessenberg matrix is considered <strong>unreduced</strong> if the superdiagonal entries are non-zero.</p>
<p><span class="math display">\[
\mathbf{H_{upper}} = \left[\begin{array}{ccccc} 
x_{1,1} &amp; x_{1,2} &amp; x_{1,3} &amp; x_{1,j} \\ 
x_{2,1} &amp; x_{2,2} &amp; x_{2,3} &amp; x_{2,j} \\ 
0 &amp; x_{3,2} &amp; \ddots &amp; x_{3,j} \\ 
0 &amp; 0 &amp; x_{i,3} &amp; x_{i,j} 
\end{array}\right]
\mathbf{H_{lower}} = \left[\begin{array}{ccccc} 
x_{1,1} &amp; c_{1,2} &amp; 0 &amp; 0 \\ 
x_{2,1} &amp; x_{2,2} &amp; x_{2,3} &amp; 0 \\ 
x_{3,1} &amp; x_{3,2} &amp; \ddots &amp; x_{3,j} \\ 
x_{4,1} &amp; x_{4,2} &amp; x_{i,3} &amp; x_{i,j} 
\end{array}\right]
\]</span></p>
<p><strong>Orthogonal Matrix</strong></p>
<p>We have described that orthogonal matrix is a matrix in which each of its column vectors results in a zero dot product with the other column vectors.</p>
<p><span class="math display">\[
A^{-1} = A^T \iff A^TA = AA^T = I
\]</span></p>
<p><span class="math display">\[
\mathbf{A^TA} = \left[\begin{array}{ccccc} 
a^t_{1}a_{1} &amp; a^t_{1}a_{2} &amp; \dots &amp; a^t_{1}a_{j} \\ 
a^t_{2}a_{1} &amp; a^t_{2}a_{2} &amp; \dots &amp; a^t_{2}a_{j} \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
a^t_{i}a_{1} &amp; a^t_{i}a_{1} &amp; \vdots &amp; a^t_{i}a_{j} 
\end{array}\right]
\]</span></p>
<p><strong>Vandermonde matrix</strong> </p>
<p>The vandermonde matrix has the following format:</p>
<p><span class="math display">\[\begin{align*}
rowwise {}&amp; \rightarrow \{1, x^1, x^2,..., x^{n-1} \} \\
columnwise &amp; \rightarrow \{1, x^1, x^2,..., x^{n-1} \}^T
\end{align*}\]</span></p>
<p>From the following equations:</p>
<p><span class="math display">\[\begin{align*}
y_0 {}&amp;= c_0 + c_1x_0^1 + c_2x_0^2 + c_3x_0^3 + ... + c_nx_0^n \\
y_1&amp; = c_0 + c_1x_1^1 + c_2x_1^2 + c_3x_1^3 + ... + c_nx_1^n \\
&amp;\vdots \\
y_n &amp;= c_0 + c_1x_n^1 + c_2x_n^2 + c_3x_n^3 + ... + c_nx_n^n
\end{align*}\]</span></p>
<p>yields the <strong>Vandermonde matrix</strong>:</p>

<p><span class="math display">\[
A = \left[\begin{array}{ccccc} 
1 &amp; x_{1,1}^1 &amp; x_{1,2}^2 &amp; \dots &amp; x_{1,n}^{n-1} \\ 
1 &amp; x_{2,1}^1 &amp; x_{2,2}^2 &amp; \dots &amp; x_{2,n}^{n-1}  \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
1 &amp; x_{n,1}^1 &amp; x_{n,2}^2 &amp; \dots &amp; x_{n,n}^{n-1} 
\end{array}\right]\ or\ 
A = \left[\begin{array}{ccccc} 
1 &amp; 1 &amp; 1 &amp; \dots &amp; 1 \\ 
x_{1,1}^1 &amp; x_{1,2}^1 &amp; x_{1,3}^2 &amp; \dots &amp; x_{1,n}^{n-1}  \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
x_{n,1}^1 &amp; x_{n,2}^1 &amp; x_{n,3}^2 &amp; \dots &amp; x_{n,n}^{n-1} 
\end{array}\right]
\]</span>
</p>
<p><strong>Jacobian Matrix</strong> </p>
<p>A matrix containing first-degree partial derivatives of a multivariate function.</p>
<p><span class="math display">\[
J = \left[\begin{array}{ccccc} 
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2}  &amp; \dots &amp; 
\frac{\partial f_1}{\partial x_n}  \\ 
\frac{\partial f_2}{\partial x_1}  &amp; \frac{\partial f_2}{\partial x_2} &amp; \dots &amp; 
\frac{\partial f_2}{\partial x_n}  \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
\frac{\partial f_m}{\partial x_1}  &amp; \frac{\partial f_m}{\partial x_2}  &amp; \dots &amp; \frac{\partial f_m}{\partial x_n} 
\end{array}\right]
\]</span></p>
<p><strong>Hessian Matrix</strong> </p>
<p>A matrix containing second-degree partial derivatives of a multivariate function.</p>
<p><span class="math display">\[
H = \left[\begin{array}{ccccc} 
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2}  &amp; \dots &amp; 
\frac{\partial^2 f}{\partial x_1 \partial x_n}  \\ 
\frac{\partial^2 f}{\partial x_2 \partial x_1}  &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \dots &amp; 
\frac{\partial^2 f}{\partial x_2 \partial x_n}  \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
\frac{\partial^2 f}{\partial x_n \partial x_1}  &amp; \frac{\partial^2 f}{\partial x_n \partial x_2}  &amp; \dots &amp; \frac{\partial^2 f}{\partial x_n^2} 
\end{array}\right]
\]</span></p>
<p><strong>Covariance and Correlation Matrix</strong>  </p>
<p>While these matrices are more commonly presented in the study of statistics and probabilities, they are nevertheless closer in representation in our study of a vector’s magnitude and direction.</p>
<p>It can be said that covariance is about the direction between variables while correlation is about the direction and magnitude between variables.</p>
<p>Below is the variance of a variable (x):</p>
<p><span class="math display">\[\begin{align}
var(x) = \sigma^2(x) = \sum(x - \bar{x})^2 \ \ \ \ \ x \in \mathbb{R}^n \label{eqn:variance}
\end{align}\]</span></p>
<p>Below is (co)variance of two variables (x,y):</p>
<p><span class="math display">\[\begin{align}
cov(x, y) = \sigma(x,y) =  \sum(x - \bar{x})(y - \bar{y}) \ \ \ \ \ x,y \in \mathbb{R}^n \label{eqn:covariance}
\end{align}\]</span></p>
<p>The covariance matrix for x,y,z variables is expressed as:</p>
<p><span class="math display">\[
\Sigma(x,y,z) = 
\left[
\begin{array}{ccc}
var(x,x) &amp; cov(x,y) &amp; cov(x,z)\\
cov(y,x) &amp; var(y,y) &amp; cov(y,z)\\
cov(z,x) &amp; cov(z,y) &amp; var(z,z)\\
\end{array}
\right] = 
\left[
\begin{array}{ccc}
\sigma(x,x) &amp; \sigma(x,y) &amp; \sigma(x,z)\\
\sigma(y,x) &amp; \sigma(y,y) &amp; \sigma(y,z)\\
\sigma(z,x) &amp; \sigma(z,y) &amp; \sigma(z,z)\\
\end{array}
\right]
\]</span>
The correlation matrix for x,y,z variables is expressed as:</p>
<p><span class="math display">\[
\rho(x,y,z) = 
\left[
\begin{array}{ccc}
\frac{var(x,x)}{\sigma^2(x)\sigma^2(x)} &amp; \frac{cov(x,y)}{\sigma^2(x)\sigma^2(y)} &amp; \frac{cov(x,z)}{\sigma^2(x)\sigma^2(z)}\\
\frac{cov(y,x)}{\sigma^2(y)\sigma^2(x)} &amp; \frac{var(y,y)}{\sigma^2(y)\sigma^2(y)} &amp; \frac{cov(y,z)}{\sigma^2(y)\sigma^2(z)}\\
\frac{cov(z,x)}{\sigma^2(z)\sigma^2(x)} &amp; \frac{cov(z,y)}{\sigma^2(z)\sigma^2(y)} &amp; \frac{var(z,z)}{\sigma^2(z)\sigma^2(z)}\\
\end{array}
\right] 
\]</span></p>
<p><strong>Cholesky-Covariance Matrix</strong> </p>
<p>This is a matrix decomposed using Cholesky decomposition against a covariance matrix, forming a lower-triangular (L) matrix. More about LU (lower-upper) decomposition discussion to follow later in this chapter.</p>
<p><span class="math display">\[
A = \left[\begin{array}{ccccc} 
x_{1,1} &amp; 0 &amp; \dots &amp; 0 \\ 
x_{2,1} &amp; x_{2,2} &amp; \dots &amp; 0 \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
x_{i,1} &amp; x_{i,2} &amp; \dots &amp; x_{i,j} 
\end{array}\right]
\]</span></p>
<p><strong>Sparse Matrix</strong> </p>
<p>A sparse matrix is a matrix that has most of its elements equal to zero. An example is a diagonal and an identity matrix. Computationally, there are methods that can be used to convert sparse matrices into structures for storage efficiency.</p>
<p><span class="math display">\[
A = \left[\begin{array}{ccccc} 
x_{1,1} &amp; 0 &amp; \dots &amp; 0 \\ 
0 &amp; x_{2,2} &amp; \dots &amp; 0 \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
0 &amp; 0 &amp; \dots &amp; x_{i,j} 
\end{array}\right]
\]</span></p>
<p><strong>Nilpotent Matrix</strong> </p>
<p>A <strong>nilpotent matrix</strong> is a matrix with the following few characteristics, among others:</p>
<ul>
<li>the determinant is zero</li>
<li>the diagonal entries are zeroes</li>
<li>the <strong>characteristic polynomial</strong> for N is <span class="math inline">\(x^n\)</span>. See <strong>Jordan form</strong> later in this chapter.</li>
</ul>
<p><strong>Unitary Matrix</strong> </p>
<p>A <strong>unitary matrix</strong> is a matrix such that its inverse equals its conjugate transpose.</p>
<p><span class="math display">\[\begin{align}
A^{-1} = A^* \label{eqn:unitary}
\end{align}\]</span></p>
<p>where * denotes conjugate transpose</p>
</div>
<div id="matrix-factorization" class="section level2">
<h2><span class="header-section-number">2.20</span> Matrix Factorization </h2>
<p>Before we jump into <strong>root finding and linear regressions</strong>, let us first cover some efficient ways to <strong>solve systems of linear equations</strong> involving matrices.</p>
<p>Given the following <strong>matrix equation</strong>:</p>
<p><span class="math display">\[\begin{align}
Ax = y \label{eqn:matrixeqn1}
\end{align}\]</span></p>
<p>we solve for x:</p>
<p><span class="math display">\[\begin{align}
A^{-1}Ax {}&amp; = A^{-1}y \label{eqn:matrixeqn2}\\
x &amp;= A^{-1}y \label{eqn:matrixeqn3}
\end{align}\]</span></p>
<p>In this chapter, we showed how to get the <strong>inverse of A</strong> by multiplying the reciprocal of its determinant by its adjugate, and then we showed how to solve the equation by performing matrix multiplication with ‘y’.</p>
<p>In this section, we cover a list of matrix decomposition methods that may help us to deal with matrices more efficiently. We start by recalling one that we have already discussed in this chapter in the context of <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong> - <strong>Eigenvalue Decomposition</strong>.</p>
<div id="eigen-spectral-decomposition-1" class="section level3">
<h3><span class="header-section-number">2.20.1</span> Eigen (Spectral) Decomposition  </h3>
<p>The idea is to decompose a <strong>matrix</strong> into its <strong>Eigenvectors</strong>, <span class="math inline">\(P\)</span>, and <strong>Eigenvalues</strong>, <span class="math inline">\(\Lambda\)</span>, forming the following equation:</p>
<p><span class="math display">\[\begin{align}
A = P\Lambda P^{-1} \label{eqn:eigendecom1}
\end{align}\]</span></p>
<p>Details of the decomposition and reconstruction of matrix is discussed in this chapter.</p>
</div>
<div id="ludecomposition" class="section level3">
<h3><span class="header-section-number">2.20.2</span> LU Decomposition (Gauss-Jordan Elimination)</h3>
<p> </p>
<p>The idea is to decompose a <strong>square matrix</strong> into its <strong>Lower-Triangular</strong> and <strong>Upper-Triangular</strong> forms, forming the equation:</p>
<p><span class="math display">\[\begin{align}
A = LU \label{eqn:ludecom1}
\end{align}\]</span></p>
<p>We transform the following sample <strong>matrix equation</strong>:</p>
<p><span class="math display">\[
\left[
\begin{array}{cccc}
a_1 &amp; a_2 &amp; a_3 &amp; a_4 \\
b_1 &amp; b_2 &amp; b_3 &amp; b_4 \\
c_1 &amp; c_2 &amp; c_3 &amp; c_4 \\
d_1 &amp; d_2 &amp; d_3 &amp; d_4 
\end{array}
\right]_{A}
\left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]_{x}
=
\left[\begin{array}{c} y_1 \\ y_2 \\ y_3 \\ y_4 \end{array}\right]_{y}
\]</span></p>
<p>into <strong>Lower and Upper Triangular</strong> form (Note that dotted entries here represent zero entries):</p>
<p><span class="math display">\[
\left[
\begin{array}{rrrr}
1 &amp; . &amp; . &amp; . \\
L_{b_1} &amp; 1 &amp; . &amp; . \\
L_{c_1} &amp; L_{c_2} &amp; 1 &amp; . \\
L_{d_1} &amp; L_{d_2} &amp; L_{d_3} &amp; 1 
\end{array}
\right]_{L_A}
\left[
\begin{array}{cccc}
U_{a_1} &amp; U_{a_2} &amp; U_{a_3} &amp; U_{a_4} \\
. &amp; U_{b_2} &amp; U_{b_3} &amp; U_{b_4} \\
. &amp; . &amp; U_{c_3} &amp; U_{c_4} \\
. &amp; . &amp; . &amp; U_{d_4} 
\end{array}
\right]_{U_A}
\left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]_{x}
=
\left[\begin{array}{c} y_1 \\ y_2 \\ y_3 \\ y_4 \end{array}\right]_{y}
\]</span></p>
<p>From here, we end up with a transformed <strong>matrix equation</strong>:</p>
<p><span class="math display">\[\begin{align}
A = LU \rightarrow LUx = y  \label{eqn:ludecom2}
\end{align}\]</span></p>
<p>And to solve for the system of equations, we have two extra formulas we can use.</p>
<p><span class="math display">\[\begin{align}
Lu_y = y, \ \ \ \ Ux = u_y \label{eqn:ludecom3}
\end{align}\]</span></p>
<p>We first solve for <span class="math inline">\(u_y\)</span> from <span class="math inline">\(Lu_y=y\)</span>, then use <span class="math inline">\(u_y\)</span> to solve for x from <span class="math inline">\(Ux=u_y\)</span>.</p>
<p><span class="math display">\[\begin{align}
u_y = L^{-1}y \rightarrow\ \ \ \ x = U^{-1}u_y \label{eqn:ludecom4}
\end{align}\]</span></p>
<p>To do that, we perform <strong>LU</strong> decomposition by <strong>Gaussian Elimination</strong> algorithm. Recall that we covered <strong>Gaussian Elimination</strong> in this chapter to reduce matrices to <strong>Echelon Form</strong>. Here, instead of just dealing with a matrix, let us transform the following system of equations:</p>
<p><span class="math display">\[
\left(\begin{array}{lll}  
1x_1 + 5x_2 + 5x_3 = 6 \\
2x_1 + 3x_2 + 4x_3 = 5 \\
3x_1 + 3x_2 + 3x_3 = 6 
 \end{array}\right)
\]</span></p>
<p>We start by first mapping the equation into a matrix form:</p>
<p><span class="math display">\[
\left[
\begin{array}{ccc}
1 &amp; 5 &amp; 5 \\
2 &amp; 3 &amp; 4 \\
3 &amp; 3 &amp; 3 \\
\end{array}
\right]_{A}
\left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}\right]_{x}
=
\left[\begin{array}{r} 6 \\ 5 \\ 6 \end{array}\right]_{y}
\]</span></p>
<p>Here, we will use an elementary matrix - the identity matrix - as a utility to help in decomposing the matrix. Let us use an augmented matrix to include vector y - so that we can validate our steps later.</p>
<p><span class="math display">\[
\left[
\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{I_A}
\left[
\begin{array}{ccc|c}
1 &amp; 5 &amp; 5 &amp; 6\\
2 &amp; 4 &amp; 5 &amp; 5\\
3 &amp; 3 &amp; 3 &amp; 6
\end{array}
\right]_{A|y} =
\left[
\begin{array}{ccc|c}
1 &amp; 5 &amp; 5 &amp; 6\\
2 &amp; 4 &amp; 5 &amp; 5\\
3 &amp; 3 &amp; 3 &amp; 6
\end{array}
\right]_{A|y}
\]</span></p>
<p>The idea is to reduce the matrix to <strong>Echelon Form</strong> using <strong>Gaussian Elimination</strong>. We work our way starting from the left-most column of the lower triangular portion of the identity matrix all the way to the right column. So, starting from the first column, we subtract 2 times of the 1st row from the 2nd row and 3 of the 1st row from the 3rd row:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|r}
1 &amp; 0 &amp; 0 \\
-2 &amp; 1 &amp; 0 \\
-3 &amp; 0 &amp; 1 
\end{array}
\right]_{l_1}
\left[
\begin{array}{rrr|r}
1 &amp; 5 &amp; 5 &amp; 6\\
2 &amp; 4 &amp; 5 &amp; 5\\
3 &amp; 3 &amp; 3 &amp; 6
\end{array}
\right]_{A} =
\left[
\begin{array}{rrr|r}
1 &amp; 5 &amp; 5 &amp; 6\\
0 &amp; -6 &amp; -5 &amp; -7\\
0 &amp; -12 &amp; -12 &amp; -12
\end{array}
\right]_{U_1|u_{y_1}}
\]</span></p>
<p>Next, we use another identity matrix as a utility matrix to solve for the 2nd column. We substract 2 of the 2nd row from the 3rd row:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; -2 &amp; 1 
\end{array}
\right]_{l_2}
\left[
\begin{array}{rrr|r}
1 &amp; 5 &amp; 5 &amp; 6\\
0 &amp; -6 &amp; -5 &amp; -7\\
0 &amp; -12 &amp; -12 &amp; -12
\end{array}
\right]_{l_1A = U_1} =
\left[
\begin{array}{rrr|r}
1 &amp; 5 &amp; 5 &amp; 6\\
0 &amp; -6 &amp; -5 &amp; -7\\
0 &amp; 0 &amp; -2 &amp; 2
\end{array}
\right]_{U_A|u_y}
\]</span></p>
<p>Since we have already achieved an upper-triangular form, <span class="math inline">\(U_A\)</span>, we don’t have to continue further. We have completed the <strong>Gaussian elimination</strong> process.</p>
<p>From this point, we need to derive the lower-triangular form, <span class="math inline">\(L_A\)</span>, using the following:</p>
<p><span class="math display">\[
L_A = (l_m \cdot l_{m-1} \cdot \ ...\ \cdot l_2 \cdot l_1)^{-1} = l_1^{-1} \cdot l_2^{-1} \cdot\ ...\  \cdot\ l_{m-1}^{-1} \cdot  l_{m}^{-1}
\]</span></p>
<p>That gives us:</p>
<p><span class="math display">\[
L_A = 
\left(
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; -2 &amp; 1 
\end{array}
\right]_{l_2}
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
-2 &amp; 1 &amp; 0 \\
-3 &amp; 0 &amp; 1 
\end{array}
\right]_{l_1}
\right)^{-1}
=
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}
\]</span></p>
<p>Overall, the <strong>LU decomposition</strong> gives us the following equation:</p>
<p><span class="math display">\[\begin{align}
A = L_AU_A \label{eqn:ludecom5}
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array}
\right]_A =
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{U_A}
\]</span></p>
<p>Using the <strong>LU</strong> matrices, let us now solve for <strong>‘x’</strong>. There are two equations we will use to solve for x. First, we solve for <span class="math inline">\(u_y\)</span> using <span class="math inline">\(L_A^{-1}y\)</span>. Then with <span class="math inline">\(u_y\)</span>, we solve for x using
Equation ().</p>
<p>Solving for <span class="math inline">\(u_y\)</span> requires <span class="math inline">\(L^{-1}\)</span> and <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
u_y = 
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
-2 &amp; 1 &amp; 0 \\
1 &amp; -2 &amp; 1 
\end{array}
\right]_{L^{-1}}
\left[\begin{array}{r} 6 \\ 5\\ 6 \end{array}\right]_{y} =
\left[\begin{array}{r} 6\\ -7 \\ 2 \end{array}\right]_{u_y}
\]</span></p>
<p>Note, in fact, that we have already solved for <span class="math inline">\(u_y\)</span> as part of the augmented matrix in the <strong>Guassian elimination</strong> portion. Let us validate in any case.</p>
<p>For that, we can use two alternative methods.</p>
<p>by <strong>Gauss forward elimination</strong>:</p>
<p><span class="math display">\[
L_A^{\{augmented\ u_y\}} = 
\left[
\begin{array}{rrr|r}
1 &amp; 0 &amp; 0 &amp; 6\\
2 &amp; 1 &amp; 0 &amp; 5\\
3 &amp; 2 &amp; 1 &amp; 6
\end{array}
\right]_{L_A|y} \rightarrow
u_y =
\left[
\begin{array}{rrr|r}
1 &amp; 0 &amp; 0 &amp; 6\\
0 &amp; 1 &amp; 0 &amp; -7\\
0 &amp; 0 &amp; 1 &amp; 2
\end{array}
\right]_{I_L|u_y}
\]</span></p>
<p>or by <strong>forward substitution</strong>:</p>
<p>Given <span class="math inline">\(L_A\)</span> and <span class="math inline">\(y\)</span>, the equation for <strong>forward substitution</strong> is:</p>
<p><span class="math display">\[\begin{align}
u_{y_1} {}&amp;= \frac{ y_1}{L_{A_{1,1}}} \leftarrow initial \label{eqn:luforward1}\\
u_{y_i} &amp;=
\frac{ \left( y_{i} - \sum_{j=1}^{i-1} L_{A_{i,j}u_{y_j}} \right)}
{L_{A_{i,i}}}, \ \ \ \ \ \text{where }i =  2,3, ... n \label{eqn:luforward2}
\end{align}\]</span></p>
<p>Finally, solving for <span class="math inline">\(x\)</span> requires <span class="math inline">\(U^{-1}\)</span> and <span class="math inline">\(u_y\)</span>:</p>
<p><span class="math display">\[
x = 
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{U_A}^{-1}
\left[\begin{array}{r} 6 \\ -7 \\ 2 \end{array}\right]_{u_y} =
\left[\begin{array}{r} 1 \\ 2 \\ -1 \end{array}\right]_{x}
\]</span></p>
<p>For that, we can use two alternative methods:</p>
<p>by <strong>Jordan backward elimination</strong>:</p>
<p><span class="math display">\[
U_A^{\{augmented\ u_y\}} = 
\left[
\begin{array}{rrr|r}
1 &amp; 5 &amp; 5 &amp; 6\\
0 &amp; -6 &amp; -5 &amp; -7 \\
0 &amp; 0 &amp; -2 &amp; 2
\end{array}
\right]_{U_A^{-1}|u_y} \rightarrow
x =
\left[
\begin{array}{rrr|r}
1 &amp; 0 &amp; 0 &amp; 1\\
0 &amp; 1 &amp; 0 &amp; 2\\
0 &amp; 0 &amp; 1 &amp; -1
\end{array}
\right]_{I_U|x}
\]</span></p>
<p>or by <strong>backward substitution</strong>:</p>
<p>Given <span class="math inline">\(U_A\)</span> and <span class="math inline">\(u_y\)</span>, the equation for <strong>backward substitution</strong> is:</p>
<p><span class="math display">\[\begin{align}
x_n {}&amp;= \frac{ u_{y_n}}{U_{A_{n,n}}} \leftarrow initial \label{eqn:luback1}\\
x_i &amp;= 
\frac{ \left(u_{y_i} - \sum_{j=i+1}^n U_{A_{i,j}x_j}\right)}
{ U_{A_{i,i}}}
, \ \ \ \ \ \text{where }i =  n-1, n-2, ..., 1 \label{eqn:luback2}
\end{align}\]</span></p>
<p>Therefore, our solution for x is:</p>
<p><span class="math display">\[
x_1 = 1,\ \ \ \ x_2 = 2, \ \ \ \ x_3 = -1
\]</span></p>
<p>For a sample implementtion of solving equations by <strong>LU decomposition</strong>, see <strong>Doolittle</strong> algorithm in next few sections.</p>
<p><strong>Gaussian Elimination with Partial Pivot (row swapping):</strong></p>
<p>Sometimes, it helps to swap rows to easily perform <strong>Gaussian elimination</strong>; especially in situations where for example we have more leading zeros in the middle of our elimination for the next rows that we need to pivot. In a case where our leading diagonal entry happens to be zero, we can swap the row with any other subsequent row that has a non-zero diagonal entry, e.g. we can swap row 2nd and row 3rd below:</p>
<p><span class="math display">\[
\left[
\begin{array}{ccc}
0 &amp; 8 &amp; 9 \\
0 &amp; 0 &amp; 6 \\
1 &amp; 2 &amp; 3 \\
\end{array}
\right]
\]</span></p>
<p>To swap 1st row with 3rd row, we use a <strong>Permutation matrix (<span class="math inline">\(P_{r_a,r_b}\)</span>)</strong> as a <strong>swap utility</strong>. See below:</p>
<p><span class="math display">\[
P_{1,3}A = \left[
\begin{array}{ccc}
0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 
\end{array}
\right]_{P_{1,3}}
\left[
\begin{array}{ccc}
0 &amp; 8 &amp; 9 \\
0 &amp; 0 &amp; 6 \\
1 &amp; 2 &amp; 3 
\end{array}
\right]_A =
\left[
\begin{array}{ccc}
1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 6 \\
0 &amp; 8 &amp; 9 
\end{array}
\right]_{A_{1,3}}
\]</span></p>
<p>To swap 2nd row with 3rd row:</p>
<p><span class="math display">\[
P_{2,3}A = \left[
\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 
\end{array}
\right]_{P_{2,3}}
\left[
\begin{array}{ccc}
1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 6 \\
0 &amp; 8 &amp; 9 
\end{array}
\right]_{A_{1,3}} =
\left[
\begin{array}{ccc}
1 &amp; 2 &amp; 3 \\
0 &amp; 8 &amp; 9 \\
0 &amp; 0 &amp; 6 
\end{array}
\right]_{A_p}
\]</span></p>
<p>Overall, we performed two swaps (permutations):</p>
<p><span class="math display">\[
P_{2,3}P_{1,3}A = A_p
\]</span></p>
<p>Now, putting the <strong>LU factorization</strong> in the picture, there are times when we need to perform <strong>partial pivoting</strong> in the middle of the operation:</p>
<p><span class="math display">\[
l_2A\ (\ P_{1,3}l_1(\ P_{1,2}A\ )\ ) = L_AU_A
\]</span>
That reads as Swap 1st and 2nd row of matrix A, then perform <strong>Guassian elimination (GE)</strong> on 1st column. Then swap the 1st and 3rd row of the resultant matrix, then coneintue with the <strong>GE</strong> on the second column.</p>
<p>Here, <strong>partial pivot</strong> may refer to the act of swapping rows then performing the <strong>GE</strong> for the row and column - where we expect the <strong>pivot</strong> to be ( the non-zero leading entry). On the other hand, <strong>full pivoting</strong> is when swapping both rows and columns.</p>
<p>On that note, see if you can get the original systems of equations above into the following matrix below using a swap utility as we discussed:</p>
<p><span class="math display">\[
\left(\begin{array}{lll}  
3x_1 + 3x_2 + 3x_3 = 6 \\
2x_1 + 3x_2 + 4x_3 = 5 \\
1x_1 + 5x_2 + 5x_3 = 6 
 \end{array}\right)
\]</span></p>
<p>There are two alternative solution for ‘x’ (see what operations to use to arrive at the solution):</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= 1,\ \ \ \ x_2 = 2, \ \ \ \ x_3 = -1\ or \\
x_1 &amp;= 1,\ \ \ \ x_2 = 1, \ \ \ \ x_3 = 0
\end{align*}\]</span></p>
<p><strong>Gaussian Elimination with Sherman-Morrison:</strong> </p>
<p>In a case where our <strong>invertible</strong> matrix is changed, for example where an entry (<strong>a rank-one</strong> entry) in the matrix is updated resulting in a new given <strong>y</strong>, call it <span class="math inline">\(y_{upd}\)</span>, we do not have to iterate through the whole process of <strong>LU factorization</strong> again. We use <strong>Sherman-Morrison</strong> formula to solve for <strong>x</strong>:</p>
<p><span class="math display">\[\begin{align}
x_1 = x_0 + \left( \frac{v^Tx_0}{1 - v^Tz} \right)z,\ \ \ 
where\ uv^T\text{ is given and }z = A^{-1}u \label{eqn:shermanmorrison1}
\end{align}\]</span></p>
<p>For example, suppose that there is an update made to our original matrix, <strong>A</strong>, in that the entry in <span class="math inline">\(A_{1,3}\)</span> (1st column, 3rd row) is changed from 1 to 3.</p>
<p><span class="math display">\[
\left[
\begin{array}{ccc|c}
3 &amp; 3 &amp; 3 &amp; 6\\
2 &amp; 4 &amp; 5 &amp;5\\
1 &amp; 5 &amp; 5 &amp; 6
\end{array}
\right]_{A|y} \rightarrow
\left[
\begin{array}{ccc|r}
3 &amp; 3 &amp; 3 &amp; 6\\
2 &amp; 4 &amp; 5 &amp; 5\\
\color{red}{3} &amp; 5 &amp; 5 &amp; 6
\end{array}
\right]_{A_{upd}|y}
\]</span></p>
<p>Suppose also that we have solved <span class="math inline">\(Ax = y\)</span> for the <strong>old x</strong> using <strong>LU factorization</strong> where:</p>
<p><span class="math display">\[
x_{old} = \left[ \begin{array}{rrr} 1 \\ 2 \\ -1 \end{array} \right]_{x_{0}}
\]</span></p>
<p>So how do we solve for the <strong>new x</strong>?</p>
<p><strong>First</strong>, let us use a <strong>rank-one</strong> update matrix expressed as <span class="math inline">\(uv^T\)</span>. A <strong>rank-one</strong> matrix means, in this case, that we are updating the 1st rank order (column-wise).</p>
<p>As an example, we use the following matrix, generated from <span class="math inline">\(uv^T\)</span>, to perform a rank-one update against the 1st column of our original matrix. The generated matrix is a representation of a perturbation in <span class="math inline">\(A_{1,3}\)</span> indicating a change in the entry from 1 to 3 (or, in fact, to change the entire 1st column where <span class="math inline">\(u\)</span> is the update vector, nx1, and <span class="math inline">\(v^T\)</span> is the rank order vector, 1xn; here, it is the rank-one or the 1st rank (column-wise)):</p>
<p><span class="math display">\[
uv^T = \left[ \begin{array}{r} 0 \\ 0 \\  -2 \\ \end{array} \right]_u 
\left[ \begin{array}{ccc} 1 &amp; 0 &amp;  0 \\ \end{array} \right]_{v^T}  =
\left[ \begin{array}{rrr} 
0 &amp; 0 &amp; 0 \\  
0 &amp; 0 &amp; 0 \\ 
-2 &amp; 0 &amp; 0  
\end{array} \right]_{uv^T}
\]</span></p>
<p><strong>Second</strong>, let us solve for z in the equation <span class="math inline">\(Az = u\)</span>.</p>
<p><span class="math display">\[
z = A^{-1}u = 
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3\\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5
\end{array}
\right]_{A}^{-1}
\left[ \begin{array}{rrr} 0 \\ 0 \\ -2 \end{array} \right]_{u} =
\left[ \begin{array}{rrr} 0.5 \\ -1.5 \\ 1.0 \end{array} \right]_{z} 
\]</span></p>
<p><strong>Third</strong>, let us now use <strong>Sherman-Morrison</strong> formula to solve for the <strong>new x</strong>,</p>
<p><span class="math display">\[
x_{new} = 
x_{old} + \left(1 - v^Tz\right)^{-1}v^Tx_{old}z =
x_{old} + \left( \frac{v^Tx_{old}}{1 - v^Tz} \right) z,
\]</span></p>
<p>for example:</p>

<p><span class="math display">\[\begin{align*}
x_{new} {}&amp;= 
\left[ \begin{array}{rrr} 1 \\ 2 \\ -1 \end{array} \right]_{x_{old}} + 
\left(
\frac{ 
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \end{array} \right]_{v^T}
\left[ \begin{array}{rrr} 1 \\ 2 \\ -1 \end{array} \right]_{x_{old}}
}
{1 - \left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \end{array} \right]_{v^T}
\left[ \begin{array}{rrr} 0.5 \\ -1.5 \\ 1.0 \end{array} \right]_{z}
}
\right)
\left[ \begin{array}{rrr} 0.5 \\ -1.5 \\ 1.0 \end{array} \right]_{z}
\\
\\
&amp;= 
\left[ \begin{array}{rrr} 1 \\ 2 \\ -1 \end{array} \right]_{x_{old}} + 
\left(\frac{1}{1 - 0.5}\right)
\left[ \begin{array}{rrr} 0.5 \\ -1.5 \\ 1.0 \end{array} \right]_{z} \\
\\
&amp;=
\left[ \begin{array}{rrr} 2 \\ -1 \\ 1 \end{array} \right]_{x_{new}}
\end{align*}\]</span>
</p>
<p>From here, we can validate the solution using the equation below, given <span class="math inline">\(uv^T\)</span> and the original <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[\begin{align}
(A - uv^T)x_{new} = y \rightarrow\ \ \
x_{new} = (A - uv^T)^{-1}y \label{eqn:shermanmorisson2}
\end{align}\]</span></p>
<p>Example:</p>

<p><span class="math display">\[
x_{new} = (A - uv^T)^{-1}y_{old} = 
\left(
\left[
\begin{array}{ccc}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array}
\right]_{A}
-
 \left[ \begin{array}{rrr} 
0 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 0 \\ 
-2 &amp; 0 &amp; 0  
\end{array} \right]_{uv^T}
 \right)^{-1}
\left[ \begin{array}{ccc} 6 \\ 5 \\  6 \\ \end{array} \right]_{y_{old}}
\]</span>
</p>
<p>We get the solution for <strong>x</strong>:</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= 2,\ \ \ \ x_2 = -1, \ \ \ \ x_3 = 1
\end{align*}\]</span></p>
<p><strong>Gaussian Elimination with Sherman-Morrison-Woodbury (SMW):</strong> </p>
<p>In <strong>Sherman-Morrison</strong> formula, we use <span class="math inline">\(uv^T\)</span> term as rank-one update matrix representing an update (a perturbation) on a specific column given vector <span class="math inline">\(v^T\)</span>, <span class="math inline">\(1 \times n\)</span>, with update values of vector <span class="math inline">\(u\)</span>, <span class="math inline">\(n \times 1\)</span>. The <strong>Woodbury</strong> formula expands on <strong>Sherman-Morrison</strong> formula in that it uses an upper <span class="math inline">\((UI)^TV^T\)</span> term to signify <strong>rank-k</strong> update matrix where <span class="math inline">\(U\)</span> has <span class="math inline">\(n \times k\)</span> dimension, <span class="math inline">\(V^T\)</span> has <span class="math inline">\(k \times n\)</span> dimension, and <span class="math inline">\(I\)</span> has <span class="math inline">\(k \times k\)</span> dimension. The formula is:</p>
<p><span class="math display">\[\begin{align}
(A - UV^T)^{-1} = A^{-1} + A^{-1}U(I - V^TA^{-1}U)^{-1}V^TA^{-1} \label{eqn:woodbury1}
\end{align}\]</span></p>
<p>where equation reduces to a <strong>Sherman-Morrison</strong> equation if the dimension of the identity matrix, I, reduces to 1x1 (where <span class="math inline">\(I=1\)</span>) and <span class="math inline">\(U\)</span> reduces to a dimension of nx1 (where <span class="math inline">\(U = u\)</span>) and <span class="math inline">\(V^T\)</span> reduces to a dimension of 1xn (where <span class="math inline">\(V=v\)</span>).</p>
<p><span class="math display">\[\begin{align}
(A - uv^T)^{-1} = A^{-1} + A^{-1}u(1 - v^TA^{-1}u)^{-1}v^TA^{-1} \label{eqn:woodbury2}
\end{align}\]</span></p>
<p>So how does <strong>SWM</strong> work?</p>
<p>Let us use the same matrix, A, but with a different perturbation. This time, the first row is multiplied by 1/3. Also, the 2nd and 3rd columns of the 2nd row are updated from 4 to 5 and 5 to 4 respectively.</p>

<p><span class="math display">\[
\left[
\begin{array}{rrr|r}
3 &amp; 3 &amp; 3 &amp; 6\\
2 &amp; 4 &amp; 5 &amp; 5\\
1 &amp; 5 &amp; 5 &amp; 6
\end{array}
\right]_{A|y_{old}} \rightarrow
\left[ 
\begin{array}{rrr|r}
\color{red}{1} &amp; \color{red}{1} &amp; \color{red}{1} &amp; 6\\
2 &amp; \color{red}{5} &amp; \color{red}{4} &amp; 5\\
1 &amp; 5 &amp; 5 &amp; 6
\end{array}
\right]_{A_{new}|y_{old}}
\]</span>
</p>
<p>Let us construct <strong>a rank-k update matrix</strong> using <span class="math inline">\(UV^T\)</span> term:</p>

<p><span class="math display">\[
UV^T = 
\left[
\begin{array}{rrr}
2 &amp; 0 &amp; 0\\
0 &amp; -1 &amp; 1\\
0 &amp; 0 &amp; 0
\end{array}
\right]_{U}
\left[
\begin{array}{rrr}
1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{V^T} = 
\left[
\begin{array}{rrr}
2 &amp; 2 &amp; 2 \\
0 &amp; -1 &amp; 1 \\
0 &amp; 0 &amp; 0
\end{array}
\right]_{UV^T}
\]</span>
</p>
<p>where K=3, N=3 so that <span class="math inline">\(U_{nxk}\)</span>, <span class="math inline">\(V_{kxn=kxn}\)</span>, and <span class="math inline">\(I_{kxk}\)</span>.</p>
<p>Derive <span class="math inline">\(z\)</span> from <span class="math inline">\(z = A^{-1}u\)</span>:</p>

<p><span class="math display">\[
z = 
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array} 
\right]_{A}^{-1} 
\left[
\begin{array}{rrr}
2 &amp; 0 &amp; 0 \\
2 &amp; -1 &amp; 1 \\
0 &amp; 0 &amp; 0 
\end{array} 
\right]_{U} 
=
\left[
\begin{array}{rrr}
1/12 &amp; 0 &amp; 0\\
1/12 &amp; 1 &amp; -1 \\
-1 &amp; -1 &amp; 1
\end{array}
\right]_{z}
\]</span>
</p>
<p>With <strong>Sherman-Morrison-Woodbury</strong> formula:</p>
<p><span class="math display">\[
x_{new} = \frac{(V^Tx_{old})_n}{(I - V^Tz)_d} = 
\left(I - V^Tz\right)_d^{-1}(V^Tx_{old})_n
\]</span></p>

<p><span class="math display">\[\begin{align*}
\left(I - V^Tz\right)_d^{-1} {}&amp;= 
\left(
\left[
\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{I}
-
 \left[ \begin{array}{rrr} 
1 &amp; 1 &amp; 1 \\ 
0 &amp; 1 &amp; 0 \\ 
0 &amp; 0 &amp; 1  
\end{array} \right]_{V^T}
\left[
\begin{array}{rrr}
1/12 &amp; 0 &amp; 0\\
1/12 &amp; 1 &amp; -1 \\
-1 &amp; -1 &amp; 1
\end{array}
\right]_{z}
\right)_d^{-1}\\
&amp;=
\left[
\begin{array}{ccc}
1/3 &amp; 0 &amp; 0 \\
-1/12 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0
\end{array}
\right]_d^{-1}
\end{align*}\]</span>
</p>
<p>Finally, solving for <span class="math inline">\(x_{new}\)</span>:</p>

<p><span class="math display">\[
x_{new} = 
\left[
\begin{array}{ccc}
1/3 &amp; 0 &amp; 0 \\
-1/12 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0
\end{array}
\right]_{d}^{-1}
\left(
\left[ \begin{array}{rrr} 
1 &amp; 1 &amp; 1 \\ 
0 &amp; 1 &amp; 0 \\ 
0 &amp; 0 &amp; 1  
\end{array} \right]_{V^T}
\left[\begin{array}{r} 1 \\ 2 \\ -1\end{array}\right]_{x_{old}}
\right)_n,
\]</span>
</p>
<p>we get the solution for <strong>x</strong>:</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= 6,\ \ \ \ x_2 = -7, \ \ \ \ x_3 = 7.
\end{align*}\]</span></p>
<p>Validate using <span class="math inline">\((A - UV^T)^{-1}y_{old}\)</span>:</p>

<p><span class="math display">\[
x_{new} = (A - UV^T)^{-1}y_{old} = 
\left(
\left[
\begin{array}{ccc}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array}
\right]_{A}
-
 \left[ \begin{array}{rrr} 
2 &amp; 2 &amp; 2 \\ 
0 &amp; -1 &amp; 1 \\ 
0 &amp; 0 &amp; 0  
\end{array} \right]_{UV^T}
 \right)^{-1}
\left[ \begin{array}{ccc} 6 \\ 5 \\  6 \\ \end{array} \right]_{y_{old}},
\]</span>
</p>
<p>we get the solution for <strong>x</strong>:</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= 6,\ \ \ \ x_2 = -7, \ \ \ \ x_3 = 7.
\end{align*}\]</span></p>
<p><strong>LU decomposition using Doolittle Algorithm:</strong> </p>
<p>One other algorithm to introduce is the <strong>Doolittle algorithm</strong> which avoids using the <strong>Gaussian Elimination</strong>.</p>
<p>Below is the notation of the algorithm, given a Matrix, <span class="math inline">\(A_{nxn}\)</span>, an initial sparse lower-triangular matrix, <span class="math inline">\(L_{nxn}\)</span>, and an initial sparse upper-triangular matrix, <span class="math inline">\(U_{nxn}\)</span>. In here, <strong>sparse</strong> denotes a matrix with entries equal to zero:</p>
<p><span class="math display">\[\begin{align*}
for\ i = 1..n\ : \\
\ \ \ \ U_{ij} {}&amp;= A_{ij} - \sum_{k=1}^i L_{ik} U_{k,j},\ for\ j = i..n \\
\\
\ \ \ \ L_{ji} &amp;= \frac{ \left( A_{ji} - \sum_{k=1}^i L_{jk} U_{k,i} \right) }{  U_{ii} }, \
\ for\ j = i..n\ and\ \ L_{jj} = 1\ \ if\ i = k
\end{align*}\]</span></p>
<p>Here is a naive implementation of the <strong>Doolittle algorithm</strong> in R code:</p>

<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1">lu_decomposition_by_doolittle &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb13-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb13-3" data-line-number="3">  u =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">*</span>n), n) <span class="co"># Sparse Matrix for Upper Triangular </span></a>
<a class="sourceLine" id="cb13-4" data-line-number="4">  l =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">*</span>n), n) <span class="co"># Sparse Matrix for Lower Triangular </span></a>
<a class="sourceLine" id="cb13-5" data-line-number="5">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb13-6" data-line-number="6">    <span class="co"># Upper Triangular Loop</span></a>
<a class="sourceLine" id="cb13-7" data-line-number="7">    <span class="cf">for</span> (j <span class="cf">in</span>  i<span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb13-8" data-line-number="8">        m_ =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb13-9" data-line-number="9">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>j) {</a>
<a class="sourceLine" id="cb13-10" data-line-number="10">            m_ =<span class="st"> </span>m_ <span class="op">+</span><span class="st"> </span>l[i,k] <span class="op">*</span><span class="st"> </span>u[k,j]</a>
<a class="sourceLine" id="cb13-11" data-line-number="11">        }</a>
<a class="sourceLine" id="cb13-12" data-line-number="12">        u[i,j] =<span class="st"> </span>A[i,j] <span class="op">-</span><span class="st"> </span>m_ </a>
<a class="sourceLine" id="cb13-13" data-line-number="13">    }</a>
<a class="sourceLine" id="cb13-14" data-line-number="14">    <span class="co"># Lower Triangular Loop</span></a>
<a class="sourceLine" id="cb13-15" data-line-number="15">    <span class="cf">for</span> (j <span class="cf">in</span> i<span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb13-16" data-line-number="16">        <span class="cf">if</span> (i <span class="op">==</span><span class="st"> </span>j) {</a>
<a class="sourceLine" id="cb13-17" data-line-number="17">            l[j,j] =<span class="st"> </span><span class="dv">1</span> <span class="co"># Diagonal Entry = 1</span></a>
<a class="sourceLine" id="cb13-18" data-line-number="18">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb13-19" data-line-number="19">            m_ =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb13-20" data-line-number="20">            <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>i) {</a>
<a class="sourceLine" id="cb13-21" data-line-number="21">                m_ =<span class="st"> </span>m_ <span class="op">+</span><span class="st"> </span>l[j,k] <span class="op">*</span><span class="st"> </span>u[k,i]</a>
<a class="sourceLine" id="cb13-22" data-line-number="22">            }</a>
<a class="sourceLine" id="cb13-23" data-line-number="23">            l[j,i] =<span class="st"> </span>( A[j,i] <span class="op">-</span><span class="st"> </span>m_ ) <span class="op">/</span><span class="st"> </span>u[i,i]</a>
<a class="sourceLine" id="cb13-24" data-line-number="24">        }</a>
<a class="sourceLine" id="cb13-25" data-line-number="25">    }</a>
<a class="sourceLine" id="cb13-26" data-line-number="26">  }</a>
<a class="sourceLine" id="cb13-27" data-line-number="27">  <span class="kw">list</span>(<span class="st">&quot;matrix&quot;</span> =<span class="st"> </span>A, <span class="st">&quot;lower&quot;</span> =<span class="st"> </span>l, <span class="st">&quot;upper&quot;</span> =<span class="st"> </span>u)</a>
<a class="sourceLine" id="cb13-28" data-line-number="28">}</a></code></pre></div>

<p>Note that the R code and algorithm do not include pivoting. The R code for the <strong>Doolittle algorithm</strong> can be modified further to include pivoting. This book does not discuss the pivoting portion of the algorithm.</p>
<p>Here is solving a system by <strong>LU decomposition</strong> for <span class="math inline">\(Ax = b\)</span>:</p>

<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb14-2" data-line-number="2">b =<span class="st"> </span><span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">5</span>,<span class="dv">6</span>)</a>
<a class="sourceLine" id="cb14-3" data-line-number="3">LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(A)  <span class="co">#derived from Doolittle section</span></a>
<a class="sourceLine" id="cb14-4" data-line-number="4">uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, b) <span class="co">#derived from REF/RREF section</span></a>
<a class="sourceLine" id="cb14-5" data-line-number="5">x =<span class="st"> </span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy) <span class="co">#derived from REF/RREF section</span></a></code></pre></div>

<p>Also, one more algorithm similar to <strong>Doolittle algorithm</strong> is the <strong>Crout’s algorithm</strong> which has diagonal entries of one for the upper triangular matrix. It may also help to be aware and understand the algorithm. This book does not discuss the details of <strong>Crout’s algorithm</strong>.</p>
</div>
<div id="ldu-factorization" class="section level3">
<h3><span class="header-section-number">2.20.3</span> LDU Factorization </h3>
<p><strong>LDU factorization</strong> is another form of <strong>LU factorization</strong>. The difference is that <strong>LDU</strong> includes a <strong>diagonal matrix</strong> derived out of decomposing the <strong>upper-triangular matrix</strong> further so that the <strong>U</strong> matrix contains a diagonal of ones.</p>

<p><span class="math display">\[
A = 
\underbrace{
\left[
\begin{array}{rrrr}
1 &amp; . &amp; . &amp; . \\
L_{b_1} &amp; 1 &amp; . &amp; . \\
L_{c_1} &amp; L_{c_2} &amp; 1 &amp; . \\
L_{d_1} &amp; L_{d_2} &amp; L_{d_3} &amp; 1 
\end{array}
\right]}_{L_A}
\underbrace{
\left[
\begin{array}{rrrr}
D_{a1}^u &amp; . &amp; . &amp; . \\
. &amp; D_{b2}^u &amp; . &amp; . \\
. &amp; . &amp; D_{c3}^u &amp; . \\
. &amp; . &amp; . &amp; D_{d4}^u
\end{array}
\right]}_{D_U}
\underbrace{
\left[
\begin{array}{cccc}
1 &amp; U_{a_2} &amp; U_{a_3} &amp; U_{a_4} \\
. &amp; 1 &amp; U_{b_3} &amp; U_{b_4} \\
. &amp; . &amp; 1 &amp; U_{c_4} \\
. &amp; . &amp; . &amp; 1 
\end{array}
\right]}_{U_A}
\]</span>
</p>
<p>For example, using the following <span class="math inline">\(A = LU\)</span>:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array}
\right]_A =
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{U_A}
\]</span>
</p>
<p>we decompose <span class="math inline">\(U_A\)</span> matrix by extracting its diagonal into a separate <strong>diagonal matrix</strong> and then replacing the diagonal of <span class="math inline">\(U_A\)</span> matrix with ones:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{U_A} \rightarrow
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; -6 &amp; 0 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{D_U}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; 1 &amp; 5/6 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{U_{new}}
\]</span>
</p>
<p>That is achieved by performing the following:</p>
<ul>
<li>Construct a <strong>diagonal matrix</strong>, <span class="math inline">\(D_U\)</span>, by extracting the diagonal entries of <span class="math inline">\(U_A\)</span>.</li>
<li>For R1, no further operation is needed because the first diagonal entry is already a one.</li>
<li>For R2, multiply R2 by -1/6; in notation, ( -1/6 * R2 <span class="math inline">\(\rightarrow\)</span> R2 ).</li>
<li>For R3, multiply R3 by -1/2; in notation, ( -1/2 * R3 <span class="math inline">\(\rightarrow\)</span> R3 ).</li>
</ul>
<p>So for the final result:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array}
\right]_A =
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; -6 &amp; 0 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{D_U}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; 1 &amp; 5/6 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{U_{new}}
\]</span>
</p>
<p>Note that <strong>LU/LDU decomposition</strong> by <strong>Gauss-Jordan Elimination</strong> works on <strong>invertible square matrices</strong>. Let us now take a look at another decomposition.</p>
</div>
<div id="qr-factorization-gram-schmidt-householder-and-givens" class="section level3">
<h3><span class="header-section-number">2.20.4</span> QR Factorization (Gram-Schmidt, Householder, and Givens) </h3>
<p>The idea is to decompose a <strong>full-rank matrix</strong> into its <strong>QR</strong> form; where <strong>Q</strong> is an <strong>orthogonal matrix</strong> and <strong>R</strong> is an <strong>upper-triangular matrix</strong>, forming the equation:</p>
<p><span class="math display">\[\begin{align}
A = QR \label{eqn:qrdecom}
\end{align}\]</span></p>
<p>We transform the following sample <strong>matrix equation</strong>:</p>

<p><span class="math display">\[
\left[
\begin{array}{cccc}
a_1 &amp; a_2 &amp; a_3 &amp; a_4 \\
b_1 &amp; b_2 &amp; b_3 &amp; b_4 \\
c_1 &amp; c_2 &amp; c_3 &amp; c_4 \\
d_1 &amp; d_2 &amp; d_3 &amp; d_4 
\end{array}
\right]_{A}
\left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]_{x}
=
\left[\begin{array}{c} y_1 \\ y_2 \\ y_3 \\ y_4 \end{array}\right]_{y}
\]</span>
</p>
<p>into <strong>QR</strong> form:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrrr}
Q_{a_1} &amp; Q_{a_2} &amp; Q_{a_3} &amp;  Q_{a_4} \\
Q_{b_1} &amp; Q_{b_2} &amp; Q_{b_3} &amp;  Q_{b_4} \\
Q_{c_1} &amp; Q_{c_2} &amp; Q_{c_3} &amp;  Q_{c_4} \\
Q_{d_1} &amp; Q_{d_2} &amp; Q_{d_3} &amp;  Q_{d_4} 
\end{array}
\right]_{Q_A}
\left[
\begin{array}{cccc}
R_{a_1} &amp; R_{a_2} &amp; R_{a_3} &amp; R_{a_4} \\
. &amp; R_{b_2} &amp; R_{b_3} &amp; R_{b_4} \\
. &amp; . &amp; R_{c_3} &amp; R_{c_4} \\
. &amp; . &amp; . &amp; R_{d_4} 
\end{array}
\right]_{R_A}
\left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]_{x}
=
\left[\begin{array}{c} y_1 \\ y_2 \\ y_3 \\ y_4 \end{array}\right]_{y}
\]</span>
</p>
<p>For a quick insight of an <strong>orthogonal matrix</strong>, <span class="math inline">\(Q\)</span>, let us use Figure  for the <strong>orthogonal projection</strong> image - left side. We cover <strong>orthogonal projection</strong> first then <strong>orthogonal reflection</strong> next.</p>
<div class="figure" style="text-align: center"><span id="fig:orthoprojectreflect"></span>
<img src="ortho_project_reflect.png" alt="Orthogonal Projection and Reflection" width="80%" />
<p class="caption">
Figure 2.22: Orthogonal Projection and Reflection
</p>
</div>
<p>The matrix representations of vectors <strong>a</strong>, <strong>b</strong>, <strong>c</strong> with <strong>p</strong> in the figure is as follows:</p>
<p><span class="math display">\[
\left[\begin{array}{cc}0 &amp; 5\\ 3 &amp; 5 \end{array}\right]_{ap}\ \ \ \ \ \ \
\left[\begin{array}{cc}1 &amp; 5\\ 4 &amp; 5 \end{array}\right]_{bp}\ \ \ \ \ \ \
\left[\begin{array}{cc}2 &amp; 5\\ 5 &amp; 5 \end{array}\right]_{cp} 
\]</span></p>
<p>The following projections apply for <strong>a</strong>, <strong>b</strong>, and <strong>c</strong> on to <strong>p</strong>:</p>

<p><span class="math display">\[\begin{align*}
a&#39; = proj_{p}a = \left(\frac{&lt;5,5&gt;^T&lt;0,3&gt;}{\|&lt;5,5&gt;\|^2}\right) &lt;5,5&gt; = \frac{15}{50} &lt;5,5&gt; = &lt;1.5,1.5&gt; \\
b&#39; = proj_{p}b = \left(\frac{&lt;5,5&gt;^T&lt;1,4&gt;}{\|&lt;5,5&gt;\|^2}\right) &lt;5,5&gt; = \frac{25}{50} &lt;5,5&gt; = &lt;2.5,2.5&gt; \\
c&#39; = proj_{p}c = \left(\frac{&lt;5,5&gt;^T&lt;2,5&gt;}{\|&lt;5,5&gt;\|^2}\right) &lt;5,5&gt; = \frac{35}{50} &lt;5,5&gt; = &lt;3.5,3.5&gt; \\
\end{align*}\]</span>
</p>
<p>The following orthogonal projections apply for <strong>a</strong>, <strong>b</strong>, and <strong>c</strong> on to <strong>p</strong>:</p>

<p><span class="math display">\[\begin{align*}
o&#39;a = a - a&#39; = &lt;0,3&gt; - &lt;1.5, 1.5&gt; = &lt;-1.5, 1.5&gt;\\
o&#39;b = b - b&#39; = &lt;1,4&gt; - &lt;2.5, 2.5&gt; = &lt;-1.5, 1.5&gt;\\
o&#39;c = c - c&#39; = &lt;2,5&gt; - &lt;3.5, 3.5&gt; = &lt;-1.5, 1.5&gt;
\end{align*}\]</span>
</p>
<p>From here, we can form a <strong>Quasi-Orthogonal matrix</strong> for each of <strong>o’a</strong>, <strong>o’b</strong>, and <strong>o’c</strong> with respect to <strong>p</strong>:</p>
<p><span class="math display">\[
\left[\begin{array}{cc}-1.5 &amp; 5\\ 1.5 &amp; 5 \end{array}\right]_{o&#39;a}\ \ \ \ \ \ \
\left[\begin{array}{cc}-1.5 &amp; 5\\ 1.5 &amp; 5 \end{array}\right]_{o&#39;b}\ \ \ \ \ \ \
\left[\begin{array}{cc}-1.5 &amp; 5\\ 1.5 &amp; 5 \end{array}\right]_{o&#39;c} 
\]</span>
Note that the matrix is only a <strong>Quasi-Orthogonal</strong> matrix. To be orthogonal, a matrix needs to meet at least the following:</p>
<p><span class="math display">\[\begin{align}
Q^{-1} = Q^T,\ \ \ Q^TQ = QQ^T = I \label{eqn:qrdecom2}
\end{align}\]</span></p>
<p>To do that, let us normalize each columns of the matrix (for now, let us use <span class="math inline">\(o&#39;a\)</span> to illustrate, though this applies to both <span class="math inline">\(o&#39;b\)</span> and <span class="math inline">\(o&#39;c\)</span> just the same):</p>
<p><span class="math display">\[\begin{align*}
q&#39;a {}&amp;= \frac{o&#39;a}{ \| o&#39;a \|_{L2}} = \frac{&lt;-1.5, 1.5&gt;}{ \|&lt;-1.5, 1.5&gt;\|} = &lt; -0.7071068, 0.7071068&gt; \\
q&#39;p &amp;= \frac{p }{ \| p \|_{L2}} = \frac{&lt;5, 5&gt;}{\|&lt;5, 5&gt;\|} = &lt; 0.7071068, 0.7071068&gt; 
\end{align*}\]</span></p>
<p>This forms the <span class="math inline">\(Q&#39;A\)</span> matrix - an <strong>orthogonal matrix</strong>:</p>
<p><span class="math display">\[
Q_A =  \left[\begin{array}{rrr}
-0.7071068 &amp; 0.7071068  \\
0.7071068 &amp; 0.7071068   
\end{array}\right] =
\left[\begin{array}{r} -0.7071068 \\ 0.7071068 \end{array}\right]_{q&#39;a}
\left[\begin{array}{r} 0.7071068 \\ 0.7071068 \end{array}\right]_{q&#39;p.}
\]</span>
To validate:</p>
<p><span class="math display">\[
Q_A^{-1} = Q_A^T,\ \ \ \ Q_A^TQ_A = Q_AQ_A^T = I.
\]</span></p>
<p>Additionally, if a matrix is a <strong>full-rank</strong> matrix, e.g. all columns are non-singular <strong>linearly independent</strong>, then an <strong>orthogonal matrix</strong> can be formed. </p>
<p>In the Figure , we sampled vectors <strong>a</strong>, <strong>b</strong>, <strong>c,</strong>, and <strong>p</strong> to form three separate <strong>orthogonal matrices</strong>. However, if we combine the vectors in one matrix, it will not form a <strong>full-rank</strong> matrix because each column is not linearly independent with the others.</p>
<p>One matrix we have used in previous <strong>QR decomposition</strong> algorithms is below which is a <strong>full-rank</strong> matrix where all columns are linearly independent:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3
\end{array}
\right]_A 
\]</span></p>
<p>In this case, we can first get the <strong>orthogonal projections</strong> of each column: </p>
<p><span class="math display">\[\begin{align*}
q_1&#39; {}&amp;= a_1 \\
q_2&#39; &amp;= a_2 - proj_{q_1&#39;}a_2 \\
q_3&#39; &amp;= a_3 - proj_{q_1&#39;}a_3 - proj_{q_2&#39;}a_3
\end{align*}\]</span></p>
<p>Equivalently, to be more in general terms, here is a list of the <strong>orthogonal basis</strong> of the projections: </p>
<p><span class="math display">\[\begin{align*}
q_1&#39; &amp;= a_1 \\
q_2&#39; &amp;= a_2 - proj_{q_1&#39;}a_2 \\
q_3&#39; &amp;= a_k - proj_{q_1&#39;}a_3 - proj_{q_2&#39;}a_3  \\
\vdots \\
q_k&#39; &amp;= a_k - proj_{q_1&#39;}a_k - proj_{q_2&#39;}a_k -\ ...\ -\  proj_{q_{k-1}&#39;}a_k   \\
\end{align*}\]</span></p>
<p>Note that the projections are subtracted from the projected vector to get the equivalent <strong>orthogonal vector (projection)</strong> perpendicular to all other <strong>orthogonal projections</strong>.</p>
<p>Finally, normalize each of the <strong>orthogonal projections</strong> to form the <strong>orthogonal matrix columns</strong>:</p>
<p><span class="math display">\[
q_1 = \frac{q_1&#39;} {\|q_1&#39;\|}\ \ \ \ \ q_2 = \frac{q_2&#39;} {\|q_2&#39;\|}\ \ \ \ \ \ 
q_3 = \frac{q_3&#39;} {\|q_3&#39;\|} \ \ \ ... \ \ \  q_k = \frac{q_k&#39;} {\|q_k&#39;\|}
\]</span></p>
<p>with that, we obtain <span class="math inline">\(Q_A\)</span> and <span class="math inline">\(R_A\)</span>:</p>
<p><span class="math display">\[
Q_A = \left[
\begin{array}{r|r|r|r|r}
q_{1a} &amp; q_{2a} &amp; q_{3a} &amp; ... &amp; q_{ka}\\
q_{1b} &amp; q_{2b} &amp; q_{3b} &amp; ... &amp; q_{kb} \\
q_{1c} &amp; q_{2c} &amp; q_{3c} &amp; ... &amp; q_{kc} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots   \\
q_{1k} &amp; q_{2k} &amp; q_{3k} &amp; ... &amp; q_{kk} \\
\end{array}
\right]\ \ \ \ \ \ \ 
R_A \leftarrow 
\begin{cases} 
r_{kj} = q_{k}^Ta_j\ \ \ if\ k\neq j \\
r_{kj} = \|q_j&#39;\|_2\ \ \ if\ k=j\ \ \ \{classic\}
\end{cases}
\]</span></p>
<p>We now illustrate <strong>QR</strong> factorization using three decomposition methods:</p>
<p><strong>Gram-Schmidt (GS) algorithm:</strong> </p>
<p>There are two versions of <strong>Gram-Schmidt</strong> algorithm: <strong>classic</strong> and <strong>modified</strong></p>
<p><span class="math display">\[
\begin{array}{l|l}
Classic\ (CGS) &amp; Modified\ (MGS) \\
------- &amp; -------- \\
\text{loop j = 1 : n} &amp;   \text{loop j = 1 : n}\\
\ \ \ \ q_j&#39; = a_j &amp; \ \ \ \ r_{jj} = \| a_j\|_{L2}\\
\ \ \ \ \text{loop k = 1 : j } &amp; \ \ \ \ q_j = a_j\ /\ \| r_{jj}   \\
\ \ \ \ \ \ \ \ \ \ r_{kj} = q_k^Tq_j&#39; &amp; \ \ \ \ \text{loop k = j : n}\\
\ \ \ \ \ \ \ \ \ \ q_j&#39; = q_j&#39; - r_{kj}q_k &amp; \ \ \ \ \ \ \ \ r_{jk} = q_j^Ta_k \\
\ \ \ \ \text{end loop} &amp; \ \ \ \ \ \ \ \ a_k = a_k - r_{jk}q_j  \\
\ \ \ \ \ r_{jj} = \| q_j&#39; \|_{L2} &amp; \ \ \ \ \text{end loop}\\
\ \ \ \ \ q_j = q_j&#39;\ /\ r_{jj} &amp; \text{end loop}\\
\text{end loop}  &amp; \\
\end{array}
\]</span></p>
<p>Let us step through the classic algorithm. Consider our previous sample matrix:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3
\end{array}
\right]_A = 
\left[\begin{array}{r} 1 \\2 \\ 3 \end{array}\right]_{a1} 
\left[\begin{array}{r} 5 \\4 \\ 3 \end{array}\right]_{a2} 
\left[\begin{array}{r} 5 \\3 \\ 3 \end{array}\right]_{a3}
\]</span>
</p>
<p><strong>First</strong>, let us handle <span class="math inline">\(q_1\)</span> and <span class="math inline">\(R_{a1}\)</span>:</p>

<p><span class="math display">\[\begin{align*}
q_1&#39; {}&amp;= a_{j=1} = &lt;1,2,3&gt;\ \ \ \leftarrow\ \ \ where\ j = 1 \\
R_{a1} &amp;= ||q_1&#39;||_{L2} = \sqrt{1^2 + 2^2 + 3^3} = \sqrt{14} \\
q_1 &amp;= \frac{1}{R_{a1}}(q_1&#39;) =  \frac{1} {\sqrt{14}}(&lt;1,2,3&gt;) \\
&amp;= \left&lt;\frac{1}{\sqrt{14}}, \frac{2}{\sqrt{14}},\frac{3}{\sqrt{14}}\right&gt;
\end{align*}\]</span>
</p>
<p><strong>Second</strong>, let us now take care of <span class="math inline">\(q_2\)</span>, <span class="math inline">\(R_{a2}\)</span>, <span class="math inline">\(R_{b2}\)</span>:</p>

<p><span class="math display">\[\begin{align*}
q_2&#39; {}&amp;= a_{j=2} = &lt;5,4,3&gt; \ \ \ \leftarrow\ \ \ where\ j = 2 \\
R_{a2} &amp;= q_1^Tq_2&#39; =  \left&lt;\frac{1}{\sqrt{14}}, \frac{2}{\sqrt{14}},\frac{3}{\sqrt{14}}\right&gt;^T
&lt;5,4,3&gt; = \sqrt{14}+ 4 \sqrt{\frac{2}{7}} \\
\\
q_2&#39;  &amp;= q_2&#39; - R_{a2}q_1 \leftarrow (I - q_1q_1^T)q_2&#39; \\
&amp;= &lt;5,4,3&gt; - \left( \sqrt{14}+ 4 \sqrt{\frac{2}{7}}\right)
\left&lt;\frac{1}{\sqrt{14}}, \frac{2}{\sqrt{14}},\frac{3}{\sqrt{14}}\right&gt; \\
&amp;= \left&lt;\frac{24}{7}, \frac{6}{7},\frac{-12}{7}\right&gt; \\
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
R_{b2} &amp;= \|q_2&#39;\|_{L2} = 
\sqrt{\left(\frac{24}{7}\right)^2 + 
\left(\frac{6}{7}\right)^2 + \left(\frac{-12}{7}\right)^2} = \frac{6\sqrt{21}}{7}
\\
q_2 &amp;= \frac{1}{R_{b2}}(q_2&#39;) = 
\frac{1}{\frac{6\sqrt{21}}{7}} \left&lt;\frac{24}{7}, \frac{6}{7},\frac{-12}{7}\right&gt;  =
\frac{7}{6\sqrt{21}}{\left&lt;\frac{24}{7}, \frac{6}{7},\frac{-12}{7}\right&gt;} \\
&amp;= \left&lt;\frac{4}{\sqrt{21}}, \frac{1}{\sqrt{21}},\frac{-2}{\sqrt{21}}\right&gt; 
\end{align*}\]</span>
</p>
<p><strong>Lastly</strong>, let us finally take care of <span class="math inline">\(q_3\)</span>, <span class="math inline">\(R_{a3}\)</span>, <span class="math inline">\(R_{b3}\)</span>, and <span class="math inline">\(R_{c3}\)</span>:</p>

<p><span class="math display">\[\begin{align*}
q_3&#39; {}&amp;= a_{j=3} = &lt;5,5,3&gt; \ \ \ \leftarrow\ \ \ where\ j = 3 \\
R_{a3} &amp;= q_1^Tq_3&#39; =  \left&lt;\frac{1}{\sqrt{14}}, \frac{2}{\sqrt{14}},\frac{3}{\sqrt{14}}\right&gt;^T
&lt;5,5,3&gt; = \sqrt{14}+ 5 \sqrt{\frac{2}{7}} \\
\\
R_{b3} &amp;= q_2^Tq_3&#39; =  \left&lt;\frac{4}{\sqrt{21}}, \frac{1}{\sqrt{21}},\frac{-2}{\sqrt{21}}\right&gt;^T
&lt;5,5,3&gt; = \frac{19}{\sqrt{21}} \\
\\
q_3&#39; &amp;= q_3&#39; - R_{a3}q_1 - R_{b3}q_2  \leftarrow (I - q_1q_1^T)(I - q_2q_2^T)q_3&#39; \\
&amp;= &lt;5,5,3&gt; - \\
&amp;\ \ \left( \sqrt{14}+ 5 \sqrt{\frac{2}{7}}\right) \left&lt;\frac{1}{\sqrt{14}}, \frac{2}{\sqrt{14}},\frac{3}{\sqrt{14}}\right&gt; -
\left(\frac{19}{\sqrt{21}}\right)
\left&lt;\frac{4}{\sqrt{21}}, \frac{1}{\sqrt{21}},\frac{-2}{\sqrt{21}}\right&gt; \\
&amp;= \left&lt;\frac{-1}{3},\frac{2}{3},\frac{-1}{3}\right&gt; \\
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
R_{c3} &amp;= \|q_3&#39;\| = \|\left&lt;\frac{-1}{3},\frac{2}{3},\frac{-1}{3}\right&gt;\| = 
\sqrt{\left(\frac{-1}{3}\right)^2,\left(\frac{2}{3}\right)^2,\left(\frac{-1}{3}\right)^2} 
=  \sqrt{\frac{2}{3}}\\
q_3 &amp;= \frac{1}{R_{c3}}(q_3&#39;) = \left(\frac{1}{\sqrt{\frac{2}{3}}}\right) \left&lt;\frac{-1}{3},\frac{2}{3},\frac{-1}{3}\right&gt; \\
&amp;= \left&lt;\frac{-1}{\sqrt{6}}, \sqrt{\frac{2}{3}}, \frac{-1}{\sqrt{6}}\right&gt;
\end{align*}\]</span>
</p>
<p>The steps above leads to the following <strong>QR</strong> form:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3
\end{array}
\right]_A =
\left(\begin{array}{rrr}
\frac{1}{\sqrt{14}} &amp; \frac{4}{\sqrt{21}} &amp; \frac{-1}{\sqrt{6}} \\ 
\frac{2}{\sqrt{14}} &amp; \frac{1}{\sqrt{21}} &amp; \sqrt{\frac{2}{3}} \\ 
\frac{3}{\sqrt{14}} &amp; \frac{-2}{\sqrt{21}} &amp; \frac{-1}{\sqrt{6}} \\
\end{array}
\right)_Q 
\left(\begin{array}{rrr}
\sqrt{14} &amp; \sqrt{14}+ 4 \sqrt{\frac{2}{7}} &amp; \sqrt{14}+ 5 \sqrt{\frac{2}{7}} \\  
. &amp; \frac{6\sqrt{21}}{7} &amp; \frac{19}{\sqrt{21}} \\ 
. &amp; . &amp; \sqrt{\frac{2}{3}}
\end{array}
\right)_R
\]</span></p>
<p>Solving for <strong>R</strong> after computing for <strong>Q</strong>, we can use the following formula (note that <span class="math inline">\(Q^{-1} = Q^T\)</span>):</p>
<p><span class="math display">\[\begin{align}
R = Q^{-1}A = Q^TA \label{eqn:gramschmidteqn}
\end{align}\]</span></p>
<p>Below are the <strong>Classic &amp; Modified Gram-Schmidt</strong> algorithms with a naive implementation in R code:</p>

<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1">qr_decomposition_by_cgs &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb15-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb15-3" data-line-number="3">  m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb15-4" data-line-number="4">  q =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, m<span class="op">*</span>n), m, <span class="dt">byrow=</span><span class="ot">TRUE</span> ) <span class="co"># Sparse Matrix for Q</span></a>
<a class="sourceLine" id="cb15-5" data-line-number="5">  r =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">*</span>n), n, <span class="dt">byrow=</span><span class="ot">TRUE</span>) <span class="co"># Sparse Matrix for R</span></a>
<a class="sourceLine" id="cb15-6" data-line-number="6">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb15-7" data-line-number="7">    qj_ =<span class="st"> </span>A[,j]</a>
<a class="sourceLine" id="cb15-8" data-line-number="8">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>j) {</a>
<a class="sourceLine" id="cb15-9" data-line-number="9">        r[k,j] =<span class="st">  </span><span class="kw">t</span>(q[,k]) <span class="op">%*%</span><span class="st"> </span>qj_</a>
<a class="sourceLine" id="cb15-10" data-line-number="10">        qj_ =<span class="st"> </span>qj_ <span class="op">-</span><span class="st"> </span>r[k,j] <span class="op">*</span><span class="st"> </span>q[,k] </a>
<a class="sourceLine" id="cb15-11" data-line-number="11">    }</a>
<a class="sourceLine" id="cb15-12" data-line-number="12">    r[j,j] =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(qj_<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb15-13" data-line-number="13">    <span class="cf">if</span> (r[j,j]<span class="op">==</span><span class="dv">0</span>) <span class="cf">break</span> <span class="co"># on linear dependency</span></a>
<a class="sourceLine" id="cb15-14" data-line-number="14">    q[,j] =<span class="st"> </span>qj_ <span class="op">/</span><span class="st"> </span>r[j,j]</a>
<a class="sourceLine" id="cb15-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb15-16" data-line-number="16">  <span class="kw">list</span>(<span class="st">&quot;A&quot;</span> =<span class="st"> </span>q <span class="op">%*%</span><span class="st"> </span>r, <span class="st">&quot;Q&quot;</span> =<span class="st"> </span>q, <span class="st">&quot;R&quot;</span> =<span class="st"> </span>r)</a>
<a class="sourceLine" id="cb15-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb15-18" data-line-number="18">qr_decomposition_by_mgs &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb15-19" data-line-number="19">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb15-20" data-line-number="20">  m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb15-21" data-line-number="21">  q =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, m<span class="op">*</span>n), m, <span class="dt">byrow=</span><span class="ot">TRUE</span> ) <span class="co"># Sparse Matrix for Q</span></a>
<a class="sourceLine" id="cb15-22" data-line-number="22">  r =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">*</span>n), n, <span class="dt">byrow=</span><span class="ot">TRUE</span>) <span class="co"># Sparse Matrix for R</span></a>
<a class="sourceLine" id="cb15-23" data-line-number="23">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb15-24" data-line-number="24">        r[j,j] =<span class="st">  </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(A[,j]<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb15-25" data-line-number="25">        <span class="cf">if</span> (r[j,j]<span class="op">==</span><span class="dv">0</span>) <span class="cf">break</span> <span class="co"># on linear dependency</span></a>
<a class="sourceLine" id="cb15-26" data-line-number="26">        q[,j] =<span class="st"> </span>A[,j] <span class="op">/</span><span class="st"> </span>r[j,j]</a>
<a class="sourceLine" id="cb15-27" data-line-number="27">        <span class="cf">for</span> (k <span class="cf">in</span> j<span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb15-28" data-line-number="28">            r[j,k] =<span class="st">   </span><span class="kw">t</span>(q[,j])  <span class="op">%*%</span><span class="st"> </span>A[,k]</a>
<a class="sourceLine" id="cb15-29" data-line-number="29">            A[,k] =<span class="st"> </span>A[,k] <span class="op">-</span><span class="st">  </span>r[j,k] <span class="op">*</span><span class="st"> </span>q[,j]</a>
<a class="sourceLine" id="cb15-30" data-line-number="30">        }</a>
<a class="sourceLine" id="cb15-31" data-line-number="31">  }</a>
<a class="sourceLine" id="cb15-32" data-line-number="32">  <span class="kw">list</span>(<span class="st">&quot;A&quot;</span> =<span class="st"> </span>q <span class="op">%*%</span><span class="st"> </span>r, <span class="st">&quot;Q&quot;</span> =<span class="st"> </span>q, <span class="st">&quot;R&quot;</span> =<span class="st"> </span>r)</a>
<a class="sourceLine" id="cb15-33" data-line-number="33">}</a>
<a class="sourceLine" id="cb15-34" data-line-number="34">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb15-35" data-line-number="35"><span class="kw">qr_decomposition_by_mgs</span>(A)</a></code></pre></div>

<p>There are other variations of the <strong>Modified Gram-Schmidt</strong> algorithm; however, the main giveaway for <strong>MGS</strong> is that it handles round-off errors seen in <strong>CGS</strong>.</p>
<p>Let us now cover the next <strong>QR decomposition</strong> algorithm - the <strong>Householder</strong> algorithm.</p>
<p><strong>Householder (Reflection) algorithm:</strong> </p>
<p>The idea is to decompose a matrix, <span class="math inline">\(A\)</span>, into the following form:</p>
<p><span class="math display">\[\begin{align}
A = Q_A \left( \begin{array}{c} R_A \\ O \end{array} \right) \label{eqn:householdereqn}
\end{align}\]</span></p>
<p>where, similar to <strong>L</strong> in <strong>LU factorization</strong> by <strong>Gaussian Elimination (GE)</strong>, <strong>Q</strong> is also a <strong>dot product</strong> accumulation of iterations in <strong>Householder</strong> algorithm.</p>
<p>Recall again <span class="math inline">\(L_A\)</span> when performing <strong>LU factorization using GE</strong>:</p>
<p><span class="math display">\[
L_A = (l_m \cdot l_{m-1} \cdot \ ...\ \cdot l_2 \cdot l_1)^{-1} = l_1^{-1} \cdot l_2^{-1} \cdot\ ...\  \cdot\ l_{m-1}^{-1} \cdot  l_{m}^{-1} 
\]</span>
Now, similarly, in <strong>Householder</strong> algorithm, we generate a set of <span class="math inline">\(H_i\)</span> (Hessenberg matrices) to eventually compose <span class="math inline">\(Q_A\)</span>.</p>
<p><span class="math display">\[
Q_A = ( H_m \cdotp H_{m-1}\ \cdotp\ . . .\ \cdotp\ H_2 \cdotp H_1)^{-1} = H_1 \cdotp H_2\ \cdotp\ . . .\ \cdotp\ H_{m-1} \cdotp\ H_m 
\]</span></p>
<p>To arrive at a set of <span class="math inline">\(H_i\)</span>, we use the following reflection formula:</p>
<p><span class="math display">\[\begin{align}
H_i = I - \frac{2v_iv_i^T}{v_i^Tv_i} \label{eqn:reflectioneqn}
\end{align}\]</span></p>
<p>where <span class="math inline">\(v_i\)</span> are reflectors of <span class="math inline">\(a_i\)</span>:</p>
<p><span class="math display">\[
v_i = a_i - sign(a_{ii})|\|a_i\|_{L2}e_i 
\]</span></p>
<p>We also can see an image of <strong>orthogonal reflection</strong> in Figure  - right side.</p>
<p>Let us use the same sample matrix, <span class="math inline">\(A\)</span>, as before and step through the <strong>Householder</strong> algorithm:</p>
<p><span class="math display">\[
A^{(1)} = A = 
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3
\end{array}
\right]_{3x3} = 
\left[\begin{array}{r} 1 \\2 \\ 3 \end{array}\right]_{a1} 
\left[\begin{array}{r} 5 \\4 \\ 3 \end{array}\right]_{a2} 
\left[\begin{array}{r} 5 \\3 \\ 3 \end{array}\right]_{a3}, \ \ \ \ where\ n=3
\]</span></p>
<p><strong>First</strong>, solve for <span class="math inline">\(H_1\)</span> by computing for <span class="math inline">\(RF_1\)</span> - reflection formula:</p>

<p><span class="math display">\[\begin{align*}
a_1 {}&amp; = \|A_{1:3,1}^{(1)}\|_{L2} = \sqrt{14} \\
\\
v_1 &amp;= a_1 - sign(A_{11}^{(1)})(a_1)(e_1) \\
&amp;= 
\left[\begin{array}{r} 1 \\2 \\ 3 \end{array}\right]_{1:3,1} - (\sqrt{14})
\left[\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\right]_{e1}
= 
\left[\begin{array}{r} -2.741657 \\ 2 \\ 3 \end{array}\right]_{v_1} \\
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
RF_1 &amp;= I - 2\left(\frac{v_1v_1^T}{v_1^Tv_1}\right)  \\
&amp;= 
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{array}
\right]_I - 2 \left(\frac{
&lt; -2.741657 , 2 , 3 &gt;&lt; -2.741657 , 2 , 3 &gt;^T
}{
&lt; -2.741657 , 2 , 3 &gt;^T&lt; -2.741657 , 2 , 3 &gt;
}\right)
\\
\\
&amp;= \left[
\begin{array}{rrr}
0.2672614 &amp; 0.5345225 &amp; 0.8017837 \\
0.5345225 &amp; 0.6100734 &amp; -0.5848899 \\
0.8017837 &amp; -0.5848899 &amp; 0.1226652
\end{array}
\right]_{RF_1} \\
\\
H_1 &amp;= RF_1  \\
A^{(2)} &amp;= H_1A^{(1)} = \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.414270 \\
0 &amp; 3.358236 &amp; 3.968310 \\
0 &amp; 2.037355 &amp; 1.452465
\end{array}
\right]_{A_2}
\end{align*}\]</span>
</p>
<p><strong>Second</strong>, solve for <span class="math inline">\(H_2\)</span> by computing for <span class="math inline">\(RF_2\)</span>:</p>

<p><span class="math display">\[\begin{align*}
a_2 {}&amp; = \|A_{2:3,2}^{(2)}\|_{L2} = 3.927921 \\
\\
v_2 &amp;= a_2 +  sign(A_{22}^{(2)})(a_2)(e_1) \\
&amp;= 
\left[\begin{array}{r} 3.358236 \\ 2.037355  \end{array}\right]_{2:3,2} - (3.927921)
\left[\begin{array}{r} 1 \\ 0  \end{array}\right]_{e1}
= 
\left[\begin{array}{r} -0.569685 \\ 2.037355 \end{array}\right]_{v_2} 
\end{align*}\]</span>
<span class="math display">\[\begin{align*}
RF_2 &amp;= I - 2\left(\frac{v_2v_2^T}{v_2^Tv_2}\right)  \\
&amp;= \left[
\begin{array}{rrr}
1 &amp; 0 \\
0 &amp; 1 \\
\end{array}
\right]_I - 2 \left(\frac{
&lt; -0.569685 , 2.037355 &gt;&lt; -0.569685 , 2.037355 &gt;^T
}{
&lt; -0.569685 , 2.037355 &gt;^T&lt; -0.569685 , 2.037355 &gt;
}\right) 
\\
\\
&amp;= \left[
\begin{array}{rrr}
0.8549653 &amp; 0.5186852 \\
0.5186852 &amp; -0.8549653 
\end{array}
\right]_{RF_2} \\
\\
H_2 &amp;= \left[
\begin{array}{rrr}
1 &amp; . \\
. &amp; RF_2 \\
\end{array}
\right] = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 0.8549653 &amp; 0.5186852 \\
0 &amp; 0.5186852 &amp; -0.8549653 
\end{array}
\right]_{H_2} \\
\\
A^{(3)} &amp;= H_2A^{(2)} = \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.4142703 \\
0 &amp; 3.927921 &amp; 4.1461394 \\
0 &amp; 0 &amp; 0.8164965 
\end{array}
\right]_{A^{(3)}}
\end{align*}\]</span>
</p>
<p><strong>Lastly</strong>, obtain <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[\begin{align*}
R {}&amp;= A^{(3)} = H_2H_1A^{(0)},\ \ \ \ \  Q = H_1H_2
\\
\\
R &amp;= \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.4142703 \\
0 &amp; 3.927921 &amp; 4.1461394 \\
0 &amp; 0 &amp; 0.8164965 
\end{array}
\right]_{R} \ \ \ \ \ \  \\
\\
Q &amp;= \left[
\begin{array}{rrr}
0.2672612 &amp; 0.8728717 &amp; -0.4082482 \\
0.5345225 &amp; 0.2182178 &amp; 0.8164967 \\
0.8017837 &amp; -0.4364360 &amp; -0.4082482 
\end{array}
\right]_{Q}
\end{align*}\]</span></p>
<p>Given a matrix, <span class="math inline">\(A_{mxn}\)</span>, let us formulate the algorithm:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{l}
Q = diag(max(m,n)) \\
R = A \\
loop\ j = 1:n \\
\ \ \ \ blk = m - j + 1 \\
\ \ \ \ if\ (blk &lt;= 1)\ break \\
\ \ \ \ e1 = zeros(blk); e1[1] = 1 \\
\ \ \ \ a = \|R_{j:m,j}\|_{L2} \\
\ \ \ \ v = R_{j:m,j} - sign(R_{jj}) * a * e1  \\
\ \ \ \ I = diag(blk) \\
\ \ \ \ H&#39; = I - constant(2 / ( v^T \cdotp v )) *  ( v \cdotp v^T ) \\
\ \ \ \ H = diag(max(m,n)) \\
\ \ \ \ H_{jm,jm} = H&#39; \\
\ \ \ \ R = HR \\
\ \ \ \ Q = QH \\
end\ loop\\
\end{array}
\end{align*}\]</span></p>
<p>Here is a naive implementation of the <strong>Householder</strong> algorithm in R code:</p>

<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1">qr_decomposition_by_householder &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb16-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb16-3" data-line-number="3">  m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb16-4" data-line-number="4">  Q =<span class="st"> </span><span class="kw">diag</span>(m)</a>
<a class="sourceLine" id="cb16-5" data-line-number="5">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb16-6" data-line-number="6">      blk =<span class="st"> </span>m <span class="op">-</span><span class="st"> </span>j <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb16-7" data-line-number="7">      <span class="cf">if</span> (blk <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>) <span class="cf">break</span></a>
<a class="sourceLine" id="cb16-8" data-line-number="8">      e1 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, blk); e1[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb16-9" data-line-number="9">      a =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(A[j<span class="op">:</span>m,j]<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb16-10" data-line-number="10">      v =<span class="st"> </span>A[j<span class="op">:</span>m,j] <span class="op">-</span><span class="st"> </span><span class="kw">sign</span>(A[j,j])  <span class="op">*</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>e1</a>
<a class="sourceLine" id="cb16-11" data-line-number="11">      I =<span class="st"> </span><span class="kw">diag</span>(blk)</a>
<a class="sourceLine" id="cb16-12" data-line-number="12">      <span class="co"># c() converts from 1x1 2D to 1D (constant)</span></a>
<a class="sourceLine" id="cb16-13" data-line-number="13">      H_ =<span class="st">  </span>I <span class="op">-</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span><span class="op">/</span>(<span class="kw">t</span>(v) <span class="op">%*%</span><span class="st"> </span>(v))) <span class="op">*</span><span class="st"> </span>(v <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(v)) </a>
<a class="sourceLine" id="cb16-14" data-line-number="14">      H =<span class="st"> </span><span class="kw">diag</span>(m) </a>
<a class="sourceLine" id="cb16-15" data-line-number="15">      H[j<span class="op">:</span>m,j<span class="op">:</span>m] =<span class="st"> </span>H_</a>
<a class="sourceLine" id="cb16-16" data-line-number="16">      A =<span class="st"> </span>H <span class="op">%*%</span><span class="st"> </span>A </a>
<a class="sourceLine" id="cb16-17" data-line-number="17">      Q =<span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span>H</a>
<a class="sourceLine" id="cb16-18" data-line-number="18">  }  </a>
<a class="sourceLine" id="cb16-19" data-line-number="19">  <span class="kw">list</span>(<span class="st">&quot;Matrix&quot;</span>=<span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span>A, <span class="st">&quot;Q&quot;</span> =<span class="st"> </span>Q, <span class="st">&quot;R&quot;</span> =<span class="st"> </span>A)</a>
<a class="sourceLine" id="cb16-20" data-line-number="20">}</a>
<a class="sourceLine" id="cb16-21" data-line-number="21">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb16-22" data-line-number="22"><span class="kw">qr_decomposition_by_householder</span>(A)</a></code></pre></div>

<p><strong>Given’s rotation algorithm:</strong> </p>
<p>The idea is to decompose a matrix, A, into the following form:</p>
<p><span class="math display">\[\begin{align}
A = QR \label{eqn:givenseqn}
\end{align}\]</span></p>
<p>For the algorithm to work, we need a transformation matrix that rotates a given 2x1 vector by the angle, <span class="math inline">\(\theta\)</span>, effectively annihilating the second item to zero.</p>
<p><span class="math display">\[
\left[
\begin{array}{rr}
cos\ \theta &amp; -sin\ \theta \\
sin\ \theta &amp; cos\ \theta \\
\end{array}
\right]_{rotate}^T
\left[ \begin{array}{rr} v_1 \\ v_2 \end{array} \right]_V =
\left[ \begin{array}{rr} r \\ 0 \end{array} \right]_{transformed}
\ \ \ \  where\ r = \sqrt{v_1^2 + v_2^2}
\]</span>
We use the below formula to compute for <span class="math inline">\(cos\ \theta\)</span> and <span class="math inline">\(\sin \theta\)</span>:</p>
<p><span class="math display">\[
cos\ \theta = \frac{v_1}{\sqrt{v_1^2 + v_2^2}}\ \ \ \ \ \ \ sin\ \theta = \frac{v_2}{\sqrt{v_1^2 + v_2^2}}
\]</span></p>
<p>To illustrate, let us use our system of equations to solve for <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[
\left(\begin{array}{lll}  
1x_1 + 5x_2 + 5x_3 = 6 \\
2x_1 + 4x_2 + 5x_3 = 5 \\
3x_1 + 3x_2 + 3x_3 = 6 \\
 \end{array}\right) \rightarrow
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 \\
\end{array}
\right]
\left[ \begin{array}{rrr} x_1 \\ x_2 \\ x_3 \end{array} \right] =
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right] 
\]</span></p>
<p>Our approach would be bottom-up, left-right, extracting each vector a time and annihilating its lower element, effectively rotating the affected rows, until we form an upper-triangular matrix:</p>
<p><span class="math display">\[\begin{align*}
\left[
\begin{array}{lll}
x_{11} &amp; x_{12} &amp; x_{13} \\
x_{21} &amp; x_{22} &amp; x_{23} \\
x_{31} &amp; x_{32} &amp; x_{33} \\
\end{array}
\right]_A {}&amp;\rightarrow \\ 
\\
\left[
\begin{array}{lll}
x_{11} &amp; x_{12} &amp; x_{13} \\
\color{blue}{\mathbf{x_{21}}} &amp; \color{blue}{\mathbf{x_{22}}} &amp; \color{blue}{\mathbf{x_{23}}} \\
\color{red}{0} &amp; \color{blue}{\mathbf{x_{32}}} &amp; \color{blue}{\mathbf{x_{33}}} \\
\end{array}
\right]_{G_1} \rightarrow 
\left[
\begin{array}{lll}
\color{blue}{\mathbf{x_{11}}} &amp; \color{blue}{\mathbf{x_{12}}} &amp; \color{blue}{\mathbf{x_{13}}} \\
\color{red}{0} &amp; \color{blue}{\mathbf{x_{22}}} &amp; \color{blue}{\mathbf{x_{23}}} \\
0 &amp; x_{32} &amp; x_{33} \\
\end{array}
\right]_{G_2} &amp;\rightarrow
\left[
\begin{array}{lll}
x_{11} &amp; x_{12} &amp; x_{13} \\
0 &amp; \color{blue}{\mathbf{x_{22}}} &amp; \color{blue}{\mathbf{x_{23}}} \\
0 &amp; \color{red}{0} &amp; \color{blue}{\mathbf{x_{33}}}\\
\end{array}
\right]_{G3}
\end{align*}\]</span></p>
<p><strong>First</strong>, we extract the vector, <span class="math inline">\(\left[ \begin{array}{rrr} x_{21} \\ x_{31} \end{array} \right] = \left[ \begin{array}{rrr} 2 \\ 3 \end{array} \right]\)</span>, to compute for <span class="math inline">\(cos\ \theta\)</span> and <span class="math inline">\(sin\ \theta\)</span> and zero out <span class="math inline">\(x_{31}\)</span>:</p>
<p><span class="math display">\[
cos\ \theta = \frac{2}{\sqrt{2^2 + 3^2}} = 0.5547002\ \ \ \ \ \ \
sin\ \theta = \frac{3}{\sqrt{2^2 + 3^2}} = 0.8320503
\]</span></p>
<p>We then improvise a transformation matrix (a given’s matrix), <span class="math inline">\(G_1\)</span> to transform matrix, A.</p>
<p><span class="math display">\[\begin{align*}
G_{1} {}&amp;=
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; cos\ \theta &amp; -sin\ \theta \\
0 &amp; sin\ \theta &amp; cos\ \theta \\
\end{array}
\right]_{utility}^T \rightarrow
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 0.5547002 &amp; -0.8320503  \\
0 &amp; 0.8320503 &amp; 0.5547002 \\
\end{array}
\right]_{rotate}^T \\
\\
A_{1} &amp;= (G_{1})^T\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 \\ 
\end{array}
\right]_A =
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_y
\end{align*}\]</span></p>
<p>We get our first transformation (annihilate lower element by rotation):</p>
<p><span class="math display">\[
A_{1} = \left[
\begin{array}{rrr}
1.000000 &amp; 5.000000 &amp; 5.000000 \\
3.605551 &amp; 4.714952 &amp; 5.269652 \\
\color{red}{0.000000} &amp; -1.664101 &amp; -2.496151 \\ 
\end{array}
\right]_{A_1} =
\left[ \begin{array}{rrr} 6.0000000 \\ 7.7658028 \\ -0.8320503 \end{array} \right]_{y_1}
\]</span></p>
<p><strong>Second</strong>, we extract the vector, <span class="math inline">\(\left[ \begin{array}{rrr} x_{11} \\ x_{21} \end{array} \right] = \left[ \begin{array}{rrr} 1.000000 \\ 3.605551 \end{array} \right]\)</span>, to compute for <span class="math inline">\(cos\ \theta\)</span> and <span class="math inline">\(sin\ \theta\)</span> and zero out <span class="math inline">\(x_{21}\)</span>:</p>
<p><span class="math display">\[
cos\ \theta = \frac{1}{\sqrt{1^2 + 3.605551^2}} = 0.2672613\ \ \ \ \ \ \
sin\ \theta = \frac{3.605551}{\sqrt{1^2 + 3.605551^2}} = 0.9636241
\]</span></p>
<p>We then improvise another transformation matrix, <span class="math inline">\(G_2\)</span>, for the transformed matrix, <span class="math inline">\(A_1\)</span>.</p>
<p><span class="math display">\[\begin{align*}
G_{2} {}&amp;= \left[
\begin{array}{rrr}
cos\ \theta &amp; -sin\ \theta &amp; 0 \\
sin\ \theta &amp; cos\ \theta &amp; 0 \\
0 &amp; 0 &amp; 1 \\
\end{array}
\right]_{utility} \rightarrow
\left[
\begin{array}{rrr}
0.2672613 &amp; -0.9636241 &amp; 0\\
0.9636241 &amp; 0.2672613 &amp; 0\\
0 &amp; 0 &amp; 1 \\
\end{array}
\right]_{rotate} \\
\\
A_{2} &amp;= (G_{2})^T
\left[
\begin{array}{rrr}
1.000000 &amp; 5.000000 &amp; 5.000000 \\
3.605551 &amp; 4.714952 &amp; 5.269652 \\
0.000000 &amp; -1.664101 &amp; -2.496151 \\ 
\end{array}
\right]_{A_1} =
\left[ \begin{array}{rrr} 6.0000000 \\ 7.7658028 \\ -0.8320503 \end{array} \right]_{y_1}
\end{align*}\]</span></p>
<p>We get our second transformation (annihilate lower element by rotation):</p>
<p><span class="math display">\[
A_{2} = \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.414270 \\
\color{red}{0.000000}  &amp; -3.557996 &amp; -3.409746 \\
\color{red}{0.000000}  &amp; -1.664101 &amp; -2.496151 \\ 
\end{array}
\right]_{A_2} =
\left[ \begin{array}{rrr} 9.0868825 \\ -3.7062460 \\ -0.8320503 \end{array} \right]_{y_2}
\]</span></p>
<p><strong>Third</strong>, we extract the vector, <span class="math inline">\(\left[ \begin{array}{rrr} x_{22} \\ x_{23} \end{array} \right] = \left[ \begin{array}{rrr} -3.557996 \\ -1.664101 \end{array} \right]\)</span>, to compute for <span class="math inline">\(cos\ \theta\)</span> and <span class="math inline">\(sin\ \theta\)</span> and zero out <span class="math inline">\(x_{23}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
cos\ \theta {}&amp;= \frac{-3.557996}{\sqrt{-3.557996^2 + -1.664101^2}} = -0.9058216\ \ \ \ \ \ \ \\
\\
sin\ \theta &amp;= \frac{-1.664101}{\sqrt{-3.557996^2 + -1.664101^2}} = -0.4236594
\end{align*}\]</span></p>
<p>We then improvise another transformation matrix, <span class="math inline">\(G_3\)</span>, for the transformed matrix, <span class="math inline">\(A_2\)</span>.</p>
<p><span class="math display">\[\begin{align*}
G_{3} {}&amp;= \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0\\
0 &amp; cos\ \theta &amp; -sin\ \theta \\
0 &amp; sin\ \theta &amp; cos\ \theta \\
\end{array}
\right]_{utility} \rightarrow
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0  \\
0 &amp; -0.9058216 &amp; 0.4236594 \\
0 &amp; -0.4236594 &amp; -0.9058216 \\
\end{array}
\right]_{rotate} \\
\\
A_{3} &amp;= (G_3)^T\left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.414270 \\
0.000000 &amp; -3.557996 &amp; -3.409746 \\
0.000000 &amp; -1.664101 &amp; -2.496151 \\ 
\end{array}
\right]_{A_2} =
\left[ \begin{array}{rrr} 9.0868825 \\ -3.7062460 \\ -0.8320503 \end{array} \right]_{y_2}
\end{align*}\]</span></p>
<p>We get our third transformation (annihilate lower element by rotation):</p>
<p><span class="math display">\[
A_{3} = \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.414270 \\
\color{red}{0.000000}  &amp; 3.927922 &amp; 4.1461398 \\
\color{red}{0.000000}  &amp; \color{red}{0.000000}  &amp; 0.8164963 \\ 
\end{array}
\right]_{A_3} =
\left[ \begin{array}{rrr} 9.0868825 \\ 3.7097037 \\ -0.8164968 \end{array} \right]_{y_3}
\]</span>
<strong>Lastly</strong>, obtain <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[\begin{align*}
R {}&amp;= G_3^TG_2^TG_1^TA,\ \ \ \ \  Q = G_1G_2G_3 
\\
\\
R &amp;= \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.4142703 \\
0 &amp; 3.927921 &amp; 4.1461394 \\
0 &amp; 0 &amp; 0.8164965 
\end{array}
\right]_{R} \ \ \ \ \ \  \\
\\
Q &amp;= \left[
\begin{array}{rrr}
0.2672612 &amp; 0.8728717 &amp; -0.4082482 \\
0.5345225 &amp; 0.2182178 &amp; 0.8164967 \\
0.8017837 &amp; -0.4364360 &amp; -0.4082482 
\end{array}
\right]_{Q}
\end{align*}\]</span></p>
<p>Given a matrix, <span class="math inline">\(A_{mxn}\)</span>, let us formulate the algorithm:</p>

<p><span class="math display">\[
\begin{array}{l|l}
\begin{array}{l}
function\ givens(v_1, v_2) \\
\ \ \ \ r = \sqrt{v_1^2 + v_2^2} \\
\ \ \ \ cos = v_1 / r \\
\ \ \ \ sin = v_2 / r \\
\ \ \ \ list&lt;cos, sin&gt; \\
end \\
\\
function\ rotate\_tool(&lt;c,s&gt;, k) \\
\ \ \ \ G = diag(m) \\
\ \ \ \ G_{k-1,k-1} = c \\
\ \ \ \ G_{k-1,k} = -s \\
\ \ \ \ G_{k,k-1} = c \\
\ \ \ \ G_{k,k} = s \\
end \\
\end{array} &amp;
\begin{array}{l}
Q = diag(m) \\
loop\ j = 1:n \\
\ \ \ \ loop\ k = m:j \\
\ \ \ \ \ \ \ \ if\ j\ &lt; k: \\ 
\ \ \ \ \ \ \ \ \ \ \ \ &lt;c,s&gt; = givens(A_{k-1,j}, A{k,j}) \\
\ \ \ \ \ \ \ \ \ \ \ \ if\ NaN(&lt;c,s&gt;)\ next\ iterate \\
\ \ \ \ \ \ \ \ \ \ \ \ G = rotate\_tool(&lt;c,s&gt;,k)  \\
\ \ \ \ \ \ \ \ \ \ \ \ A = G^TA \leftarrow \{annihilate\ lower\ element\}\\
\ \ \ \ \ \ \ \ \ \ \ \ Q = QG \\
\ \ \ \ end\ loop \\
end\ loop\\
\end{array}
\end{array}
\]</span>
</p>
<p>Here is a naive implementation of the <strong>Givens</strong> algorithm in R code:</p>

<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1">qr_decomposition_by_givens &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb17-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3">  m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb17-4" data-line-number="4">  givens &lt;-<span class="st"> </span><span class="cf">function</span>(v1, v2) {</a>
<a class="sourceLine" id="cb17-5" data-line-number="5">      r =<span class="st"> </span><span class="kw">sqrt</span>(v1<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>v2<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb17-6" data-line-number="6">      cos =<span class="st"> </span>v1 <span class="op">/</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb17-7" data-line-number="7">      sin =<span class="st"> </span>v2 <span class="op">/</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb17-8" data-line-number="8">      <span class="kw">list</span>(<span class="st">&quot;cos&quot;</span>=cos,<span class="st">&quot;sin&quot;</span>=sin)</a>
<a class="sourceLine" id="cb17-9" data-line-number="9">  }</a>
<a class="sourceLine" id="cb17-10" data-line-number="10">  rotate_tool &lt;-<span class="st"> </span><span class="cf">function</span>(cs, k) {</a>
<a class="sourceLine" id="cb17-11" data-line-number="11">      G =<span class="st"> </span><span class="kw">diag</span>(m)  </a>
<a class="sourceLine" id="cb17-12" data-line-number="12">      G[k<span class="dv">-1</span>,k<span class="dv">-1</span>] =<span class="st"> </span>cs<span class="op">$</span>cos</a>
<a class="sourceLine" id="cb17-13" data-line-number="13">      G[k<span class="dv">-1</span>,k] =<span class="st"> </span><span class="op">-</span>cs<span class="op">$</span>sin</a>
<a class="sourceLine" id="cb17-14" data-line-number="14">      G[k,k<span class="dv">-1</span>] =<span class="st"> </span>cs<span class="op">$</span>sin</a>
<a class="sourceLine" id="cb17-15" data-line-number="15">      G[k,k] =<span class="st"> </span>cs<span class="op">$</span>cos</a>
<a class="sourceLine" id="cb17-16" data-line-number="16">      G</a>
<a class="sourceLine" id="cb17-17" data-line-number="17">  }</a>
<a class="sourceLine" id="cb17-18" data-line-number="18">  Q =<span class="st"> </span><span class="kw">diag</span>(m)  </a>
<a class="sourceLine" id="cb17-19" data-line-number="19">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb17-20" data-line-number="20">      <span class="cf">for</span> (k <span class="cf">in</span> m<span class="op">:</span>j) {</a>
<a class="sourceLine" id="cb17-21" data-line-number="21">          <span class="cf">if</span> (j <span class="op">&lt;</span><span class="st"> </span>k ) {</a>
<a class="sourceLine" id="cb17-22" data-line-number="22">              cs =<span class="st"> </span><span class="kw">givens</span>(A[k<span class="dv">-1</span>,j],A[k,j])</a>
<a class="sourceLine" id="cb17-23" data-line-number="23">              <span class="cf">if</span> (<span class="kw">is.nan</span>(cs<span class="op">$</span>cos) <span class="op">||</span><span class="st"> </span><span class="kw">is.nan</span>(cs<span class="op">$</span>sin)) <span class="cf">next</span></a>
<a class="sourceLine" id="cb17-24" data-line-number="24">              G =<span class="st"> </span><span class="kw">rotate_tool</span>(cs,k)</a>
<a class="sourceLine" id="cb17-25" data-line-number="25">              A =<span class="st"> </span><span class="kw">t</span>(G) <span class="op">%*%</span><span class="st"> </span>A</a>
<a class="sourceLine" id="cb17-26" data-line-number="26">              Q =<span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span>G</a>
<a class="sourceLine" id="cb17-27" data-line-number="27">          }</a>
<a class="sourceLine" id="cb17-28" data-line-number="28">      }</a>
<a class="sourceLine" id="cb17-29" data-line-number="29">  }</a>
<a class="sourceLine" id="cb17-30" data-line-number="30">  <span class="kw">list</span>(<span class="st">&quot;Matrix&quot;</span> =<span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span>A, <span class="st">&quot;Q&quot;</span> =<span class="st"> </span>Q, <span class="st">&quot;R&quot;</span> =<span class="st"> </span>A)</a>
<a class="sourceLine" id="cb17-31" data-line-number="31">}</a>
<a class="sourceLine" id="cb17-32" data-line-number="32">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb17-33" data-line-number="33"><span class="kw">qr_decomposition_by_givens</span>(A)</a></code></pre></div>

<p>One recent evolution of Givens rotation is <strong>Generalized Givens Rotation (GGR)</strong> for <strong>QR decomposition</strong>. This topic is left for readers to follow and investigate.</p>
</div>
<div id="cholesky-factorization" class="section level3">
<h3><span class="header-section-number">2.20.5</span> Cholesky Factorization </h3>
<p>The idea is to decompose an <strong>invertible symmetric positive definite square matrix</strong> into its <span class="math inline">\(LL^T\)</span> form or <span class="math inline">\(LDL^T\)</span> form, if and only if <span class="math inline">\(U=L^T\)</span>, forming the equation:</p>
<p><span class="math display">\[\begin{align}
A = LL^T = LDL^T\ \ \ \ \ \ \ \ \ \  \text{where}\ L\ \text{is called Cholesky Factor} \label{eqn:choleskyeqn}
\end{align}\]</span></p>
<p>This has the advantage over <strong>LU factorization</strong> only if the <strong>square matrix</strong> is <strong>symmetric positive definite</strong> (a <strong>Hermitian</strong> matrix). We only work on one matrix instead of two, saving storage computationally.</p>
<p>To perform <strong>Cholesky factorization</strong>, we only need to perform the <strong>Gaussian elimination</strong> method for <strong>LU factorization</strong> to get the <strong>lower-triangular matrix</strong>.</p>
<p>For example, using <strong>LU factorization by GE</strong>, we get the following:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array}
\right]_A =
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{U_A}
\]</span></p>
<p>We use <span class="math inline">\(L_A\)</span> and its transpose:</p>
<p><span class="math display">\[
A = LL^T = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}\left[
\begin{array}{rrr}
1 &amp; 2 &amp; 3 \\
0 &amp; 1 &amp; 2 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{L_A^T}
\]</span></p>
</div>
<div id="svd-factorization" class="section level3">
<h3><span class="header-section-number">2.20.6</span> SVD Factorization </h3>
<p>The idea is to decompose a matrix, A, into its <span class="math inline">\(U\Sigma V^T\)</span> form; where <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are both <strong>orthogonal matrices</strong>, and <span class="math inline">\(\Sigma\)</span> is a <strong>diagonal matrix</strong> of <strong>Eigenvalues</strong>, forming the equation:</p>
<p><span class="math display">\[\begin{align}
A = U\Sigma V^T \label{eqn:svdeqn}
\end{align}\]</span></p>
<p>We extract the <strong>singular positive real values</strong> of a matrix into <span class="math inline">\(\Sigma\)</span>; hence, the reason for the term <strong>singular value decomposition</strong>.</p>

<p><span class="math display">\[
A_{mxn} =
\left[
\begin{array}{rrrr}
u_{11} &amp; \ldots &amp; u_{1m} \\
u_{21} &amp; \ldots &amp; u_{2m} \\
\vdots &amp; \ddots &amp; \vdots \\
u_{m1} &amp; \ldots&amp; u_{mm}\\
\end{array}
\right]_{U_{mxm}}
\left[
\begin{array}{rrrr}
\sigma_{11} &amp; . &amp; . &amp; . \\
. &amp; \sigma_{22} &amp; . &amp; . \\
. &amp; . &amp; \sigma_{33} &amp; . \\
. &amp; . &amp; . &amp; \sigma_{mn}
\end{array}
\right]_{\Sigma_{mxn}}
\left[
\begin{array}{rrrr}
v_{11}  &amp; \ldots &amp; v_{1n} \\
v_{21}  &amp; \ldots &amp; v_{2n} \\
\vdots  &amp; \ddots &amp; \vdots \\
v_{n1}  &amp; \ldots&amp; v_{nn}\\
\end{array}
\right]_{V_{nxn}}^T
\]</span>
</p>
<p>Recall the <strong>Eigen equation</strong> (), where <span class="math inline">\(v\)</span> is an <strong>Eigenvector</strong> and <span class="math inline">\(\lambda\)</span> is an <strong>Eigenvalue</strong>. We know that <span class="math inline">\(A\)</span> is a transformation matrix - it is a utility to scale the Eigenvector <span class="math inline">\(v\)</span>. We also know that <span class="math inline">\(\lambda\)</span> is a transformation scalar - it is a utility to scale the same Eigenvector <span class="math inline">\(v\)</span>. In essence, both matrices <span class="math inline">\(A\)</span> and scalar <span class="math inline">\(\lambda\)</span> do fulfill the same purpose - to scale the Eigenvector <span class="math inline">\(v\)</span>. In fact, we can say that, based on the formula, we can derive an <strong>Eigen decomposition</strong> formula:</p>
<p><span class="math display">\[\begin{align}
A = v \lambda v^{-1} = P \Lambda P^{-1} \label{eqn:eigeneqn}
\end{align}\]</span></p>
<p>Now, let us introduce related formula in the context of <strong>SVD</strong> decomposition:</p>
<p><span class="math display">\[\begin{align}
A \cdotp v = \sigma \cdotp u,\ \ \ \ \ A^T \cdotp u = \sigma \cdotp v \label{eqn:svdeqn}
\end{align}\]</span></p>
<p>We can think of matrix A as a transformation matrix such that vector <span class="math inline">\(v\)</span>, gets transformed into vector <span class="math inline">\(u\)</span> by rotation; additionally, the transformed vector <span class="math inline">\(u\)</span> is scaled by <span class="math inline">\(\sigma\)</span>. Of course, we do not have to deal with just one vector <span class="math inline">\(v\)</span> or one vector <span class="math inline">\(u\)</span>. We can deal with a matrix <span class="math inline">\(V\)</span>, a matrix <span class="math inline">\(U\)</span>, and a matrix <span class="math inline">\(\Sigma\)</span> where:</p>
<p><span class="math display">\[\begin{align*}
U_m &amp;= \{u_1, u_2, u_3,..., u_m\} \rightarrow \text{an orthogonal matrix of m column vectors of length m} \\
\Sigma_m &amp;= \{\sigma_1, \sigma_2, \sigma_3, ..., \sigma_n\} \rightarrow \text{a diagonal matrix of n positive diagonal entries} \\
V_n {}&amp;= \{v_1, v_2, v_3, ..., v_n\} \rightarrow \text{an orthogonal matrix of n column vectors of length n} \\
\end{align*}\]</span></p>
<p>With that, we can show the formula as:</p>
<p><span class="math display">\[\begin{align}
A \cdotp V = U \cdotp \Sigma,\ \ \ \ \  A^T \cdotp U = V \cdotp \Sigma \label{eqn:eigenmatrixeqn1}
\end{align}\]</span></p>
<p>And because <span class="math inline">\(V\)</span> and <span class="math inline">\(U\)</span> are orthogonal matrices, it goes to show that:</p>
<p><span class="math display">\[\begin{align}
V^T = V^{-1},
\ \ \ \ \ \ \ \ V^TV = VV^T = I,
\ \ \ \ \ \ \ \ U^T = U^{-1},
\ \ \ \ \ \ \ \ U^TU = UU^T = I \label{eqn:eigenmatrixeqn2}
\end{align}\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[\begin{align}
A = U\Sigma V^T,
\ \ \ \ \ \ \ \ \ \ \ \ A^{T} = V\Sigma^TU^T,
\ \ \ \ \ \ \ \ \ \ \ \ A^{-1} = V\Sigma^{-1}U^T  \label{eqn:eigenmatrixeqn3}
\end{align}\]</span></p>
<p>So how do we solve for <span class="math inline">\(U\)</span>, <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(V\)</span>.</p>
<p><strong>First</strong>, we perform some mathematical transformations by adding <span class="math inline">\(A^T\)</span> to both sides of the equation:</p>

<p><span class="math display">\[\begin{align}
A^TA &amp;= A^{(1)} = A^T(U\Sigma V^T) = (U\Sigma V^T)^T(U\Sigma V^T)\\
A^TA &amp;= A^{(1)} =  (V\Sigma^TU^T)(U\Sigma V^T)  = V\Sigma^TU^TU\Sigma V^T \\
A^TA &amp;= A^{(1)} =  V\Sigma^T\Sigma V^T &amp; \{V^TV = I\} \\
A^TA &amp;= A^{(1)} =  \color{blue}{V\Sigma^2V^T} &amp; \{\Sigma^T\Sigma = \Sigma^2\}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
AA^T &amp;= A^{(2)} =  (U\Sigma V^T)A^T = (U\Sigma V^T)(U\Sigma V^T)^T\\
AA^T &amp;= A^{(2)} =  (U\Sigma V^T)(V\Sigma^TU^T) = U\Sigma V^TV\Sigma^TU^T  \\
AA^T &amp;= A^{(2)} =  U\Sigma \Sigma^TU^T &amp; \{U^TU = I\}\\
AA^T &amp;= A^{(2)} =  \color{blue}{U\Sigma^2U^T} &amp; \{\Sigma\Sigma^T = \Sigma^2\}
\end{align}\]</span>
</p>
<p><strong>Second</strong>, we transform the formulas into an <strong>Eigen equation</strong> by adding <span class="math inline">\(V\)</span> and <span class="math inline">\(U\)</span> to both sides of the equation, respectively:</p>

<p><span class="math display">\[\begin{align}
A^{(1)}V &amp;= \color{blue}{V\Sigma^2V^T}V &amp; let\ A^TA = A^{(1)} \\
A^{(1)}V &amp;= V\Sigma^2 &amp; \{V^TV = I\} \\
A^{(1)}V &amp;= \Sigma^2V &amp; \{A \cdot v = \lambda \cdot v, where\ \lambda = \Sigma^2 \} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
A^{(2)}U &amp;= \color{blue}{U\Sigma^2U^T}U &amp; let\ AA^T = A^{(2)} \\
A^{(2)}U &amp;= U\Sigma^2 &amp; \{U^TU = I\} \\
A^{(2)}U &amp;= \Sigma^2U &amp; \{A \cdot u = \lambda \cdot u, where\ \lambda = \Sigma^2 \}
\end{align}\]</span>
</p>
<p><strong>Third</strong>, recall the derived equations below from <strong>Eigen Equation</strong> () and ():</p>
<p><span class="math display">\[
(A - \lambda I)v = 0 \ \ \ \ \ \ \ \ \ \leftarrow \ \ \ A v = \lambda v
\]</span></p>
<p>Equivalently, we get:</p>
<p><span class="math display">\[\begin{align}
(A^{(1)} - \Sigma^2 I)V = 0 \ \ \ \ \ \ \ \ \ \leftarrow \ \ \ A^{(1)} V = \Sigma^2 V \\
(A^{(2)} - \Sigma^2 I)U = 0 \ \ \ \ \ \ \ \ \ \leftarrow \ \ \ A^{(2)} U = \Sigma^2 U 
\end{align}\]</span></p>
<p>We solve for <span class="math inline">\(\Sigma\)</span> first by using the below <strong>characteristic equations</strong>:</p>
<p><span class="math display">\[\begin{align}
det(A^{(1)} - \Sigma^2 I) = |A^{(1)} - \Sigma^2 I| = 0 \\
det(A^{(2)} - \Sigma^2 I) = |A^{(2)} - \Sigma^2 I| = 0 
\end{align}\]</span></p>
<p>One highlight to note is that <span class="math inline">\(\Sigma\)</span> or <span class="math inline">\(\sigma\)</span> is in a positive decreasing order:</p>
<p><span class="math display">\[
\sigma^1 \geq \sigma^2 \geq \sigma^3 \geq \  ... \ \geq \sigma^{n-1} \geq q^{n} \geq 0 
\]</span></p>
<p>Once we get the <span class="math inline">\(\Sigma\)</span>, we then plug into the equations below to get <span class="math inline">\(V\)</span> and <span class="math inline">\(U\)</span>:</p>
<p><span class="math display">\[\begin{align}
(A^{(1)} - \Sigma^2 I)V = 0 \\
(A^{(2)} - \Sigma^2 I)U = 0 
\end{align}\]</span></p>
<p>From there, in the context of a linear system of equations, we can plug those matrices, <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span>, and <span class="math inline">\(\Sigma\)</span> into the equation to solve for <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\begin{align}
Ax = y \rightarrow\ \ \ \ \ \ \ x = A^{-1}y \rightarrow\ \ \ \ \ \ x = V\Sigma^{-1}U^Ty \label{eqn:eigenmatrixeqn3}
\end{align}\]</span></p>
<p>Let us use our sample matrix A to illustrate <strong>SVD</strong> decomposition.</p>
<p><strong>First</strong>, let us compose for <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span>:</p>

<p><span class="math display">\[
A = \left[\begin{array}{lll}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array}\right],\ \ \ 
A^TA = A^{(1)} = \left[\begin{array}{lll}
14 &amp; 22 &amp; 24 \\ 
22 &amp; 50 &amp; 54 \\
24 &amp; 54 &amp; 59 
\end{array}\right]_{A^{(1)}},
\]</span></p>
<p><span class="math display">\[
AA^T = A^{(2)} = \left[\begin{array}{lll}
51 &amp; 47 &amp; 33 \\
47 &amp; 45 &amp; 33 \\
33 &amp; 33 &amp; 27  
\end{array}\right]_{A^{(2)}}
\]</span>
</p>
<p><strong>Second</strong>, let us solve for <span class="math inline">\(\Sigma^2\)</span>:</p>
<p>Solving for <span class="math inline">\(\Sigma^2\)</span> based on <span class="math inline">\(A^{(1)}\)</span> and <span class="math inline">\(V\)</span>:</p>

<p><span class="math display">\[
det(A^{(1)} - \Sigma^2 I) = |A^{(1)} - \Sigma^2 I| = 0 \\
\]</span></p>
<p><span class="math display">\[
\left|
\left[\begin{array}{lll}
14 &amp; 22 &amp; 24 \\
22 &amp; 50 &amp; 54 \\
24 &amp; 54 &amp; 59 
\end{array}\right]_{A^{(1)}} - \Sigma^2 
\left[\begin{array}{lll}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}\right]_I
\right| = 
\left|
\left[\begin{array}{lll}
14-\Sigma^2 &amp; 22 &amp; 24 \\
22 &amp; 50 - \Sigma^2 &amp; 54 \\
24 &amp; 54 &amp; 59 - \Sigma^2 
\end{array}\right]
\right|
= 0
\]</span>
</p>
<p>Use the following <strong>characteristic equation</strong> of matrix <span class="math inline">\(A^{(1)}\)</span> for <span class="math inline">\(\Sigma^2\)</span>:</p>
<p><span class="math display">\[
-\Sigma^6 + 123\Sigma^4-500\Sigma^2 + 144 = 0
\]</span></p>
<p>Therefore, by computing for the <strong>roots</strong>, we get our <span class="math inline">\(\Sigma^2\)</span> and <strong>Eigen values</strong>, <span class="math inline">\(\Sigma\)</span>, for <span class="math inline">\(A^{(1)}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\Sigma^2 {}&amp;= &lt;118.80150,\ 3.88663,\ 0.31187&gt;\\
\Sigma &amp;= &lt; 10.89961, 1.971454, 0.5584532&gt;
\end{align*}\]</span></p>
<p>Let us also solve for <span class="math inline">\(\Sigma^2\)</span> based on <span class="math inline">\(A^{(2)}\)</span> and <span class="math inline">\(U\)</span> (as a validation):</p>
<p><span class="math display">\[
det(A^{(2)} - \Sigma^2 I) = |A^{(2)} - \Sigma^2 I| = 0 \\
\]</span></p>

<p><span class="math display">\[
\left|
\left[\begin{array}{lll}
51 &amp; 47 &amp; 33 \\
47 &amp; 45 &amp; 33 \\
33 &amp; 33 &amp; 27 
\end{array}\right]_{A^{(2)}} - \Sigma^2 
\left[\begin{array}{lll}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}\right]_I
\right| = 
\left|
\left[\begin{array}{lll}
51-\Sigma^2 &amp; 47 &amp; 33 \\
47 &amp; 45 - \Sigma^2 &amp; 33 \\
33 &amp; 33 &amp; 27 - \Sigma^2 
\end{array}\right]
\right|
= 0
\]</span>
</p>
<p>That takes us to the following <strong>characteristic polynomial</strong> of matrix <span class="math inline">\(A^{(2)}\)</span>for <span class="math inline">\(\Sigma^2\)</span>:</p>
<p><span class="math display">\[
p(\Sigma) = -\Sigma^6 + 123\Sigma^4-500\Sigma^2 + 144
\]</span></p>
<p>Notice that the <strong>characteristic polynomial</strong> of the matrix <span class="math inline">\(A^{(2)}\)</span> is the same as that obtained from <span class="math inline">\(A^{(1)}\)</span>. This makes the two <strong>matrices</strong> considered as <strong>similar matrices</strong>.</p>
<p><strong>Third</strong>, let us derive the <strong>Eigenvectors</strong> for <span class="math inline">\(V^{(a)}\)</span> - using <strong>Gaussian Elimination</strong>:</p>

<p><span class="math display">\[
(A^{(1)} - \Sigma^2I)V^{(a)} = 
\left(
\left[\begin{array}{lll}
14 &amp; 22 &amp; 24 \\
22 &amp; 50  &amp; 54 \\
24 &amp; 54 &amp; 59 
\end{array}\right]_{A^{(2)}} - \Sigma^2 
\left[\begin{array}{lll}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}\right]_I
\right)V
= 0
\]</span></p>
<p><span class="math display">\[\begin{align*}
v_1 {}&amp;= \left[
\begin{array}{rrr}
-104.80 &amp; 22 &amp; 24 \\
22 &amp; -68.80 &amp; 54 \\
24 &amp; 54 &amp; -59.80
\end{array}
\right] = \left[
\begin{array}{rrr}
1  &amp; 0 &amp; -0.422097 \\
0 &amp; 1 &amp; -0.919837 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
0.422097  \\ 0.919837 \\ 1
\end{array}
\right]
\\
v_2 &amp;= \left[
\begin{array}{rrr}
10.11 &amp; 22 &amp; 24 \\
22 &amp; 46.11 &amp; 54 \\
24 &amp; 54 &amp; 55.11
\end{array}
\right] \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 4.608032 \\
0 &amp; 1 &amp; -1.027396 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
-4.608032  \\ 1.027396 \\ 1
\end{array}
\right]
\\ 
v_3 &amp;= \left[
\begin{array}{rrr}
13.69 &amp; 22 &amp; 24 \\
22 &amp; 49.69 &amp; 54 \\
24 &amp; 54 &amp; 58.69
\end{array}
\right] = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0.023021 \\
0 &amp; 1 &amp; 1.076586 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
-0.023021  \\ -1.076586 \\ 1
\end{array}
\right]
\end{align*}\]</span>
</p>
<p><strong>Fourth</strong>, let us derive the <strong>Eigenvectors</strong> for <span class="math inline">\(V^{(b)}\)</span> - using <strong>Gaussian Elimination</strong>:</p>

<p><span class="math display">\[
(A^{(2)} - \Sigma^2I)U^{(a)} = 
\left(
\left[\begin{array}{lll}
51 &amp; 47 &amp; 33 \\
47 &amp; 45 &amp; 33 \\
33 &amp; 33 &amp; 27 
\end{array}\right]_{A^{(2)}} - \Sigma^2 
\left[\begin{array}{lll}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}\right]_I
\right)U
= 0
\]</span></p>
<p><span class="math display">\[\begin{align*}
u_1 {}&amp;= \left[
\begin{array}{rrr}
-67.80 &amp; 47 &amp; 33 \\
47 &amp; -73.80 &amp; 33 \\
33 &amp; 33 &amp; -91.80
\end{array}
\right] = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; -1.426354 \\
0 &amp; 1 &amp; -1.355510 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
1.426354 \\ 1.355510 \\ 1 
\end{array}
\right]
\\
u_2 &amp;= \left[
\begin{array}{rrr}
47.11 &amp; 47 &amp; 33 \\
47 &amp; 41.11 &amp; 33 \\
33 &amp; 33 &amp; 23.11
\end{array}
\right] = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0.714159 \\
0 &amp; 1 &amp; -0.013754 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
-0.714159 \\ 0.013754 \\ 1 
\end{array}
\right]
\\
u_3 &amp;= \left[
\begin{array}{rrr}
50.69 &amp; 47 &amp; 33 \\
47 &amp; 44.69 &amp; 33 \\
33 &amp; 33 &amp; 26.69
\end{array}
\right] = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; -1.358512 \\
0 &amp; 1 &amp; 2.167243 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
1.358512 \\ -2.167243 \\ 1 
\end{array}
\right]\\
\end{align*}\]</span>
</p>
<p><strong>Fifth</strong>, normalize each <strong>column vectors</strong> to convert to <strong>orthonormal</strong> matrices for <span class="math inline">\(V^{(a)}\)</span>, and <span class="math inline">\(U^{(a)}\)</span>:</p>

<p><span class="math display">\[\begin{align*}
\Sigma {}&amp;= \left[\begin{array}{rrr} 10.89961 &amp; 1.971454 &amp; 0.5584532 \end{array}\right] \cdotp I \\
\\
U^{(a)} &amp;= \left[\begin{array}{rrr} 
1.426354 &amp; -0.714159 &amp; 1.358512 \\
1.355510 &amp; 0.013754 &amp; -2.167243 \\
1 &amp; 1 &amp; 1
\end{array}\right] \\
&amp;= \left[\begin{array}{rrr} 
0.6462171 &amp; -0.58113351 &amp; 0.4946590 \\
0.6141209 &amp; 0.01119206 &amp; -0.7891327 \\
0.4530552 &amp; 0.81373127 &amp; 0.3641182
\end{array}\right]
\\
\\ 
V^{(a)} &amp;= \left[\begin{array}{rrr} 
0.422097 &amp; -4.608032 &amp; -0.023021 \\
0.919837 &amp; 1.027396 &amp; -1.076586 \\
1 &amp; 1 &amp; 1
\end{array}\right] \\
&amp;= \left[\begin{array}{rrr} 
0.2966733 &amp; -0.9548505 &amp; -0.01566538 \\
0.6465127 &amp; 0.2128912 &amp; -0.73259736 \\
0.7028558 &amp; 0.2072144 &amp; 0.68048197
\end{array}\right]
\end{align*}\]</span>
</p>
<p><strong>Finally</strong>, let us get the actual <span class="math inline">\(V\)</span> and <span class="math inline">\(U\)</span>:</p>
<p>If we use <span class="math inline">\(V^{(a)}\)</span> for <span class="math inline">\(V\)</span>, then use the following formula to get <span class="math inline">\(U\)</span>:</p>
<p><span class="math display">\[\begin{align}
u_i = \frac{A \cdot v_i}{\Sigma},
\end{align}\]</span></p>
<p>obtaining the following:</p>
<p><span class="math display">\[\begin{align*}
V^{(a)} = V {}&amp;= 
\left[\begin{array}{rrr} 
0.2966733 &amp; -0.9548505 &amp; -0.01566538 \\
0.6465127 &amp; 0.2128912 &amp; -0.73259736 \\
0.7028558 &amp; 0.2072144 &amp; 0.68048197
\end{array}\right] \\
\left[\begin{array}{r} u_1, u_2, ... u_n \end{array}\right] =
U &amp;= \left[\begin{array}{rrr} 
0.6462171 &amp; 0.58113351 &amp; -0.4946590 \\
0.6141209 &amp; -0.01119206 &amp; 0.7891327 \\
0.4530552 &amp; -0.81373127 &amp; -0.3641182
\end{array}\right]
\end{align*}\]</span></p>
<p>If we use <span class="math inline">\(U^{(a)}\)</span> for <span class="math inline">\(U\)</span>, then use the following formula to get <span class="math inline">\(V\)</span>:</p>
<p><span class="math display">\[\begin{align}
v_i = \frac{A^T \cdot u_i}{\Sigma}, 
\end{align}\]</span></p>
<p>obtaining the following matrices:</p>
<p><span class="math display">\[\begin{align*}
U^{(a)} = U {}&amp;= 
\left[\begin{array}{rrr} 
0.6462171 &amp; -0.58113351 &amp; 0.4946590 \\
0.6141209 &amp; 0.01119206 &amp; -0.7891327 \\
0.4530552 &amp; 0.81373127 &amp; 0.3641182
\end{array}\right] \\
\left[\begin{array}{r} v_1, v_2, ... v_n \end{array}\right] = 
V &amp;= 
\left[\begin{array}{rrr} 
0.2966733 &amp; 0.9548505 &amp; 0.01566538 \\
0.6465127 &amp; -0.2128912 &amp; 0.73259736 \\
0.7028558 &amp; -0.2072144 &amp; -0.68048197
\end{array}\right]
\end{align*}\]</span></p>
<p>We can validate by using Equation .</p>
<p>Here is an implementation of <strong>SVD</strong> in R code using R’s library (notice a third option produced by the library in addition to the two previous solutions we provided - also notice the discrepancy in accuracy):</p>

<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,   <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,   <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb18-2" data-line-number="2">(<span class="dt">M =</span> <span class="kw">svd</span>(A))</a></code></pre></div>
<pre><code>## $d
## [1] 10.899610  1.971455  0.558449
## 
## $u
##            [,1]        [,2]       [,3]
## [1,] -0.6462172  0.58113330  0.4946589
## [2,] -0.6141207 -0.01119169 -0.7891327
## [3,] -0.4530552 -0.81373143  0.3641183
## 
## $v
##            [,1]       [,2]        [,3]
## [1,] -0.2966734 -0.9548505  0.01566518
## [2,] -0.6465125  0.2128913  0.73259732
## [3,] -0.7028559  0.2072144 -0.68048201</code></pre>

<p>Reconstructing the original matrix, we get the following:</p>

<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1">M<span class="op">$</span>u <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(M<span class="op">$</span>d) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(M<span class="op">$</span>v)</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    5    5
## [2,]    2    4    5
## [3,]    3    3    3</code></pre>

<p>In <strong>numerical methods</strong> chapter, we discuss <strong>iterative</strong> algorithms that can be used to compute for SVD numerically for matrices with very high dimensions.</p>
<p>Apart from solving for a linear system of equations, there are other applications of <strong>SVD</strong> decomposition; to name a few: PCA (primary component analysis) and LSA (latent semantics analysis). Also in machine learning, when we are presented with a high dimensionality of features, we may need to reduce common features (or features with multicollinearity) using <strong>SVD</strong>.</p>
</div>
<div id="jordan-decomposition" class="section level3">
<h3><span class="header-section-number">2.20.7</span> Jordan Decomposition </h3>
<p>The idea is to decompose a <strong>matrix</strong> into its <span class="math inline">\(VJV^{-1}\)</span> form; where <span class="math inline">\(J\)</span> is a matrix of <strong>jordan canonical form</strong> and <span class="math inline">\(V\)</span> is an <strong>invertible</strong> matrix of <strong>eigenvectors</strong> (with possible <strong>generalized eigenvectors</strong>), forming the equation:</p>
<p><span class="math display">\[\begin{align}
A = VJV^{-1} \label{eqn:jordaneqn}
\end{align}\]</span></p>
<p>A <strong>jordan canonical form</strong> has the following:</p>
<p><span class="math display">\[
J = \left[\begin{array}{cccccc}
\lambda &amp; 1 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; \lambda  &amp; 1 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 0 &amp; \lambda &amp; 1&amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp;\ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \lambda &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \lambda
\end{array}\right]
\]</span></p>
<p>Recall <strong>rank and nullity</strong> in this chapter. We described <strong>rank</strong> as the dimension of the column space; complementary of which is the <strong>nullity</strong> which is the dimension of the null space. We illustrate <strong>Jordan</strong> decomposition around these two spaces, but more importantly around the <strong>Eigen equation</strong> ().</p>
<p>Let us manipulate the equation to reduce to the following format (note that <span class="math inline">\(\mathbf{\vec{v}}\)</span> is expressed on the basis of a generalized eigenvector):</p>
<p><span class="math display">\[
\therefore A_\lambda v = 0\ \ \ \ \leftarrow\ \ \ \ \ where\ (A - \lambda I) = A_{\lambda} 
\]</span>
In this case, <span class="math inline">\(A_\lambda\)</span> is what we call the <strong>characteristic matrix</strong>.</p>
<p>We are interested in the <strong>Eigenspace</strong> - the <strong>Nullspace</strong> of the <strong>characteristic matrix</strong> (See Equation ).</p>
<p>Now to illustrate <strong>Jordan decomposition</strong>, having the <strong>characteristic matrix</strong> format in mind, and given a 4x4 matrix, let us construct several samples (or cases) of <strong>Jordan form</strong> first.</p>
<p><strong>First Case</strong>: Suppose that the matrix is <strong>rank-three</strong>. The <strong>nullity is one</strong> in this case. So forming the equations:</p>

<p><span class="math display">\[
\begin{array}{ccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \  A \cdotp v_1 = \lambda \times v_1\\
A_\lambda^2 v_2 = 0 \rightarrow\ \   A \cdotp v_2 = v_1 + \lambda \times v_2\\
A_\lambda^3 v_3 = 0 \rightarrow\ \  A \cdotp v_3 = v_2 + \lambda \times v_3 \\
A_\lambda^4 v_4 = 0 \rightarrow\ \   A \cdotp v_4 = v_3 + \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{ccccc}
\lambda &amp; 1  &amp; . &amp; .  \\
. &amp; \lambda  &amp; 1 &amp; . \\
. &amp; . &amp; \lambda  &amp; 1  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right]=
\left[\begin{array}{ccccc} 
\color{red}{\square} &amp; \color{red}{\square}  &amp; \color{red}{\square} &amp; \color{red}{\square}  \\
\color{red}{\square} &amp;  . &amp; . &amp; \color{red}{\square} \\
\color{red}{\square} &amp; . &amp;  .  &amp; \color{red}{\square} \\
\color{red}{\square} &amp; \color{red}{\square}  &amp; \color{red}{\square} &amp; \color{red}{\square} \\
\end{array}\right]_{nxn}
\end{array}
\end{array}
\]</span>
</p>
<p>We have one full block of size four (four <strong>Eigenvectors</strong>).</p>
<p>Here we have assumed that <span class="math inline">\(\lambda\)</span> has an <strong>algebraic multiplicity</strong> of four, e.g. <span class="math inline">\((\lambda - 1)^4\)</span>, in the <strong>Eigenspace</strong>, but the <strong>geometric multiplicity</strong> is one, e.g. <span class="math inline">\(dim(N(\lambda - 1)) = 1\)</span>. Because of that, the <strong>jordan form</strong> has only one full <strong>jordan block</strong> with one <strong>standard eigenvector</strong>, <span class="math inline">\(v_1\)</span>, and three non-zero <strong>generalized eigenvectors</strong>, <span class="math inline">\(v_2, v_3, v_4\)</span>. If a <strong>jordan block</strong> contains <strong>generalized eigenvectors</strong> it means there is a chain in the block.</p>
<p><span class="math display">\[
(v, A_\lambda v, A_\lambda^2 v, A_\lambda^3 v, ..., A_\lambda^{n-1} v)
\]</span></p>
<p>In the example above, we notice <span class="math inline">\(A_\lambda^4 v_4 = 0\)</span>. That is derived as such:</p>
<p><span class="math display">\[\begin{align*}
A \cdotp v_4 {}&amp;= v_3 + \lambda \times v_4 \\
(A - \lambda I) \cdotp v_4 &amp;= v_3 \leftarrow let\ (A - \lambda I) = A_\lambda\\
A_\lambda \cdotp v_4 &amp;= v_3 \leftarrow \ \ \ A_\lambda v_4 \neq 0\ \ \text{\{ not in Eigenspace}\}\\
A_\lambda^2 \cdotp v_4 &amp;= A_\lambda v_3 = v_2 \leftarrow \ \ \ A_\lambda^2 v_4 \neq 0\ \ \ \text{\{ add }A_\lambda\text{ to each side \}} \\
A_\lambda^3 \cdotp v_4 &amp;= A_\lambda^2 v_3 = A_\lambda v_2 = v_1 \leftarrow \ \ \ A_\lambda^3 v_4 \neq 0 \\
A_\lambda^4 \cdotp v_4 &amp;= A_\lambda^3 v_3 = A_\lambda^2 v_2 = A_\lambda v_1 = 0 \leftarrow \ \ \ A_\lambda^4 v_4 = 0 \\
\end{align*}\]</span></p>
<p><strong>Second Case</strong>: Suppose that the matrix is <strong>rank-two</strong> instead. The <strong>nullity is two</strong> in this case. So forming the equations:</p>

<p><span class="math display">\[
\begin{array}{cccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \  A \cdotp v_1 = \lambda \times v_1\\
A_\lambda v_2 = 0 \rightarrow\ \   A \cdotp v_2 = \lambda \times v_2\\
A_\lambda^2 v_3 = 0 \rightarrow\ \   A \cdotp v_3 = v_2 + \lambda \times v_3 \\
A_\lambda^3 v_4 = 0 \rightarrow\ \  A \cdotp v_4 = v_3 + \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{llll}
\lambda &amp; .  &amp; . &amp; .  \\
. &amp; \lambda  &amp; 1 &amp; . \\
. &amp; . &amp; \lambda  &amp; 1  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right] =
\left[\begin{array}{ccccc}
\color{red}{\square} &amp; .  &amp; . &amp; .  \\
. &amp; \color{green}{\square}  &amp; \color{green}{\square} &amp; \color{green}{\square} \\
. &amp; \color{green}{\square} &amp; .  &amp; \color{green}{\square}  \\
. &amp; \color{green}{\square} &amp; \color{green}{\square} &amp; \color{green}{\square} \\
\end{array}\right]_{nxn}
\end{array}
\end{array}
\]</span>
</p>
<p>We have two <strong>Jordan blocks</strong>. The first block has size 1. The other block has a size three.</p>
<p>If suppose we have the following equations, still for <strong>rank-two</strong>, then we form a different system of equations:</p>

<p><span class="math display">\[
\begin{array}{ccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ A \cdotp v_1 = \lambda_1 \times v_1\\
A_\lambda^2 v_2 = 0 \rightarrow\ A \cdotp v_2 = v_1 + \lambda_1 \times v_2\\
A_\lambda v_3 = 0 \rightarrow\ A \cdotp v_3 = \lambda_2 \times v_3 \\
A_\lambda^2 v_4 = 0 \rightarrow\ A \cdotp v_4 = v_3 + \lambda_2 \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{llll}
\lambda_1 &amp; 1  &amp; . &amp; .\\
. &amp; \lambda_1  &amp; . &amp; .\\
. &amp; . &amp; \lambda_2  &amp; 1\\
. &amp; . &amp; . &amp; \lambda_2\\
\end{array}\right] =
\left[\begin{array}{ccccc}
\color{red}{\square} &amp; \color{red}{\square}   &amp; . &amp; .  \\
\color{red}{\square}  &amp; \color{red}{\square}  &amp; . &amp; . \\
.  &amp; . &amp; \color{green}{\square}  &amp; \color{green}{\square}  \\
. &amp; . &amp; \color{green}{\square} &amp; \color{green}{\square} \\
\end{array}\right]_{nxn}
\end{array}
\end{array}
\]</span>
</p>
<p>We still have two <strong>Jordan blocks</strong>. Each block size has size two. This could be a case where we have two <span class="math inline">\(\lambda s\)</span>, e.g. <span class="math inline">\((\lambda-1)^2(\lambda-2)^2\)</span>; but each <span class="math inline">\(\lambda\)</span> may only have one <strong>geometric multiplicity</strong>; thus may need one <strong>generalized eigenvector</strong> for each <span class="math inline">\(\lambda\)</span>.</p>
<p><strong>Third Case</strong>: Suppose that the matrix is <strong>rank-one</strong>. The <strong>nullity is three</strong> in this case. So forming the equations:</p>

<p><span class="math display">\[
\begin{array}{ccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \  A \cdotp v_1 =  \lambda \times v_1\\
A_\lambda v_2 = 0 \rightarrow\ \  A \cdotp v_2 = \lambda \times v_2\\
A_\lambda v_3 = 0 \rightarrow\ \   A \cdotp v_3 = \lambda \times v_3 \\
A_\lambda^2 v_4 = 0 \rightarrow\ \   A \cdotp v_4 = v_3 + \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{ccccc}
\lambda &amp; .  &amp; . &amp; .  \\
. &amp; \lambda  &amp; . &amp; . \\
. &amp; . &amp; \lambda  &amp; 1  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right] =
\left[\begin{array}{ccccc}
\color{red}{\square} &amp; .  &amp; . &amp; .  \\
. &amp; \color{green}{\square}  &amp; . &amp; . \\
. &amp; . &amp; \color{blue}{\square}  &amp; \color{blue}{\square} \\
. &amp; . &amp; \color{blue}{\square} &amp; \color{blue}{\square} \\
\end{array}\right]_{nxn}
\end{array}
\end{array}
\]</span>
</p>
<p>We have three <strong>Jordan blocks</strong>. The first two blocks have size one. The last block has size two.</p>
<p>If suppose we have the following equations, still for <strong>rank-one</strong>, then we form a different system of equations:</p>

<p><span class="math display">\[
\begin{array}{ccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \  A \cdotp v_1 =  \lambda \times v_1\\
A_\lambda^2 v_2 = 0 \rightarrow\ \   A \cdotp v_2 = v1 + \lambda \times v_2\\
A_\lambda v_3 = 0 \rightarrow\ \   A \cdotp v_3 = \lambda \times v_3 \\
A_\lambda^2 v_4 = 0 \rightarrow\ \  A \cdotp v_4 = \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{ccccc}
\lambda &amp; 1  &amp; . &amp; .  \\
. &amp; \lambda  &amp; . &amp; . \\
. &amp; . &amp; \lambda  &amp; .  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right] =
\left[\begin{array}{ccccc}
\color{blue}{\square}  &amp; \color{blue}{\square} &amp; . &amp; .\\
\color{blue}{\square} &amp; \color{blue}{\square} &amp; . &amp; .\\
.  &amp; . &amp;  \color{red}{\square} &amp; .\\
. &amp; .  &amp; . &amp; \color{green}{\square}\\
\end{array}\right]_{nxn}
\end{array}
\end{array}
\]</span>
</p>
<p>We still have three <strong>Jordan blocks</strong>. The first block has size two, the two other blocks have size 1 each.</p>
<p><strong>Fourth Case</strong>: Suppose that the matrix is <strong>rank-zero</strong> (all elements are zero). The <strong>nullity is four</strong> in this case. So forming the equations:</p>

<p><span class="math display">\[
\begin{array}{ccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \  A \cdotp v_1 = \lambda \times v_1\\
A_\lambda v_2 = 0 \rightarrow\ \  A \cdotp v_2 = \lambda \times v_2\\
A_\lambda v_3 = 0 \rightarrow\ \  A \cdotp v_3 = \lambda \times v_3 \\
A_\lambda v_4 = 0 \rightarrow\ \  A \cdotp v_4 = \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{ccccc}
\lambda &amp; .  &amp; . &amp; .  \\
. &amp; \lambda  &amp; . &amp; . \\
. &amp; . &amp; \lambda  &amp; .  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right] = 
\left[\begin{array}{ccccc}
\color{red}{\square} &amp; .  &amp; . &amp; .  \\
. &amp; \color{green}{\square}  &amp; . &amp; . \\
. &amp; . &amp; \color{blue}{\square}  &amp; .  \\
. &amp; . &amp; . &amp; \color{brown}{\square} \\
\end{array}\right]_{nxn}
\end{array}
\end{array}
\]</span>
</p>
<p>We have four Jordan blocks. Each block has size 1.</p>
<p>The key point to all this is about the choice of which case to use that dictates the set of equations and thus the number of <strong>Jordan blocks</strong> that determine the <strong>Jordan form</strong> to use.</p>
<p>To know the number of <strong>Jordan blocks</strong>, we need to work on the <strong>Eigenspace</strong> of the <strong>characteristic matrix</strong>: <span class="math inline">\(N(A - \lambda I)\)</span></p>
<p>Let us illustrate that by first evaluating the <strong>RREF</strong> (reduced row echelon form) of the <strong>characteristic matrix</strong> for the <strong>free variables</strong> (Note here that we work on the <strong>Nullspace</strong>, <span class="math inline">\(N(A -\lambda I) \leftarrow (A - \lambda I)v = 0\)</span>:</p>
<p><span class="math display">\[
A = \left[\begin{array}{cccc}
1 &amp; 1 &amp; 9 &amp; 0\\
0 &amp; 4 &amp; 3 &amp; 0\\
0 &amp; 3 &amp; 4 &amp; 0 \\
0 &amp; 6 &amp; 1 &amp; 1 \\ 
\end{array}\right]
\]</span></p>
<p>Our <strong>characteristic polynomial</strong> in the equation gives us:</p>
<p><span class="math display">\[\begin{align*}
\lambda^4 - 10\lambda^3+24\lambda^2 - 22\lambda + 7 {}&amp;= 0\\
(\lambda-7)(\lambda-1)^3 &amp;=0
\end{align*}\]</span></p>
<p>However, our <strong>minimal polynomial</strong> is below (as that is the minimal polynomials whose corresponding characteristic matrices, when multiplied together, results in zero matrices):</p>
<p><span class="math display">\[
(\lambda-7)(\lambda-1)^2 =0
\]</span></p>
<p>That gives us a clue that we have one <strong>Jordan block</strong> for <span class="math inline">\(\lambda=7\)</span> and two <strong>Jordan blocks</strong> for <span class="math inline">\(\lambda=1\)</span> based on the <strong>algebraic multiplicity</strong> of the <strong>minimal polynomials</strong>. That can further be validated by finding the <strong>regular Eigenvectors</strong> of the <strong>characteristic matrix</strong> of each corresponding <strong>Eigenvalues</strong>.</p>
<p>Here, we know our <strong>Eigenvalues</strong>: <span class="math inline">\(\lambda=7\)</span> and <span class="math inline">\(\lambda=1\)</span>. Now, we need to find our <strong>Eigenvectors</strong>.</p>
<p><strong>First</strong>, let us solve for <span class="math inline">\(v_4\)</span> using <span class="math inline">\(\lambda=7\)</span>:</p>
<p><span class="math display">\[\begin{align*}
(A - \lambda I) {}&amp;= 
\left[\begin{array}{rrrr}
-6 &amp; 1 &amp; 9 &amp; 0\\
0 &amp; -3 &amp; 3 &amp; 0\\
0 &amp; 3 &amp; -3 &amp; 0 \\
0 &amp; 6 &amp; 1 &amp; -6 \\
\end{array}\right]
\rightarrow RREF(A - (7)I) \rightarrow 
\left[\begin{array}{rrrr}
1 &amp; 0 &amp; 0 &amp; -10/7\\
0 &amp; 1 &amp; 0 &amp; -6/7\\
0 &amp; 0 &amp; 1 &amp; -6/7 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
\end{array}\right] \\
\end{align*}\]</span></p>
<p>The <strong>RREF</strong> of the matrix shows one free variable: <strong>r</strong>. Because <strong>r</strong> is a free variable, we can assume any value. In our case, we use <span class="math inline">\(r=7\)</span> only to discard the denominator 7 from the other variables.</p>
<p><span class="math display">\[
\text{solution for x}\ \begin{cases}
x_1 + \frac{-10}{7}x_4 = 0 \\
x_2 + \frac{-10}{7}x_4 = 0 \\
x_3 + \frac{-10}{7}x_4 = 0
\end{cases}\ \ \text{simplied into}\ \
\begin{cases}
x_1 = \frac{10}{7}r \\
x_2 = \frac{6}{7}r \\
x_3 = \frac{6}{7}r \\
x_4 = r = 7\\
\end{cases} \rightarrow
x = 
r\left[\begin{array}{rrrrr}10 \\ 6 \\ 6 \\ 7 \end{array}\right]_{v_4}
\]</span></p>
<p>That gives us the <strong>standard eigenvector</strong> for <span class="math inline">\(\lambda=7\)</span>:</p>
<p><span class="math display">\[
v_4 = \left[\begin{array}{rrrrr} 10 &amp; 6 &amp; 6 &amp; 7\end{array}\right]^T
\]</span></p>
<p><strong>Second</strong>, let us solve for <span class="math inline">\(v_3\)</span> using <span class="math inline">\(\lambda=1\)</span>:</p>
<p>Here, we know that our <strong>algebraic multiplicity</strong> for <span class="math inline">\(\lambda=1\)</span> is three. However, if we compute for the number of <strong>Eigenvectors</strong>, we only get two <strong>Eigenvectors</strong>.</p>
<p><span class="math display">\[\begin{align*}
(A -\lambda I) {}&amp;= 
\left[\begin{array}{cccc}
0 &amp; 1 &amp; 9 &amp; 0\\
0 &amp; 3 &amp; 3 &amp; 0\\
0 &amp; 3 &amp; 3 &amp; 0 \\
0 &amp; 6 &amp; 1 &amp; 0 \\
\end{array}\right]
\rightarrow RREF(A - (1)I) \rightarrow 
\left[\begin{array}{rrrr}
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
\end{array}\right]
\left[\begin{array}{rrrrr}x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]
\end{align*}\]</span></p>
<p>The <strong>RREF</strong> of the matrix shows two free variables: <strong>r</strong> and <strong>s</strong>. Because they are free variables, we can assume values, <span class="math inline">\(r=1, s=1\)</span>:</p>
<p><span class="math display">\[
\text{solution for x}\ \begin{cases}
x_2 = 0 \\
x_3 = 0
\end{cases}\ \ \text{simplied into}\ \
\begin{cases}
x_1 = r \\
x_2 = 0 \\
x_3 = 0 \\
x_4 = s \\
\end{cases} \rightarrow
x = 
r\left[\begin{array}{rrrrr}1 \\ 0 \\ 0 \\ 0 \end{array}\right]_{v_1} +
s\left[\begin{array}{rrrrr}0 \\ 0 \\ 0 \\ 1 \end{array}\right]_{v_2}
\]</span></p>
<p>That gives us a set of only two <strong>eigenvectors</strong> for <span class="math inline">\(\lambda=1\)</span>:</p>
<p><span class="math display">\[
v_1 = \left[\begin{array}{rrrr} 1 &amp; 0 &amp; 0 &amp; 0\end{array}\right]^T, \ \ \ \ 
v_2 = \left[\begin{array}{rrrr} 0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]^T
\]</span></p>
<p>Overall, our <strong>Eigenspace</strong>, <span class="math inline">\(N(A - \lambda I)\)</span>, spans only <strong>three Eigenvectors</strong> <span class="math inline">\(\rightarrow\)</span> <strong>Span {</strong> <span class="math inline">\(v_1, v_2, v_4\)</span> <strong>}</strong>.</p>
<p><span class="math display">\[
N(A - \lambda I) = Span
\left\{
\left[\begin{array}{r}1 \\ 0 \\ 0 \\ 0 \end{array}\right]_{v_1}
\left[\begin{array}{r}0 \\ 0 \\ 0 \\ 1 \end{array}\right]_{v_2}
\left[\begin{array}{r}10 \\ 6 \\ 6 \\ 7 \end{array}\right]_{v_4}
\right\}
\]</span></p>
<p>where dim(<span class="math inline">\(N(A - \lambda I)\)</span>) = 3 indicating a <strong>geometric multiplicity</strong> of three which equals <strong>three Jordan blocks</strong>.</p>
<p>In the context of <strong>solutions</strong>, the three <strong>eigenvectors</strong> are found to be solutions in the <strong>Nullspace</strong>, and so the system for which <span class="math inline">\((A - \lambda I) = 0\)</span> does hold. On the contrary, they do not form a solution in the <strong>column space</strong> of the <strong>characteristic matrix</strong>, thus the system for which <span class="math inline">\((A - \lambda I) \neq 0\)</span> does not hold.</p>
<p>With all that being said, we come down now to the following <strong>Jordan form</strong>:</p>

<p><span class="math display">\[
\begin{array}{ccccc} 
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \ \ \ A \cdotp v_1 =  \lambda \times v_1\\
A_\lambda v_2 = 0 \rightarrow\ \ \ \  A \cdotp v_2 = \lambda \times v_2\\
A_\lambda^2 v_3 = 0 \rightarrow\ \ \ \  A \cdotp v_3 = v_2 + \lambda \times v_3 \\
A_\lambda v_4 = 0 \rightarrow\ \ \ \  A \cdotp v_4 = \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{ccccc}
\lambda &amp; .  &amp; . &amp; .  \\
. &amp; \lambda  &amp; 1 &amp; . \\
. &amp; . &amp; \lambda  &amp; .  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right] =
\left[\begin{array}{ccccc}
\color{red}{\square} &amp; .  &amp; . &amp; .  \\
. &amp; \color{blue}{\square}  &amp; \color{blue}{\square} &amp; .\\
. &amp; \color{blue}{\square} &amp; \color{blue}{\square} &amp; .\\
. &amp; . &amp; . &amp; \color{green}{\square}  
\end{array}\right]
\end{array}
\end{array}
\]</span>
</p>
<p>That is because <span class="math inline">\(\lambda=1\)</span> gives us only two <strong>Eigenvectors</strong>. We need one <strong>generalized Eigenvector</strong>, <span class="math inline">\(v_3\)</span>; that makes one of the <strong>Jordan blocks</strong> size of two.</p>
<p>To solve for <span class="math inline">\(v_3\)</span>, let us, therefore, use the equation:</p>
<p><span class="math display">\[
A_\lambda^2 v_3 = 0 \rightarrow\ \ \ \  A \cdotp v_3 = v_2 + \lambda \times v_3  
\]</span></p>
<p>The <strong>regular eigenvector</strong> <span class="math inline">\(v_2\)</span> will be disposed of a new one because <span class="math inline">\(v_2\)</span> participates now in a <strong>Jordan block</strong> of size two instead of this vector being isolated to its own <strong>Jordan block</strong>.</p>
<p>Let us put aside <span class="math inline">\(v_2\)</span> for now to solve for <span class="math inline">\(v_3\)</span>. To do that, we simplify the equation further:</p>
<p><span class="math display">\[\begin{align*}
A  v_3 {}&amp;= v_2 + \lambda \times v_3 \\
(A - \lambda I) v_3 &amp;= v_2 \\
A_\lambda v_3 &amp;= v_2 \\
\end{align*}\]</span></p>
<p>However, that does not give us a solution in the <strong>Null-space</strong> for <span class="math inline">\(v_3\)</span>; therefore, let us convert the equation into one with <strong>nilpotent polynomial</strong>. The trick is to add <span class="math inline">\(A_\lambda\)</span> to each side of the equation.</p>
<p><span class="math display">\[\begin{align*}
A_\lambda v_3 {}&amp;= v_2 \\
A_\lambda  A_\lambda v_3 &amp;= A_\lambda v_2 \\
A_\lambda^2 v_3 &amp;= A_\lambda v_2 \\
A_\lambda^2 v_3 &amp;= 0, \ \ \ \ \ \because A_\lambda v_2 = 0
\end{align*}\]</span></p>
<p>Our <strong>characteristic matrix</strong> corresponding to <span class="math inline">\(\lambda=1\)</span> becomes:</p>
<p><span class="math display">\[
(A\lambda)^2v_3 = 
\left[
\begin{array}{cccc}
0 &amp; 30 &amp; 30 &amp; 0 \\
0 &amp; 18 &amp; 18 &amp; 0 \\
0 &amp; 18 &amp; 18 &amp; 0 \\
0 &amp; 21 &amp; 21 &amp; 0 \\
\end{array}
\right]v_3 = 0 \rightarrow
\begin{cases}
30_{x_2} + 30_{x_3} = 0 \\
18_{x_2} + 18_{x_3} = 0 \\
18_{x_2} + 18_{x_3} = 0 \\
21_{x_2} + 21_{x_3} = 0
\end{cases} \rightarrow
\begin{cases}
x_2 = -x_3  
\end{cases}
\]</span>
That gives us our <span class="math inline">\(v_3\)</span> vector:</p>
<p><span class="math display">\[
v_3 = \left[\begin{array}{rrrr}0 &amp; 1 &amp; -1 &amp; 0 \end{array}\right]^T
\]</span></p>
<p><strong>Third</strong>, let us now solve for <span class="math inline">\(v_2\)</span>. We know that we can solve for <span class="math inline">\(v_2\)</span> using the following previous equation:</p>
<p><span class="math display">\[
A \cdotp v_3 = v_2 + \lambda \times v_3\ \ \ \ \rightarrow\ \ \ \ (A - \lambda I)v_3 = v_2 
\]</span>
Therefore, our new <span class="math inline">\(v_2\)</span> becomes:</p>
<p><span class="math display">\[
v2 = (A - \lambda I)v_3 =
\left[
\begin{array}{cccc}
0 &amp; 1 &amp; 9 &amp; 0 \\
0 &amp; 3 &amp; 3 &amp; 0 \\
0 &amp; 3 &amp; 3 &amp; 0 \\
0 &amp; 6 &amp; 1 &amp; 0 \\
\end{array}
\right]_{(A - \lambda I)}
\left[\begin{array}{ccc} 0 \\ 1 \\ -1 \\ 0 \end{array} \right]_{v_3} =  
\left[\begin{array}{ccc} -8 \\ 0 \\ 0 \\ 5 \end{array} \right]_{v_2}   
\]</span></p>
<p>Let us validate if <span class="math inline">\(v_2\)</span> is the end of the <strong>Jordan block</strong> chain - though we mentioned that the size is two.</p>
<p><span class="math display">\[
(A - \lambda I)v_2 =
\left[
\begin{array}{cccc}
0 &amp; 1 &amp; 9 &amp; 0 \\
0 &amp; 3 &amp; 3 &amp; 0 \\
0 &amp; 3 &amp; 3 &amp; 0 \\
0 &amp; 6 &amp; 1 &amp; 0 \\
\end{array}
\right]_{(A - \lambda I)}
\left[\begin{array}{ccc} -8 \\ 0 \\ 0 \\ 5 \end{array} \right]_{v_2} =  
\left[\begin{array}{ccc} 0 \\ 0 \\ 0 \\ 0 \end{array} \right]_{0}   
\]</span>
That shows that the new <span class="math inline">\(v_2\)</span> is also a solution in the <strong>Eigenspace</strong>.</p>
<p><strong>Finally</strong>, we have obtained our <span class="math inline">\(V\)</span> and <span class="math inline">\(J\)</span>:</p>
<p><span class="math display">\[
J = \left[\begin{array}{rrrr}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 7\\
\end{array}\right],\ \ \ \ \ 
V = \left[\begin{array}{rrrr}
1 &amp; -8 &amp; 0 &amp; 10\\
0 &amp; 0 &amp; 1 &amp; 6 \\
0 &amp; 0 &amp; -1 &amp; 6\\
0 &amp; 5 &amp; 0 &amp; 7\\
\end{array}\right]
\]</span>
What we just did by using a new <span class="math inline">\(v_2\)</span> and <span class="math inline">\(v_3\)</span> to form <span class="math inline">\(V\)</span> is what we call a <strong>change of basis</strong>.</p>
<p>With that, we can reconstruct our original matrix <span class="math inline">\(A\)</span> using Equation ():</p>
<p>As an exercise, try the following matrix to arrive at the following hints:</p>
<p><span class="math display">\[
A = \left[\begin{array}{cccc}
1 &amp; 6 &amp; 1 &amp; 0\\
0 &amp; 3 &amp; 4 &amp; 0\\
0 &amp; 4 &amp; 3 &amp; 0 \\
0 &amp; 1 &amp; 9 &amp; 1 \\ 
\end{array}\right]
\]</span>
Hint for the <strong>characteristic equation</strong></p>
<p><span class="math display">\[\begin{align*}
\lambda^4-8\lambda^3+6\lambda^2 + 8\lambda -7 {}&amp;= 0 \\
(\lambda^3-9\lambda^2+15\lambda + -7)(x+1) &amp;= 0 \\
(\lambda^2-8\lambda+7)(x-1)(x+1) &amp;= 0 \\
(x-1)(x-7)(x-1)(x+1) &amp;= 0 \\
(x-7)(x-1)^2(x+1) &amp;= 0 \\
\end{align*}\]</span></p>
<p>Hint for <strong>J</strong> and <strong>V</strong>:</p>
<p><span class="math display">\[
J = \left[\begin{array}{rrrr}
7 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; -1\\
\end{array}\right],\ \ \ \ \ 
V = \left[\begin{array}{rrrr}
7 &amp; 1 &amp; 0 &amp; 5\\
6 &amp; 0 &amp; 0 &amp; -2 \\
6 &amp; 0 &amp; 0 &amp; 2\\
10 &amp; 0 &amp; 1 &amp; -8\\
\end{array}\right]
\]</span></p>
<p>Note that other literature use <strong>transposed Jordan form</strong> instead and thus carry on altered methods, e.g.:</p>
<p><span class="math display">\[
J = \left[\begin{array}{cccccc}
\lambda &amp; 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
1 &amp; \lambda  &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 &amp; \lambda &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; \lambda &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp;\ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; \lambda
\end{array}\right]
\]</span></p>
</div>
<div id="other-decomposition" class="section level3">
<h3><span class="header-section-number">2.20.8</span> Other Decomposition</h3>
<p>There are other decompositions that are equally important in solving problems. This book only introduces the decomposition equations and does not discuss the methods in detail.</p>
<ul>
<li>Schur Decomposition</li>
</ul>
<p>The idea is to decompose a <strong>matrix</strong> into its <span class="math inline">\(QUQ^{T} or\ QUQ^{-1}\)</span> form; where <span class="math inline">\(Q\)</span> is an <strong>orthogonal matrix</strong> and <span class="math inline">\(U\)</span> is an <strong>Upper Triangular matrix</strong>, forming the equation:</p>
<p><span class="math display">\[\begin{align}
A = QUQ^{-1} \label{eqn:schureqn}
\end{align}\]</span></p>
<ul>
<li>Hessenberg Decomposition</li>
</ul>
<p>The idea is to decompose a <strong>matrix</strong> into its <span class="math inline">\(PHP^T\)</span> form; where <span class="math inline">\(H\)</span> is an <strong>Hessenberg matrix</strong> and <span class="math inline">\(P\)</span> is a <strong>unitary matrix</strong>, forming the equation:</p>
<p><span class="math display">\[\begin{align}
A = PHP^{T} \label{eqn:hessenbergeqn}
\end{align}\]</span></p>
<ul>
<li>Polar Decomposition</li>
</ul>
<p>The idea is to decompose a <strong>matrix</strong> into its <span class="math inline">\(UP\)</span> form; where <span class="math inline">\(U\)</span> is a <strong>unitary matrix</strong> and <span class="math inline">\(P\)</span> is a <strong>positive semi-definite Hermitian matrix</strong>, forming the equation:</p>
<p><span class="math display">\[\begin{align}
A = UP \label{eqn:polareqn}
\end{align}\]</span></p>
<p>A matrix, being regarded as a transformation utility, can stretch or rotate a vector:</p>
<p><span class="math display">\[
Av = b\ \ \ \ \text{ where b is a transformed vector}
\]</span>
The matrix can also be regarded as a deformation utility. It deforms a vector. Polar decomposition can be useful when factoring a deformation matrix and thus has a geometric representation.</p>
</div>
</div>
<div id="software-libraries" class="section level2">
<h2><span class="header-section-number">2.21</span> Software libraries    </h2>
<p>Computationally, there are software libraries built to allow the use of optimized low-level operations and subroutines for performing linear algebra, including computation for eigenvectors and eigenvalues, and other matrix factorization (or decompositions).</p>
<ul>
<li><strong>BLAS</strong> - <strong>B</strong>asic <strong>L</strong>inear <strong>A</strong>lgebra <strong>S</strong>ubroutine</li>
<li><strong>LAPACK</strong> - <strong>L</strong>inear <strong>A</strong>lgebra <strong>PACK</strong>age</li>
<li><strong>LINPACK</strong> - <strong>LIN</strong>ear Algebra <strong>PACK</strong>age</li>
<li><strong>EISPACK</strong> - <strong>EI</strong>gen<strong>S</strong>ystem <strong>PACK</strong>age</li>
</ul>
<p>Because these built-in libraries are served with the purpose of optimizing linear algebra operations and subroutines, it is perhaps ideal to use them instead of re-implementing the operation or algorithm.</p>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">2.22</span> Summary</h2>
<p>We know that <strong>eigenvectors</strong> form the <strong>basis</strong>. We know that <strong>basis</strong> spans vectors that are <strong>linearly independent</strong>. We know that vectors that are <strong>linearly independent</strong> are therefore unique. Gathering data with many unknown variables - or features - helps to identify the unique ones and eliminate redundancy (those that are marked as <strong>linearly dependent</strong>).</p>
<p>Scaling unique vectors via <strong>eigenvalues</strong> give more magnitude to the vectors, or weights. Vectors with greater magnitude represent the primary components of a matrix. It helps to narrow down unique vectors to the more important (or relevant) ones.</p>
<p>For a matrix that is decomposed, in terms of a system of linear equations, the resulting <strong>eigenvalues</strong> are the <strong>unknown variables</strong>. And the three <strong>coefficients</strong> across each row represent elements in each <strong>column eigenvector</strong>, e.g.:</p>
<p><span class="math display">\[
1x + 2y + 3z = 4\\
8x + 7y + 6z = 5\\
9x + 10y + 11z = 12
\]</span></p>
<p>While there exist software subroutines allowing us to computationally perform linear algebra, it helps to also understand the reliability of certain linear algebra operations. It pays to be wary of the performance implications when dealing with linear algebra. That said, we cover approximation and optimization topics in the next chapter.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="numerical-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="numerical-linear-algebra.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
