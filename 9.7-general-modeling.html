<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.7 General Modeling | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="9.7 General Modeling | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.7 General Modeling | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-03-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="9.6-featureengineering.html"/>
<link rel="next" href="9.8-supervised-vs.unsupervised-learning.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat-and-disclaimer.html"><a href="caveat-and-disclaimer.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i><em>Bibliography</em></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="general-modeling" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.7</span> General Modeling<a href="9.7-general-modeling.html#general-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section introduces a few important <strong>operations</strong> behind <strong>Computational learning</strong>; in particular, we cover the <strong>Learning</strong>, <strong>Testing</strong>, and <strong>Validation</strong> operations.</p>
<div id="training-learning" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.1</span> Training (Learning)<a href="9.7-general-modeling.html#training-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Training a model starts with a dataset - we call this fitting the model to data. The <strong>model</strong> can be a <strong>linear model</strong> such that we <strong>fit</strong> a <strong>line</strong> through the data points. In <strong>Machine Learning (ML)</strong>, it is common to split a dataset into subsets and use a portion of the split for training while the rest are held out for validation and test. The portion of the split for training is called <strong>training set</strong>. We use the <strong>training set</strong> to <strong>train or fit a model</strong>. An example of <strong>fitting a model</strong> is illustrated in <strong>Ordinary Least Square (OLS)</strong>. Geometrically, we randomly place a line through a random set of data points and adjust the line until it meets the minimum computed Least-Squares - a measure for the <strong>goodness of fit</strong>. The <strong>fitted line</strong> is a geometric representation of an <strong>ML model</strong> in the form of the following equation - this accounts for <strong>noise</strong> denoted by the symbol epsilon <span class="math inline">\(\epsilon\)</span>, which also stands for error: </p>
<p><span class="math display" id="eq:equate1110063">\[\begin{align}
h_{\theta}(X) = \hat{f}(x) = \sum_{i=1}^n \theta x_i + \epsilon, \tag{9.67} 
\end{align}\]</span></p>
<p>where <strong>h(.)</strong> stands for <strong>hypothetical value</strong> - the <span class="math inline">\(\mathbf{yhat}\ (\hat{y})\)</span> and the <span class="math inline">\(\theta\)</span> symbol stands for <strong>coefficient</strong> - the <strong>weight</strong> multiplied to features.</p>
<p>Without <strong>noise</strong>, the data points align perfectly well such that a line can go through all of them. Such a case can then be expressed as a <strong>linear expression</strong> like so (in its most simple linear form):</p>
<p><span class="math display" id="eq:equate1110064">\[\begin{align}
y = mx + b \tag{9.68} 
\end{align}\]</span></p>
<p>However, an <strong>ML model</strong> does not just consist of a <strong>linear expression</strong>. The <strong>linear expression</strong> above comes with two model parameters: <strong>m</strong> for slope and <strong>b</strong> for intercept. We also treat these parameters as <strong>weights</strong> or <strong>coefficients</strong> - in some cases, it is denoted by the symbol <strong>theta</strong> <span class="math inline">\(\theta\)</span>. When we say <strong>fit the line</strong> or <strong>fit the model</strong> or <strong>train the model</strong>, we mean to use the <strong>linear expression</strong> while adjusting the <strong>weights</strong>. For every adjustment of the weights, we measure and take note of all the distances of the data points to the fitted line - such distance is called <strong>residual</strong>. The one adjustment that renders the least square distance effectively determines the final <strong>ML model</strong>. For example:</p>
<p><span class="math display" id="eq:equate1110065">\[\begin{align}
\epsilon = y - \hat{y}\ \ \rightarrow \ \ \ MSE = \frac{1}{n}\sum \epsilon^2 \tag{9.69} 
\end{align}\]</span></p>
<p>The final <strong>ML model</strong> in terms of <strong>linear regression</strong> comes with the following sufficient <strong>ML</strong> artifacts (other implementation of models come with additional supplementary artifacts, e.g.Â for <strong>ML model provenance</strong>):</p>
<ul>
<li>The linear equation, e.g. <span class="math inline">\(\hat{y} = \beta_0 + \beta_1 x\)</span></li>
<li>The coefficient values, e.g. <span class="math inline">\(\beta_0 = 30, \beta_1 = 50\)</span></li>
<li>The training set: <span class="math inline">\(\{x, y\}_{i=1}^n\)</span></li>
<li>The features: x</li>
<li>The fitted values: (<span class="math inline">\(\hat{y}\)</span>)</li>
<li>The residual: <span class="math inline">\(\epsilon\)</span></li>
</ul>
<p>Note that by running the following R code below, a linear model in R using the function <strong>lm(.)</strong> captures the following content (preserved in a <strong>list</strong> structure): coefficients, residuals, effects, rank, fitted.values, qr, df.residual, xlevels, call, formula, terms, and others.</p>

<div class="sourceCode" id="cb1186"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1186-1" data-line-number="1"><span class="co"># Using mtcars dataset to illustrate linear modeling</span></a>
<a class="sourceLine" id="cb1186-2" data-line-number="2">my.model =<span class="st"> </span><span class="kw">lm</span>(<span class="dt">formula =</span> mpg <span class="op">~</span><span class="st"> </span>cyl <span class="op">+</span><span class="st"> </span>am, <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb1186-3" data-line-number="3"><span class="kw">saveRDS</span>(my.model, <span class="st">&quot;./my_model.rds&quot;</span>)   <span class="co"># save our model</span></a>
<a class="sourceLine" id="cb1186-4" data-line-number="4">my.model =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;./my_model.rds&quot;</span>)  <span class="co"># retrieve our model</span></a>
<a class="sourceLine" id="cb1186-5" data-line-number="5"><span class="kw">names</span>(my.model)</a></code></pre></div>
<pre><code>##  [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;       &quot;rank&quot;         
##  [5] &quot;fitted.values&quot; &quot;assign&quot;        &quot;qr&quot;            &quot;df.residual&quot;  
##  [9] &quot;xlevels&quot;       &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;</code></pre>

<p>Our definition of a <strong>model</strong> should be all-encompassing to include all necessary ingredients to reconstruct our model. The list of artifacts above exemplifies that. However, Chapter <strong>14</strong> (<strong>Distributed Computation</strong>) covers <strong>Open Standards</strong>, which describe standard model specifications that guide how models can be portable and sharable. At the very least, a model should meet the minimum required content for inferencing.</p>
</div>
<div id="validation-tuning" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.2</span> Validation (Tuning) <a href="9.7-general-modeling.html#validation-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>fitting a model</strong>, it is not unusual to encounter <strong>overfitting</strong>. That means the model is too tailored to fit the <strong>training set</strong> perfectly well but loses generality; thus, it cannot be used for new <strong>unseen</strong> data. If this happens, additional adjustment is needed to the model to minimize the overfit. This adjustment is called <strong>tuning</strong> the model.</p>
<p><strong>Tuning</strong> is essentially needed for optimization. Primarily, we tune <strong>hyperparameters</strong>. A combination of <strong>cross-validation</strong> and calibration of hyperparameters helps improve the performance of our models. In two separate sections a little later ahead, we discuss more of <strong>cross-validation</strong> and <strong>tuning</strong> by using <strong>regularization</strong> techniques.</p>
<p>Now, one way to overcome overfitting is to use <strong>cross-validation</strong>. The idea is to split the dataset into <strong>k</strong> sets (see Figure <a href="9.7-general-modeling.html#fig:crossvalidation">9.64</a>). We then perform model fitting by <strong>holding out</strong> the first set and using the rest of the sets for training. After which, we perform another model fitting by <strong>holding out</strong> the second set and using the rest of the sets. We continue to do so iteratively until completing several iterations, e.g., five iterations.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:crossvalidation"></span>
<img src="crossvalidation.png" alt="5-Fold Cross-Validation" width="60%" />
<p class="caption">
Figure 9.64: 5-Fold Cross-Validation
</p>
</div>
<p>After the last iteration, we then analyze the result. We choose the model that renders the best result. In this case, for <strong>linear regression</strong>, we choose one with least error using <span class="math inline">\(\mathbf{\text{RMSE}, \text{R}^2,}\text{ and }\mathbf{\text{MAE}}\)</span>.</p>
</div>
<div id="testing-assessing" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.3</span> Testing (Assessing) <a href="9.7-general-modeling.html#testing-assessing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After a model is trained, our next goal is to assess the performance (e.g., predictive power) of the model. Here, we treat the <strong>holdout set</strong> as a <strong>testing set</strong>, which contains a list of predictive variables only - the features. Unlike the <strong>training set</strong>, we do not account for the <strong>response</strong> variable in the <strong>testing set</strong>.</p>
<p>Ideally, the values in the <strong>response</strong> variables in the <strong>testing set</strong> are preserved as <strong>ground truth</strong> for later evaluation. For example, we have covered <strong>Auc on ROC</strong> under the <strong>Feature Selection</strong> section as a method we can use to compare the predicted result of our test over the actual values.</p>
<p>In terms of the proportionality of our <strong>testing set</strong> over <strong>training set</strong>, it may be safe to assume that there is no specific hard rule in the manner in which we split our dataset, but it seems sensical to assume (in a general sense, but not always) that smaller datasets may not have to be split into many folds. A simple 2-fold split helps when the first fold becomes the training set and the second fold becomes a <strong>holdout set</strong>. The proportion may be arbitrary depending on necessity, e.g., <strong>training set</strong> may accommodate 75% of the dataset, and 25% goes into a <strong>holdout set</strong>.</p>
<p>The question now comes when we use the <strong>holdout set</strong>. If the set is used to assess our modelâs performance (e.g., predictive power) and shows high results (e.g., high accuracy or low error), it may not necessarily signify a good thing. <strong>Overfitting</strong> is a good example. To ensure that accuracy holds, we perform multiple tests using <strong>cross-validation</strong>.</p>
<p>Below is an example of splitting the dataset <strong>BreastCancer</strong> in 5-fold using <strong>createFolds(.)</strong> function.</p>

<div class="sourceCode" id="cb1188"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1188-1" data-line-number="1"><span class="kw">require</span>(caret)</a>
<a class="sourceLine" id="cb1188-2" data-line-number="2"><span class="kw">require</span>(mlbench)</a>
<a class="sourceLine" id="cb1188-3" data-line-number="3">n=<span class="dv">80</span></a>
<a class="sourceLine" id="cb1188-4" data-line-number="4">X =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span> n)</a>
<a class="sourceLine" id="cb1188-5" data-line-number="5"><span class="co"># note that the output is the location</span></a>
<a class="sourceLine" id="cb1188-6" data-line-number="6"><span class="co"># (index of corresponding data points)</span></a>
<a class="sourceLine" id="cb1188-7" data-line-number="7">(<span class="dt">fold.indices =</span> <span class="kw">createFolds</span>(X, <span class="dt">k =</span> <span class="dv">5</span>, <span class="dt">returnTrain=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## $Fold1
##  [1]  2  8 11 14 22 24 25 40 44 45 51 59 62 65 76 78
## 
## $Fold2
##  [1]  4  7  9 10 21 26 29 37 46 49 52 60 67 70 72 75
## 
## $Fold3
##  [1]  1  3 15 16 23 28 33 38 41 48 56 57 61 63 64 79
## 
## $Fold4
##  [1]  5  6 13 18 30 31 35 36 47 53 55 58 68 69 77 80
## 
## $Fold5
##  [1] 12 17 19 20 27 32 34 39 42 43 50 54 66 71 73 74</code></pre>

<p>Given the 5-folds, we can use the 1st fold for our test set and the rest of the folds for our training set like so:</p>
<div class="sourceCode" id="cb1190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1190-1" data-line-number="1"><span class="co"># choose the first fold for our holdout set.</span></a>
<a class="sourceLine" id="cb1190-2" data-line-number="2">holdout.set =<span class="st"> </span>X[fold.indices<span class="op">$</span>Fold1] </a>
<a class="sourceLine" id="cb1190-3" data-line-number="3"><span class="co"># choose the other folds for our training set</span></a>
<a class="sourceLine" id="cb1190-4" data-line-number="4">training.set =<span class="st"> </span>X[<span class="op">-</span>fold.indices<span class="op">$</span>Fold1]</a>
<a class="sourceLine" id="cb1190-5" data-line-number="5"><span class="kw">c</span>(<span class="st">&quot;HoldOut&quot;</span>=<span class="kw">length</span>(holdout.set), <span class="st">&quot;Training&quot;</span>=<span class="kw">length</span>(training.set))</a></code></pre></div>
<pre><code>##  HoldOut Training 
##       16       64</code></pre>
</div>
<div id="cross-validation-cv" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.4</span> Cross-Validation (CV)  <a href="9.7-general-modeling.html#cross-validation-cv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us further extend the concept of <strong>Cross-Validation (CV)</strong>. There are a few other reasons why we need <strong>CV</strong> of which we mention the following three reasons:</p>
<ul>
<li><strong>Model Selection and Tuning</strong></li>
<li><strong>Hyperparameter Selection and Tuning</strong></li>
<li><strong>Algorithm Selection and Tuning</strong></li>
</ul>
<p>Here, we discuss two <strong>CV</strong> techniques. The first <strong>CV</strong> technique is called the <strong>K-fold Cross-Validation</strong>. We split the dataset into k-folds. See Figure <a href="9.7-general-modeling.html#fig:crossvalidation">9.64</a> which shows a 5-fold split. </p>
<p>There is no hard rule when choosing <strong>k</strong> for splitting the dataset. However, care must be considered to avoid <strong>data leakage</strong> - a topic introduced in the <strong>EDA</strong> section. It is easy to <strong>contaminate</strong> our dataset, especially when using <strong>cross-validation</strong>. This happens if the algorithms and hyperparameters are not well-tuned.</p>
<p>To tune our algorithm and hyperparameter and select the best combination, it is ideal for performing a <strong>repeated K-fold CV</strong>. For example, Figure <a href="9.7-general-modeling.html#fig:modelselection">9.65</a> shows a 1-iteration evaluation of five different attempts to tune a model, e.g., given some set of tunable parameters. In practice, we can use multiple iterations across several different configurations (e.g., one combination of a chosen model and hyperparameters).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:modelselection"></span>
<img src="modelselection.png" alt="Repeated 5-Fold Cross-Validation (Model Selection)" width="60%" />
<p class="caption">
Figure 9.65: Repeated 5-Fold Cross-Validation (Model Selection)
</p>
</div>
<p>On the other hand, we are not limited to only one algorithm for evaluation. Figure <a href="9.7-general-modeling.html#fig:algoselection">9.66</a> illustrates an example of evaluating the performance of different algorithms using a <strong>repeated K-fold CV</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:algoselection"></span>
<img src="algoselection.png" alt="Repeated 5-Fold Cross-Validation (Algorithm Selection)" width="60%" />
<p class="caption">
Figure 9.66: Repeated 5-Fold Cross-Validation (Algorithm Selection)
</p>
</div>
<p>The second <strong>CV</strong> technique is called the <strong>Leave One Out Cross Validation (LOOCV)</strong>. It is also referred to as the <strong>Jackknife</strong> approach. The idea is simply to <strong>hold out</strong> a data point and use the rest of the dataset for training. Recall in <strong>linear regression</strong> how we geometrically fit a line through data points. Such a line is fitted by adjusting its slope and intercept. To derive a good fit, we use the <strong>Jackknife</strong> approach in which we perform the following steps:  </p>
<ol style="list-style-type: decimal">
<li>Using the entire dataset, we compute the initial values of the slope and intercept.</li>
<li>We then hold out one data point and use the rest of the dataset to compute the slope and intercept values.</li>
<li>We repeat step 2 by holding out the next data point until all the data points have taken turns being held out.</li>
<li>Compute the average of all the computed slopes and intercepts.</li>
</ol>
<p>Such an approach aims to balance <strong>bias</strong> and <strong>variance</strong>.</p>
</div>
<div id="bias-and-variance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.5</span> Bias and Variance <a href="9.7-general-modeling.html#bias-and-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us use a popular diagram used in discussing <strong>bias</strong> and <strong>variance</strong>. See Figure <a href="9.7-general-modeling.html#fig:biasvariance">9.67</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:biasvariance"></span>
<img src="biasvariance.png" alt="Variance and Bias" width="60%" />
<p class="caption">
Figure 9.67: Variance and Bias
</p>
</div>
<p>In <strong>fitting a model</strong>, we rely on measuring the <strong>error</strong> of our fit against some true value. A breakdown (or decomposition) of the <strong>error</strong> is essential in understanding how we may deal with <strong>underfitting</strong> or <strong>overfitting</strong> a model. To compute for the <strong>total error</strong> (e.g., <strong>MSE</strong>), we use the following decomposition:</p>
<p><span class="math display" id="eq:equate1110066">\[\begin{align}
\text{Total Error}(x) = Bias^2 +  Variance + \text{Irreducible Error} \tag{9.70} 
\end{align}\]</span></p>
<p><strong>Bias</strong> and <strong>Variance</strong> are derived from the following equations:</p>
<p><span class="math display" id="eq:equate1110067">\[\begin{align}
Bias^2 = \left[\mathbb{E}[\hat{f}(x)] - f(x) \right]^2 
\ \ \ \ \ \ \ \ \ \ \ \ \ \
Variance = \mathbb{E}\left[ \left(\hat{f}(x) - \mathbb{E}[\hat{f}(x)]\right)^2\right]  \tag{9.71} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\underbrace{\hat{y} = \hat{f}(x)}_{
\begin{array}{l}
\text{Estimated Response} \\
\text{Learned from X}
\end{array}
}
\ \ \ \ \ \ \ \ \ \
\underbrace{y = {f}(x) }_{\text{Actual Response}}
\ \ \ \ \ \ \ \ \ \ \
\underbrace{\text{Irreducible Error} = \sigma^2_e}_{Noise}
\]</span></p>
<p>Let us design a simple dataset and create a 3-fold dataset to illustrate <strong>bias</strong> and <strong>variance</strong>.</p>

<div class="sourceCode" id="cb1192"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1192-1" data-line-number="1"><span class="kw">require</span>(caret)</a>
<a class="sourceLine" id="cb1192-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2019</span>)</a>
<a class="sourceLine" id="cb1192-3" data-line-number="3">n =<span class="st"> </span><span class="dv">10</span>; k =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb1192-4" data-line-number="4">f &lt;-<span class="st"> </span><span class="cf">function</span>(X) { <span class="kw">sqrt</span>(X) }</a>
<a class="sourceLine" id="cb1192-5" data-line-number="5">X =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span> n <span class="op">*</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1192-6" data-line-number="6">E =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n <span class="op">*</span><span class="st"> </span>k, <span class="dv">0</span>, <span class="fl">0.1</span>)      <span class="co"># Irreducible Error</span></a>
<a class="sourceLine" id="cb1192-7" data-line-number="7">Y =<span class="st"> </span><span class="kw">f</span>(X)                  <span class="co"># Actual Values without Noise</span></a>
<a class="sourceLine" id="cb1192-8" data-line-number="8">Y.observed =<span class="st"> </span><span class="kw">f</span>(X) <span class="op">+</span><span class="st"> </span>E     <span class="co"># Observed Values with Noise</span></a>
<a class="sourceLine" id="cb1192-9" data-line-number="9">fold.indices =<span class="st"> </span><span class="kw">createFolds</span>(X, <span class="dt">k =</span> k, <span class="dt">returnTrain=</span><span class="ot">FALSE</span>)</a></code></pre></div>

<p>Given the 3 folds, we have the plot in Figure <a href="9.7-general-modeling.html#fig:biasfold">9.68</a>:</p>

<div class="sourceCode" id="cb1193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1193-1" data-line-number="1"><span class="kw">require</span>(ModelMetrics)</a>
<a class="sourceLine" id="cb1193-2" data-line-number="2">Y.bias =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, k)</a>
<a class="sourceLine" id="cb1193-3" data-line-number="3">Y.var  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, k)</a>
<a class="sourceLine" id="cb1193-4" data-line-number="4">Y.mse  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, k)</a>
<a class="sourceLine" id="cb1193-5" data-line-number="5"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="fl">1.3</span>), </a>
<a class="sourceLine" id="cb1193-6" data-line-number="6">      <span class="dt">xlab=</span><span class="st">&quot;Predictor&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Response&quot;</span>, </a>
<a class="sourceLine" id="cb1193-7" data-line-number="7">     <span class="dt">main=</span><span class="st">&quot;Cross Validation&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1193-8" data-line-number="8"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1193-9" data-line-number="9"><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {</a>
<a class="sourceLine" id="cb1193-10" data-line-number="10">  indices =<span class="st"> </span>fold.indices[[j]] </a>
<a class="sourceLine" id="cb1193-11" data-line-number="11">  <span class="co"># choose the j-fold for our holdout set.</span></a>
<a class="sourceLine" id="cb1193-12" data-line-number="12">  X.holdout.set =<span class="st"> </span>X[indices]; Y.holdout.set =<span class="st"> </span>Y.observed[indices]</a>
<a class="sourceLine" id="cb1193-13" data-line-number="13">  <span class="co"># choose the other folds for our training set</span></a>
<a class="sourceLine" id="cb1193-14" data-line-number="14">  X.training.set =<span class="st"> </span>X[<span class="op">-</span>indices]; Y.training.set =<span class="st"> </span>Y.observed[<span class="op">-</span>indices]</a>
<a class="sourceLine" id="cb1193-15" data-line-number="15">  <span class="co"># fit a linear model</span></a>
<a class="sourceLine" id="cb1193-16" data-line-number="16">  Y.model =<span class="st"> </span><span class="kw">lm</span>(Y.training.set <span class="op">~</span><span class="st"> </span>X.training.set) </a>
<a class="sourceLine" id="cb1193-17" data-line-number="17">  Y.predicted =<span class="st"> </span><span class="kw">predict.lm</span>(Y.model, </a>
<a class="sourceLine" id="cb1193-18" data-line-number="18">                <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">X.training.set  =</span> X.holdout.set))</a>
<a class="sourceLine" id="cb1193-19" data-line-number="19">  Y.bias[j]   =<span class="st"> </span><span class="kw">mean</span>( ( <span class="kw">mean</span>(Y.predicted)  <span class="op">-</span><span class="st"> </span>Y.holdout.set )<span class="op">^</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb1193-20" data-line-number="20">  Y.var[j]    =<span class="st"> </span><span class="kw">sum</span>( (Y.predicted <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(Y.predicted))<span class="op">^</span><span class="dv">2</span> ) <span class="op">*</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>(n<span class="dv">-1</span>) </a>
<a class="sourceLine" id="cb1193-21" data-line-number="21">  Y.mse[j]    =<span class="st"> </span><span class="kw">mean</span>((Y.predicted <span class="op">-</span><span class="st"> </span>Y.holdout.set)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1193-22" data-line-number="22">  <span class="kw">points</span>(X.holdout.set, Y.holdout.set, <span class="dt">col=</span>j)</a>
<a class="sourceLine" id="cb1193-23" data-line-number="23">  <span class="kw">lines</span>(X.training.set, Y.model<span class="op">$</span>fitted.values, <span class="dt">col=</span>j)</a>
<a class="sourceLine" id="cb1193-24" data-line-number="24">}</a>
<a class="sourceLine" id="cb1193-25" data-line-number="25"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, </a>
<a class="sourceLine" id="cb1193-26" data-line-number="26">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;Train 1&quot;</span>, <span class="st">&quot;Train 2&quot;</span>, <span class="st">&quot;Train 3&quot;</span>),</a>
<a class="sourceLine" id="cb1193-27" data-line-number="27">    <span class="dt">col=</span><span class="kw">seq</span>(<span class="dv">1</span>,k), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">1</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:biasfold"></span>
<img src="DS_files/figure-html/biasfold-1.png" alt="Cross Validation" width="70%" />
<p class="caption">
Figure 9.68: Cross Validation
</p>
</div>

<p>There are situations in which we find that no matter how good we fit our model, we cannot reduce the error. This so-called <strong>irreducible error</strong> could indicate a dataset with a certain amount of noise - some perturbation that could be the norm in a system. Perhaps we may acknowledge that additional data processing, adjustment, or transformation may be required before fitting a model - all that with one thing in mind. That is to do our best to improve our modelâs <strong>goodness of fit</strong> by balancing <strong>bias</strong> and <strong>variance</strong> - a trade-off. That is only as long as we do not fall into <strong>underfitting</strong> or <strong>overfitting</strong> conditions.</p>

<div class="sourceCode" id="cb1194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1194-1" data-line-number="1">(<span class="dt">measure =</span> <span class="kw">data.frame</span>( <span class="dt">Train =</span> <span class="kw">seq</span>(<span class="dv">1</span>,k), </a>
<a class="sourceLine" id="cb1194-2" data-line-number="2">    <span class="dt">bias =</span> Y.bias, <span class="dt">variance =</span> Y.var, <span class="dt">mse =</span> Y.mse))</a></code></pre></div>
<pre><code>##   Train    bias variance     mse
## 1     1 0.05536  0.07679 0.02174
## 2     2 0.08065  0.08024 0.01423
## 3     3 0.10337  0.05376 0.01895</code></pre>

<p>When it comes to <strong>losing generality</strong>, there is a correlation between <strong>generalizability</strong> and <strong>variance</strong>. If the dataset deviates or varies a lot, we may still be able to come up with a model that can predict. Otherwise, perhaps we can split the dataset into groups and use different modeling algorithms for each group such that when we merge the models into an aggregated model - an ensemble model, it will be able to predict.</p>
<p>In a <strong>linear regression</strong>, if we start to fit a line in such a manner that the line starts to curve or adjust towards every data point (basically introducing a higher degree of freedom), then it can be guaranteed that our model will just not fit any new unseen data. The model loses <strong>generalizability</strong>. See Figure <a href="9.7-general-modeling.html#fig:goodnessoffit">9.69</a>.</p>

<div class="sourceCode" id="cb1196"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1196-1" data-line-number="1">fit =<span class="st"> </span><span class="cf">function</span>(x) { x<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb1196-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1196-3" data-line-number="3">N =<span class="st"> </span><span class="dv">40</span></a>
<a class="sourceLine" id="cb1196-4" data-line-number="4">x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span>N)</a>
<a class="sourceLine" id="cb1196-5" data-line-number="5">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="fl">0.1</span>) <span class="co"># Noise</span></a>
<a class="sourceLine" id="cb1196-6" data-line-number="6">y =<span class="st"> </span><span class="kw">fit</span>(x)                     <span class="co"># f(x) &lt;- Actual Response </span></a>
<a class="sourceLine" id="cb1196-7" data-line-number="7">y.hat =<span class="st"> </span>y <span class="op">+</span><span class="st"> </span>e</a>
<a class="sourceLine" id="cb1196-8" data-line-number="8"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb1196-9" data-line-number="9">      <span class="dt">xlab=</span><span class="st">&quot;Predictor&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Response&quot;</span>, </a>
<a class="sourceLine" id="cb1196-10" data-line-number="10">     <span class="dt">main=</span><span class="st">&quot;Goodness of Fit&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1196-11" data-line-number="11"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1196-12" data-line-number="12"><span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1196-13" data-line-number="13"><span class="kw">lines</span>(x, y, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1196-14" data-line-number="14"><span class="kw">lines</span>(<span class="kw">lowess</span>(x, y.hat, <span class="dt">f=</span><span class="fl">0.02</span>), <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb1196-15" data-line-number="15"><span class="kw">points</span>(x, y.hat, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">pch=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb1196-16" data-line-number="16"><span class="co">### Legend</span></a>
<a class="sourceLine" id="cb1196-17" data-line-number="17"><span class="kw">legend</span>(<span class="op">-</span><span class="fl">0.2</span>, <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1196-18" data-line-number="18">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;Fitted Line (Optimal)&quot;</span>, <span class="st">&quot;Underfit (High Bias)&quot;</span>,</a>
<a class="sourceLine" id="cb1196-19" data-line-number="19">              <span class="st">&quot;Overfit (High Variance)&quot;</span>),</a>
<a class="sourceLine" id="cb1196-20" data-line-number="20">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>),  <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:goodnessoffit"></span>
<img src="DS_files/figure-html/goodnessoffit-1.png" alt="Goodness of Fit" width="70%" />
<p class="caption">
Figure 9.69: Goodness of Fit
</p>
</div>

<p>Similarly, in a case where a <strong>decision tree</strong> has too many branches or as the tree depth increases, there is a tendency to overfit since every leaf tends toward every data point. Once again, this indicates that the tree model will not fit any new (unseen) set of data points.</p>
<p>We continue to discuss <strong>Overfitting</strong> in subsequent chapters. We will introduce hyperparameters and network layers starting with Chapter <strong>12</strong> (<strong>Computational Deep Learning I</strong>). The more we begin to tailor the network against many layers where each layer filters toward every data point, the more we overfit.</p>
</div>
<div id="loss-and-cost-functions" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.6</span> Loss and Cost Functions  <a href="9.7-general-modeling.html#loss-and-cost-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When we calculate our estimates, intuitively, we quantify the error in our estimates and adjust our operations as necessary. Additionally, we define functions that allow us to measure the difference of our estimates based on error, adjust as necessary, and numerically determine how much of an error we can tolerate.</p>
<p><strong>Loss (Error) function</strong> </p>
<p><strong>Loss function</strong> measures the difference between our estimated and actual values. The larger the difference, the larger the error. We use an <strong>error</strong> function to compute our error. <strong>Squared Error Loss</strong> (e.g. <strong>SSE</strong>, <strong>MSE</strong>, <strong>MAE</strong>) and <strong>Huber Loss</strong> are examples of <strong>Loss functions</strong> for regression. A general formula for a <strong>Loss function</strong> in regression is expressed as such:</p>
<p><span class="math display" id="eq:equate1110068">\[\begin{align}
\underbrace{Lik(\theta) = \sum(y - \hat{y})^2}_{\text{squared error loss}} \tag{9.72} 
\end{align}\]</span></p>
<p>Note that <strong>classification problems</strong> introduce other forms of measures for Loss functions based on <strong>Information Theory</strong> and <strong>Bayesian Theory</strong>. Examples of measures are the <strong>Gini Index</strong>, <strong>Cross-Entropy</strong>, and <strong>Information Gain</strong>, which are used for tree classifications, <strong>Logit Loss</strong> for binary classification, and <strong>Hinge Loss</strong> for SVM. We cover this in the <strong>Classification</strong> section later. Below are examples of <strong>Hinge Loss</strong> function and <strong>Logit Loss</strong> function respectively:</p>
<p><span class="math display" id="eq:equate1110069">\[\begin{align}
\underbrace{Lik(\theta) = max\left\{0, 1 - f(x)\right\}}_{\text{hinge loss}}\ \ \ \ \ \ \ \ \ 
\underbrace{Lik(\theta) = max\left\{1 + exp(- f(x))\right\}}_{\text{log loss}} \tag{9.73} 
\end{align}\]</span></p>
<p><strong>Objective (Cost) function</strong> </p>
<p><strong>Objective function</strong> measures the Cost of fitting a model to data. Intuitively, our goal is to reduce the <strong>Cost</strong>. In this respect, it is understandable that we determine our tolerance level - how much <strong>Cost</strong> is acceptable. <strong>Cost</strong> is a function of the <strong>loss</strong> that we incur and the complexity of our model to fit. If our model is too complex, there is a chance to overfit. As shown in the previous section, we can use <strong>Lasso</strong> or <strong>Ridge</strong> regularization for linear regression to balance complexity. A general formula for the objective function is expressed as such: </p>
<p><span class="math display" id="eq:equate1110070">\[\begin{align}
J(\theta) = \theta^{\text{*}} = \text{arg}\ \underset{\theta}{\text{min}}\ Lik(\theta)  
\ \ \ \ \ \ \ \ \ \ \ \ \ \ 
J(\theta)_R = \theta^{\text{*}}_R = \underbrace{ \text{arg}\ \underset{\theta}{\text{min}}
\left\{Lik(\theta) + \lambda(\theta)\right\}  }_{\text{with regularization}} \tag{9.74} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is a regularization function</p>
<p><strong>Stopping Criterion function</strong> </p>
<p>We use an <strong>objective</strong> function to find the minimum error. However, we might wonder why we have to find a minimum error when we can go straight to zero error, meaning we just get the estimated value equal to the actual value. Indeed, that would be ideal if we knew the actual value. Unfortunately, in some cases, we do not have the actual value. All we have are data points, and the most basic method we can use is to average the data points and estimate the actual value based on the average. However, we know that average will not always give accurate results when our data points are inherently skewed. In this sense, we need better methods to find a minimum error that is optimal enough to describe the accuracy of our estimate. That said, either we keep looking for that optimal minimum error, or at some point, we have to <strong>stop</strong>. Furthermore, this is where we need to determine the exit criterion to <strong>stop</strong>? We use a stopping criterion function to determine our stopping point - which we also term our <strong>tolerance level</strong>.</p>
<p>Note that the three general functions ( loss, cost, stopping criterion) are commonly used and emphasized in <strong>Machine Learning</strong>. Most algorithms are readily available in many languages and are implemented in different ways. Here, we use a library called <strong>caret</strong> to illustrate two functions, namely <strong>trainControl(.)</strong> and <strong>train(.)</strong>. The former is used to control <strong>cross-validation</strong> training. The latter is used to train a model by injecting a selection of algorithms and corresponding hyperparameter settings.</p>
<p>The <strong>trainControl(.)</strong> function allows us to set parameters that control how we train our model. Such parameters include the validation method, number of splits to a dataset, and others.</p>
<div class="sourceCode" id="cb1197"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1197-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb1197-2" data-line-number="2">tr.control =<span class="st"> </span>caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,  <span class="dt">number =</span> <span class="dv">5</span>)</a></code></pre></div>
<p>We then plug the control parameters to <strong>train(.)</strong>. The function allows us to set the <strong>algorithm</strong> to use for the training such as <strong>general linear model (glm)</strong>, <strong>gradient boosting machine (gbm)</strong>, <strong>regression forest (rf)</strong>, etc.</p>
<p>The <strong>tr.tuneGrid</strong> is a configuration of <strong>regularization choices</strong> and a selection of <strong>lambdas</strong> as discussed in previous chapter. We then use <strong>train(.)</strong> to train our model with <strong>centering</strong> and <strong>scaling</strong> for pre-processing. Notice the use of <strong>expand.grid(.)</strong>. That allows multiple hyperparameters to produce different unique combinations. In our case, we use alpha and lambda. We then train the model and determine which combination renders the optimal fit.</p>

<div class="sourceCode" id="cb1198"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1198-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a></code></pre></div>
<pre><code>## Loaded glmnet 3.0-2</code></pre>
<div class="sourceCode" id="cb1200"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1200-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1200-2" data-line-number="2">predict =<span class="st"> </span>stats<span class="op">::</span>predict</a>
<a class="sourceLine" id="cb1200-3" data-line-number="3">tr.tune =<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">alpha =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">1</span>), <span class="dt">lambda =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1200-4" data-line-number="4">(<span class="dt">tr.model =</span> caret<span class="op">::</span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>hp <span class="op">+</span><span class="st"> </span>cyl, <span class="dt">data=</span>mtcars, </a>
<a class="sourceLine" id="cb1200-5" data-line-number="5">          <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;scale&quot;</span>, <span class="st">&quot;center&quot;</span> ),</a>
<a class="sourceLine" id="cb1200-6" data-line-number="6">          <span class="dt">method=</span><span class="st">&quot;glmnet&quot;</span>, <span class="dt">tuneGrid =</span> tr.tune, <span class="dt">trControl =</span> tr.control))</a></code></pre></div>
<pre><code>## glmnet 
## 
## 32 samples
##  3 predictor
## 
## Pre-processing: scaled (3), centered (3) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 25, 27, 25, 27, 24 
## Resampling results across tuning parameters:
## 
##   alpha  lambda  RMSE   Rsquared  MAE  
##   0.0    0.0     2.599  0.8681    2.164
##   0.0    0.5     2.600  0.8682    2.165
##   0.0    1.0     2.614  0.8692    2.168
##   0.5    0.0     2.619  0.8650    2.192
##   0.5    0.5     2.625  0.8659    2.198
##   0.5    1.0     2.670  0.8662    2.213
##   1.0    0.0     2.621  0.8647    2.195
##   1.0    0.5     2.670  0.8618    2.236
##   1.0    1.0     2.808  0.8580    2.326
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 0 and lambda = 0.</code></pre>

<p>The output shows an optimal model based on the lowest <strong>RMSE</strong> (e.g., the choice of performance metric) using ridge regression (<strong>alpha=0.0</strong>) and <strong>lambda=0.0</strong>. We can then use the model for prediction.</p>
<div class="sourceCode" id="cb1202"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1202-1" data-line-number="1">tr.predict =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(tr.model, <span class="dt">newx =</span> x)</a></code></pre></div>
</div>
<div id="global-and-local-minima" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.7</span> Global and Local Minima  <a href="9.7-general-modeling.html#global-and-local-minima" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, in the context of a <strong>Loss function</strong>, when we deal with data, we need to perform the estimation in most cases. We try our best to predict an estimated value of our data close to an actual value. Because we are estimating, our result will come with some inaccuracies or errors. Intuitively, especially in cases where we cannot afford errors, we need to minimize these errors. Errors can be costly and so when we say minimize error, we may also be minimizing cost.</p>
<p>We will be covering <strong>Error functions</strong> and <strong>Loss functions</strong> in later chapters. However, for now, let us use Figure <a href="9.7-general-modeling.html#fig:localminima">9.70</a> to talk about the basic concepts of Global and Local Minima. The local minima is at <span class="math inline">\(x = 1\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:localminima"></span>
<img src="localminima.png" alt="Global and Local Minima" width="60%" />
<p class="caption">
Figure 9.70: Global and Local Minima
</p>
</div>
<p>In training a good model, we notice that the result of our <strong>Loss function</strong> for every iteration during training begins to follow a convex curve when charted. We hope to find the minimum point somewhere along the convex curve - that is our minimum error. However, there may be cases when the curve does not necessarily form a single convex curve. It may form multiple convex shapes or saddle areas, each with multiple minimum points - we call these points our local minima. Our hope in this situation is to find the global minima - the smallest minima. </p>
<p>We shall see how we minimize <strong>Loss functions</strong> and avoid local minima when we discuss <strong>Neural Network</strong>, which involves using gradient descent and all other optimization strategies.</p>
</div>
<div id="regularization" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.7.8</span> Regularization<a href="9.7-general-modeling.html#regularization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Regularization</strong> is mostly used as an added <strong>tuning</strong> technique during <strong>learning</strong>. The idea is to calibrate hyperparameters and thus tune modeling algorithms. In doing so, we effectively optimize our models and reduce model complexity. It can, therefore, also effectively disqualify features by penalizing coefficients (a.l.a embedded-based feature selection in essence). </p>
<p>Recall that <strong>Regularization</strong> is a way to reward or penalize. In linear regression, we use regularization to reward or penalize coefficients through the <strong>Loss functions</strong>, either diminishing or amplifying the effect of the <strong>Loss</strong>. Our goal is to guide coefficients, so the model fits even more optimal among data points. Note that <strong>OLS</strong> fits a model by adjusting two <strong>model parameters</strong>, also called <strong>coefficients</strong>, namely <strong>slope</strong> and <strong>interface</strong>. We add weight to the two coefficients by introducing a tunable <strong>regularization hyperparameter</strong>, namely the <strong>lambda</strong> <span class="math inline">\(\lambda\)</span>. We then use an updated <strong>Loss</strong> function that accommodates the hyperparameter to specifically minimize error, e.g., <strong>MSE</strong> (mean square error); hence, calibration is about minimizing our objective (or loss) function.</p>
<p>Let us review three <strong>regularization</strong> techniques in this section, namely <strong>ridge</strong>, <strong>lasso</strong>, and <strong>elastic net</strong>. First, let us mention a couple of <strong>loss</strong> functions we use during <strong>cross-validation</strong>.</p>
<ul>
<li><strong>L1-norm vs L2-norm</strong>  </li>
</ul>
<p>The <strong>least absolute deviation (LAD)</strong> is referred to as <strong>L1-norm</strong> and is expressed mathematically as: </p>
<p><span class="math display" id="eq:equate1110071">\[\begin{align}
LS_{L1-norm} = \sum_{i=1} |y_{i} - f(x_{i})| \tag{9.75} 
\end{align}\]</span></p>
<p>The <strong>least squared error (LSE)</strong> is referred to as <strong>L2-norm</strong> and is expressed mathematically as: </p>
<p><span class="math display" id="eq:equate1110072">\[\begin{align}
LS_{L2-norm} = \sum_{i=1} (y_{i} - f(x_{i}))^2 \tag{9.76} 
\end{align}\]</span></p>
<p>The difference in the formula is between using absolute difference versus using squared difference.</p>
<ul>
<li><strong>L1-loss vs L2-loss</strong>  </li>
</ul>
<p>The <strong>mean absolute error (MAE)</strong> is referred to as L1-loss and is expressed mathematically as: </p>
<p><span class="math display" id="eq:equate1110073">\[\begin{align}
MAE_{L1} = \frac{LS_{L1-norm}}{N} \tag{9.77} 
\end{align}\]</span></p>
<p>The <strong>mean squared error (MSE)</strong> is referred to as L2-loss and is expressed mathematically as:  </p>
<p><span class="math display" id="eq:equate1110074">\[\begin{align}
MSE_{L2} = \frac{LS_{L2-norm}}{N} \tag{9.78} 
\end{align}\]</span></p>
<p>As for <strong>Regularization</strong>, we focus on using <strong>lambda</strong> as a tuning hyperparameter. The first to cover is <strong>Ridge</strong> regularization.</p>
<p><strong>Ridge Regularization (alpha=0)</strong> </p>
<p>With <strong>Ridge regularization</strong>, we have an updated formula for coefficients <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p><span class="math display" id="eq:equate1110075">\[\begin{align}
\hat{\beta}_{ridge} = 
\underbrace{(X^{T}X + \lambda I)^{-1}X^{T}y}_{\text{Regularized OLS thru Ridge}}
\ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ \ \underbrace{(X^{T}X)^{-1}X^{T}y}_{OLS} \tag{9.79} 
\end{align}\]</span></p>
<p>Our least square objective <span class="math inline">\(LS_{(objective)}\)</span> is expressed as:</p>
<p><span class="math display" id="eq:equate1110076">\[\begin{align}
LS_{(objective)} = | y - X\beta|^2 = \sum_{i=1}^n (y_{i} - x_{i}\beta)^2, \tag{9.80} 
\end{align}\]</span></p>
<p>We modify the function to accommodate the <strong>Ridge regularization</strong> like so:</p>
<p><span class="math display" id="eq:equate1110077">\[\begin{align}
RSS_{(\beta_{ridge})} = |y - X\beta|^2 + \lambda \beta^2 = 
\underbrace{\sum_{i=1}^n (y_{i} - x_{i}\beta)^2}_{\text{RSS}} + 
\underbrace{\lambda \sum_{i=1}^n \beta_i^2}_{\text{Penalty Term}} \tag{9.81} 
\end{align}\]</span></p>
<p>Notice the addition of <strong>lambda</strong> denoted as <span class="math inline">\(\lambda I\)</span>. This tuning hyperparameter allows shrinking the coefficients, <span class="math inline">\(\beta s\)</span>, making Ridge coefficients lesser than Least-Square coefficients.</p>
<p>Our objective function for <strong>Ridge regression</strong> is therefore expressed as such:</p>
<p><span class="math display" id="eq:equate1110078">\[\begin{align}
\hat{\beta}_{ridge} = \text{arg}\ \underset{\beta}{\mathrm{min}}\ RSS_{(\beta_{ridge})} \tag{9.82} 
\end{align}\]</span></p>
<p>To illustrate, let us use the <strong>glmnet(.)</strong> function to generate a <strong>General Linear Regression model</strong>. We provide <strong>alpha</strong> <span class="math inline">\((\alpha)\)</span> equal to 0 to indicate we are using <strong>ridge regression</strong> (see Figure <a href="9.7-general-modeling.html#fig:ridgereg1">9.71</a>).</p>

<div class="sourceCode" id="cb1203"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1203-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a>
<a class="sourceLine" id="cb1203-2" data-line-number="2">x =<span class="st">  </span><span class="kw">data.matrix</span>( mtcars[,<span class="op">!</span>(<span class="kw">names</span>(mtcars) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>))] )</a>
<a class="sourceLine" id="cb1203-3" data-line-number="3">y =<span class="st">  </span>mtcars[, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)]</a>
<a class="sourceLine" id="cb1203-4" data-line-number="4">ridge.model =<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1203-5" data-line-number="5"><span class="kw">plot</span>(ridge.model, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>,  <span class="dt">label=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1203-6" data-line-number="6"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">legend=</span><span class="kw">colnames</span>(x), <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1203-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ridgereg1"></span>
<img src="DS_files/figure-html/ridgereg1-1.png" alt="Ridge Regularization" width="70%" />
<p class="caption">
Figure 9.71: Ridge Regularization
</p>
</div>

<p>Figure <a href="9.7-general-modeling.html#fig:ridgereg1">9.71</a> shows a plot of <strong>coefficients</strong>, each corresponding to a feature variable in the <strong>mtcars</strong> dataset. Each coefficient converges toward zero from left to right as <strong>log lambda</strong> increases, virtually eliminating the features. If we settle the <strong>log lambda</strong> at 4, we can see that <strong>am</strong>, <strong>drat</strong>, <strong>vs</strong>, <strong>gear</strong>, <strong>disp</strong>, <strong>cyl</strong>, <strong>wt</strong>, and <strong>qsec</strong> features all have coefficients that are non-zero. On the other hand, <strong>qsec</strong> is close to being zero. In other words, it seems only eight out of ten are important features just based on observing the plot.</p>
<p>Let us perform <strong>cross-validation</strong> using <strong>cv.glmnet(.)</strong> function to confirm our assessment of the eight coefficients. Here, we split the <strong>mtcars</strong> dataset into 5 sets (see Figure <a href="9.7-general-modeling.html#fig:ridgereg2">9.72</a>).</p>

<div class="sourceCode" id="cb1204"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1204-1" data-line-number="1">cv.ridge =<span class="st"> </span><span class="kw">cv.glmnet</span>(x, y, <span class="dt">nfolds=</span><span class="dv">5</span>, <span class="dt">type=</span><span class="st">&quot;mse&quot;</span>, <span class="dt">alpha=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1204-2" data-line-number="2"><span class="kw">plot</span>(cv.ridge)</a>
<a class="sourceLine" id="cb1204-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ridgereg2"></span>
<img src="DS_files/figure-html/ridgereg2-1.png" alt="Ridge Regularization (CV)" width="70%" />
<p class="caption">
Figure 9.72: Ridge Regularization (CV)
</p>
</div>

<p>In Figure <a href="9.7-general-modeling.html#fig:ridgereg2">9.72</a>, there are two vertical dotted lines. The first dotted line represents the minimum lambda, and the second dotted line represents <strong>one standard error</strong> (<strong>1se</strong>) from the minimum lambda. It also shows that the range between the minimum lambda and the lambda is one standard error away, covering a minimum <strong>MSE</strong>, making any lambdas within the range candidates for regularization. </p>

<div class="sourceCode" id="cb1205"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1205-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;Minimum Lambda&quot;</span>=<span class="st"> </span>(<span class="dt">lambda.min =</span> cv.ridge<span class="op">$</span>lambda.min),</a>
<a class="sourceLine" id="cb1205-2" data-line-number="2">  <span class="st">&quot;1se Lambda&quot;</span> =<span class="st"> </span>(<span class="dt">lambda.1se =</span> cv.ridge<span class="op">$</span>lambda<span class="fl">.1</span>se))</a></code></pre></div>
<pre><code>## Minimum Lambda     1se Lambda 
##          2.747          9.206</code></pre>

<p>Note that as we use <strong>cross-validation</strong>, we need to compute the <strong>MSE</strong> between <strong>training sets</strong>. The equivalent specific <strong>lambdas</strong> for <strong>min</strong> and <strong>1se</strong> are computed using the following:</p>

<div class="sourceCode" id="cb1207"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1207-1" data-line-number="1">idx.min =<span class="st"> </span><span class="kw">which</span>(cv.ridge<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>lambda.min)</a>
<a class="sourceLine" id="cb1207-2" data-line-number="2">idx<span class="fl">.1</span>se =<span class="st"> </span><span class="kw">which</span>(cv.ridge<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>lambda<span class="fl">.1</span>se) </a>
<a class="sourceLine" id="cb1207-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;MSE@lambda.min&quot;</span> =<span class="st"> </span>cv.ridge<span class="op">$</span>cvm[idx.min],</a>
<a class="sourceLine" id="cb1207-4" data-line-number="4">  <span class="st">&quot;MSE@lambda.1se&quot;</span> =<span class="st"> </span>cv.ridge<span class="op">$</span>cvm[idx<span class="fl">.1</span>se],</a>
<a class="sourceLine" id="cb1207-5" data-line-number="5">  <span class="st">&quot;1se&quot;</span> =<span class="st"> </span>cv.ridge<span class="op">$</span>cvsd[idx.min],</a>
<a class="sourceLine" id="cb1207-6" data-line-number="6">  <span class="st">&quot;MSE(our 1se)&quot;</span>  =<span class="st"> </span>cv.ridge<span class="op">$</span>cvm[idx.min] <span class="op">+</span><span class="st"> </span>cv.ridge<span class="op">$</span>cvsd[idx.min]</a>
<a class="sourceLine" id="cb1207-7" data-line-number="7">  )</a></code></pre></div>
<pre><code>## MSE@lambda.min MSE@lambda.1se            1se   MSE(our 1se) 
##          6.881          7.965          1.228          8.109</code></pre>

<p>If we choose <strong>lambda.1se</strong> (9.2061), then we have the following coefficients for the features:</p>

<div class="sourceCode" id="cb1209"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1209-1" data-line-number="1"><span class="kw">coef</span>(cv.ridge, <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>)</a></code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                    1
## (Intercept) 19.85948
## cyl         -0.36149
## disp        -0.00524
## hp          -0.00962
## drat         0.98359
## wt          -0.86947
## qsec         0.15136
## vs           0.87153
## am           1.17967
## gear         0.49498
## carb        -0.36970</code></pre>

<p>Equivalently, we can arbitrarily plug the value of <strong>lambda.1se</strong> and run <strong>regression</strong> like so:</p>
<div class="sourceCode" id="cb1211"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1211-1" data-line-number="1">ridge.model =<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha=</span><span class="dv">0</span>, <span class="dt">lambda=</span>lambda<span class="fl">.1</span>se)</a>
<a class="sourceLine" id="cb1211-2" data-line-number="2"><span class="kw">coef</span>(ridge.model)</a></code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                    s0
## (Intercept) 19.862051
## cyl         -0.361608
## disp        -0.005244
## hp          -0.009626
## drat         0.984155
## wt          -0.869465
## qsec         0.151322
## vs           0.870986
## am           1.179181
## gear         0.494695
## carb        -0.369557</code></pre>
<p>Afterwhich, we can then perform prediction using the ridge model:</p>
<div class="sourceCode" id="cb1213"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1213-1" data-line-number="1">ridge.predict =<span class="st"> </span><span class="kw">predict.glmnet</span>(ridge.model, <span class="dt">newx =</span> x)</a></code></pre></div>
<p>As one can see, based on our <strong>cross-validation</strong>, it seems that our <strong>cross-validation</strong> does show that all ten coefficients are used as long as our lambda between the range <strong>lambda.min</strong> and <strong>lambda.1se</strong>, which fall under a minimum <strong>MSE</strong>. It does not force the <strong>coefficients</strong> to become zero. Therefore, we still see that all ten coefficients are kept. We will show <strong>LASSO</strong> regularization next, which forces <strong>coefficients</strong> to become zero.</p>
<p>A couple of notes to be aware of:</p>
<ul>
<li><p>Increase in ridge lambda decreases variance but increases bias.</p></li>
<li><p>Ridge avoids overfitting by penalizing coefficients. It means it shrinks beta coefficients reducing MSE and predicted error.</p></li>
</ul>
<p><strong>Lasso Regularization (alpha=1)</strong> </p>
<p><strong>LASSO</strong> stands for <strong>Least Absolute Selection and Shrinkage Operation</strong>. This <strong>regularization</strong> technique also decreases variance but increases bias. However, unlike <strong>Ridge regularization</strong>, it forces coefficients to become zero. Because of that, coefficients that are not significant tend toward zero and are eliminated, so in a way, this removes unwanted input variables.</p>
<p>Similarly, with <strong>Lasso regularization</strong>, we have an updated formula for coefficients <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p><span class="math display" id="eq:equate1110079">\[\begin{align}
\hat{\beta}_{lasso} = 
\underbrace{(X^{T}X + \lambda I)^{-1}X^{T}y}_{\text{Regularized OLS thru Lasso}}
\ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ \ \underbrace{(X^{T}X)^{-1}X^{T}y}_{OLS} \tag{9.83} 
\end{align}\]</span></p>
<p>Our least square objective <span class="math inline">\(LS_{(objective)}\)</span> is expressed as:</p>
<p><span class="math display" id="eq:equate1110080">\[\begin{align}
LS_{objective} = | y - X\beta|^2 = \sum_{i=1}^n (y_{i} - x_{i}\beta)^2,  \tag{9.84} 
\end{align}\]</span></p>
<p>We modify the function to accommodate the <strong>ridge regularization</strong> like so:</p>
<p><span class="math display" id="eq:equate1110081">\[\begin{align}
RSS_{(\beta_{lasso})} = |y - X\beta|^2 + \lambda |\beta| = 
\underbrace{\sum_{i=1}^n (y_{i} - x_{i}\beta)^2}_{\text{RSS}} + 
\underbrace{\lambda \sum_{i=1}^n |\beta_i|}_{\text{Penalty Term}},   \tag{9.85} 
\end{align}\]</span></p>
<p>Our objective function for <strong>Ridge regression</strong> is therefore expressed as such:</p>
<p><span class="math display" id="eq:equate1110082">\[\begin{align}
\hat{\beta}_{lasso} = \text{arg}\ \underset{\beta}{\mathrm{min}}\ RSS_{(\beta_{lasso})}. \tag{9.86} 
\end{align}\]</span></p>
<p>To illustrate, we provide <strong>alpha</strong> <span class="math inline">\((\alpha)\)</span> equal to 1 to indicate we are using <strong>LASSO regression</strong> (see Figure <a href="9.7-general-modeling.html#fig:lassoreg1">9.73</a>).</p>

<div class="sourceCode" id="cb1214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1214-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a>
<a class="sourceLine" id="cb1214-2" data-line-number="2">x =<span class="st">  </span><span class="kw">data.matrix</span>( mtcars[,<span class="op">!</span>(<span class="kw">names</span>(mtcars) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>))] )</a>
<a class="sourceLine" id="cb1214-3" data-line-number="3">y =<span class="st">  </span>mtcars[, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)]</a>
<a class="sourceLine" id="cb1214-4" data-line-number="4">lasso.model =<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1214-5" data-line-number="5"><span class="kw">plot</span>(lasso.model, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>,  <span class="dt">label=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1214-6" data-line-number="6"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">legend=</span><span class="kw">colnames</span>(x), <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1214-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lassoreg1"></span>
<img src="DS_files/figure-html/lassoreg1-1.png" alt="Lasso Regularization" width="70%" />
<p class="caption">
Figure 9.73: Lasso Regularization
</p>
</div>

<p>Figure <a href="9.7-general-modeling.html#fig:lassoreg1">9.73</a> shows a plot of <strong>coefficients</strong>, each corresponding to a feature variable in the <strong>mtcars</strong> dataset. From left to right, each coefficient converges toward zero as <strong>log lambda</strong> increases, virtually eliminating the feature. Unlike <strong>ridge</strong>, we can see in the plot that given a <strong>log lambda</strong> equal to zero, the number of coefficients (see top labels) is equal to three. Just by observation, the three features corresponding to the coefficients are <strong>wt</strong>, <strong>cyl</strong>, and <strong>hp</strong>.</p>
<p>Let us perform <strong>cross-validation</strong> using <strong>cv.glmnet(.)</strong> function to confirm our assessment of the three coefficients. Here, as before, we split the <strong>mtcars</strong> dataset into 5 sets (see Figure <a href="9.7-general-modeling.html#fig:lassoreg2">9.74</a>).</p>
<div class="sourceCode" id="cb1215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1215-1" data-line-number="1">cv.lasso =<span class="st"> </span><span class="kw">cv.glmnet</span>(x, y, <span class="dt">nfolds=</span><span class="dv">5</span>, <span class="dt">type=</span><span class="st">&quot;mse&quot;</span>, <span class="dt">alpha=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1215-2" data-line-number="2"><span class="kw">plot</span>(cv.lasso)</a>
<a class="sourceLine" id="cb1215-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lassoreg2"></span>
<img src="DS_files/figure-html/lassoreg2-1.png" alt="Lasso Regularization (CV)" width="70%" />
<p class="caption">
Figure 9.74: Lasso Regularization (CV)
</p>
</div>
<p>In Figure <a href="9.7-general-modeling.html#fig:lassoreg2">9.74</a>, we see the range between the minimum lambda and the lambda that is one standard error away, which covers a minimum <strong>MSE</strong>, making any lambdas within the range candidates for regularization.</p>
<div class="sourceCode" id="cb1216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1216-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;Minimum Lambda&quot;</span>=<span class="st"> </span>(<span class="dt">lambda.min =</span> cv.lasso<span class="op">$</span>lambda.min),</a>
<a class="sourceLine" id="cb1216-2" data-line-number="2">  <span class="st">&quot;1se Lambda&quot;</span> =<span class="st"> </span>(<span class="dt">lambda.1se =</span> cv.lasso<span class="op">$</span>lambda<span class="fl">.1</span>se))</a></code></pre></div>
<pre><code>## Minimum Lambda     1se Lambda 
##         0.1246         1.1617</code></pre>
<p>It can be observed that the vertical dotted line for the <strong>lambda.1se</strong> points towards a lesser number of coefficients.</p>
<p>Note that as we are using <strong>cross-validation</strong>, we need to compute the <strong>MSE</strong> between <strong>training sets</strong>. The equivalent specific <strong>lambdas</strong> for <strong>min</strong> and <strong>1se</strong> are computed using the following:
</p>
<div class="sourceCode" id="cb1218"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1218-1" data-line-number="1">idx.min =<span class="st"> </span><span class="kw">which</span>(cv.lasso<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>lambda.min)</a>
<a class="sourceLine" id="cb1218-2" data-line-number="2">idx<span class="fl">.1</span>se =<span class="st"> </span><span class="kw">which</span>(cv.lasso<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>lambda<span class="fl">.1</span>se) </a>
<a class="sourceLine" id="cb1218-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;MSE@lambda.min&quot;</span> =<span class="st"> </span>cv.lasso<span class="op">$</span>cvm[idx.min],</a>
<a class="sourceLine" id="cb1218-4" data-line-number="4">  <span class="st">&quot;MSE@lambda.1se&quot;</span> =<span class="st"> </span>cv.lasso<span class="op">$</span>cvm[idx<span class="fl">.1</span>se],</a>
<a class="sourceLine" id="cb1218-5" data-line-number="5">  <span class="st">&quot;1se&quot;</span> =<span class="st"> </span>cv.lasso<span class="op">$</span>cvsd[idx.min],</a>
<a class="sourceLine" id="cb1218-6" data-line-number="6">  <span class="st">&quot;MSE(our 1se)&quot;</span>  =<span class="st"> </span>cv.lasso<span class="op">$</span>cvm[idx.min] <span class="op">+</span><span class="st"> </span>cv.lasso<span class="op">$</span>cvsd[idx.min]</a>
<a class="sourceLine" id="cb1218-7" data-line-number="7">  )</a></code></pre></div>
<pre><code>## MSE@lambda.min MSE@lambda.1se            1se   MSE(our 1se) 
##         8.7300         9.5551         0.8342         9.5642</code></pre>

<p>If we choose <strong>lambda.1se</strong> (1.1617), then we have the following coefficients for the features:</p>
<div class="sourceCode" id="cb1220"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1220-1" data-line-number="1"><span class="kw">coef</span>(cv.lasso, <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>)</a></code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                     1
## (Intercept) 34.758210
## cyl         -0.860295
## disp         .       
## hp          -0.008835
## drat         .       
## wt          -2.501672
## qsec         .       
## vs           .       
## am           .       
## gear         .       
## carb         .</code></pre>
<p>Equivalently, we can arbitrarily plug the value of <strong>lambda.1se</strong> and run <strong>regression</strong> like so:</p>
<div class="sourceCode" id="cb1222"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1222-1" data-line-number="1">lasso.model =<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha=</span><span class="dv">1</span>, <span class="dt">lambda=</span>lambda<span class="fl">.1</span>se)</a>
<a class="sourceLine" id="cb1222-2" data-line-number="2"><span class="kw">coef</span>(lasso.model)</a></code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                    s0
## (Intercept) 34.757287
## cyl         -0.859728
## disp         .       
## hp          -0.008845
## drat         .       
## wt          -2.502011
## qsec         .       
## vs           .       
## am           .       
## gear         .       
## carb         .</code></pre>
<p>Notice that we only see three features listed, namely <strong>cyl</strong>, <strong>hp</strong>, and <strong>wt</strong>.</p>
<p>We can then perform prediction using the lasso model:</p>
<div class="sourceCode" id="cb1224"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1224-1" data-line-number="1">lasso.predict =<span class="st"> </span><span class="kw">predict.glmnet</span>(lasso.model, <span class="dt">newx =</span> x)</a></code></pre></div>
<p><strong>Elastic Net Regularization (alpha=0.5)</strong> </p>
<p>We leave readers to investigate <strong>Elastic Net</strong>. This technique meets in between both <strong>Ridge</strong> and <strong>LASSO</strong> regularization. It accepts <strong>alpha</strong> equal to 0.5, e.g. (see Figure <a href="9.7-general-modeling.html#fig:elasticnet1">9.75</a>).</p>
<div class="sourceCode" id="cb1225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1225-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a>
<a class="sourceLine" id="cb1225-2" data-line-number="2">x =<span class="st">  </span><span class="kw">data.matrix</span>( mtcars[,<span class="op">!</span>(<span class="kw">names</span>(mtcars) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>))] )</a>
<a class="sourceLine" id="cb1225-3" data-line-number="3">y =<span class="st">  </span>mtcars[, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)]</a>
<a class="sourceLine" id="cb1225-4" data-line-number="4">elastic.model =<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb1225-5" data-line-number="5"><span class="kw">plot</span>(elastic.model, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>,  <span class="dt">label=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1225-6" data-line-number="6"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">legend=</span><span class="kw">colnames</span>(x), <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x))</a>
<a class="sourceLine" id="cb1225-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:elasticnet1"></span>
<img src="DS_files/figure-html/elasticnet1-1.png" alt="Elastic Net Regularization" width="70%" />
<p class="caption">
Figure 9.75: Elastic Net Regularization
</p>
</div>
<p>And for <strong>cross-validation</strong> of <strong>Elastic Net</strong>, see Figure <a href="9.7-general-modeling.html#fig:elasticnet2">9.76</a>.</p>
<div class="sourceCode" id="cb1226"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1226-1" data-line-number="1">cv.elastic =<span class="st"> </span><span class="kw">cv.glmnet</span>(x, y, <span class="dt">nfolds=</span><span class="dv">5</span>, <span class="dt">type=</span><span class="st">&quot;mse&quot;</span>, <span class="dt">alpha=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb1226-2" data-line-number="2"><span class="kw">plot</span>(cv.elastic)</a>
<a class="sourceLine" id="cb1226-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:elasticnet2"></span>
<img src="DS_files/figure-html/elasticnet2-1.png" alt="Elastic Net Regularization (CV)" width="70%" />
<p class="caption">
Figure 9.76: Elastic Net Regularization (CV)
</p>
</div>
<p>We also leave readers to investigate <strong>Coordinate Descent</strong> for <strong>Lasso</strong>, which may be helpful for multivariate multinomial predictors.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="9.6-featureengineering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="9.8-supervised-vs.unsupervised-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
