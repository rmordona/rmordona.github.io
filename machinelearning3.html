<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Computational Learning III | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Computational Learning III | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Computational Learning III | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2023-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="machinelearning2.html"/>
<link rel="next" href="deeplearning1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#acknowledgment-and-motivations"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#caveat"><i class="fa fa-check"></i><b>0.2</b> Caveat</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i><b>0.3</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.4" data-path="mathematical-notation.html"><a href="mathematical-notation.html#notation"><i class="fa fa-check"></i><b>0.4</b> Notation</a></li>
<li class="chapter" data-level="0.5" data-path="mathematical-notation.html"><a href="mathematical-notation.html#number-system"><i class="fa fa-check"></i><b>0.5</b> Number System</a></li>
<li class="chapter" data-level="0.6" data-path="mathematical-notation.html"><a href="mathematical-notation.html#implementation"><i class="fa fa-check"></i><b>0.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numericalmethods.html"><a href="numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="numericalmethods.html"><a href="numericalmethods.html#closed-form-equation"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="numericalmethods.html"><a href="numericalmethods.html#analytical-and-numerical-solutions"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="numericalmethods.html"><a href="numericalmethods.html#significant-figures"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="numericalmethods.html"><a href="numericalmethods.html#accuracy"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="numericalmethods.html"><a href="numericalmethods.html#precision"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="numericalmethods.html"><a href="numericalmethods.html#stability-and-sensitivity"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="numericalmethods.html"><a href="numericalmethods.html#stiffness-and-implicitness"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="numericalmethods.html"><a href="numericalmethods.html#conditioning-and-posedness"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="linearalgebra.html"><a href="linearalgebra.html#system-of-linear-equations"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="linearalgebra.html"><a href="linearalgebra.html#scalar-vector-and-matrix-tensor"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition-and-multiplication"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="linearalgebra.html"><a href="linearalgebra.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="linearalgebra.html"><a href="linearalgebra.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linearalgebra.html"><a href="linearalgebra.html#magnitude-direction-unit-vectors"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-combination-and-independence"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="linearalgebra.html"><a href="linearalgebra.html#space-span-and-basis"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="linearalgebra.html"><a href="linearalgebra.html#determinants"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="linearalgebra.html"><a href="linearalgebra.html#minors-cofactors-and-adjugate-forms"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="linearalgebra.html"><a href="linearalgebra.html#inverse-form-and-row-echelon-form"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-transformations"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="linearalgebra.html"><a href="linearalgebra.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="linearalgebra.html"><a href="linearalgebra.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="linearalgebra.html"><a href="linearalgebra.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="linearalgebra.html"><a href="linearalgebra.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="linearalgebra.html"><a href="linearalgebra.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="linearalgebra.html"><a href="linearalgebra.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="linearalgebra.html"><a href="linearalgebra.html#rank-and-nullity"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="linearalgebra.html"><a href="linearalgebra.html#singularity-and-triviality"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="linearalgebra.html"><a href="linearalgebra.html#orthogonality-and-orthonormality"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="linearalgebra.html"><a href="linearalgebra.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-reconstruction-using-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="linearalgebra.html"><a href="linearalgebra.html#diagonalizability-of-a-matrix"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="linearalgebra.html"><a href="linearalgebra.html#trace-of-a-square-matrix"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="linearalgebra.html"><a href="linearalgebra.html#algebraic-and-geometric-multiplicity"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="linearalgebra.html"><a href="linearalgebra.html#types-of-matrices"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-factorization"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="linearalgebra.html"><a href="linearalgebra.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="linearalgebra.html"><a href="linearalgebra.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="linearalgebra.html"><a href="linearalgebra.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="linearalgebra.html"><a href="linearalgebra.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="linearalgebra.html"><a href="linearalgebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="linearalgebra.html"><a href="linearalgebra.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="linearalgebra.html"><a href="linearalgebra.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="linearalgebra.html"><a href="linearalgebra.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="linearalgebra.html"><a href="linearalgebra.html#software-libraries"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="linearalgebra.html"><a href="linearalgebra.html#summary"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#iteration-and-convergence"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-root-and-fixed-point-by-iteration"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-solutions-to-systems-of-eqs-by-iteration-ax-b"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialregression"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-polynomial-functions-by-series-expansion"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialinterpolation"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialsmoothing"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-optimization"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#summary-1"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numericalcalculus.html"><a href="numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#introductory-calculus"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-integration"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-differentiation"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-ordinary-differential-equations"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="numericalcalculus.html"><a href="numericalcalculus.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="numericalcalculus.html"><a href="numericalcalculus.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="numericalcalculus.html"><a href="numericalcalculus.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="numericalcalculus.html"><a href="numericalcalculus.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="numericalcalculus.html"><a href="numericalcalculus.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="numericalcalculus.html"><a href="numericalcalculus.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-functional-differential-equations"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-partial-differential-equations"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-fourier-series-and-transform"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#summary-2"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="numericalprobability.html"><a href="numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="numericalprobability.html"><a href="numericalprobability.html#approximation-based-on-random-chances"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="numericalprobability.html"><a href="numericalprobability.html#distribution"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="numericalprobability.html"><a href="numericalprobability.html#mass-and-density"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="numericalprobability.html"><a href="numericalprobability.html#probability"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-density-function-pdf"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-mass-function-pmf"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="numericalprobability.html"><a href="numericalprobability.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="numericalprobability.html"><a href="numericalprobability.html#special-functions"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="numericalprobability.html"><a href="numericalprobability.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="numericalprobability.html"><a href="numericalprobability.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="numericalprobability.html"><a href="numericalprobability.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="numericalprobability.html"><a href="numericalprobability.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="numericalprobability.html"><a href="numericalprobability.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="numericalprobability.html"><a href="numericalprobability.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="numericalprobability.html"><a href="numericalprobability.html#distributiontypes"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="numericalprobability.html"><a href="numericalprobability.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="numericalprobability.html"><a href="numericalprobability.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="numericalprobability.html"><a href="numericalprobability.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="numericalprobability.html"><a href="numericalprobability.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="numericalprobability.html"><a href="numericalprobability.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="numericalprobability.html"><a href="numericalprobability.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="numericalprobability.html"><a href="numericalprobability.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="numericalprobability.html"><a href="numericalprobability.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="numericalprobability.html"><a href="numericalprobability.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="numericalprobability.html"><a href="numericalprobability.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="numericalprobability.html"><a href="numericalprobability.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="numericalprobability.html"><a href="numericalprobability.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="numericalprobability.html"><a href="numericalprobability.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="numericalprobability.html"><a href="numericalprobability.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="numericalprobability.html"><a href="numericalprobability.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="numericalprobability.html"><a href="numericalprobability.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="numericalprobability.html"><a href="numericalprobability.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="numericalprobability.html"><a href="numericalprobability.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="numericalprobability.html"><a href="numericalprobability.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="numericalprobability.html"><a href="numericalprobability.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="numericalprobability.html"><a href="numericalprobability.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="numericalprobability.html"><a href="numericalprobability.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="numericalprobability.html"><a href="numericalprobability.html#summary-3"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="statistics.html"><a href="statistics.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="statistics.html"><a href="statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="statistics.html"><a href="statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="statistics.html"><a href="statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="statistics.html"><a href="statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="statistics.html"><a href="statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="statistics.html"><a href="statistics.html#inferential-statistics"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="statistics.html"><a href="statistics.html#the-significance-of-difference"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="statistics.html"><a href="statistics.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="statistics.html"><a href="statistics.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="statistics.html"><a href="statistics.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="statistics.html"><a href="statistics.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="statistics.html"><a href="statistics.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="statistics.html"><a href="statistics.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="statistics.html"><a href="statistics.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearson’s Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="statistics.html"><a href="statistics.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="statistics.html"><a href="statistics.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="statistics.html"><a href="statistics.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="statistics.html"><a href="statistics.html#post-hoc-analysis"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="statistics.html"><a href="statistics.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="statistics.html"><a href="statistics.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="statistics.html"><a href="statistics.html#multiple-comparison-tests"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="statistics.html"><a href="statistics.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="statistics.html"><a href="statistics.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="statistics.html"><a href="statistics.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="statistics.html"><a href="statistics.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="statistics.html"><a href="statistics.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="statistics.html"><a href="statistics.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="statistics.html"><a href="statistics.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="statistics.html"><a href="statistics.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="statistics.html"><a href="statistics.html#statistical-modeling"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="statistics.html"><a href="statistics.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="statistics.html"><a href="statistics.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="statistics.html"><a href="statistics.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="statistics.html"><a href="statistics.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="statistics.html"><a href="statistics.html#regression-analysis"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="statistics.html"><a href="statistics.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="statistics.html"><a href="statistics.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="statistics.html"><a href="statistics.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="statistics.html"><a href="statistics.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="statistics.html"><a href="statistics.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="statistics.html"><a href="statistics.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="statistics.html"><a href="statistics.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="statistics.html"><a href="statistics.html#the-significance-of-regression"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="statistics.html"><a href="statistics.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="statistics.html"><a href="statistics.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="statistics.html"><a href="statistics.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="statistics.html"><a href="statistics.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="statistics.html"><a href="statistics.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="statistics.html"><a href="statistics.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="statistics.html"><a href="statistics.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="statistics.html"><a href="statistics.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="statistics.html"><a href="statistics.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="statistics.html"><a href="statistics.html#inference-for-regression"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="statistics.html"><a href="statistics.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="statistics.html"><a href="statistics.html#summary-4"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian.html"><a href="bayesian.html#probability-1"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian.html"><a href="bayesian.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian.html"><a href="bayesian.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian.html"><a href="bayesian.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian.html"><a href="bayesian.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="bayesian.html"><a href="bayesian.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian.html"><a href="bayesian.html#probability-rules"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian.html"><a href="bayesian.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian.html"><a href="bayesian.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian.html"><a href="bayesian.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="bayesian.html"><a href="bayesian.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="bayesian.html"><a href="bayesian.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="bayesian.html"><a href="bayesian.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="bayesian.html"><a href="bayesian.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="bayesian.html"><a href="bayesian.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="bayesian.html"><a href="bayesian.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian.html"><a href="bayesian.html#bayes-theorem"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian.html"><a href="bayesian.html#naïve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naïve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian.html"><a href="bayesian.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="bayesian.html"><a href="bayesian.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian.html"><a href="bayesian.html#conjugacy"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian.html"><a href="bayesian.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian.html"><a href="bayesian.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian.html"><a href="bayesian.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="bayesian.html"><a href="bayesian.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="bayesian.html"><a href="bayesian.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="bayesian.html"><a href="bayesian.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="bayesian.html"><a href="bayesian.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="bayesian.html"><a href="bayesian.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="bayesian.html"><a href="bayesian.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="bayesian.html"><a href="bayesian.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="bayesian.html"><a href="bayesian.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="bayesian.html"><a href="bayesian.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian.html"><a href="bayesian.html#information-theory"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian.html"><a href="bayesian.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian.html"><a href="bayesian.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian.html"><a href="bayesian.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian.html"><a href="bayesian.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian.html"><a href="bayesian.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian.html"><a href="bayesian.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian.html"><a href="bayesian.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bayesian.html"><a href="bayesian.html#bayesianinference"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="bayesian.html"><a href="bayesian.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="bayesian.html"><a href="bayesian.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="bayesian.html"><a href="bayesian.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="bayesian.html"><a href="bayesian.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian2.html"><a href="bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-models"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="bayesian2.html"><a href="bayesian2.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="bayesian2.html"><a href="bayesian2.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="bayesian2.html"><a href="bayesian2.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="bayesian2.html"><a href="bayesian2.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="bayesian2.html"><a href="bayesian2.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="bayesian2.html"><a href="bayesian2.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="bayesian2.html"><a href="bayesian2.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="bayesian2.html"><a href="bayesian2.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="bayesian2.html"><a href="bayesian2.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="bayesian2.html"><a href="bayesian2.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="bayesian2.html"><a href="bayesian2.html#simulation-and-sampling"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="bayesian2.html"><a href="bayesian2.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="bayesian2.html"><a href="bayesian2.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="bayesian2.html"><a href="bayesian2.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="bayesian2.html"><a href="bayesian2.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="bayesian2.html"><a href="bayesian2.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="bayesian2.html"><a href="bayesian2.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-analysis"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian2.html"><a href="bayesian2.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian2.html"><a href="bayesian2.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian2.html"><a href="bayesian2.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="bayesian2.html"><a href="bayesian2.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian2.html"><a href="bayesian2.html#summary-5"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="machinelearning1.html"><a href="machinelearning1.html#observation-and-measurement"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="machinelearning1.html"><a href="machinelearning1.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="machinelearning1.html"><a href="machinelearning1.html#input-data"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="machinelearning1.html"><a href="machinelearning1.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="machinelearning1.html"><a href="machinelearning1.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="machinelearning1.html"><a href="machinelearning1.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="machinelearning1.html"><a href="machinelearning1.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="machinelearning1.html"><a href="machinelearning1.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="machinelearning1.html"><a href="machinelearning1.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="machinelearning1.html"><a href="machinelearning1.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="machinelearning1.html"><a href="machinelearning1.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machinelearning1.html"><a href="machinelearning1.html#primitive-methods"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="machinelearning1.html"><a href="machinelearning1.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="machinelearning1.html"><a href="machinelearning1.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="machinelearning1.html"><a href="machinelearning1.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="machinelearning1.html"><a href="machinelearning1.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="machinelearning1.html"><a href="machinelearning1.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="machinelearning1.html"><a href="machinelearning1.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="machinelearning1.html"><a href="machinelearning1.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="machinelearning1.html"><a href="machinelearning1.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="machinelearning1.html"><a href="machinelearning1.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="machinelearning1.html"><a href="machinelearning1.html#distance-metrics"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="machinelearning1.html"><a href="machinelearning1.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="machinelearning1.html"><a href="machinelearning1.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="machinelearning1.html"><a href="machinelearning1.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="machinelearning1.html"><a href="machinelearning1.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="machinelearning1.html"><a href="machinelearning1.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="machinelearning1.html"><a href="machinelearning1.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="machinelearning1.html"><a href="machinelearning1.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="machinelearning1.html"><a href="machinelearning1.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="machinelearning1.html"><a href="machinelearning1.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="machinelearning1.html"><a href="machinelearning1.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="machinelearning1.html"><a href="machinelearning1.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="machinelearning1.html"><a href="machinelearning1.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="machinelearning1.html"><a href="machinelearning1.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="machinelearning1.html"><a href="machinelearning1.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="machinelearning1.html"><a href="machinelearning1.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="machinelearning1.html"><a href="machinelearning1.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="machinelearning1.html"><a href="machinelearning1.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="machinelearning1.html"><a href="machinelearning1.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="machinelearning1.html"><a href="machinelearning1.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="machinelearning1.html"><a href="machinelearning1.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="machinelearning1.html"><a href="machinelearning1.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="machinelearning1.html"><a href="machinelearning1.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="machinelearning1.html"><a href="machinelearning1.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="machinelearning1.html"><a href="machinelearning1.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="machinelearning1.html"><a href="machinelearning1.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="machinelearning1.html"><a href="machinelearning1.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureengineering"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="machinelearning1.html"><a href="machinelearning1.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="machinelearning1.html"><a href="machinelearning1.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="machinelearning1.html"><a href="machinelearning1.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="machinelearning1.html"><a href="machinelearning1.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="machinelearning1.html"><a href="machinelearning1.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="machinelearning1.html"><a href="machinelearning1.html#general-modeling"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="machinelearning1.html"><a href="machinelearning1.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="machinelearning1.html"><a href="machinelearning1.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="machinelearning1.html"><a href="machinelearning1.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="machinelearning1.html"><a href="machinelearning1.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="machinelearning1.html"><a href="machinelearning1.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="machinelearning1.html"><a href="machinelearning1.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="machinelearning1.html"><a href="machinelearning1.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="machinelearning1.html"><a href="machinelearning1.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="machinelearning1.html"><a href="machinelearning1.html#supervised-vs.unsupervised-learning"><i class="fa fa-check"></i><b>9.8</b> Supervised vs. Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="machinelearning1.html"><a href="machinelearning1.html#summary-6"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="machinelearning2.html"><a href="machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="machinelearning2.html"><a href="machinelearning2.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="machinelearning2.html"><a href="machinelearning2.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="machinelearning2.html"><a href="machinelearning2.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="machinelearning2.html"><a href="machinelearning2.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="machinelearning2.html"><a href="machinelearning2.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="machinelearning2.html"><a href="machinelearning2.html#binary-classification-supervised"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="machinelearning2.html"><a href="machinelearning2.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="machinelearning2.html"><a href="machinelearning2.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="machinelearning2.html"><a href="machinelearning2.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="machinelearning2.html"><a href="machinelearning2.html#multi-class-classification-supervised"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="machinelearning2.html"><a href="machinelearning2.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="machinelearning2.html"><a href="machinelearning2.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="machinelearning2.html"><a href="machinelearning2.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="machinelearning2.html"><a href="machinelearning2.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="machinelearning2.html"><a href="machinelearning2.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="machinelearning3.html"><a href="machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="machinelearning3.html"><a href="machinelearning3.html#clustering-unsupervised"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="machinelearning3.html"><a href="machinelearning3.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="machinelearning3.html"><a href="machinelearning3.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="machinelearning3.html"><a href="machinelearning3.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="machinelearning3.html"><a href="machinelearning3.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="machinelearning3.html"><a href="machinelearning3.html#meta-learning"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="machinelearning3.html"><a href="machinelearning3.html#natural-language-processing-nlp"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="machinelearning3.html"><a href="machinelearning3.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="machinelearning3.html"><a href="machinelearning3.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="machinelearning3.html"><a href="machinelearning3.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="machinelearning3.html"><a href="machinelearning3.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="machinelearning3.html"><a href="machinelearning3.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="machinelearning3.html"><a href="machinelearning3.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="machinelearning3.html"><a href="machinelearning3.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="machinelearning3.html"><a href="machinelearning3.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-forecasting"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="machinelearning3.html"><a href="machinelearning3.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="machinelearning3.html"><a href="machinelearning3.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="machinelearning3.html"><a href="machinelearning3.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="machinelearning3.html"><a href="machinelearning3.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="machinelearning3.html"><a href="machinelearning3.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="machinelearning3.html"><a href="machinelearning3.html#recommender-systems"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deeplearning1.html"><a href="deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="deeplearning1.html"><a href="deeplearning1.html#simple-perceptron"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="deeplearning1.html"><a href="deeplearning1.html#adaptive-linear-neuron-adaline"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="deeplearning1.html"><a href="deeplearning1.html#multi-layer-perceptron-mlp"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="deeplearning1.html"><a href="deeplearning1.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="deeplearning1.html"><a href="deeplearning1.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="deeplearning1.html"><a href="deeplearning1.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="deeplearning1.html"><a href="deeplearning1.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="deeplearning1.html"><a href="deeplearning1.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="deeplearning1.html"><a href="deeplearning1.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="deeplearning1.html"><a href="deeplearning1.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="deeplearning1.html"><a href="deeplearning1.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="deeplearning1.html"><a href="deeplearning1.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="deeplearning1.html"><a href="deeplearning1.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="deeplearning1.html"><a href="deeplearning1.html#convolutional-neural-network-cnn"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="deeplearning1.html"><a href="deeplearning1.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="deeplearning1.html"><a href="deeplearning1.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="deeplearning1.html"><a href="deeplearning1.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="deeplearning1.html"><a href="deeplearning1.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="deeplearning1.html"><a href="deeplearning1.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="deeplearning1.html"><a href="deeplearning1.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="deeplearning1.html"><a href="deeplearning1.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="deeplearning1.html"><a href="deeplearning1.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="deeplearning1.html"><a href="deeplearning1.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="deeplearning1.html"><a href="deeplearning1.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="deeplearning1.html"><a href="deeplearning1.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="deeplearning2.html"><a href="deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="deeplearning2.html"><a href="deeplearning2.html#residual-network-resnet"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="deeplearning2.html"><a href="deeplearning2.html#recurrent-neural-network-rnn"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="deeplearning2.html"><a href="deeplearning2.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="deeplearning2.html"><a href="deeplearning2.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="deeplearning2.html"><a href="deeplearning2.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-rnn"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-bidirectional-rnn"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-neural-network-tnn"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="deeplearning2.html"><a href="deeplearning2.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="deeplearning2.html"><a href="deeplearning2.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="deeplearning2.html"><a href="deeplearning2.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="deeplearning2.html"><a href="deeplearning2.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="deeplearning2.html"><a href="deeplearning2.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="deeplearning2.html"><a href="deeplearning2.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="deeplearning2.html"><a href="deeplearning2.html#applications-using-tnn-and-rnn"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="deeplearning2.html"><a href="deeplearning2.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="deeplearning2.html"><a href="deeplearning2.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="deeplearning2.html"><a href="deeplearning2.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="deeplearning2.html"><a href="deeplearning2.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="deeplearning2.html"><a href="deeplearning2.html#generative-adversarial-network-gan"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-reinforcement-network-dqn"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="deeplearning2.html"><a href="deeplearning2.html#summary-8"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="distributedcomputation.html"><a href="distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#integration-and-interoperability"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#ml-pipelines"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-standards"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="distributedcomputation.html"><a href="distributedcomputation.html#general-summary"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>15</b> Appendix</a><ul>
<li class="chapter" data-level="15.1" data-path="appendix.html"><a href="appendix.html#appendix-a"><i class="fa fa-check"></i><b>15.1</b> Appendix A</a><ul>
<li class="chapter" data-level="15.1.1" data-path="appendix.html"><a href="appendix.html#trigonometry"><i class="fa fa-check"></i><b>15.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="15.1.2" data-path="appendix.html"><a href="appendix.html#logarithms"><i class="fa fa-check"></i><b>15.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="15.1.3" data-path="appendix.html"><a href="appendix.html#category-theory"><i class="fa fa-check"></i><b>15.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="appendix.html"><a href="appendix.html#appendix-b"><i class="fa fa-check"></i><b>15.2</b> Appendix B</a><ul>
<li class="chapter" data-level="15.2.1" data-path="appendix.html"><a href="appendix.html#on-random-chances"><i class="fa fa-check"></i><b>15.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="15.2.2" data-path="appendix.html"><a href="appendix.html#on-replacements"><i class="fa fa-check"></i><b>15.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="15.2.3" data-path="appendix.html"><a href="appendix.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>15.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="15.2.4" data-path="appendix.html"><a href="appendix.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>15.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="15.2.5" data-path="appendix.html"><a href="appendix.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>15.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="15.2.6" data-path="appendix.html"><a href="appendix.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>15.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="15.2.7" data-path="appendix.html"><a href="appendix.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>15.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="appendix.html"><a href="appendix.html#appendix-c"><i class="fa fa-check"></i><b>15.3</b> Appendix C</a></li>
<li class="chapter" data-level="15.4" data-path="appendix.html"><a href="appendix.html#appendix-d"><i class="fa fa-check"></i><b>15.4</b> Appendix D</a><ul>
<li class="chapter" data-level="15.4.1" data-path="appendix.html"><a href="appendix.html#lubridate-library"><i class="fa fa-check"></i><b>15.4.1</b> Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machinelearning3" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 11</span> Computational Learning III<a href="machinelearning3.html#machinelearning3" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
            autoNumber: "AMS",
            formatNumber: function (n) {return '11.'+n}
      } 
  }
});
</script>
<p>In this chapter, we continue to discuss <strong>Computational Learning</strong>, emphasizing <strong>Clustering</strong>. We then cover a few <strong>applications of Computational Learning</strong>.</p>
<div id="clustering-unsupervised" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.1</span> Clustering (Unsupervised) <a href="machinelearning3.html#clustering-unsupervised" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here, we switch context to discuss datasets that are <strong>unlabeled</strong>. Our goal is to determine similarities among data points. The idea is to group similar (or homogeneous) data points to form <strong>clusters</strong> and hopefully be able to <strong>annotate</strong> and <strong>profile</strong> such discovered clusters (or structures). The similarity of data points is measured depending on the domain, which we expound on further.</p>
<p>For our case, let us narrow down our discussion to only three types of <strong>clustering methods</strong>, though other literature may have other types of Clustering:</p>
<ul>
<li><p><strong>Partition-based Clustering</strong> - this method of Clustering requires an input of <strong>K</strong> number of clusters. The algorithm then splits the dataset into <strong>K</strong> number of clusters. The core idea of this method is to then optimize each cluster by computing the distance of the local data points in each cluster based on a chosen metric criterion (e.g., Euclidean distance measurement). Until clusters are optimized, data points are iteratively relocated from cluster to cluster. Then the clusters are re-evaluated. An example of such a method is <strong>K-mean</strong>. </p></li>
<li><p><strong>Hierarchical-based Clustering</strong> - this method does not require an input of <strong>K</strong> number of clusters. There are two types of <strong>Hierarchical Clustering</strong>: </p>
<ul>
<li><p><strong>Divisive Clustering</strong> - the idea is to group all data points into one cluster and divide the cluster into smaller clusters as necessary. We repeat the process until a <strong>stopping criterion</strong>. This Clustering is also called the <strong>top-bottom</strong> approach.</p></li>
<li><p><strong>Agglomerative Clustering</strong> - this is a <strong>bottom-up</strong> approach in which clusters are merged until a <strong>stopping criterion</strong>. From the onset, it treats each data point as a cluster. It then compares such small clusters based on chosen metric criteria and merges clusters to form larger clusters. The idea is to have a <strong>stopping criterion</strong> to determine the optimal number of clusters formed. </p></li>
</ul></li>
<li><p><strong>Density-based Clustering</strong> - this method does not also require an input of <strong>K</strong> number of clusters. Rather it calculates the density of data points in terms of closeness. Data points that are closer to each other form a cluster. The <strong>Closeness</strong> of data points is measured with chosen distance metrics. An example of such a method is <strong>DBSCAN</strong>. </p></li>
</ul>
<p>Note that it helps to perform <strong>Exploratory Data Analysis (EDA)</strong> against the dataset first. This is to scale data points and deal with outliers as they tend to affect distance or similarity measures.</p>
<p>In Chapter <strong>9</strong> (<strong>Computational Learning I</strong>), we have a few lists of <strong>Distance Metrics</strong>, which cover <strong>Euclidean</strong>, <strong>Manhattan</strong>, and <strong>Minkowski</strong> distance measurements. For specific domains, we leave readers to investigate other relevant distance measurements.</p>
<div id="k-means-clustering" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.1.1</span> K-means (clustering) <a href="machinelearning3.html#k-means-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>K-means</strong> is a <strong>partition-based clustering</strong> method formulated by Stuart P Lloyd in 1957. It is also regarded as a <strong>centroid clustering</strong> method because it finds the <strong>most</strong> representative of all data points within each cluster and designates it to be the <strong>centroid</strong>. In this section, we follow <strong>Lloyd’s k-means</strong> algorithm (here, we can use a multi-dimensional dataset):</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{{(x_i^{(1)},...,x_i^{(D)})}\}_{i=1}^n\\
\ \ \ \text{number of clusters}: K\\
\mathbf{Algorithm}:\\
\ \ \ \mu_1,...,\mu_k \leftarrow \text{initial centroids} &amp; \text{(randomly chosen)}\\
\ \ \ \text{while stopping criterion not met} \\
\ \ \ \ \ \ \ \ \ \omega_1, ..., \omega_n = 0 &amp; \text{(initialize cluster)}\\
\ \ \ \ \ \ \ \ \ \text{loop i in 1 : N} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ k = \text{arg} \underset{k&#39;}{\text{min}} \|x_i - \mu_{k&#39;} \|^2_2 &amp; \text{(euclidean distance)}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \omega_i \leftarrow k  &amp; \text{(relocation of clusters)}\\
\ \ \ \ \ \ \ \ \ \text{end loop} \\
\ \ \ \ \ \ \ \ \ \text{loop k in 1 : K} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \mu_k = \frac{1}{|\omega_{i..n}=k|} \sum_{i,\omega_i = k} x_i &amp; \text{(recomputation of centroids)}\\
\ \ \ \ \ \ \ \ \ \text{end loop} \\
\ \ \ \text{end while loop}\\
\ \ \ \text{Output: } C = \{\omega_1,...,\omega_n\}
\end{array}
\]</span></p>
<p>Here, our algorithm uses a vector denoted by <span class="math inline">\(\omega\)</span> such that its length equals the number of data points. Each element in the vector corresponds to the <strong>k</strong> cluster in which the corresponding data point belongs. The relocation of each data point to the corresponding <strong>k</strong> cluster is determined by choosing the distance closest to a <strong>k-centroid</strong>. When all data points are relocated, a new set of <strong>k-centroids</strong> is recalculated. The process repeats until such that the recalculated <strong>k-centroids</strong> do not change.</p>
<p>The key to achieving a good cluster is to provide the number of <strong>K</strong> clusters and their corresponding <strong>centroids</strong>. Awareness of the domain helps to correctly (though arbitrarily) choose the <strong>K</strong> clusters and the <strong>centroids</strong> needed to optimally perform the <strong>K-means</strong> method.</p>
<p>Let us now review our example implementation of <strong>K-means</strong> algorithm:</p>

<div class="sourceCode" id="cb1566"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1566-1" data-line-number="1">my.kmeans &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">K=</span><span class="dv">3</span>, <span class="dt">limit=</span><span class="dv">50</span>, <span class="dt">tol=</span><span class="fl">1e-5</span>) {</a>
<a class="sourceLine" id="cb1566-2" data-line-number="2">  N =<span class="st"> </span><span class="kw">nrow</span>(x); D =<span class="st"> </span><span class="kw">ncol</span>(x)</a>
<a class="sourceLine" id="cb1566-3" data-line-number="3">  compute.centroid &lt;-<span class="st"> </span><span class="cf">function</span>(omega) {</a>
<a class="sourceLine" id="cb1566-4" data-line-number="4">      centroid =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>K, <span class="dt">ncol=</span>D, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1566-5" data-line-number="5">      <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1566-6" data-line-number="6">          idx =<span class="st"> </span><span class="kw">which</span>(omega <span class="op">==</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1566-7" data-line-number="7">          centroid[k,] =<span class="st"> </span><span class="kw">apply</span>(x[idx,], <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1566-8" data-line-number="8">      }</a>
<a class="sourceLine" id="cb1566-9" data-line-number="9">      centroid</a>
<a class="sourceLine" id="cb1566-10" data-line-number="10">  }</a>
<a class="sourceLine" id="cb1566-11" data-line-number="11">  initialize &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb1566-12" data-line-number="12">      omega  =<span class="st"> </span><span class="kw">sample</span>(K, <span class="dt">size=</span>N, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1566-13" data-line-number="13">      <span class="kw">compute.centroid</span>(omega)</a>
<a class="sourceLine" id="cb1566-14" data-line-number="14">  }</a>
<a class="sourceLine" id="cb1566-15" data-line-number="15">  distance &lt;-<span class="st"> </span><span class="cf">function</span>(x, centroid) {</a>
<a class="sourceLine" id="cb1566-16" data-line-number="16">    <span class="kw">sqrt</span>(<span class="kw">sum</span>((x <span class="op">-</span><span class="st"> </span>centroid)<span class="op">^</span><span class="dv">2</span>)) <span class="co"># euclidean</span></a>
<a class="sourceLine" id="cb1566-17" data-line-number="17">  }</a>
<a class="sourceLine" id="cb1566-18" data-line-number="18">  stopping.criterion &lt;-<span class="st"> </span><span class="cf">function</span>(centroid, old.centroid) {</a>
<a class="sourceLine" id="cb1566-19" data-line-number="19">      <span class="kw">abs</span>( <span class="kw">sum</span>(centroid <span class="op">-</span><span class="st"> </span>old.centroid) )  </a>
<a class="sourceLine" id="cb1566-20" data-line-number="20">  }</a>
<a class="sourceLine" id="cb1566-21" data-line-number="21">  algorithm &lt;-<span class="st"> </span><span class="cf">function</span>(x, K, limit, tol) {</a>
<a class="sourceLine" id="cb1566-22" data-line-number="22">    iter =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1566-23" data-line-number="23">    old.centroid =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>K, <span class="dt">ncol=</span>D, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1566-24" data-line-number="24">    centroid =<span class="st"> </span><span class="kw">initialize</span>(x) <span class="co"># initialize centroid</span></a>
<a class="sourceLine" id="cb1566-25" data-line-number="25">    <span class="cf">while</span> (<span class="kw">stopping.criterion</span>(centroid, old.centroid) <span class="op">&gt;</span><span class="st"> </span>tol) {</a>
<a class="sourceLine" id="cb1566-26" data-line-number="26">        omega =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb1566-27" data-line-number="27">        <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1566-28" data-line-number="28">             closest =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, K)</a>
<a class="sourceLine" id="cb1566-29" data-line-number="29">             <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1566-30" data-line-number="30">               closest[k] =<span class="st"> </span><span class="kw">distance</span>(x[i,], centroid[k,])</a>
<a class="sourceLine" id="cb1566-31" data-line-number="31">             }</a>
<a class="sourceLine" id="cb1566-32" data-line-number="32">             k =<span class="st"> </span><span class="kw">which.min</span>(closest)</a>
<a class="sourceLine" id="cb1566-33" data-line-number="33">             omega[i] =<span class="st"> </span>k</a>
<a class="sourceLine" id="cb1566-34" data-line-number="34">        }</a>
<a class="sourceLine" id="cb1566-35" data-line-number="35">        old.centroid =<span class="st"> </span>centroid</a>
<a class="sourceLine" id="cb1566-36" data-line-number="36">        centroid =<span class="st"> </span><span class="kw">compute.centroid</span>(omega)</a>
<a class="sourceLine" id="cb1566-37" data-line-number="37">        iter =<span class="st"> </span>iter <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1566-38" data-line-number="38">        <span class="cf">if</span> (iter <span class="op">&gt;=</span><span class="st"> </span>limit) <span class="cf">break</span>  <span class="co"># hard stop</span></a>
<a class="sourceLine" id="cb1566-39" data-line-number="39">    }</a>
<a class="sourceLine" id="cb1566-40" data-line-number="40">    omega</a>
<a class="sourceLine" id="cb1566-41" data-line-number="41">  }</a>
<a class="sourceLine" id="cb1566-42" data-line-number="42">  <span class="kw">algorithm</span>(x, K, limit, tol)</a>
<a class="sourceLine" id="cb1566-43" data-line-number="43">}</a></code></pre></div>

<p>To illustrate, let us generate two clusters using the following dataset.</p>

<div class="sourceCode" id="cb1567"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1567-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1567-2" data-line-number="2">N         =<span class="st"> </span><span class="dv">200</span>; v =<span class="st"> </span><span class="dv">1</span> <span class="co"># variance</span></a>
<a class="sourceLine" id="cb1567-3" data-line-number="3">x1.blue   =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); x2.blue =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v)</a>
<a class="sourceLine" id="cb1567-4" data-line-number="4">x1.red    =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); x2.red  =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v)</a>
<a class="sourceLine" id="cb1567-5" data-line-number="5">x         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(x1.blue, x1.red), <span class="kw">c</span>(x2.blue, x2.red)) </a></code></pre></div>

<p>Let us now plot the result of <strong>my.kmean(.)</strong> (see Figure <a href="machinelearning3.html#fig:kmeanp">11.1</a>).</p>

<div class="sourceCode" id="cb1568"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1568-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1568-2" data-line-number="2">K =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1568-3" data-line-number="3">clusters =<span class="st"> </span><span class="kw">my.kmeans</span>(x, <span class="dt">K=</span>K, <span class="dt">limit=</span><span class="dv">15</span>)</a>
<a class="sourceLine" id="cb1568-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x[,<span class="dv">1</span>]), <span class="dt">ylim=</span><span class="kw">range</span>(x[,<span class="dv">2</span>]),</a>
<a class="sourceLine" id="cb1568-5" data-line-number="5">     <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">main=</span><span class="st">&quot;K-mean Clustering&quot;</span>)</a>
<a class="sourceLine" id="cb1568-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1568-7" data-line-number="7"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">v=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1568-8" data-line-number="8">color =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;green&quot;</span>)</a>
<a class="sourceLine" id="cb1568-9" data-line-number="9"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1568-10" data-line-number="10">  idx =<span class="st"> </span><span class="kw">which</span>(clusters <span class="op">==</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1568-11" data-line-number="11">  <span class="kw">points</span>(x[idx,<span class="dv">1</span>], x[idx,<span class="dv">2</span>], <span class="dt">col=</span>color[k], <span class="dt">pch=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb1568-12" data-line-number="12">}</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kmeanp"></span>
<img src="DS_files/figure-html/kmeanp-1.png" alt="K-mean Clustering" width="70%" />
<p class="caption">
Figure 11.1: K-mean Clustering
</p>
</div>

<p>We leave readers to also investigate the <strong>Hartigan K-means</strong> method.</p>
</div>
<div id="hierarchical-clustering" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.1.2</span> Hierarchical (clustering) <a href="machinelearning3.html#hierarchical-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Hierarchical Clustering</strong> is a <strong>data mining</strong> technique to group similar data points to form a <strong>clustr</strong>. We begin this discussion with an illustration using the following data set.</p>

<div class="sourceCode" id="cb1569"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1569-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1569-2" data-line-number="2">v           =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dv">50</span>, <span class="dv">100</span>), <span class="dv">8</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1569-3" data-line-number="3">x           =<span class="st"> </span><span class="kw">matrix</span>(v, <span class="dt">nrow=</span><span class="dv">4</span>, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1569-4" data-line-number="4"><span class="kw">rownames</span>(x) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;data.point&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb1569-5" data-line-number="5"><span class="kw">colnames</span>(x) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;feature&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1569-6" data-line-number="6">x</a></code></pre></div>
<pre><code>##             feature1 feature2
## data.point1       82       70
## data.point2       81       74
## data.point3       56       53
## data.point4       56       70</code></pre>

<p>The first step in clustering is to determine similarities. In our discussion, we use <strong>distance</strong> to measure the <strong>proximity (closeness)</strong> of data points. In particular, we use <strong>Euclidean distance</strong>.</p>
<p>As an illustration, an approach to computing the distance from any data point to any other data point is to plug the data points into a diagonal <strong>distance matrix</strong> like so:</p>

<div class="sourceCode" id="cb1571"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1571-1" data-line-number="1">(<span class="dt">dist.matrix =</span> <span class="kw">dist</span>(x, <span class="dt">diag=</span><span class="ot">TRUE</span>, <span class="dt">method =</span> <span class="st">&quot;euclidean&quot;</span>))</a></code></pre></div>
<pre><code>##             data.point1 data.point2 data.point3 data.point4
## data.point1       0.000                                    
## data.point2       4.123       0.000                        
## data.point3      31.064      32.650       0.000            
## data.point4      26.000      25.318      17.000       0.000</code></pre>

<p>Each entry is calculated using the <strong>Euclidean distance</strong> (albeit other distance measurements can be tried such as <strong>Manhattan</strong>, <strong>Minkowski</strong>, <strong>Correlation</strong>, and so on):</p>
<p><span class="math display">\[\begin{align}
Dist_i = \sqrt{\sum^D_{d=1}\left(x_1^{(d)} - x_2^{(d)}\right)^2}\ \ \ \ \ 
\text{where D is dimension (no. of features)}
\end{align}\]</span></p>
<p>For example, the distance between data points 1 and 2 is:</p>

<div class="sourceCode" id="cb1573"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1573-1" data-line-number="1"><span class="kw">sqrt</span>(<span class="kw">sum</span>((x[<span class="dv">1</span>,] <span class="op">-</span><span class="st"> </span>x[<span class="dv">2</span>,])<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 4.123</code></pre>

<p>Note that building the <strong>hierarchical cluster</strong> starts with a pair-wise approach. We pair data points based on which pairs have the closest distance (in increasing order). The decision to merge (or cluster) a pair of data points is based on the above <strong>distance matrix</strong>. Notice that the smallest (closest) distance in the matrix is 4.1231 between data.point2 and data.point1. Therefore, they are merged first.</p>

<div class="sourceCode" id="cb1575"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1575-1" data-line-number="1">min.dist =<span class="st"> </span><span class="kw">max</span>(dist.matrix)</a>
<a class="sourceLine" id="cb1575-2" data-line-number="2">(<span class="dt">lab  =</span> <span class="kw">rownames</span>(<span class="kw">which</span>(<span class="kw">as.matrix</span>(dist.matrix)<span class="op">==</span>min.dist, <span class="dt">arr.ind=</span><span class="ot">TRUE</span>)))</a></code></pre></div>
<pre><code>## [1] &quot;data.point3&quot; &quot;data.point2&quot;</code></pre>
<pre><code>##      1      6      5      3      2      4 
##  4.123 17.000 25.318 26.000 31.064 32.650</code></pre>

<p>In R, we can use the function <strong>order(.)</strong> to order the distances in increasing order. So then, the next smallest distance is 17 between data.point4 and data.point3. They are merged next.</p>

<div class="sourceCode" id="cb1578"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1578-1" data-line-number="1">o =<span class="st"> </span><span class="kw">order</span>(dist.matrix); d =<span class="st"> </span>dist.matrix[o]; <span class="kw">names</span>(d) =<span class="st"> </span>o; d</a></code></pre></div>
<pre><code>##      1      6      5      3      2      4 
##  4.123 17.000 25.318 26.000 31.064 32.650</code></pre>
<div class="sourceCode" id="cb1580"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1580-1" data-line-number="1">next.min.dist =<span class="st"> </span>d[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb1580-2" data-line-number="2">(<span class="dt">lab  =</span> <span class="kw">rownames</span>(<span class="kw">which</span>(<span class="kw">as.matrix</span>(dist.matrix)<span class="op">==</span>next.min.dist, <span class="dt">arr.ind=</span><span class="ot">TRUE</span>)))</a></code></pre></div>
<pre><code>## [1] &quot;data.point4&quot; &quot;data.point3&quot;</code></pre>

<p>All pairs that are formed are considered clusters. The next step is to merge the pairs (clusters) to form larger clusters. When merging, we have three methods:</p>
<ul>
<li><p><strong>Single</strong> - each data point from one cluster is paired with data points from other clusters. We then evaluate the distance between pairs with the closest distance. Finally, we merge the two clusters having the closest pair.</p></li>
<li><p><strong>Complete</strong> - each data point from one cluster is paired with data points from other clusters. We then evaluate the distance between pairs with the farthest distance. Finally, we merge the two clusters having the farthest pair.</p></li>
<li><p><strong>Average</strong> - the centroid of one cluster is paired with the centroid of another cluster. We then evaluate which distance between centroids has the closest distance. Finally, we merge the two clusters having the closest centroids.</p></li>
</ul>
<p>Let us see the plot for the formation of two clusters using a <strong>Dendrogram</strong> for the formation of two clusters. See Figure <a href="machinelearning3.html#fig:hierarchycls1">11.2</a>). In the figure, we use <strong>Complete</strong> as a method to <strong>merge</strong> clusters.</p>

<div class="sourceCode" id="cb1582"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1582-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb1582-2" data-line-number="2">my.hierarchy.cluster =<span class="st"> </span><span class="kw">hclust</span>(dist.matrix, <span class="dt">method=</span><span class="st">&quot;complete&quot;</span>)</a>
<a class="sourceLine" id="cb1582-3" data-line-number="3"><span class="kw">plot</span>(my.hierarchy.cluster,  <span class="dt">ylab=</span><span class="st">&quot;Height&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Clusters&quot;</span>,</a>
<a class="sourceLine" id="cb1582-4" data-line-number="4">     <span class="dt">main=</span><span class="st">&quot;Hierarchical Clustering (Dendogram)&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hierarchycls1"></span>
<img src="DS_files/figure-html/hierarchycls1-1.png" alt="(Complete) Hierarchical Clustering (Dendogram)" width="70%" />
<p class="caption">
Figure 11.2: (Complete) Hierarchical Clustering (Dendogram)
</p>
</div>

<p>Let us plot using <strong>Average</strong> as method to <strong>merge</strong> clusters (see Figure <a href="machinelearning3.html#fig:hierarchycls1">11.2</a>).</p>

<div class="sourceCode" id="cb1583"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1583-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb1583-2" data-line-number="2">my.hierarchy.cluster =<span class="st"> </span><span class="kw">hclust</span>(dist.matrix, <span class="dt">method=</span><span class="st">&quot;average&quot;</span>)</a>
<a class="sourceLine" id="cb1583-3" data-line-number="3"><span class="kw">plot</span>(my.hierarchy.cluster,  <span class="dt">ylab=</span><span class="st">&quot;Height&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Clusters&quot;</span>,</a>
<a class="sourceLine" id="cb1583-4" data-line-number="4">     <span class="dt">main=</span><span class="st">&quot;Hierarchical Clustering (Dendogram)&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hierarchycls2"></span>
<img src="DS_files/figure-html/hierarchycls2-1.png" alt="(Average) Hierarchical Clustering (Dendogram)" width="70%" />
<p class="caption">
Figure 11.3: (Average) Hierarchical Clustering (Dendogram)
</p>
</div>

<p>For other variants of <strong>Hierarchical Clustering</strong>, we leave readers to investigate <strong>Balanced Iterative Reducing and Clustering (BIRCH)</strong> method, <strong>Clustering Using Representatives (CURE)</strong> method, and <strong>Chameleon</strong> method.</p>
</div>
<div id="dbscan-clustering" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.1.3</span> DBSCAN (clustering) <a href="machinelearning3.html#dbscan-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>DBSCAN</strong>, introduced by Martin Ester et al. <span class="citation">(<a href="bibliography.html#ref-ref159m">1996</a>)</span>, is short for <strong>Density-based spatial clustering of applications with noise</strong> and is a density-based clustering algorithm. The algorithm works by clustering data points using two parameters:</p>
<ul>
<li><strong>minpts</strong> - This number determines the minimum allowable density of a region. If a region contains minpts=5, then the region is considered dense.</li>
<li><strong>radius</strong><span class="math inline">\((\epsilon)\)</span> - This number determines the neighborhood proximity of a selected data point to other data points. Any data point is considered a neighbor if it is reachable by at least an <span class="math inline">\(\epsilon\)</span> closer to a selected data point.</li>
</ul>
<p>There are three distinct types of points:</p>
<ul>
<li><strong>Core point</strong> - A point that has at least <strong>minpts</strong> neighbors, including itself, within an <span class="math inline">\(\epsilon\)</span> distance from itself.</li>
<li><strong>Border point</strong> - A point that belongs to a neighborhood but is not a core point.</li>
<li><strong>Noise point</strong> - A point that does not belong to a neighborhood; thus, it is neither a core point nor a border point.</li>
</ul>
<p>In Figure <a href="machinelearning3.html#fig:dbscan">11.4</a>, we see seven core points forming a more extensive cluster with two border points. Also, there are three outliers in the figure.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dbscan"></span>
<img src="dbscan.png" alt="DBSCAN" width="70%" />
<p class="caption">
Figure 11.4: DBSCAN
</p>
</div>
<p><strong>DBSCAN</strong> has the following algorithm:</p>

<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{{(x_i^{(1)},...,x_i^{(D)})}\}_{i=1}^n\\
\ \ \ \text{radius}: \epsilon\\
\ \ \ \text{minimum points to form dense cluster: } \mathbf{minpts}\\
\mathbf{Algorithm}:\\
\ \ \ \mathbf{\text{foreach}}\ \text{unvisited point }x \in \mathbf{X}\\
\ \ \ \ \ \ \ \ \ \text{Mark }x\text{ as }\mathbf{ VISITED}\\
\ \ \ \ \ \ \ \ \ \text{N }\leftarrow \text{Neighbors}(x,\epsilon)\\
\ \ \ \ \ \ \ \ \ \text{if sizeof(N) &lt; }\mathbf{minpts}\text{ then}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{Mark } x\text{ as }\mathbf{NOISE}\\
\ \ \ \ \ \ \ \ \ \text{else}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \omega \leftarrow \{x\} &amp; \text{(add point to new cluster)}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{ExpandCluster}(x, N, \omega, \epsilon, \mathbf{minpts})\\
\ \ \ \ \ \ \ \ \ \text{end if}\\
\ \ \ \text{end loop}\\
\ \ \ \text{Output: } C = \{\omega_1,...,\omega_k\}\\
\end{array}
\]</span>
</p>
<p>where:</p>

<p><span class="math display">\[
\begin{array}{ll}
\text{ExpandCluster}(x, N, \omega, \epsilon, \mathbf{minpts}):\\
\ \ \ \ \ \ \ \mathbf{\text{foreach}}\text{ point x}&#39; \in N\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{\text{if }}x&#39;\text{ is not visited then}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{Mark }x&#39;\text{ as }\mathbf{ VISITED}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{N}&#39;\leftarrow \text{Neighbors}(x&#39;,\epsilon)\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{\text{if }}\text{ sizeof}(N&#39;) \ge\mathbf{minpts}\text{ then}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ N \leftarrow N \cup N&#39;  &amp; \text{(merge neighbors)}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{end if}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{end if}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{\text{if }}x&#39;\text{ is not a cluster member}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \omega \leftarrow \omega \cup\{x&#39;\}  &amp; \text{(add point to cluster)}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{end if}\\
\ \ \ \ \ \ \ \text{end loop}\\
\end{array}
\]</span>
</p>
<p>Below is our example implementation of <strong>DBSCAN</strong>:</p>

<div class="sourceCode" id="cb1584"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1584-1" data-line-number="1"><span class="kw">library</span>(dequer)</a>
<a class="sourceLine" id="cb1584-2" data-line-number="2">my.dbscan &lt;-<span class="st"> </span><span class="cf">function</span>(x, eps, minpts) {</a>
<a class="sourceLine" id="cb1584-3" data-line-number="3">  n =<span class="st"> </span><span class="kw">nrow</span>(x); K =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1584-4" data-line-number="4">  noise   =<span class="st"> </span><span class="ot">NULL</span>; visited =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">FALSE</span>, n)</a>
<a class="sourceLine" id="cb1584-5" data-line-number="5">  omega   =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1584-6" data-line-number="6">  is.member &lt;-<span class="st"> </span><span class="cf">function</span>(i) { omega[i] <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>  }</a>
<a class="sourceLine" id="cb1584-7" data-line-number="7">  distance  &lt;-<span class="st"> </span><span class="cf">function</span>(x, y) { <span class="kw">sqrt</span>(<span class="kw">sum</span>((x <span class="op">-</span><span class="st"> </span>y)<span class="op">^</span><span class="dv">2</span>)) } <span class="co"># euclidean</span></a>
<a class="sourceLine" id="cb1584-8" data-line-number="8">  neighbors &lt;-<span class="st"> </span><span class="cf">function</span>(i, eps) {</a>
<a class="sourceLine" id="cb1584-9" data-line-number="9">    N =<span class="st"> </span><span class="kw">queue</span>()</a>
<a class="sourceLine" id="cb1584-10" data-line-number="10">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1584-11" data-line-number="11">       <span class="cf">if</span> (j <span class="op">!=</span><span class="st"> </span>i) {</a>
<a class="sourceLine" id="cb1584-12" data-line-number="12">         proximity =<span class="st"> </span><span class="kw">distance</span>(x[i,], x[j,]) </a>
<a class="sourceLine" id="cb1584-13" data-line-number="13">         <span class="cf">if</span> (proximity <span class="op">&lt;=</span><span class="st"> </span>eps) {  <span class="kw">pushback</span>(N, j) }</a>
<a class="sourceLine" id="cb1584-14" data-line-number="14">    }  }</a>
<a class="sourceLine" id="cb1584-15" data-line-number="15">    N</a>
<a class="sourceLine" id="cb1584-16" data-line-number="16">  }</a>
<a class="sourceLine" id="cb1584-17" data-line-number="17">  merge.neighbors &lt;-<span class="st"> </span><span class="cf">function</span>(N1, N2) {</a>
<a class="sourceLine" id="cb1584-18" data-line-number="18">      N =<span class="st"> </span><span class="kw">queue</span>(); p =<span class="st"> </span><span class="ot">NULL</span>; q =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1584-19" data-line-number="19">      <span class="cf">while</span> (<span class="kw">length</span>(N1)) { p =<span class="st"> </span><span class="kw">c</span>(p, <span class="kw">pop</span>(N1)) }</a>
<a class="sourceLine" id="cb1584-20" data-line-number="20">      <span class="cf">while</span> (<span class="kw">length</span>(N2)) { q =<span class="st"> </span><span class="kw">c</span>(q, <span class="kw">pop</span>(N2)) }</a>
<a class="sourceLine" id="cb1584-21" data-line-number="21">      p =<span class="st"> </span><span class="kw">union</span>(p, q)             <span class="co"># eliminate duplicates</span></a>
<a class="sourceLine" id="cb1584-22" data-line-number="22">      <span class="cf">for</span> (i <span class="cf">in</span> p) <span class="kw">pushback</span>(N, i) <span class="co"># form one queue</span></a>
<a class="sourceLine" id="cb1584-23" data-line-number="23">      N</a>
<a class="sourceLine" id="cb1584-24" data-line-number="24">  }</a>
<a class="sourceLine" id="cb1584-25" data-line-number="25">  expandCluster &lt;-<span class="st"> </span><span class="cf">function</span>(p, K, N) {</a>
<a class="sourceLine" id="cb1584-26" data-line-number="26">    omega[p] &lt;&lt;-<span class="st"> </span>K</a>
<a class="sourceLine" id="cb1584-27" data-line-number="27">    <span class="cf">while</span> (<span class="kw">length</span>(N)) {</a>
<a class="sourceLine" id="cb1584-28" data-line-number="28">      j =<span class="st"> </span><span class="kw">pop</span>(N)</a>
<a class="sourceLine" id="cb1584-29" data-line-number="29">      <span class="cf">if</span> (visited[j] <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb1584-30" data-line-number="30">        visited[j] &lt;&lt;-<span class="st"> </span><span class="ot">TRUE</span>   </a>
<a class="sourceLine" id="cb1584-31" data-line-number="31">        N.j =<span class="st"> </span><span class="kw">neighbors</span>(j, eps)</a>
<a class="sourceLine" id="cb1584-32" data-line-number="32">        <span class="cf">if</span> (<span class="kw">length</span>(N.j) <span class="op">&gt;=</span><span class="st"> </span>minpts) { N =<span class="st"> </span><span class="kw">merge.neighbors</span>(N, N.j) }</a>
<a class="sourceLine" id="cb1584-33" data-line-number="33">      }</a>
<a class="sourceLine" id="cb1584-34" data-line-number="34">      <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.member</span>(j)) { omega[j] &lt;&lt;-<span class="st"> </span>K }</a>
<a class="sourceLine" id="cb1584-35" data-line-number="35">    }</a>
<a class="sourceLine" id="cb1584-36" data-line-number="36">  }</a>
<a class="sourceLine" id="cb1584-37" data-line-number="37">  algorithm &lt;-<span class="st"> </span><span class="cf">function</span>() {</a>
<a class="sourceLine" id="cb1584-38" data-line-number="38">    <span class="cf">for</span> (p <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1584-39" data-line-number="39">        <span class="cf">if</span> (visited[p] <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb1584-40" data-line-number="40">            visited[p] &lt;&lt;-<span class="st"> </span><span class="ot">TRUE</span>    </a>
<a class="sourceLine" id="cb1584-41" data-line-number="41">            N =<span class="st"> </span><span class="kw">neighbors</span>(p, eps)</a>
<a class="sourceLine" id="cb1584-42" data-line-number="42">            <span class="cf">if</span> (<span class="kw">length</span>(N) <span class="op">&lt;</span><span class="st"> </span>minpts) { </a>
<a class="sourceLine" id="cb1584-43" data-line-number="43">              noise &lt;&lt;-<span class="st"> </span><span class="kw">c</span>(noise, p)</a>
<a class="sourceLine" id="cb1584-44" data-line-number="44">            } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1584-45" data-line-number="45">              K &lt;&lt;-<span class="st"> </span>K <span class="op">+</span><span class="st"> </span><span class="dv">1</span>  <span class="co"># new cluster</span></a>
<a class="sourceLine" id="cb1584-46" data-line-number="46">              <span class="kw">expandCluster</span>(p, K, N)</a>
<a class="sourceLine" id="cb1584-47" data-line-number="47">    }   }   }</a>
<a class="sourceLine" id="cb1584-48" data-line-number="48">    omega</a>
<a class="sourceLine" id="cb1584-49" data-line-number="49">  }</a>
<a class="sourceLine" id="cb1584-50" data-line-number="50">  <span class="kw">algorithm</span>()</a>
<a class="sourceLine" id="cb1584-51" data-line-number="51">}</a></code></pre></div>

<p>Let us use the dataset below for our <strong>DBSCAN</strong> implementation:</p>

<div class="sourceCode" id="cb1585"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1585-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1585-2" data-line-number="2">N         =<span class="st"> </span><span class="dv">200</span>; v =<span class="st"> </span><span class="dv">1</span> <span class="co"># variance</span></a>
<a class="sourceLine" id="cb1585-3" data-line-number="3">x1.blue   =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); x2.blue =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v) </a>
<a class="sourceLine" id="cb1585-4" data-line-number="4">x1.red    =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); x2.red  =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v) </a>
<a class="sourceLine" id="cb1585-5" data-line-number="5">x         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(x1.blue, x1.red), <span class="kw">c</span>(x2.blue, x2.red)) </a>
<a class="sourceLine" id="cb1585-6" data-line-number="6">clusters =<span class="st"> </span><span class="kw">my.dbscan</span>(x, <span class="dt">eps =</span> <span class="dv">1</span>, <span class="dt">minpts =</span> <span class="dv">15</span>)</a></code></pre></div>

<p>Figure <a href="machinelearning3.html#fig:dbscancls">11.5</a> shows the plot of the cluster.</p>

<div class="sourceCode" id="cb1586"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1586-1" data-line-number="1">K =<span class="st"> </span><span class="kw">length</span>(<span class="kw">unique</span>(clusters)) <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1586-2" data-line-number="2"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x[,<span class="dv">1</span>]), <span class="dt">ylim=</span><span class="kw">range</span>(x[,<span class="dv">2</span>]),</a>
<a class="sourceLine" id="cb1586-3" data-line-number="3">     <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">main=</span><span class="st">&quot;DBSCAN Clustering&quot;</span>)</a>
<a class="sourceLine" id="cb1586-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1586-5" data-line-number="5"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">v=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1586-6" data-line-number="6">color =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb1586-7" data-line-number="7"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1586-8" data-line-number="8">  idx =<span class="st"> </span><span class="kw">which</span>(clusters <span class="op">==</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1586-9" data-line-number="9">  <span class="kw">points</span>(x[idx,<span class="dv">1</span>], x[idx,<span class="dv">2</span>], <span class="dt">col=</span>color[k <span class="op">+</span><span class="st"> </span><span class="dv">1</span>], <span class="dt">pch=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb1586-10" data-line-number="10">}</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dbscancls"></span>
<img src="DS_files/figure-html/dbscancls-1.png" alt="DBSCAN Clustering" width="70%" />
<p class="caption">
Figure 11.5: DBSCAN Clustering
</p>
</div>

</div>
<div id="quality-of-clustering" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.1.4</span> Quality of Clustering<a href="machinelearning3.html#quality-of-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the challenges in using a clustering method such as <strong>K-means</strong> is determining the optimal number of clusters to input. In this section, let us discuss three methods to evaluate the <strong>goodness of a cluster</strong>.</p>
<ul>
<li><strong>Elbow Method</strong> - this method is one of the common measures to determine the quality of clusters. The name <strong>elbow</strong> becomes apparent once we plot the result of our evaluation. Here, we use <strong>WSS</strong> metrics to measure the sum of squares within clusters. For example, in Figure <a href="machinelearning3.html#fig:elbow">11.6</a>, the <strong>elbow</strong> shape is found at cluster equal to 2. Therefore, the optimal number of clusters is 2. </li>
</ul>

<div class="sourceCode" id="cb1587"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1587-1" data-line-number="1">elbow.method &lt;-<span class="cf">function</span>(x, K) {</a>
<a class="sourceLine" id="cb1587-2" data-line-number="2">  clusters =<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dt">centers=</span>K, <span class="dt">nstart=</span><span class="dv">25</span>)</a>
<a class="sourceLine" id="cb1587-3" data-line-number="3">  clusters<span class="op">$</span>tot.withinss</a>
<a class="sourceLine" id="cb1587-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb1587-5" data-line-number="5">K =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb1587-6" data-line-number="6">wss =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, K)</a>
<a class="sourceLine" id="cb1587-7" data-line-number="7"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) { wss[k]  =<span class="st"> </span><span class="kw">elbow.method</span>(x, k) }</a>
<a class="sourceLine" id="cb1587-8" data-line-number="8"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">1</span><span class="op">:</span>K), <span class="dt">ylim=</span><span class="kw">range</span>(wss),</a>
<a class="sourceLine" id="cb1587-9" data-line-number="9">     <span class="dt">ylab=</span><span class="st">&quot;within clusters sum of squares&quot;</span>, </a>
<a class="sourceLine" id="cb1587-10" data-line-number="10">     <span class="dt">xlab=</span><span class="st">&quot;number of clusters (k)&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Elbow Method&quot;</span>)</a>
<a class="sourceLine" id="cb1587-11" data-line-number="11"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1587-12" data-line-number="12"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">v=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1587-13" data-line-number="13"><span class="kw">points</span>(<span class="kw">seq</span>(<span class="dv">1</span>,K), wss, <span class="dt">pch=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb1587-14" data-line-number="14"><span class="kw">lines</span>(<span class="kw">seq</span>(<span class="dv">1</span>,K), wss,  <span class="dt">type=</span><span class="st">&quot;b&quot;</span>)</a>
<a class="sourceLine" id="cb1587-15" data-line-number="15"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:elbow"></span>
<img src="DS_files/figure-html/elbow-1.png" alt="Elbow Method" width="70%" />
<p class="caption">
Figure 11.6: Elbow Method
</p>
</div>

<ul>
<li><p><strong>Silhouette Method</strong> - the quality of clustering can be measured using the <strong>Silhouette</strong> method which measures a <strong>Silhouette Coefficient</strong>. There are two average distances to consider here and are called <strong>Cohesion</strong> and <strong>Separation</strong> respectively denoted by <span class="math inline">\(\mathbf{a(x)}\)</span> and <span class="math inline">\(\mathbf{b(x)}\)</span> where:  </p></li>
<li><span class="math inline">\(\mathbf{a(x)}\)</span> refers to <strong>Cohesion</strong> taking the average distance of x to all other vectors within clusters.</li>
<li><p><span class="math inline">\(\mathbf{b(x)}\)</span> refers to <strong>Separation</strong> taking the average distance of x to the vectors between clusters.</p></li>
</ul>
<p>The <strong>Silhouette width</strong>, namely <strong>s(x)</strong> is expressed as such:</p>
<p><span class="math display">\[\begin{align}
S(x) = \frac{b(x) - a(x)}{max\left(a(x), b(x)\right)}\ \ \ \ \leftarrow
\begin{cases}
-1 &amp; \text{bad}\\
0 &amp; \text{indifferent}\\
1 &amp; \text{good}
\end{cases} \label{eqn:eqnnumber502}
\end{align}\]</span></p>
<p>We then take the average, resulting in our <strong>Silhouette Coefficient</strong>:</p>
<p><span class="math display">\[\begin{align}
SC = \frac{1}{N}\sum_{i=1}^N S(x)
\end{align}\]</span></p>
<p>The goal is to find the minimum coefficient. To illustrate, let us review Figure <a href="machinelearning3.html#fig:silhouette">11.7</a>. Notice that the minimum coefficient is at 3.</p>

<div class="sourceCode" id="cb1588"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1588-1" data-line-number="1"><span class="kw">library</span>(cluster)</a>
<a class="sourceLine" id="cb1588-2" data-line-number="2">silhouette.score &lt;-<span class="cf">function</span>(x, K) {</a>
<a class="sourceLine" id="cb1588-3" data-line-number="3">  clusters =<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dt">centers=</span>K, <span class="dt">nstart=</span><span class="dv">25</span>)</a>
<a class="sourceLine" id="cb1588-4" data-line-number="4">  ss       =<span class="st"> </span><span class="kw">silhouette</span>(clusters<span class="op">$</span>cluster, <span class="kw">as.matrix</span>(<span class="kw">dist</span>(x)))</a>
<a class="sourceLine" id="cb1588-5" data-line-number="5">  <span class="kw">mean</span>(ss[, <span class="dv">3</span>])</a>
<a class="sourceLine" id="cb1588-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb1588-7" data-line-number="7">K =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb1588-8" data-line-number="8">sc =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, K )</a>
<a class="sourceLine" id="cb1588-9" data-line-number="9"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) { sc[k] =<span class="st"> </span><span class="kw">silhouette.score</span>(x, k <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) }</a>
<a class="sourceLine" id="cb1588-10" data-line-number="10"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">1</span><span class="op">:</span>K), <span class="dt">ylim=</span><span class="kw">range</span>(sc),</a>
<a class="sourceLine" id="cb1588-11" data-line-number="11">     <span class="dt">ylab=</span><span class="st">&quot;Silhouette Coefficient&quot;</span>, </a>
<a class="sourceLine" id="cb1588-12" data-line-number="12">     <span class="dt">xlab=</span><span class="st">&quot;number of clusters (k)&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Silhouette Method&quot;</span>)</a>
<a class="sourceLine" id="cb1588-13" data-line-number="13"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1588-14" data-line-number="14"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">v=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1588-15" data-line-number="15"><span class="kw">points</span>(<span class="kw">seq</span>(<span class="dv">1</span>,K), sc, <span class="dt">pch=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb1588-16" data-line-number="16"><span class="kw">lines</span>(<span class="kw">seq</span>(<span class="dv">1</span>,K), sc,  <span class="dt">type=</span><span class="st">&quot;b&quot;</span>)</a>
<a class="sourceLine" id="cb1588-17" data-line-number="17"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">3</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:silhouette"></span>
<img src="DS_files/figure-html/silhouette-1.png" alt="Silhouette Method" width="70%" />
<p class="caption">
Figure 11.7: Silhouette Method
</p>
</div>

<p><strong>Calinski-Harabasz Index</strong> - this measure is also called <strong>Variance Ratio Criterion</strong> which calculates the ratio of dispersion <span class="citation">(Baarsch J. <a href="bibliography.html#ref-ref1359j">2012</a>)</span>. The formula is as follows:  </p>
<p><span class="math display">\[
s(k) = \frac{tr(B_k)}{tr(Wk)} \times \frac{\left( N -k\right)}{\left(k-1\right)}
,\ \ \ \ \ \ \text{k = number of clusters}
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\underbrace{Bk = \sum_{j=1}^k n_j\left(\mu_j - \mu\right) \left(\mu_j- \mu\right)^\text{T}}_{\text{Between Clusters}}
\ \ \ \ \ \ \ \
\underbrace{Wk =  \sum_{j=1}^{k} \sum_{i=1}^{n_j} \left(x_i - \mu_j\right) \left(x_i - \mu_j\right)^\text{T}}_{\text{Within  Cluster j}}
\]</span></p>
<p>and where <span class="math inline">\(N\)</span> is the number of samples in the dataset, <span class="math inline">\(\mu\)</span> is the center of the dataset, <span class="math inline">\(n_j\)</span> is the number of samples in cluster j, and <span class="math inline">\(\mu_j\)</span> is the center of the cluster j. The <span class="math inline">\(tr(.)\)</span> is a function that yields a trace matrix.</p>
<p>A higher score indicates a better clustering.</p>
<p>We leave readers to investigate other methods of evaluating clustering algorithms such as <strong>Bayesian Information Criterion (BIC)</strong>, <strong>Davies-Bouldin Index (DBI)</strong>, and <strong>Dunn’s index</strong>.</p>
</div>
</div>
<div id="meta-learning" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.2</span> Meta-Learning <a href="machinelearning3.html#meta-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Meta-Learning</strong> is the process of learning how to learn. In other words, the idea of being adaptive to changes goes beyond just being able to identify regressions or being able to recognize patterns and classify them. Self-adaptation, self-preservation, and self-healing are a few of many things that allow a system to advance farther into being autonomous. This book does not cover such topics; however, they are essential ingredients in advancing fundamental knowledge of computational learning. We thus leave readers to investigate them.</p>
<p>In the following three sections, we introduce use cases commonly discussed in Machine Learning. Let us start with <strong>NLP</strong>.</p>
</div>
<div id="natural-language-processing-nlp" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.3</span> Natural Language Processing (NLP)  <a href="machinelearning3.html#natural-language-processing-nlp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Natural Language Processing (NLP)</strong> is covered in <strong>Information Retrieval and Text Mining</strong> Theory. However, other literature covers <strong>NLP</strong> in <strong>Artificial Intelligence</strong> that deals with the computational processing of human languages based on <strong>Word Embeddings</strong> - a topic worth covering in the last chapter.</p>
<p>A better way to explain <strong>NLP</strong> is to start with a simple list of sample documents - consider this as our <strong>corpus</strong> for now:</p>

<div class="sourceCode" id="cb1589"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1589-1" data-line-number="1">D1 =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;There is a big animal in my house.&quot;</span>)</a>
<a class="sourceLine" id="cb1589-2" data-line-number="2">D2 =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;The animal has great big ears, great big eyes, great big teeth.&quot;</span>)</a>
<a class="sourceLine" id="cb1589-3" data-line-number="3">D3 =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;A wolf is an animal.&quot;</span>)</a>
<a class="sourceLine" id="cb1589-4" data-line-number="4">D4 =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Little Red Riding Hood met a Wolf.&quot;</span>)</a>
<a class="sourceLine" id="cb1589-5" data-line-number="5">(<span class="dt">documents =</span> <span class="kw">c</span>(D1, D2, D3, D4))</a></code></pre></div>
<pre><code>## [1] &quot;There is a big animal in my house.&quot;                             
## [2] &quot;The animal has great big ears, great big eyes, great big teeth.&quot;
## [3] &quot;A wolf is an animal.&quot;                                           
## [4] &quot;Little Red Riding Hood met a Wolf.&quot;</code></pre>

<p>In our sample list of documents, we extract words and convert the list into a vector of words - we call this vector as <strong>bag of words</strong>. It is raw and needs cleanup. Our goal is to perform pre-processing which includes dealing with <strong>tokenization</strong>, <strong>case-sensitivity</strong>, <strong>stopwords</strong>, <strong>N-grams</strong>, <strong>stemming</strong>, and <strong>lemmatization</strong> to name a few.</p>
<div id="pre-processing-texts" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.1</span> Pre-Processing Texts<a href="machinelearning3.html#pre-processing-texts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Part of the pre-processing text is to tokenize terms in documents. <strong>Tokenization</strong> is extracting units of text information called <strong>tokens</strong>. A token is represented as a <strong>word</strong> (or a <strong>term</strong>) in a document. Below, we extract words and map them into a vector.</p>

<div class="sourceCode" id="cb1591"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1591-1" data-line-number="1">(<span class="dt">words2vector =</span> <span class="kw">unlist</span>( <span class="kw">strsplit</span>(documents, <span class="dt">split=</span><span class="st">&#39; &#39;</span>)))</a></code></pre></div>
<pre><code>##  [1] &quot;There&quot;   &quot;is&quot;      &quot;a&quot;       &quot;big&quot;     &quot;animal&quot;  &quot;in&quot;     
##  [7] &quot;my&quot;      &quot;house.&quot;  &quot;The&quot;     &quot;animal&quot;  &quot;has&quot;     &quot;great&quot;  
## [13] &quot;big&quot;     &quot;ears,&quot;   &quot;great&quot;   &quot;big&quot;     &quot;eyes,&quot;   &quot;great&quot;  
## [19] &quot;big&quot;     &quot;teeth.&quot;  &quot;A&quot;       &quot;wolf&quot;    &quot;is&quot;      &quot;an&quot;     
## [25] &quot;animal.&quot; &quot;Little&quot;  &quot;Red&quot;     &quot;Riding&quot;  &quot;Hood&quot;    &quot;met&quot;    
## [31] &quot;a&quot;       &quot;Wolf.&quot;</code></pre>

<p>Now, there is pre-processing to be made when tokenizing documents. Let us cover a few of them:</p>
<p><strong>Case-Sensitivity</strong></p>
<p>If we do not care much about <strong>case sensitive characters</strong>, we may choose to set all words in small cases:</p>

<div class="sourceCode" id="cb1593"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1593-1" data-line-number="1">(<span class="dt">documents =</span> <span class="kw">tolower</span>(documents))</a></code></pre></div>
<pre><code>## [1] &quot;there is a big animal in my house.&quot;                             
## [2] &quot;the animal has great big ears, great big eyes, great big teeth.&quot;
## [3] &quot;a wolf is an animal.&quot;                                           
## [4] &quot;little red riding hood met a wolf.&quot;</code></pre>

<p><strong>Punctuations</strong></p>
<p>Note that documents have <strong>punctuations</strong> or special whitespaces that we need to remove.</p>

<div class="sourceCode" id="cb1595"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1595-1" data-line-number="1">(<span class="dt">documents =</span> <span class="kw">gsub</span>(<span class="st">&quot;[[:punct:]]&quot;</span>, <span class="st">&quot;&quot;</span>, documents))</a></code></pre></div>
<pre><code>## [1] &quot;there is a big animal in my house&quot;                           
## [2] &quot;the animal has great big ears great big eyes great big teeth&quot;
## [3] &quot;a wolf is an animal&quot;                                         
## [4] &quot;little red riding hood met a wolf&quot;</code></pre>

<p><strong>StopWords</strong></p>
<p>Apparently, we also notice that certain terms such as (<strong>is</strong>, <strong>a</strong>, <strong>an</strong>, <strong>in</strong>, <strong>the</strong>) are common in documents that may give little to no <strong>information</strong>. We can treat them as <strong>stopwords</strong> and can be removed. For <strong>stopwords</strong>, let us use <strong>anti_join(.)</strong> function from <strong>dplyr</strong> library. Also, let us use <strong>tibble(.)</strong> to convert the documents to <strong>tibble</strong> data frame. Here, we use <strong>unnest_tokens(.)</strong> from <strong>tidytext</strong> library to tokenize.</p>

<div class="sourceCode" id="cb1597"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1597-1" data-line-number="1"><span class="kw">library</span>(tidytext)</a>
<a class="sourceLine" id="cb1597-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb1597-3" data-line-number="3">stopwords =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;is&quot;</span>, <span class="st">&quot;a&quot;</span>, <span class="st">&quot;an&quot;</span>, <span class="st">&quot;has&quot;</span>, <span class="st">&quot;in&quot;</span>, <span class="st">&quot;the&quot;</span>, <span class="st">&quot;there&quot;</span>)</a>
<a class="sourceLine" id="cb1597-4" data-line-number="4">stopwords.frame  =<span class="st"> </span><span class="kw">tibble</span>( <span class="dt">term =</span> stopwords)</a>
<a class="sourceLine" id="cb1597-5" data-line-number="5">doc.frame =<span class="st">  </span><span class="kw">tibble</span>(<span class="dt">docid =</span> <span class="kw">c</span>(<span class="st">&#39;D1&#39;</span>,<span class="st">&#39;D2&#39;</span>,<span class="st">&#39;D3&#39;</span>,<span class="st">&#39;D4&#39;</span>), <span class="dt">term =</span> documents)</a>
<a class="sourceLine" id="cb1597-6" data-line-number="6">tokenized.table =<span class="st">  </span>doc.frame  <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1597-7" data-line-number="7"><span class="st">        </span><span class="kw">unnest_tokens</span>(term, term) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1597-8" data-line-number="8"><span class="st">        </span><span class="kw">anti_join</span>(stopwords.frame,  <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;term&quot;</span> =<span class="st"> &quot;term&quot;</span> ))</a></code></pre></div>

<p>We now have the final pre-processed list of words above.</p>

<div class="sourceCode" id="cb1598"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1598-1" data-line-number="1">preprocessed.doc &lt;-<span class="st"> </span><span class="cf">function</span>(t) {</a>
<a class="sourceLine" id="cb1598-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(tokenized.table)</a>
<a class="sourceLine" id="cb1598-3" data-line-number="3">  ids =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;D1&quot;</span>, <span class="st">&quot;D2&quot;</span>, <span class="st">&quot;D3&quot;</span>, <span class="st">&quot;D4&quot;</span>)</a>
<a class="sourceLine" id="cb1598-4" data-line-number="4">  docs =<span class="st"> </span><span class="kw">rep</span>(<span class="st">&quot;&quot;</span>, n)</a>
<a class="sourceLine" id="cb1598-5" data-line-number="5">  <span class="cf">for</span> (i  <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(ids)) {</a>
<a class="sourceLine" id="cb1598-6" data-line-number="6">    idx =<span class="st"> </span><span class="kw">which</span>(tokenized.table<span class="op">$</span>docid <span class="op">==</span><span class="st"> </span>ids[i])</a>
<a class="sourceLine" id="cb1598-7" data-line-number="7">    docs[i] =<span class="st"> </span><span class="kw">paste0</span>(tokenized.table<span class="op">$</span>term[idx], <span class="dt">sep=</span><span class="st">&quot; &quot;</span>, <span class="dt">collapse=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1598-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb1598-9" data-line-number="9">  docs</a>
<a class="sourceLine" id="cb1598-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb1598-11" data-line-number="11">(<span class="dt">documents =</span> <span class="kw">preprocessed.doc</span>(tokenized.table))</a></code></pre></div>
<pre><code>## [1] &quot;big animal my house &quot;                                 
## [2] &quot;animal great big ears great big eyes great big teeth &quot;
## [3] &quot;wolf animal &quot;                                         
## [4] &quot;little red riding hood met wolf &quot;</code></pre>

<p><strong>N-Gram</strong> </p>
<p>At times, in <strong>Text Mining</strong>, it helps to consider also a group of words and not just individual words. For example, a sequence of one, two, and three words is called a <strong>unigram</strong>, <strong>bigram</strong>, and <strong>trigram</strong>, respectively. For example, below, we generate a sequence of <strong>bigram</strong> tokens.</p>

<div class="sourceCode" id="cb1600"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1600-1" data-line-number="1">tokens =<span class="st"> </span>doc.frame  <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1600-2" data-line-number="2"><span class="st">        </span><span class="kw">unnest_tokens</span>(term, term, <span class="dt">token =</span> <span class="st">&quot;ngrams&quot;</span>, <span class="dt">n =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1600-3" data-line-number="3"><span class="st">        </span>dplyr<span class="op">::</span><span class="kw">count</span>(term, <span class="dt">sort=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1600-4" data-line-number="4"><span class="kw">c</span>(tokens)</a></code></pre></div>
<pre><code>## $term
##  [1] &quot;great big&quot;   &quot;a wolf&quot;      &quot;a big&quot;       &quot;an animal&quot;  
##  [5] &quot;animal has&quot;  &quot;animal in&quot;   &quot;big animal&quot;  &quot;big ears&quot;   
##  [9] &quot;big eyes&quot;    &quot;big teeth&quot;   &quot;ears great&quot;  &quot;eyes great&quot; 
## [13] &quot;has great&quot;   &quot;hood met&quot;    &quot;in my&quot;       &quot;is a&quot;       
## [17] &quot;is an&quot;       &quot;little red&quot;  &quot;met a&quot;       &quot;my house&quot;   
## [21] &quot;red riding&quot;  &quot;riding hood&quot; &quot;the animal&quot;  &quot;there is&quot;   
## [25] &quot;wolf is&quot;    
## 
## $n
##  [1] 3 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</code></pre>

<p><strong>Stemming</strong> and <strong>Lemmatization</strong>  </p>
<p>In the <strong>English</strong> language, we use <strong>inflection forms</strong> for <strong>parts of speech</strong>. For example, the <strong>adjective</strong> form of <strong>beauty</strong> is <strong>beautiful</strong>. The past participle of <strong>run</strong> is <strong>running</strong>. The plural form of <strong>chocolate</strong> is <strong>chocolates</strong>. Now, it may be necessary to reduce such terms into their <strong>stem</strong> (or <strong>root</strong>) form, e.g. <strong>beauty</strong>, <strong>run</strong>, and <strong>chocolate</strong>. This method is called <strong>Stemming</strong> and <strong>Lemmatization</strong>.</p>
<p><strong>Stemming</strong> cuts the <strong>suffix</strong> and <strong>prefix</strong> of words. For example, <strong>stemming</strong> cuts the suffix <strong>ing</strong> from the term <strong>cooking</strong>. A good choice of stemmer is important to avoid <strong>over stemming</strong> or <strong>under stemming</strong>. For example, a poor <strong>stemmer</strong> may cut <strong>driving</strong> into <strong>driv</strong>.</p>
<p><strong>Lemmatization</strong> takes into account the morphological structure of words. For example, if we simply cut the prefix <strong>ing</strong> for <strong>running</strong>, we may end up with <strong>runn</strong>. <strong>Lemmatization</strong> allows us to transform <strong>running</strong> into <strong>run</strong>, <strong>swimming</strong> into <strong>swim</strong>, <strong>driving</strong> into <strong>drive</strong>. Also, words such as <strong>am</strong>, <strong>is</strong>, <strong>are</strong> get converted into <strong>be</strong>.</p>
<p>To illustrate, let us use the <strong>quanteda</strong> library to perform all the <strong>pre-processing</strong> methods we discussed so far, including stemming and lemmatization. Here, we use <strong>dfm(.)</strong> function.</p>
<p>First, we generate a document corpus using <strong>corpus(.)</strong> function from <strong>quanteda</strong> library:</p>

<div class="sourceCode" id="cb1602"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1602-1" data-line-number="1"><span class="kw">library</span>(quanteda)</a>
<a class="sourceLine" id="cb1602-2" data-line-number="2">doc.frame =<span class="st">  </span><span class="kw">tibble</span>(<span class="dt">docid =</span> <span class="kw">c</span>(<span class="st">&#39;D1&#39;</span>,<span class="st">&#39;D2&#39;</span>,<span class="st">&#39;D3&#39;</span>,<span class="st">&#39;D4&#39;</span>), <span class="dt">term =</span> <span class="kw">c</span>(D1, D2, D3, D4))</a>
<a class="sourceLine" id="cb1602-3" data-line-number="3">doc.corpus =<span class="st"> </span><span class="kw">corpus</span>(doc.frame, <span class="dt">docid_field =</span> <span class="st">&quot;docid&quot;</span>, <span class="dt">text_field =</span> <span class="st">&quot;term&quot;</span>)</a>
<a class="sourceLine" id="cb1602-4" data-line-number="4"><span class="kw">summary</span>(doc.corpus)</a></code></pre></div>
<pre><code>## Corpus consisting of 4 documents, showing 4 documents:
## 
##  Text Types Tokens Sentences
##    D1     9      9         1
##    D2    10     15         1
##    D3     6      6         1
##    D4     8      8         1</code></pre>

<p>Now, we can perform the pre-processing:</p>

<div class="sourceCode" id="cb1604"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1604-1" data-line-number="1">doc.tokens =<span class="st"> </span><span class="kw">tokens</span>(doc.corpus, <span class="dt">remove_punct =</span> <span class="ot">TRUE</span>,  <span class="dt">remove_numbers =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1604-2" data-line-number="2">doc.tokens =<span class="st"> </span><span class="kw">tokens_select</span>(doc.tokens, stopwords, <span class="dt">case_insensitive =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1604-3" data-line-number="3">                           <span class="dt">selection=</span><span class="st">&#39;remove&#39;</span>)</a>
<a class="sourceLine" id="cb1604-4" data-line-number="4">doc.tokens =<span class="st"> </span><span class="kw">tokens_wordstem</span>(doc.tokens)</a>
<a class="sourceLine" id="cb1604-5" data-line-number="5">doc.tokens</a></code></pre></div>
<pre><code>## Tokens consisting of 4 documents.
## D1 :
## [1] &quot;big&quot;  &quot;anim&quot; &quot;my&quot;   &quot;hous&quot;
## 
## D2 :
##  [1] &quot;anim&quot;  &quot;great&quot; &quot;big&quot;   &quot;ear&quot;   &quot;great&quot; &quot;big&quot;   &quot;eye&quot;   &quot;great&quot;
##  [9] &quot;big&quot;   &quot;teeth&quot;
## 
## D3 :
## [1] &quot;wolf&quot; &quot;anim&quot;
## 
## D4 :
## [1] &quot;Littl&quot; &quot;Red&quot;   &quot;Ride&quot;  &quot;Hood&quot;  &quot;met&quot;   &quot;Wolf&quot;</code></pre>

<p>One point to make is that <strong>stemming</strong> results in the term <strong>hous</strong> for <strong>house</strong> and <strong>anim</strong> for <strong>animal</strong>, which may be correct but not as expected.</p>
<p>Another point to make is that <strong>quanteda</strong> has its list of <strong>stopwords</strong>:</p>

<div class="sourceCode" id="cb1606"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1606-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">stopwords</span>(<span class="dt">source =</span> <span class="st">&quot;smart&quot;</span>), <span class="dt">n=</span><span class="dv">25</span>)  <span class="co"># listing only 25 stopwords</span></a></code></pre></div>
<pre><code>##  [1] &quot;a&quot;           &quot;a&#39;s&quot;         &quot;able&quot;        &quot;about&quot;      
##  [5] &quot;above&quot;       &quot;according&quot;   &quot;accordingly&quot; &quot;across&quot;     
##  [9] &quot;actually&quot;    &quot;after&quot;       &quot;afterwards&quot;  &quot;again&quot;      
## [13] &quot;against&quot;     &quot;ain&#39;t&quot;       &quot;all&quot;         &quot;allow&quot;      
## [17] &quot;allows&quot;      &quot;almost&quot;      &quot;alone&quot;       &quot;along&quot;      
## [21] &quot;already&quot;     &quot;also&quot;        &quot;although&quot;    &quot;always&quot;     
## [25] &quot;am&quot;</code></pre>

</div>
<div id="ranking-and-scoring" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.2</span> Ranking and Scoring <a href="machinelearning3.html#ranking-and-scoring" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the simplest pieces of information we can gather from a sequence of pre-processed words is the corresponding ranks based on frequency. For example, we show that the terms <strong>big</strong> and <strong>great</strong> are ranked at the top below.</p>

<div class="sourceCode" id="cb1608"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1608-1" data-line-number="1">words =<span class="st">  </span><span class="kw">unlist</span>( <span class="kw">strsplit</span>(documents, <span class="dt">split=</span><span class="st">&#39; &#39;</span>))</a>
<a class="sourceLine" id="cb1608-2" data-line-number="2">(<span class="dt">ranked.words =</span> <span class="kw">sort</span>(<span class="kw">table</span>(words), <span class="dt">decreasing=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>## words
##    big animal  great   wolf   ears   eyes   hood  house little    met 
##      4      3      3      2      1      1      1      1      1      1 
##     my    red riding  teeth 
##      1      1      1      1</code></pre>

<p>We can also just quickly validate the result by using <strong>count(.)</strong>.</p>

<div class="sourceCode" id="cb1610"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1610-1" data-line-number="1"><span class="kw">library</span>(tidytext)</a>
<a class="sourceLine" id="cb1610-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb1610-3" data-line-number="3">stopwords.frame =<span class="st"> </span><span class="kw">tibble</span>( <span class="dt">term =</span> stopwords)</a>
<a class="sourceLine" id="cb1610-4" data-line-number="4">tokens =<span class="st"> </span>tokenized.table  <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1610-5" data-line-number="5"><span class="st">        </span>dplyr<span class="op">::</span><span class="kw">count</span>(term, <span class="dt">sort=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1610-6" data-line-number="6"><span class="kw">c</span>(tokens)</a></code></pre></div>
<pre><code>## $term
##  [1] &quot;big&quot;    &quot;animal&quot; &quot;great&quot;  &quot;wolf&quot;   &quot;ears&quot;   &quot;eyes&quot;   &quot;hood&quot;  
##  [8] &quot;house&quot;  &quot;little&quot; &quot;met&quot;    &quot;my&quot;     &quot;red&quot;    &quot;riding&quot; &quot;teeth&quot; 
## 
## $n
##  [1] 4 3 3 2 1 1 1 1 1 1 1 1 1 1</code></pre>

<p><strong>Ranking based on TF-IDF</strong> </p>
<p>Ranking can be achieved using <strong>TF-IDF</strong>, short for <strong>Term Frequency - Inverse Document Frequency</strong>. It is a method to rank <strong>relevant</strong> <strong>words</strong> (or <strong>terms</strong>) found in documents. A simple ranking method is to count for the occurrence of terms - the frequency - which we demonstrated in the previous section. The result only provides the <strong>frequency</strong> as the basis for our ranking. However, it does not necessarily provide accurate relevance of the terms to the four documents from where they come. <strong>TF-IDF</strong> uses a formula to compute for <strong>relevance</strong>:   </p>

<p><span class="math display">\[\begin{align}
\text{score}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t,D)
\end{align}\]</span>
</p>
<p>where:</p>

<p><span class="math display">\[\begin{align}
\text{TF}(t, d) &amp;= \frac{\text{count}(t\ \in\ d)}{\text{count}(d)} = \frac{\text{Freq of the term }\mathbf{t}\ \text{ in d}}{\text{No of words in d}}  \\
\text{IDF}(t,D) &amp;= \log_e \left(\frac{N}{df_t \text{ = count}(d\ \in\ D:\ t\ \in\ d)}\right)\\
&amp;= \log_e \left( \frac{\text{Total No of Documents}}{\text{No of Documents containing the term }\mathbf{t}} \right) 
\end{align}\]</span>
</p>
<p>and where:</p>
<ul>
<li><span class="math inline">\(\mathbf{\text{TF}(t,d)}\)</span> is the frequency of word (t) found in document (d).</li>
<li><span class="math inline">\(\mathbf{\text{IDF}(t,D)}\)</span> is the number of documents containing word (t). Its intent is to give high score (relevance) to rare words.</li>
</ul>
<p>Note that stopwords such as <strong>the</strong>, <strong>a</strong>, and <strong>an</strong> are high in frequency and are not rare. In <strong>TF-IDF</strong>, the inclusion of the <strong>IDF</strong> formula in the equation allows us to give low scores (or low relevance) to stopwords; thus, we choose not to remove the <strong>stopwords</strong> in our corpus as they get pushed to lower relevance.</p>
<p>To illustrate <strong>TF-IDF</strong>, let us calculate the <strong>TF-IDF</strong> weight for the terms <strong>big</strong> in D1 versus in D2 using the <strong>pre-processed</strong> documents.</p>

<p><span class="math display">\[
\begin{array}{ll}
\text{score}(big, D1) &amp;= TF(big, D1) \times IDF(big,D) \\
&amp;= \frac{\text{Freq of the term }\mathbf{big}\ \text{ in D1}}{\text{No of words in D1}} \times 
    \log_e \left( \frac{\text{Total No of Documents}}{\text{No of Docs containing the term }\mathbf{big}} \right) \\
&amp;= \frac{1}{4} \times \log_e \left( \frac{4}{2} \right) \\ 
&amp;= 0.25 \times \log_e(2)\\
&amp;= 0.25 \times 0.6931472 = 0.1732868\\
\\
\text{score}(big, D2) &amp;= \frac{3}{10} \times \log_e \left( \frac{4}{2} \right) \\ 
&amp;= 0.30 \times \log_e(2)\\
&amp;= 0.30 \times 0.6931472 = 0.2079442 
\end{array}
\]</span>
</p>
<p>The calculations show that the term <strong>big</strong> has a 17.3% relevance in D2 and has an 8.66% relevance in D1, indicating that the term is more relevant in D2 than in D1.</p>
<p>An enhanced variance of <strong>TF-IDF</strong> called <strong>Okapi BM25</strong> <span class="citation">(S.E. Robertson et al. <a href="bibliography.html#ref-ref984se">1994</a>)</span> has the following formula below that accounts for other considerations when ranking terms:  </p>

<p><span class="math display">\[\begin{align}
score(D,Q)_{(BM25)} = \underbrace{ \sum_{t\ \in\ Q} \frac{TF_{t,d} \times \left(k + 1\right)}{TF_{t,d} + k \times \left(1-b+b \times \frac{dl}{adl}\right)}}_{\text{TF}} \times 
\underbrace{\log_e \left(\frac{N - df_t + 0.5}{df_t + 0.5}\right)}_{\text{IDF}} 
\end{align}\]</span>
</p>
<p>The first consideration is to account for <strong>Term Saturation</strong>, which modifies the <strong>TF</strong> into the following formula with the idea that as the frequency of a given term fully reaches some maximum level, any further increase in frequency may not provide any further benefit. Therefore the <strong>k</strong> parameter controls the effect of <strong>TF</strong>.</p>
<p><span class="math display">\[\begin{align}
TF(t, d)\ \text{modified into} \rightarrow \frac{TF(t,d) \times (k + 1)}{TF(t,d) + k}
\end{align}\]</span></p>
<p>Figure <a href="machinelearning3.html#fig:tfidf1">11.8</a> illustrates the effect of different values of <strong>k</strong>:</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tfidf1"></span>
<img src="DS_files/figure-html/tfidf1-1.png" alt="TF (Term Saturation" width="70%" />
<p class="caption">
Figure 11.8: TF (Term Saturation
</p>
</div>

<p>The second consideration accounts for <strong>Document Length</strong>. The <strong>TF</strong> term gets modified further like so:</p>

<p><span class="math display">\[\begin{align}
TF(t, d)\ \text{modified into} \rightarrow \frac{TF(t,d) \times (k + 1 ) }{TF(t,d) + k \times \frac{dl}{adl}}
\end{align}\]</span>
</p>
<p>where <strong>dl</strong> is the document length and <strong>adl</strong> is the average document length. The relevance of a term in a document is much higher if the length is shorter than if it is longer; thus, a higher weight is given to terms in shorter documents. On the other hand, if <strong>document length</strong> is not as important, we can disable or reduce the weight using the <strong>b</strong> parameter, giving a value between [0,1].</p>

<p><span class="math display">\[\begin{align}
TF(t, d)\ \text{modified into} \rightarrow \frac{TF(t,d) \times (k + 1)}{TF(t,d) + k \times \left(1 - b + b \times\frac{dl}{adl}\right)}
\end{align}\]</span>
</p>
<p>The last consideration is the adjustment of <strong>IDF</strong>. This adjustment can be regarded as an additional tune-up for <strong>term relevance</strong> from <strong>Lucene</strong> variance.</p>

<p><span class="math display">\[\begin{align}
IDF = \log_e\left(\frac{N}{df_t}\right) \propto \log_e \left( \frac{1+ N - df_t + 0.5}{df_t + 0.5} \right)  
\end{align}\]</span>
</p>
<p>Let us apply <strong>Okapi BM25</strong> to our previous example (assume k=2 and b=0.70):</p>

<p><span class="math display">\[
\begin{array}{ll}
\text{score}(big, D1) &amp;= TF(big, D1) \times IDF(big,D) \\
&amp;= \frac{\frac{1}{4} \times (2 + 1)}{ \frac{1}{4}  + 2 \times \left(1 - 0.70 + 0.70 \times \frac{4}{5.5}\right)} \times \log_e \left( \frac{1 + 4 - 2 + 0.5}{2 + 0.5} \right) \\
&amp;= \frac{0.75}{1.868182} \times \log_e(\frac{3.5}{2.5})\\
&amp;= 0.1350801 
\\
\text{score}(big, D2) &amp;= \frac{\frac{3}{10} \times (2 + 1)}{ \frac{3}{10}  + 2 \times \left(1 - 0.70 + 0.70 \times \frac{10}{5.5}\right)} \times \log_e \left( \frac{1 + 4 - 2 + 0.5}{2 + 0.5} \right) \\
&amp;= \frac{0.9}{3.445455} \times \log_e(\frac{3.5}{2.5})\\
&amp;= 0.08789115
\end{array}
\]</span>
</p>
<p>The document lengths for <strong>d1</strong> and <strong>d2</strong> are 4 and 10 respectively. The average length is 5.5 based on the computation below:</p>

<div class="sourceCode" id="cb1612"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1612-1" data-line-number="1">D    =<span class="st"> </span>documents</a>
<a class="sourceLine" id="cb1612-2" data-line-number="2">dl   =<span class="st"> </span><span class="cf">function</span> (d) { <span class="kw">length</span>( <span class="kw">unlist</span>(<span class="kw">strsplit</span>(d, <span class="st">&quot; &quot;</span>)) ) }</a>
<a class="sourceLine" id="cb1612-3" data-line-number="3">(<span class="dt">adl =</span> ( <span class="kw">dl</span>(D[<span class="dv">1</span>]) <span class="op">+</span><span class="st"> </span><span class="kw">dl</span>(D[<span class="dv">2</span>]) <span class="op">+</span><span class="st"> </span><span class="kw">dl</span>(D[<span class="dv">3</span>]) <span class="op">+</span><span class="st"> </span><span class="kw">dl</span>(D[<span class="dv">4</span>]) ) <span class="op">/</span><span class="st"> </span><span class="dv">4</span>)</a></code></pre></div>
<pre><code>## [1] 5.5</code></pre>

<p>Let us now re-evaluate our original ranked words and rank them using additional functions from <strong>tidytext</strong> library. Here, we use <strong>bind_tf_idf(.)</strong> function to calculate <strong>TF-IDF</strong> score.</p>

<div class="sourceCode" id="cb1614"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1614-1" data-line-number="1"><span class="kw">library</span>(tidytext)</a>
<a class="sourceLine" id="cb1614-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb1614-3" data-line-number="3">tokens =<span class="st"> </span>tokenized.table  <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1614-4" data-line-number="4"><span class="st">        </span><span class="kw">group_by</span>(docid, term) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1614-5" data-line-number="5"><span class="st">        </span><span class="kw">tally</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1614-6" data-line-number="6"><span class="st">        </span><span class="kw">arrange</span>(<span class="kw">desc</span>(n))</a>
<a class="sourceLine" id="cb1614-7" data-line-number="7"><span class="kw">head</span>( tokens <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_tf_idf</span>(term, docid, n) ) <span class="co"># display top 5</span></a></code></pre></div>
<pre><code>## # A tibble: 6 x 6
## # Groups:   docid [2]
##   docid term       n    tf   idf tf_idf
##   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1 D2    big        3  0.3  0.693 0.208 
## 2 D2    great      3  0.3  1.39  0.416 
## 3 D1    animal     1  0.25 0.288 0.0719
## 4 D1    big        1  0.25 0.693 0.173 
## 5 D1    house      1  0.25 1.39  0.347 
## 6 D1    my         1  0.25 1.39  0.347</code></pre>

<p>We leave readers to evaluate other variants of <strong>BM25</strong> and its other counterparts.</p>
<p><strong>Scoring Based on Cumulative Gain</strong></p>
<p>While <strong>TF-IDF</strong> provides us the ability to rank words, we use two popular <strong>ranking</strong> measures to determine the <strong>quality of ranking</strong>, namely <strong>mean Average Precision (mAP)</strong> and <strong>Normalized Discounted Cumulative Gain (nDCG)</strong>. In this section, let us cover <strong>nDCG</strong> and defer the discussion of <strong>mAP</strong> in the <strong>Recommender System (Image Classification)</strong> section.   </p>
<p>We begin with the idea that a <strong>search engine</strong> retrieves a list of documents based on a given user query. In <strong>search engines</strong>, we rank retrieved articles based on relevance to our <strong>query</strong>. The <strong>relevance</strong> of each retrieved documents can be categorically labeled as: <strong>1 - not relevant</strong>, <strong>2 - fairly relevant</strong>, <strong>3 - quite relevant</strong>. The value given to a document is called the <strong>Gain</strong>. However, we are after the <strong>cumulative gain</strong> to score the effectiveness of the retrieval, for which we use the following <strong>nDCG</strong> Scoring:</p>
<p><span class="math display">\[\begin{align}
nDCG = \frac{DCG_{(n)}}{IDCG_{(n)}}\ \ \ \ \ \ \text{where n is number of retrieved documents}
\end{align}\]</span></p>
<p>The score depends on the <strong>Cumulative Gain</strong> and <strong>Discounted Cumulative Gain</strong> and <strong>Ideal Discounted Cumulative Gain (IDC)</strong> expressed respectively below:    </p>

<p><span class="math display">\[\begin{align}
\mathbf{CG} = \sum_{i=1}^n G_i\ \ \ \ \  \mathbf{DCG} = \sum_{i=1}^n \frac{G_i}{\log_e(i + 1)}\\
\mathbf{IDCG} =  \sum \left[\text{sort}\_\text{terms} \left\{\frac{G_i}{\log_e(i + 1)}\right\}_{i=1}^n\right]
\end{align}\]</span>
</p>
<p>To illustrate, suppose we have the following documents and their relevance (the <strong>Gain</strong>):</p>
<p><span class="math display">\[
D_{1} = 3\ \ \ \ \ \ 
D_{2} = 1\ \ \ \ \ \ 
D_{3} = 2\ \ \ \ \ \ 
D_{4} = 0\ \ \ \ \ \ 
D_{5} = 2\ \ \ \ \ \ 
\]</span></p>
<p>The <strong>Cumulative Gain (CG)</strong> result is:</p>

<p><span class="math display">\[\begin{align}
\mathbf{CG} = \sum_{i=1}^n G_i = 3 + 1 + 2 + 0 + 3 = 9
\end{align}\]</span>
</p>
<p>The <strong>Discounted Cumulative Gain (DCG)</strong> is:</p>

<p><span class="math display">\[\begin{align}
\mathbf{DCG}_{(5)} &amp;= \sum_{i=1}^n \frac{G_i}{\log_e(i + 1)} \\ 
&amp;=\frac{3}{\log_e(2)} + \frac{1}{\log_e(3)} +  \frac{2}{\log_e(4)}  
\frac{0}{\log_e(5)} +  \frac{3}{\log_e(6)} = 8.355351 \nonumber
\end{align}\]</span>
</p>
<p>The <strong>Ideal (sorted) Discounted Cumulative Gain (IDCG)</strong> is:</p>

<p><span class="math display">\[\begin{align}
\mathbf{IDCG}_{(5)} &amp;= \sum_{i=1}^n \frac{G_i}{\log_e(i + 1)}\\ 
&amp;= \frac{3}{\log_e(2)} + \frac{3}{\log_e(3)} +  \frac{2}{\log_e(4)} + 
\frac{1}{\log_e(5)} +  \frac{0}{\log_e(6)} = 9.059608 \nonumber
\end{align}\]</span>
</p>
<p><strong>Finally</strong>, the <strong>nDCG</strong> score is:</p>

<p><span class="math display">\[\begin{align}
\mathbf{nDCG}_{(5)} = \frac{DCG_{(5)}}{IDCG_{(5)}} = \frac{8.355351}{9.059608} = 0.9222641
\end{align}\]</span>
</p>
<p>Based on the score, we can say that the <strong>search engine</strong> demonstrates 92.23% effectiveness in retrieving the relevant documents.</p>
</div>
<div id="document-similarity" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.3</span> Document Similarity <a href="machinelearning3.html#document-similarity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For <strong>Information Retrieval</strong>, a classic representation of texts is in the form of a <strong>Vector Space Model (VSM)</strong>. The model is achieved by casting each term in the texts into its corresponding numerical equivalence that becomes helpful in measuring similarity at some point. Our goal is to use <strong>VSM</strong> to determine term similarities or word relationships.  </p>
<p>We start with a generic similarity function like so:</p>
<p><span class="math display">\[\begin{align}
\mathbf{\text{similarity}}\left(d^{(1)}, d^{(2)}\right)\ \ \ \ \ \ \ \ \text{where }\mathbf{d^{(1)}}\ \text{is 1st document and }\mathbf{d^{(2)}}\ \text{is 2nd document.}
\end{align}\]</span></p>
<p>Note that we can also compare a given query relevant to a retrieved document like so:</p>
<p><span class="math display">\[\begin{align}
\mathbf{\text{similarity}}(q, d)\ \ \ \ \ \ \ \ \text{where }\mathbf{q}\ \text{is query and }\mathbf{d}\ \text{is document.}
\end{align}\]</span></p>
<p>The goal is to determine if the given query is similar (or <strong>relevant</strong>) to a <strong>retrieved</strong> document. Here, a good measure of similarity is based on <strong>cosine similarity</strong>; albeit, we also can use <strong>euclidean distance</strong> and so on for comparison.</p>
<p><span class="math display">\[\begin{align}
\mathbf{\text{similarity}}(q, d) = \mathbf{\text{cosine}}(\vec{q}, \vec{d}) = \frac{\vec{q} \cdot \vec{d}}{\|\vec{q}\|\|\vec{d}\|}  =
\frac{\sum_{i=1}^{|v|} q_i d_i}{\sqrt{\sum_{i=1}^{|v|}q_i^2} \sqrt{\sum_{i=1}^{|v|} d_i^2}}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(|v|\)</span> represents the number of unique features (unique terms).</li>
<li><span class="math inline">\(\mathbf{q_i}\)</span> is a calculated numerical value, e.g. <strong>tf-idf</strong> score for ith term in the query (<strong>q</strong>).</li>
<li><span class="math inline">\(\mathbf{d_i}\)</span> is a calculated numerical value, e.g. <strong>tf-idf</strong> score for ith term in the document (<strong>d</strong>).</li>
</ul>
<p>To illustrate, let us cast our original document corpus into <strong>Document Feature Matrix (DFM)</strong>, also called <strong>Document Term Matrix (DTM)</strong> using <strong>dfm(.)</strong> function from <strong>quanteda</strong> library. <strong>Pre-processing</strong> can also be applied through the function (below, we disable stemming):    </p>

<div class="sourceCode" id="cb1616"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1616-1" data-line-number="1">doc.dfm =<span class="st">  </span><span class="kw">dfm</span>(doc.corpus, <span class="dt">tolower =</span> <span class="ot">TRUE</span>,  <span class="dt">remove_punct =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb1616-2" data-line-number="2">                    <span class="dt">remove_numbers =</span> <span class="ot">TRUE</span>,  <span class="dt">stem         =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb1616-3" data-line-number="3">                    <span class="dt">remove         =</span> stopwords</a>
<a class="sourceLine" id="cb1616-4" data-line-number="4">                )</a>
<a class="sourceLine" id="cb1616-5" data-line-number="5">(<span class="dt">dtm.doc =</span> <span class="kw">data.frame</span>(<span class="kw">as.matrix</span>(doc.dfm)))[,<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>] <span class="co"># display in data frame </span></a></code></pre></div>
<pre><code>##    big animal my house great ears eyes teeth
## D1   1      1  1     1     0    0    0     0
## D2   3      1  0     0     3    1    1     1
## D3   0      1  0     0     0    0    0     0
## D4   0      0  0     0     0    0    0     0</code></pre>

<p>We can convert the data frame into a <strong>Term Document Matrix (TDM)</strong> using transpose (we can come back to this form in a later section for <strong>LSA</strong>):</p>

<div class="sourceCode" id="cb1618"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1618-1" data-line-number="1">(<span class="dt">tdm.doc =</span> <span class="kw">data.frame</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(dtm.doc))))</a></code></pre></div>
<pre><code>##        D1 D2 D3 D4
## big     1  3  0  0
## animal  1  1  1  0
## my      1  0  0  0
## house   1  0  0  0
## great   0  3  0  0
## ears    0  1  0  0
## eyes    0  1  0  0
## teeth   0  1  0  0
## wolf    0  0  1  1
## little  0  0  0  1
## red     0  0  0  1
## riding  0  0  0  1
## hood    0  0  0  1
## met     0  0  0  1</code></pre>

<p>We can use the <strong>topfeatures(.)</strong> function with <strong>DFM</strong> to rank terms. It shows the same result as before.</p>

<div class="sourceCode" id="cb1620"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1620-1" data-line-number="1"><span class="kw">topfeatures</span>(doc.dfm) </a></code></pre></div>
<pre><code>##    big animal  great   wolf     my  house   ears   eyes  teeth little 
##      4      3      3      2      1      1      1      1      1      1</code></pre>

<p>For <strong>cosine similarity</strong>, we can use the <strong>dfm_weight(.)</strong> function to obtain the same result as <strong>dfm(.)</strong></p>

<div class="sourceCode" id="cb1622"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1622-1" data-line-number="1"><span class="kw">as.matrix</span>( <span class="kw">dfm_weight</span>(doc.dfm) )</a></code></pre></div>
<pre><code>##     features
## docs big animal my house great ears eyes teeth wolf little red riding
##   D1   1      1  1     1     0    0    0     0    0      0   0      0
##   D2   3      1  0     0     3    1    1     1    0      0   0      0
##   D3   0      1  0     0     0    0    0     0    1      0   0      0
##   D4   0      0  0     0     0    0    0     0    1      1   1      1
##     features
## docs hood met
##   D1    0   0
##   D2    0   0
##   D3    0   0
##   D4    1   1</code></pre>

<p>Using the <strong>DTM</strong> above, the corresponding <strong>Cosine Similarity</strong> between the document D1 and D2 is: </p>
<p><span class="math display">\[\begin{align}
\vec{d^{(1)}} \cdot \vec{d^{(2)}}  &amp;= \sum_{i=1}^{|v|} d_i^{(1)} d_i^{(2)}\\
&amp;=(1\times 3) + (1 \times 1) + (1\times 0) + (1\times 0) + (0 \times 3) + \nonumber \\
&amp;\ ( 0 \times 1 ) +  ( 0 \times 1 ) + ( 0 \times 1 ) \nonumber \\
&amp;= 4 \nonumber \\
\nonumber \\
\sqrt{\sum_{i=1}^{|v|} (d^{{(1)}})^2} &amp;= \sqrt{1^2 + 1^2 + 1^2 + 1^2}  = 2\\
\nonumber \\
\sqrt{\sum_{i=1}^{|v|} (d^{{(2)}})^2} &amp;= \sqrt{3^2 + 1^2 +  3^2 + 1^2 + 1^2 + 1^2}  = 4.690416\\
\nonumber \\
\mathbf{\text{cosine}}(\vec{q}, \vec{d}) &amp;= 4\ /\ (\ 2 \times 4.690416\ ) = 0.4264014
\end{align}\]</span></p>
<p>A value of one means that the two documents are similar.</p>
<p>Alternatively, we can also generate a <strong>Document Term Matrix</strong> with <strong>TF-IDF</strong> score:</p>

<div class="sourceCode" id="cb1624"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1624-1" data-line-number="1">tfidf =<span class="st"> </span><span class="kw">dfm_tfidf</span>(doc.dfm, <span class="dt">scheme_tf =</span> <span class="st">&quot;prop&quot;</span>, <span class="dt">base=</span><span class="kw">exp</span>(<span class="dv">1</span>), <span class="dt">force=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1624-2" data-line-number="2"><span class="kw">round</span>( <span class="kw">as.matrix</span>( tfidf ), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##     features
## docs  big animal   my house great ears eyes teeth wolf little  red
##   D1 0.17   0.07 0.35  0.35  0.00 0.00 0.00  0.00 0.00   0.00 0.00
##   D2 0.21   0.03 0.00  0.00  0.42 0.14 0.14  0.14 0.00   0.00 0.00
##   D3 0.00   0.14 0.00  0.00  0.00 0.00 0.00  0.00 0.35   0.00 0.00
##   D4 0.00   0.00 0.00  0.00  0.00 0.00 0.00  0.00 0.12   0.23 0.23
##     features
## docs riding hood  met
##   D1   0.00 0.00 0.00
##   D2   0.00 0.00 0.00
##   D3   0.00 0.00 0.00
##   D4   0.23 0.23 0.23</code></pre>

<p>Using the <strong>DTM</strong> above, the corresponding <strong>cosine similarity</strong> between the document D1 and D2 is:</p>
<p><span class="math display">\[\begin{align}
\vec{d^{(1)}} \cdot \vec{d^{(2)}}  &amp;= \sum_{i=1}^{|v|} d_i^{(1)} d_i^{(2)}\\
&amp;=(0.17\times 0.21) + (0.07 \times 0.03) + (0.35\times 0) + (0.35\times 0)  \nonumber \\
&amp;\ \ \  + (0 \times 0.42)  + ( 0 \times 0.14 ) +  ( 0 \times 0.14 ) + ( 0 \times 0.14 ) \nonumber \\
&amp;= 0.0378 \nonumber\\
\nonumber \\
\sqrt{\sum_{i=1}^{|v|} (d^{{(1)}})^2} &amp;= \sqrt{0.17^2 + 0.07^2 + 0.35^2 + 0.35^2 }  = 0.5280152\\
\nonumber \\
\sqrt{\sum_{i=1}^{|v|} (d^{{(2)}})^2} &amp;= \sqrt{0.03^2 + 0.42^2 + 0.14^2 + 0.14^2 + 0.14^2}  = 0.4859012\\
\nonumber \\ 
\mathbf{\text{cosine}}(\vec{q}, \vec{d}) &amp;= 0.0378\ /\ (\ 0.5280152\times 0.4859012\ ) = 0.1473321
\end{align}\]</span></p>
<p>The <strong>cosine similarity</strong> of the two documents is adjusted using <strong>TF-IDF</strong>. Notice that with <strong>TF-IDF</strong>, the <strong>similarity</strong> measurements drop even lower, indicating dissimilarity between the two documents, though human intuition seems there seems to be some relation between the two documents. We can revisit this in <strong>Latent Semantic Analysis (LSA)</strong>.</p>
</div>
<div id="linguistic-analysis" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.4</span> Linguistic Analysis <a href="machinelearning3.html#linguistic-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When it comes to <strong>Speech Recognition</strong>, we emphasize <strong>Linguistics</strong>, defined by <strong>Oxford language</strong> and <strong>Google Dictionary</strong> to be the scientific study of language and structure, including the study of <strong>morphology</strong> (language form), <strong>Syntax</strong>, <strong>Phonetics</strong>, and <strong>Semantics</strong> which includes both <strong>Lexical</strong> structure (language vocabulary for meaning and relationship) and <strong>Conceptual</strong> structure (language vocabulary for meaning and cognition). <strong>Wikipedia</strong> also includes <strong>Contextual</strong> structure, including social, historical, cultural, and other properties that influence the language.</p>
<p>We may not be able to cover them all in this book as they deserve their own merits. Instead, the next few sections will touch only on <strong>Lexical</strong> and <strong>Semantic</strong> analysis basics. This, in particular, covers <strong>Parts of Speach (POS)</strong> and <strong>Phrases</strong> (also called <strong>Chunks</strong>).</p>
<p>Also, we cover additional insight about <strong>Speech Recognition</strong> in Chapter <strong>13</strong> (<strong>Computational Deep Learning II</strong>).</p>
</div>
<div id="lexical-analysis" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.5</span> Lexical Analysis <a href="machinelearning3.html#lexical-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For <strong>NLP</strong>, getting the closest insight about the content (or possibly common key themes) of a set of documents is essential. A straightforward way to determine the closest topic is to count the frequency of all terms found in the documents to rank them accordingly.</p>
<p>This section extends our discussion on <strong>pre-processing</strong> of words and <strong>TF-IDF</strong> starting with <strong>POS tagging</strong> - the processes of labeling or annotating words with <strong>Parts of Speech</strong>.</p>
<p><strong>POS Tagging</strong> </p>
<p>There should be more than 15 to 20 different tags, each corresponding to a particular <strong>Part of Speech</strong>. The following <strong>(POS) tags</strong> are common (albeit we only show a few tags):</p>
<ul>
<li><strong>NN</strong> - Noun</li>
<li><strong>NNS</strong> - Noun Plural</li>
<li><strong>VBD</strong> - Verb</li>
<li><strong>JJ</strong> - Adjective</li>
<li><strong>RB</strong> - Adverb</li>
<li><strong>PRP</strong> - Possessive Pronoun</li>
</ul>
<p>To illustrate, we use <strong>treetag(.)</strong> function from the <strong>koRpus</strong> library for POS tagging:</p>

<div class="sourceCode" id="cb1626"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1626-1" data-line-number="1"><span class="kw">library</span>(koRpus)</a>
<a class="sourceLine" id="cb1626-2" data-line-number="2"><span class="kw">library</span>(koRpus.lang.en)</a>
<a class="sourceLine" id="cb1626-3" data-line-number="3"><span class="kw">library</span>(tm)</a>
<a class="sourceLine" id="cb1626-4" data-line-number="4"><span class="kw">library</span>(SnowballC)</a>
<a class="sourceLine" id="cb1626-5" data-line-number="5"><span class="kw">set.kRp.env</span>(<span class="dt">TT.cmd=</span><span class="st">&quot;~/nlp/cmd/tree-tagger-english&quot;</span> , <span class="dt">lang=</span><span class="st">&quot;en&quot;</span>, </a>
<a class="sourceLine" id="cb1626-6" data-line-number="6">             <span class="dt">preset=</span><span class="st">&quot;en&quot;</span>, <span class="dt">treetagger=</span><span class="st">&quot;kRp.env&quot;</span>, <span class="dt">format=</span><span class="st">&quot;file&quot;</span>, <span class="dt">TT.tknz=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1626-7" data-line-number="7">             <span class="dt">add.desc =</span> <span class="ot">FALSE</span>, <span class="dt">encoding=</span><span class="st">&quot;UTF-8&quot;</span>)</a>
<a class="sourceLine" id="cb1626-8" data-line-number="8">tagged.text =<span class="st"> </span><span class="kw">treetag</span>( <span class="st">&quot;~/nlp/documents.txt&quot;</span> , </a>
<a class="sourceLine" id="cb1626-9" data-line-number="9">                       <span class="dt">stopwords =</span> tm<span class="op">::</span><span class="kw">stopwords</span>(<span class="st">&quot;en&quot;</span>),</a>
<a class="sourceLine" id="cb1626-10" data-line-number="10">                       <span class="dt">stemmer   =</span> SnowballC<span class="op">::</span>wordStem) </a>
<a class="sourceLine" id="cb1626-11" data-line-number="11">tagged.text[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">2</span><span class="op">:</span><span class="dv">10</span>] <span class="co"># Display the 1st 10 lines</span></a></code></pre></div>
<pre><code>##     token  tag  lemma lttr      wclass desc  stop  stem idx
## 1   There  EX0  there    5 existential   NA  TRUE There   1
## 2      is  VBZ     be    2        verb   NA  TRUE     i   2
## 3       a  AT0      a    1     article   NA  TRUE     a   3
## 4     big  AJ0    big    3   adjective   NA FALSE   big   4
## 5  animal  NN1 animal    6        noun   NA FALSE  anim   5
## 6      in  PRP     in    2 preposition   NA  TRUE    in   6
## 7      my  DPS      i    2  determiner   NA  TRUE    my   7
## 8   house  NN1  house    5        noun   NA FALSE  hous   8
## 9       . SENT      .    1    fullstop   NA FALSE     .   9
## 10    The  AT0    the    3     article   NA  TRUE   The  10</code></pre>

<p>We can then show the mapping between terms (tokens) and tags like so:</p>

<div class="sourceCode" id="cb1628"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1628-1" data-line-number="1">pos_tag &lt;-<span class="st"> </span><span class="cf">function</span>(tokens, tags) { <span class="kw">paste0</span>(tokens, <span class="st">&quot;/&quot;</span>, tags) }</a>
<a class="sourceLine" id="cb1628-2" data-line-number="2">tokens =<span class="st"> </span>tagged.text<span class="op">@</span>tokens<span class="op">$</span>token</a>
<a class="sourceLine" id="cb1628-3" data-line-number="3">tags   =<span class="st"> </span>tagged.text<span class="op">@</span>tokens<span class="op">$</span>tag</a>
<a class="sourceLine" id="cb1628-4" data-line-number="4"><span class="kw">pos_tag</span>(tokens, tags)</a></code></pre></div>
<pre><code>##  [1] &quot;There/EX0&quot;  &quot;is/VBZ&quot;     &quot;a/AT0&quot;      &quot;big/AJ0&quot;    &quot;animal/NN1&quot;
##  [6] &quot;in/PRP&quot;     &quot;my/DPS&quot;     &quot;house/NN1&quot;  &quot;./SENT&quot;     &quot;The/AT0&quot;   
## [11] &quot;animal/NN1&quot; &quot;has/VBZ&quot;    &quot;great/AJ0&quot;  &quot;big/AJ0&quot;    &quot;ears/NN2&quot;  
## [16] &quot;,/PUN&quot;      &quot;great/AJ0&quot;  &quot;big/AJ0&quot;    &quot;eyes/NN2&quot;   &quot;,/PUN&quot;     
## [21] &quot;great/AJ0&quot;  &quot;big/AJ0&quot;    &quot;teeth/NN2&quot;  &quot;./SENT&quot;     &quot;A/AT0&quot;     
## [26] &quot;wolf/NN1&quot;   &quot;is/VBZ&quot;     &quot;an/AT0&quot;     &quot;animal/NN1&quot; &quot;./SENT&quot;    
## [31] &quot;Little/DT0&quot; &quot;Red/AJ0&quot;    &quot;Riding/NP0&quot; &quot;Hood/NP0&quot;   &quot;met/VBD&quot;   
## [36] &quot;a/AT0&quot;      &quot;Wolf/NP0&quot;   &quot;./SENT&quot;</code></pre>

<p>There are a few notes to make about the output above. First, the document.txt file contains four sentences, as in the case we have previously. Each term in the output above maps to its corresponding sentence.</p>
<p>Secondly, note that we used 3rd-party stopwords and stemmers from <strong>tm</strong> library and <strong>SnowballC</strong> library, respectively. We also can use our <strong>stopwords</strong> as an option.</p>
<p>Lastly, note that <strong>treetagger</strong> is a 3rd-party software externally used by <strong>koRpus</strong>.</p>
<p><strong>Annotation</strong></p>
<p>Different structures of a text can be annotated. We can annotate <strong>Documents</strong>, <strong>Paragraphs</strong>, <strong>Sentences</strong>, <strong>Phrases</strong>, <strong>Clauses</strong>, <strong>Words</strong>. Only a few will be explained. Moreover, we use a common library called <strong>NLP</strong> and <strong>openNLP</strong>.</p>
<p>To illustrate, let us use <strong>Annotator</strong> functions from <strong>openNLP</strong> and <strong>NLP</strong> libraries for annotation. At the same time, we show an alternative way to perform <strong>POS tagging</strong>, and in addition, to handle <strong>Chunking</strong>. Below, we annotate texts based on sentence structure, word structure, and POS forms.</p>

<div class="sourceCode" id="cb1630"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1630-1" data-line-number="1"><span class="kw">library</span>(NLP)</a>
<a class="sourceLine" id="cb1630-2" data-line-number="2"><span class="kw">library</span>(openNLP)</a>
<a class="sourceLine" id="cb1630-3" data-line-number="3"><span class="kw">library</span>(openNLPmodels.en)</a>
<a class="sourceLine" id="cb1630-4" data-line-number="4">text =<span class="st">  </span><span class="kw">as.String</span>( doc.corpus ) <span class="co"># as.String(doc.corpus)</span></a>
<a class="sourceLine" id="cb1630-5" data-line-number="5">sentence.annotator =<span class="st"> </span><span class="kw">Maxent_Sent_Token_Annotator</span>() </a>
<a class="sourceLine" id="cb1630-6" data-line-number="6">word.annotator     =<span class="st"> </span><span class="kw">Maxent_Word_Token_Annotator</span>() </a>
<a class="sourceLine" id="cb1630-7" data-line-number="7">pos.annotator      =<span class="st"> </span><span class="kw">Maxent_POS_Tag_Annotator</span>(<span class="dt">probs=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1630-8" data-line-number="8">annotated.doc  =<span class="st"> </span><span class="kw">annotate</span>(<span class="dt">s =</span> text, <span class="kw">list</span>(sentence.annotator, word.annotator))</a>
<a class="sourceLine" id="cb1630-9" data-line-number="9">pos_tagged.doc =<span class="st"> </span><span class="kw">annotate</span>(<span class="dt">s =</span> text, pos.annotator, annotated.doc)</a></code></pre></div>

<p>Now, let us show a summary of the annotations for the part of speech - the <strong>POS tags</strong>:</p>

<div class="sourceCode" id="cb1631"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1631-1" data-line-number="1"><span class="kw">head</span>(pos_tagged.doc, <span class="dt">n=</span><span class="dv">10</span>) <span class="co"># display only first 10 lines</span></a></code></pre></div>
<pre><code>##  id type     start end features
##   1 sentence     1  34 constituents=&lt;&lt;integer,9&gt;&gt;
##   2 sentence    36  98 constituents=&lt;&lt;integer,15&gt;&gt;
##   3 sentence   100 119 constituents=&lt;&lt;integer,6&gt;&gt;
##   4 sentence   121 154 constituents=&lt;&lt;integer,8&gt;&gt;
##   5 word         1   5 POS=EX, POS_prob=0.9517
##   6 word         7   8 POS=VBZ, POS_prob=0.9993
##   7 word        10  10 POS=DT, POS_prob=0.9909
##   8 word        12  14 POS=JJ, POS_prob=0.9979
##   9 word        16  21 POS=NN, POS_prob=0.9799
##  10 word        23  24 POS=IN, POS_prob=0.9801</code></pre>

<p>To map tokens (terms) to tags, we code the following:</p>

<div class="sourceCode" id="cb1633"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1633-1" data-line-number="1">text.string =<span class="st"> </span><span class="kw">as.String</span>(doc.corpus)</a>
<a class="sourceLine" id="cb1633-2" data-line-number="2">words       =<span class="st"> </span><span class="kw">subset</span>(pos_tagged.doc, type<span class="op">==</span><span class="st">&quot;word&quot;</span>)</a>
<a class="sourceLine" id="cb1633-3" data-line-number="3">tags        =<span class="st"> </span><span class="kw">sapply</span>(words<span class="op">$</span>features, <span class="st">&#39;[[&#39;</span>,<span class="st">&quot;POS&quot;</span> )</a>
<a class="sourceLine" id="cb1633-4" data-line-number="4">tokens      =<span class="st"> </span>text.string[<span class="kw">subset</span>(pos_tagged.doc, type<span class="op">==</span><span class="st">&quot;word&quot;</span>)]</a>
<a class="sourceLine" id="cb1633-5" data-line-number="5"><span class="kw">pos_tag</span>(tokens, tags)</a></code></pre></div>
<pre><code>##  [1] &quot;There/EX&quot;   &quot;is/VBZ&quot;     &quot;a/DT&quot;       &quot;big/JJ&quot;     &quot;animal/NN&quot; 
##  [6] &quot;in/IN&quot;      &quot;my/PRP$&quot;    &quot;house/NN&quot;   &quot;./.&quot;        &quot;The/DT&quot;    
## [11] &quot;animal/NN&quot;  &quot;has/VBZ&quot;    &quot;great/JJ&quot;   &quot;big/JJ&quot;     &quot;ears/NNS&quot;  
## [16] &quot;,/,&quot;        &quot;great/JJ&quot;   &quot;big/JJ&quot;     &quot;eyes/NNS&quot;   &quot;,/,&quot;       
## [21] &quot;great/JJ&quot;   &quot;big/JJ&quot;     &quot;teeth/NNS&quot;  &quot;./.&quot;        &quot;A/DT&quot;      
## [26] &quot;wolf/NN&quot;    &quot;is/VBZ&quot;     &quot;an/DT&quot;      &quot;animal/NN&quot;  &quot;./.&quot;       
## [31] &quot;Little/NNP&quot; &quot;Red/NNP&quot;    &quot;Riding/VBG&quot; &quot;Hood/NNP&quot;   &quot;met/VBD&quot;   
## [36] &quot;a/DT&quot;       &quot;Wolf/NNP&quot;   &quot;./.&quot;</code></pre>

<p>The output shows that the token <strong>big</strong> maps to the <strong>JJ</strong> tag, and the token <strong>animal</strong> maps to <strong>NN</strong>.</p>
<p>Note that the tag used by <strong>openNLP</strong>, such as <strong>JJ</strong> for adjectives, is different
from the tag used by <strong>treetag</strong>, e.g., <strong>AJ0</strong>. There are a few differences to observe likewise.</p>
<p><strong>Chunking</strong> and <strong>Syntactic Parsing</strong>  </p>
<p><strong>Chunking</strong> is extracting and parsing texts to group terms into phrases and is often guided by the corresponding language syntax and structure. For example, a <strong>clause</strong> may represent a combination of adjective and noun tagged as ‘JJ+NN’ in that order. A <strong>phrase</strong>, on the other hand, is made up of <strong>clauses</strong>.</p>
<p>Let us show a summary of the <strong>Chunks</strong>:</p>

<div class="sourceCode" id="cb1635"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1635-1" data-line-number="1">chunk.annotator    =<span class="st"> </span><span class="kw">Maxent_Chunk_Annotator</span>(<span class="dt">probs =</span> <span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb1635-2" data-line-number="2">chunked.doc    =<span class="st"> </span><span class="kw">annotate</span>(text, chunk.annotator, pos_tagged.doc)</a>
<a class="sourceLine" id="cb1635-3" data-line-number="3"><span class="kw">head</span>(chunked.doc, <span class="dt">n=</span><span class="dv">10</span>) <span class="co"># display only first 10 lines</span></a></code></pre></div>
<pre><code>##  id type     start end features
##   1 sentence     1  34 constituents=&lt;&lt;integer,9&gt;&gt;
##   2 sentence    36  98 constituents=&lt;&lt;integer,15&gt;&gt;
##   3 sentence   100 119 constituents=&lt;&lt;integer,6&gt;&gt;
##   4 sentence   121 154 constituents=&lt;&lt;integer,8&gt;&gt;
##   5 word         1   5 POS=EX, POS_prob=0.9517, chunk_tag=B-NP,
##                        chunk_prob=0.9736
##   6 word         7   8 POS=VBZ, POS_prob=0.9993, chunk_tag=B-VP,
##                        chunk_prob=0.9909
##   7 word        10  10 POS=DT, POS_prob=0.9909, chunk_tag=B-NP,
##                        chunk_prob=0.9962
##   8 word        12  14 POS=JJ, POS_prob=0.9979, chunk_tag=I-NP,
##                        chunk_prob=0.9939
##   9 word        16  21 POS=NN, POS_prob=0.9799, chunk_tag=I-NP,
##                        chunk_prob=0.9898
##  10 word        23  24 POS=IN, POS_prob=0.9801, chunk_tag=B-PP,
##                        chunk_prob=0.992</code></pre>

<p>Note that <strong>Maxent_Chunk_Annotator(.)</strong> requires a model file for language <strong>en</strong>. This function can be installed from <strong><a href="https://datacube.wu.ac.at/" class="uri">https://datacube.wu.ac.at/</a></strong> like so:</p>
<div class="sourceCode" id="cb1637"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1637-1" data-line-number="1"><span class="kw">install.packages</span>(<span class="st">&quot;openNLPmodels.en&quot;</span>,   </a>
<a class="sourceLine" id="cb1637-2" data-line-number="2">                 <span class="dt">repos =</span> <span class="st">&quot;http://datacube.wu.ac.at/&quot;</span>,  <span class="dt">type =</span> <span class="st">&quot;source&quot;</span>) </a></code></pre></div>
<p>We can begin parsing the text using the annotated chunks and create a parsed tree. However, it is worth mentioning that parsing is an expensive operation and may require CPU and memory resources. For that reason, specifically for the parser used in <strong>openNLP</strong>, we adjust the memory requirement - being so, the parser uses java virtual machine (JVM). From there, we can invoke the annotator.</p>

<div class="sourceCode" id="cb1638"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1638-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">java.parameters =</span> <span class="st">&quot;-Xmx4096m&quot;</span>)</a>
<a class="sourceLine" id="cb1638-2" data-line-number="2">parse.annotator =<span class="st"> </span><span class="kw">Parse_Annotator</span>()</a></code></pre></div>

<p>Then we can annotate. Here, we used the annotated chunk texts.</p>

<div class="sourceCode" id="cb1639"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1639-1" data-line-number="1">parsed.doc   =<span class="st"> </span><span class="kw">parse.annotator</span>(text, chunked.doc)</a>
<a class="sourceLine" id="cb1639-2" data-line-number="2">parsed.texts =<span class="st"> </span><span class="kw">sapply</span>(parsed.doc<span class="op">$</span>features, <span class="st">`</span><span class="dt">[[</span><span class="st">`</span>, <span class="st">&quot;parse&quot;</span>)</a>
<a class="sourceLine" id="cb1639-3" data-line-number="3">parsed.tree  =<span class="st"> </span><span class="kw">lapply</span>(parsed.texts , Tree_parse)</a></code></pre></div>

<p>The parsed tree of the 1st document has the text format below:</p>

<div class="sourceCode" id="cb1640"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1640-1" data-line-number="1">parsed.tree[[<span class="dv">1</span>]]</a></code></pre></div>
<pre><code>## (TOP
##   (S
##     (NP (EX There))
##     (VP
##       (VBZ is)
##       (NP
##         (NP (DT a) (JJ big) (NN animal))
##         (PP (IN in) (NP (PRP$ my) (NN house)))))
##     (. .)))</code></pre>

<p>The equivalent graph form is shown in Figure <a href="machinelearning3.html#fig:parsedtree1">11.9</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:parsedtree1"></span>
<img src="parsedtree1.png" alt="Parsed Tree 1" width="60%" />
<p class="caption">
Figure 11.9: Parsed Tree 1
</p>
</div>
<p>Similarly, the parsed tree of the 4th document has the text format below:</p>

<div class="sourceCode" id="cb1642"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1642-1" data-line-number="1">parsed.tree[[<span class="dv">4</span>]]</a></code></pre></div>
<pre><code>## (TOP
##   (S
##     (NP (NNP Little) (NNP Red) (VBG Riding) (NNP Hood))
##     (VP (VBD met) (NP (DT a) (NNP Wolf)))
##     (. .)))</code></pre>

<p>The equivalent graph form is shown in Figure <a href="machinelearning3.html#fig:parsedtree4">11.10</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:parsedtree4"></span>
<img src="parsedtree4.png" alt="Parsed Tree 4" width="60%" />
<p class="caption">
Figure 11.10: Parsed Tree 4
</p>
</div>
</div>
<div id="semantic-analysis" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.6</span> Semantic Analysis <a href="machinelearning3.html#semantic-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In a sense, <strong>Semantics Analysis</strong> reminds us of <strong>Clustering</strong>, which depends on the similarities of members in a cluster and the grouping of members in their respective clusters. In <strong>Semantic Analysis</strong>, we are also interested in determining the relationship of members within and between clusters. In the context of <strong>NLP</strong>, clusters may represent groups of words (possibly the syntactic structures - e.g., parsed tree - as we have shown in the previous section) and their relationship to the documents. Let us start with one of the first popular classic methods called <strong>Latent Semantic Analysis (LSA)</strong>.</p>
<p>Note that we do not <strong>Semantic Relationships</strong> in this book. We leave readers to investigate the types of <strong>Semantic Relationships</strong>, such as <strong>Synonymy</strong>, <strong>Antonymy</strong>, <strong>Homonymy</strong>, and <strong>Metonymy</strong>, which may be covered in the study of <strong>Linguistics</strong>. Such <strong>relationships</strong> may require knowledge-based inputs as references for training our models. We may present here basic relationship measures in comparing similarities based on weighted words (or chunks) in documents.</p>
<p><strong>Latent Semantic Analysis (LSA)</strong>  </p>
<p><strong>LSA</strong> is an NLP technique formulated by Susan T. Dumais et al. <span class="citation">(<a href="bibliography.html#ref-ref867d">1988</a>)</span> for semantic analysis. It is dependent upon terms found in documents. With <strong>LSA</strong>, our goal is to determine the relationship between documents. To achieve our goal, let us recall a <strong>Matrix Decomposition</strong> method in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>) called <strong>Singular Value Decomposition (SVD)</strong>.  </p>
<p><span class="math display">\[\begin{align}
\mathbf{M}_{(\text{n x m})} = \mathbf{U}_{(\text{n x p})}\mathbf{\Sigma}_{(\text{p x p})}\mathbf{V}^{\text{T}}_{(\text{p x m})}
\ \ \ \ \ \
where\ \mathbf{\Sigma}\ \text{is diagonal matrix}
\end{align}\]</span></p>
<p>The formula can be interpreted based on Figure <a href="machinelearning3.html#fig:lsamatrix">11.11</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lsamatrix"></span>
<img src="lsa_matrix.png" alt="Latent Semantic Analysis" width="100%" />
<p class="caption">
Figure 11.11: Latent Semantic Analysis
</p>
</div>
<p>Previously, we are able to derive the <strong>DTM</strong> and <strong>TDM</strong> representations of our relevant terms. Here, both our <strong>DTM</strong> and <strong>TDM</strong> contain <strong>TF-IDF</strong> weights instead of basic <strong>Term Frequency</strong>.</p>

<div class="sourceCode" id="cb1644"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1644-1" data-line-number="1">dtm.doc =<span class="st"> </span>tfidf =<span class="st"> </span><span class="kw">dfm_tfidf</span>(doc.dfm, <span class="dt">scheme_tf =</span> <span class="st">&quot;prop&quot;</span>, <span class="dt">base=</span><span class="kw">exp</span>(<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb1644-2" data-line-number="2">                            <span class="dt">force=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1644-3" data-line-number="3">tdm.doc =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(dtm.doc)))</a>
<a class="sourceLine" id="cb1644-4" data-line-number="4">tdm.doc</a></code></pre></div>
<pre><code>##             D1      D2     D3     D4
## big    0.17329 0.20794 0.0000 0.0000
## animal 0.07192 0.02877 0.1438 0.0000
## my     0.34657 0.00000 0.0000 0.0000
## house  0.34657 0.00000 0.0000 0.0000
## great  0.00000 0.41589 0.0000 0.0000
## ears   0.00000 0.13863 0.0000 0.0000
## eyes   0.00000 0.13863 0.0000 0.0000
## teeth  0.00000 0.13863 0.0000 0.0000
## wolf   0.00000 0.00000 0.3466 0.1155
## little 0.00000 0.00000 0.0000 0.2310
## red    0.00000 0.00000 0.0000 0.2310
## riding 0.00000 0.00000 0.0000 0.2310
## hood   0.00000 0.00000 0.0000 0.2310
## met    0.00000 0.00000 0.0000 0.2310</code></pre>

<p>Following <strong>SVD</strong>, we should be able to decompose <strong>TDM</strong> and obtain the <strong>Topic-To-Document</strong> matrix.</p>

<div class="sourceCode" id="cb1646"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1646-1" data-line-number="1">(<span class="dt">M =</span> <span class="kw">svd</span>(tdm.doc))</a></code></pre></div>
<pre><code>## $d
## [1] 0.5604 0.5391 0.4869 0.3598
## 
## $u
##          [,1]     [,2]     [,3]     [,4]
##  [1,] 0.47681 -0.05787  0.05537 -0.03755
##  [2,] 0.14739  0.05267 -0.07076  0.37164
##  [3,] 0.43794 -0.04288 -0.49816 -0.06325
##  [4,] 0.43794 -0.04288 -0.49816 -0.06325
##  [5,] 0.51567 -0.07286  0.60890 -0.01185
##  [6,] 0.17189 -0.02429  0.20297 -0.00395
##  [7,] 0.17189 -0.02429  0.20297 -0.00395
##  [8,] 0.17189 -0.02429  0.20297 -0.00395
##  [9,] 0.07005  0.36649 -0.01581  0.84684
## [10,] 0.03971  0.41202  0.01415 -0.16440
## [11,] 0.03971  0.41202  0.01415 -0.16440
## [12,] 0.03971  0.41202  0.01415 -0.16440
## [13,] 0.03971  0.41202  0.01415 -0.16440
## [14,] 0.03971  0.41202  0.01415 -0.16440
## 
## $v
##         [,1]     [,2]     [,3]     [,4]
## [1,] 0.70809 -0.06671 -0.69989 -0.06566
## [2,] 0.69480 -0.09445  0.71290 -0.01025
## [3,] 0.08116  0.24965 -0.03216  0.96439
## [4,] 0.09632  0.96141  0.02982 -0.25599</code></pre>

<p>We reduce the decomposed matrices such that we only consider the first two columns:</p>

<div class="sourceCode" id="cb1648"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1648-1" data-line-number="1">reduced.dtm =<span class="st"> </span>M<span class="op">$</span>u[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span> ] <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(M<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span> ]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(M<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span> ])</a>
<a class="sourceLine" id="cb1648-2" data-line-number="2"><span class="kw">rownames</span>(reduced.dtm)  =<span class="st"> </span><span class="kw">rownames</span>(tdm.doc)</a>
<a class="sourceLine" id="cb1648-3" data-line-number="3"><span class="kw">colnames</span>(reduced.dtm)  =<span class="st"> </span><span class="kw">colnames</span>(tdm.doc)</a>
<a class="sourceLine" id="cb1648-4" data-line-number="4">reduced.dtm</a></code></pre></div>
<pre><code>##               D1        D2       D3        D4
## big    0.1912700  0.188585 0.013895 -0.004262
## animal 0.0565857  0.054700 0.013792  0.035253
## my     0.1753109  0.172691 0.014145  0.001409
## house  0.1753109  0.172691 0.014145  0.001409
## great  0.2072291  0.204479 0.013645 -0.009933
## ears   0.0690764  0.068160 0.004548 -0.003311
## eyes   0.0690764  0.068160 0.004548 -0.003311
## teeth  0.0690764  0.068160 0.004548 -0.003311
## wolf   0.0146147  0.008612 0.052513  0.193743
## little 0.0009392 -0.005518 0.057261  0.215703
## red    0.0009392 -0.005518 0.057261  0.215703
## riding 0.0009392 -0.005518 0.057261  0.215703
## hood   0.0009392 -0.005518 0.057261  0.215703
## met    0.0009392 -0.005518 0.057261  0.215703</code></pre>

<p>With the new coefficients reflected in our <strong>TDM</strong> obtained through <strong>SVD</strong>, let us compute for <strong>cosine similarity</strong> to see any effect. Here, we use a 3rd-party library called <strong>lsa</strong> for our similarity calculation:</p>

<div class="sourceCode" id="cb1650"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1650-1" data-line-number="1"><span class="kw">library</span>(lsa)</a>
<a class="sourceLine" id="cb1650-2" data-line-number="2">original.dtm =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">dfm_weight</span>(doc.dfm) )</a>
<a class="sourceLine" id="cb1650-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;original&quot;</span> =<span class="st"> </span>lsa<span class="op">::</span><span class="kw">cosine</span>(original.dtm [<span class="dv">1</span>,], original.dtm [<span class="dv">2</span>,]),</a>
<a class="sourceLine" id="cb1650-4" data-line-number="4">  <span class="st">&quot;new&quot;</span> =<span class="st"> </span>lsa<span class="op">::</span><span class="kw">cosine</span>(reduced.dtm[<span class="dv">1</span>,], reduced.dtm[<span class="dv">2</span>,]))</a></code></pre></div>
<pre><code>## original      new 
##   0.4264   0.9016</code></pre>

<p>Notice that with <strong>LSA</strong>, our two documents (D1 and D2) are similar. Hence, there is a 90.16% relationship between the two documents. However, this latent relationship is presented only as a percentage - the interpretation of such a relationship is unknown. Therefore, let us review <strong>pLSA</strong> to see how the interpretation is considered.</p>
<p><strong>Probabilistic Latent Semantic Analysis (pLSA)</strong>  </p>
<p><strong>pLSA</strong> is formulated by Thomas Hofmann <span class="citation">(<a href="bibliography.html#ref-ref220t">1999</a>)</span>, and it introduces the idea of an <strong>Aspect model</strong> (See Figure <a href="machinelearning3.html#fig:aspectmodel">11.12</a> adapted from David M. Blei <span class="citation">(<a href="bibliography.html#ref-ref866b">2012</a>)</span>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:aspectmodel"></span>
<img src="aspectmodel.png" alt="pLSA (Aspect Model)" width="100%" />
<p class="caption">
Figure 11.12: pLSA (Aspect Model)
</p>
</div>
<p>The model can also be represented in the form of a <strong>Probabilistic Graphical Model (PGM)</strong> (see Figure <a href="machinelearning3.html#fig:plsapgm">11.13</a>), particularly formulation 1.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:plsapgm"></span>
<img src="plsapgm.png" alt="pLSA (PGM)" width="80%" />
<p class="caption">
Figure 11.13: pLSA (PGM)
</p>
</div>
<p>If we are to follow the <strong>PGM</strong> figure, we have two random variables denoted by <strong>Z</strong> and <strong>W</strong>. The former follows a multinomial distribution denoted by <span class="math inline">\(\theta_m\)</span>, and the latter follows a second multinomial distribution denoted by <span class="math inline">\(\phi_k\)</span>. Hence, we can express them as such:</p>
<p><span class="math display">\[
Z_{mn} \sim Multinomial(\theta_m)\ \ \ \ \ \ \ \ \ \ \ W_{mn} \sim Multinomial(\phi_k)
\]</span></p>
<p>Our goal is to be able to generate the topic proportionality (<span class="math inline">\(\theta_m\)</span> ) per-document and word distribution (<span class="math inline">\(\phi_k\)</span>) per-topic (see Figure <a href="machinelearning3.html#fig:plsadisrib">11.14</a>) based on our given <strong>Corpus</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:plsadisrib"></span>
<img src="plsadistrib.png" alt="pLSA (Multinomial Distribution)" width="80%" />
<p class="caption">
Figure 11.14: pLSA (Multinomial Distribution)
</p>
</div>
<p>We first determine the probability of seeing a document and a word appearing together to achieve our goal. We can fashion this using a joint distribution but with two similar formulations:</p>
<p>The first formulation allows us to determine the likelihood of seeing a document in which we can also find a word based on the distribution of topics.</p>
<p><span class="math display">\[\begin{align}
P(d, w) = P(d)\sum_{z \in Z} P(w|z)P(z|d)
\end{align}\]</span></p>
<p>The second formulation follows the <strong>LSA</strong> SVD-based matrix model such that we have the following:</p>
<p><span class="math display">\[\begin{align}
P(d, w) = \sum_z \underbrace{P(z)}_{\mathbf{\text{U}}}
    \underbrace{P(d|z)}_{\mathbf{\text{S}}}\underbrace{P(w|z)}_{\mathbf{\text{V}}^T}
\end{align}\]</span></p>
<p>Determining the probabilities or their corresponding parameters needs to be done heuristically. The most common method uses <strong>Expectation-Maximization (EM)</strong>; albeit, for <strong>LDA</strong> in the next section, we can use <strong>Gibbs Sampling</strong> due to the <strong>bayesian a-priori</strong> hyperparameters introduced.</p>
<p>Let us consider <strong>EM</strong> based on the two formulations for <strong>pLSA</strong>. See below <span class="citation">(Hong L. <a href="bibliography.html#ref-ref929l">2012</a>)</span> - derivation is not included:</p>
<p><span class="math display">\[
\begin{array}{l}
\mathbf{\text{Formulation 1:}}\\
\ \ \ \ P(d, w) = P(d)\sum_z P(w|z)P(z|d)\\
\mathbf{\text{E-Step:}}\\
P(z|w,d) = \frac{P(w|z)P(z|d)}
              {\sum_z P(w|z)P(z|d)}\\
\mathbf{\text{M-Step:}}\\
P(d) = \frac{n(d)}{\sum_d n(d)}\\
P(w|z) = \frac{\sum_d n(d,w) P(z|w,d)}
             {\sum_w \sum_d n(d,w) P(z|w,d)} \\
P(z|d) = \frac{\sum_w n(d,w)P(z|w,d)}{n(d)}
\\  \\ \\
\end{array}
\left|
\begin{array}{l}
\mathbf{\text{Formulation 2:}}\\
\ \ \ \ P(d, w) = \sum_zP(z) P(d|z)P(w|z)\\
\mathbf{\text{E-Step:}}\\
P(z|w,d) = \frac{P(w,z,d)}{P(w,d)} \\
\ \ \ \ \ \ \ \ \ \ \ \  =\frac{P(w|z)P(d|z)P(z)}
            {\sum_z P(w|z)P(d|z)P(z)}
\\
\mathbf{\text{M-Step:}}\\
P(z) = \frac{\sum_d \sum_w n(d,w)P(z|w,d)}
          {\sum_d \sum_w n(d,w)}\\
P(w|z) = \frac{\sum_d  n(d,w)P(z|w,d)}
          {\sum_w \sum_d \sum_w n(d,w)}\\
P(d|z) = \frac{\sum_w  n(d,w)P(z|w,d)}
          {\sum_d \sum_w \sum_w n(d,w)}\\
\end{array}
\right.
\]</span></p>
<p>We perform iterative calculations and updates to maximize the log-likelihood. For the first formulation, we use the following log-likelihood:</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}_1 = \sum_d \sum_w n(d, w) \log_e \left[P(d) \sum_z P(w|z) P(z|d)\right]
\end{align}\]</span></p>
<p>For the second formulation, we use the following log-likelihood:</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}_2 = \sum_d \sum_w n(d, w) \log_e \left[ \sum_zP(z)  P(d|z)P(w|z)\right]
\end{align}\]</span></p>
<p>Instead of giving an example to illustrate <strong>pLSA</strong>, let us rather extend our discussion by introducing <strong>Latent Dirichlet Allocation (LDA)</strong> which is an enhanced variant of <strong>pLSA</strong> in that it uses <strong>Naive Bayes</strong> for which both <span class="math inline">\(\theta_m\)</span> and <span class="math inline">\(\phi_k\)</span> follow a <strong>Dirichlet</strong> distribution using two corresponding <strong>Prior</strong> hyperparameters, namely <strong>alpha</strong> (<span class="math inline">\(\alpha\)</span>) and <strong>beta</strong> (<span class="math inline">\(\beta\)</span>).</p>
<p><strong>Latent Dirichlet Allocation (LDA)</strong>  </p>
<p><strong>LDA</strong> is a popular algorithm commonly used in <strong>topic modeling</strong> formulated by David M. Blei, Andrew Y. Ng, and Michael I. Jordan <span class="citation">(<a href="bibliography.html#ref-ref209d">2003</a>)</span>. As with <strong>pLSA</strong>, the grand idea is to <strong>classify</strong> a bag of words gathered from a collection of documents called <strong>corpus</strong> into a set of arbitrarily chosen topics. </p>
<p>Let us first review the <strong>PGM</strong> for <strong>LDA</strong> (see Figure <a href="machinelearning3.html#fig:ldapgm">11.15</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ldapgm"></span>
<img src="ldapgm.png" alt="LDA (PGM)" width="90%" />
<p class="caption">
Figure 11.15: LDA (PGM)
</p>
</div>
<p>The two <strong>Dirichlet</strong> parameters are used to provide the initial multinomial distributions, e.g. sampling a <strong>Dirichlet distribution</strong> as <strong>priors</strong> for <span class="math inline">\(\theta_m\)</span> and <span class="math inline">\(\phi_k\)</span>.</p>
<p><span class="math display">\[\begin{align}
\theta_m \sim Dir(\alpha)\ \ \ \ \ \ \ \ \ \ \
\phi_k \sim Dir(\beta)
\end{align}\]</span></p>
<p>To illustrate, let us use <strong>icml-nips-iclr-dataset</strong> dataset sourced from <strong>github/cqql</strong> - a note that all contents of the datasets comes from ICML, NIPS, and ICLR 2016-2018. For our dataset, we are only interested in the <strong>title</strong> for papers published in 2018. Assume for a moment that each title represents one document.</p>

<div class="sourceCode" id="cb1652"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1652-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb1652-2" data-line-number="2">icml.nips.iclr =<span class="st"> </span><span class="kw">read.csv</span>(<span class="dt">file =</span> <span class="st">&quot;~/Documents/nlp/papers.csv&quot;</span>, </a>
<a class="sourceLine" id="cb1652-3" data-line-number="3">                          <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1652-4" data-line-number="4">yr.idx       =<span class="st"> </span><span class="kw">which</span>(icml.nips.iclr[,<span class="kw">c</span>(<span class="st">&quot;Year&quot;</span>)] <span class="op">==</span><span class="st"> &quot;2018&quot;</span>)</a>
<a class="sourceLine" id="cb1652-5" data-line-number="5">doc.titles  =<span class="st"> </span><span class="kw">unique</span>(icml.nips.iclr[yr.idx,<span class="kw">c</span>(<span class="st">&quot;Title&quot;</span>)])</a>
<a class="sourceLine" id="cb1652-6" data-line-number="6"><span class="kw">strwrap</span>(<span class="kw">head</span>(doc.titles), <span class="dt">width=</span><span class="dv">70</span>, <span class="dt">exdent=</span><span class="dv">5</span>)</a></code></pre></div>
<pre><code>## [1] &quot;Spline Filters For End-to-End Deep Learning&quot;                          
## [2] &quot;Non-linear motor control by local learning in spiking neural networks&quot;
## [3] &quot;Implicit Quantile Networks for Distributional Reinforcement Learning&quot; 
## [4] &quot;An Inference-Based Policy Gradient Method for Learning Options&quot;       
## [5] &quot;Predict and Constrain: Modeling Cardinality in Deep Structured&quot;       
## [6] &quot;     Prediction&quot;                                                      
## [7] &quot;Differentially Private Matrix Completion Revisited&quot;</code></pre>

<p>In this illustration, let us use <strong>tm</strong> to perform the pre-processing, ignoring stemming for a moment.</p>

<div class="sourceCode" id="cb1654"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1654-1" data-line-number="1"><span class="kw">library</span>(tm)</a>
<a class="sourceLine" id="cb1654-2" data-line-number="2">no.docs          =<span class="st"> </span><span class="kw">length</span>(doc.titles)</a>
<a class="sourceLine" id="cb1654-3" data-line-number="3">doc_ids          =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;D&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>, no.docs))</a>
<a class="sourceLine" id="cb1654-4" data-line-number="4">doc.dfm          =<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">doc_id =</span> doc_ids, <span class="dt">text =</span> doc.titles)</a>
<a class="sourceLine" id="cb1654-5" data-line-number="5">doc.dfm.source   =<span class="st"> </span><span class="kw">DataframeSource</span>(doc.dfm)</a>
<a class="sourceLine" id="cb1654-6" data-line-number="6">doc.corpus       =<span class="st"> </span><span class="kw">Corpus</span>(doc.dfm.source)</a>
<a class="sourceLine" id="cb1654-7" data-line-number="7">preprocessed.doc =<span class="st"> </span><span class="kw">tm_map</span>(doc.corpus, <span class="kw">content_transformer</span>(tolower))</a>
<a class="sourceLine" id="cb1654-8" data-line-number="8">preprocessed.doc =<span class="st"> </span><span class="kw">tm_map</span>(preprocessed.doc, removePunctuation, </a>
<a class="sourceLine" id="cb1654-9" data-line-number="9">                          <span class="dt">preserve_intra_word_dashes =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1654-10" data-line-number="10">preprocessed.doc =<span class="st"> </span><span class="kw">tm_map</span>(preprocessed.doc, removeNumbers)</a>
<a class="sourceLine" id="cb1654-11" data-line-number="11">preprocessed.doc =<span class="st"> </span><span class="kw">tm_map</span>(preprocessed.doc, removeWords, <span class="kw">stopwords</span>(<span class="st">&quot;english&quot;</span>))</a>
<a class="sourceLine" id="cb1654-12" data-line-number="12"><span class="co">#preprocessed.doc = tm_map(preprocessed.doc, stemDocument, language = &quot;en&quot;)</span></a>
<a class="sourceLine" id="cb1654-13" data-line-number="13">preprocessed.doc =<span class="st"> </span><span class="kw">tm_map</span>(preprocessed.doc, stripWhitespace)</a>
<a class="sourceLine" id="cb1654-14" data-line-number="14">preprocessed.doc</a></code></pre></div>
<pre><code>## &lt;&lt;SimpleCorpus&gt;&gt;
## Metadata:  corpus specific: 1, document level (indexed): 0
## Content:  documents: 1966</code></pre>

<p>Now, let us introduce the use of the <strong>topicmodels</strong> package to help us with <strong>LDA</strong>. In this exercise, we limit our solution to only four topics over words with a minimum frequency of 1. From there, we cast the pre-processed document into <strong>DTM</strong> format.</p>

<div class="sourceCode" id="cb1656"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1656-1" data-line-number="1"><span class="kw">library</span>(topicmodels)</a>
<a class="sourceLine" id="cb1656-2" data-line-number="2">min.Freq =<span class="st"> </span><span class="dv">1</span>; K =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb1656-3" data-line-number="3">DTM =<span class="st"> </span><span class="kw">DocumentTermMatrix</span>(preprocessed.doc, </a>
<a class="sourceLine" id="cb1656-4" data-line-number="4">      <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">bounds =</span> <span class="kw">list</span>(<span class="dt">global =</span> <span class="kw">c</span>(min.Freq, <span class="ot">Inf</span>))))</a></code></pre></div>

<p>We can use a couple of methods to meet our goals, as pointed out in the <strong>pLSA</strong> section. We can use <strong>Variational EM (VEM)</strong> or <strong>Gibbs Sampling</strong>.  </p>
<p>For <strong>VEM</strong>, recall <strong>Variational Inference</strong> in Chapter <strong>7</strong> (<strong>Bayesian Computation I</strong>) under <strong>Bayesian Inference</strong> Section. Using <strong>LDA(.)</strong> function, we issue a set of controls, including tolerance levels for convergence and maximum iteration, with the seed being set.</p>

<div class="sourceCode" id="cb1657"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1657-1" data-line-number="1">tol =<span class="st"> </span><span class="fl">1e-3</span></a>
<a class="sourceLine" id="cb1657-2" data-line-number="2">em.control =<span class="st"> </span>var.control =<span class="st"> </span><span class="kw">list</span>(<span class="dt">iter.max =</span> <span class="dv">500</span>,  <span class="dt">tol =</span> tol)</a>
<a class="sourceLine" id="cb1657-3" data-line-number="3">lda.vem.control =<span class="st"> </span><span class="kw">list</span>(  <span class="dt">nstart =</span> <span class="dv">1</span>, <span class="dt">seed =</span> <span class="dv">2018</span>,  <span class="dt">verbose=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb1657-4" data-line-number="4">             <span class="dt">var  =</span> var.control,  <span class="dt">em   =</span> em.control, <span class="dt">initialize =</span> <span class="st">&quot;random&quot;</span>)</a>
<a class="sourceLine" id="cb1657-5" data-line-number="5">topic.model =<span class="st"> </span><span class="kw">LDA</span>(DTM, K, <span class="dt">method=</span><span class="st">&quot;VEM&quot;</span>, <span class="dt">control=</span>lda.vem.control)</a></code></pre></div>
<pre><code>## **** em iteration 1 ****
## document 1966
## new alpha = 13.80105
## **** em iteration 2 ****
## document 1966
## new alpha = 15.10332
## **** em iteration 3 ****
## document 1966
## new alpha = 16.39318
## final e step document 1966</code></pre>

<p>Let us output the list of latent topics generated using the <strong>VEM</strong> method, restricting to only the top 15 words per topic.</p>

<div class="sourceCode" id="cb1659"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1659-1" data-line-number="1">(<span class="dt">topic.matrix =</span> <span class="kw">terms</span>(topic.model, <span class="dv">15</span>))</a></code></pre></div>
<pre><code>##       Topic 1        Topic 2         Topic 3        
##  [1,] &quot;learning&quot;     &quot;learning&quot;      &quot;learning&quot;     
##  [2,] &quot;neural&quot;       &quot;deep&quot;          &quot;neural&quot;       
##  [3,] &quot;optimization&quot; &quot;networks&quot;      &quot;deep&quot;         
##  [4,] &quot;data&quot;         &quot;optimization&quot;  &quot;variational&quot;  
##  [5,] &quot;stochastic&quot;   &quot;generative&quot;    &quot;adversarial&quot;  
##  [6,] &quot;efficient&quot;    &quot;reinforcement&quot; &quot;networks&quot;     
##  [7,] &quot;networks&quot;     &quot;via&quot;           &quot;models&quot;       
##  [8,] &quot;inference&quot;    &quot;adversarial&quot;   &quot;recurrent&quot;    
##  [9,] &quot;network&quot;      &quot;bayesian&quot;      &quot;via&quot;          
## [10,] &quot;via&quot;          &quot;training&quot;      &quot;reinforcement&quot;
## [11,] &quot;descent&quot;      &quot;models&quot;        &quot;training&quot;     
## [12,] &quot;prediction&quot;   &quot;stochastic&quot;    &quot;sparse&quot;       
## [13,] &quot;unsupervised&quot; &quot;inference&quot;     &quot;generative&quot;   
## [14,] &quot;models&quot;       &quot;model&quot;         &quot;linear&quot;       
## [15,] &quot;machine&quot;      &quot;gradient&quot;      &quot;using&quot;        
##       Topic 4        
##  [1,] &quot;networks&quot;     
##  [2,] &quot;neural&quot;       
##  [3,] &quot;using&quot;        
##  [4,] &quot;gradient&quot;     
##  [5,] &quot;models&quot;       
##  [6,] &quot;via&quot;          
##  [7,] &quot;learning&quot;     
##  [8,] &quot;reinforcement&quot;
##  [9,] &quot;adversarial&quot;  
## [10,] &quot;data&quot;         
## [11,] &quot;model&quot;        
## [12,] &quot;estimation&quot;   
## [13,] &quot;bayesian&quot;     
## [14,] &quot;deep&quot;         
## [15,] &quot;fast&quot;</code></pre>

<p>For the <strong>Gibbs Sampling</strong> method, recall the <strong>Simulation and Sampling</strong> methods in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>). Using <strong>LDA(.)</strong>, we introduce <strong>Burn-In</strong> and <strong>Thinning</strong>. Note that we use <strong>Gibbs sampling</strong> with 600 sampling iterations - including <strong>Burn-in</strong> - but for simplicity, we omit <strong>Thinning</strong> for now.</p>

<div class="sourceCode" id="cb1661"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1661-1" data-line-number="1"><span class="kw">library</span>(topicmodels)</a>
<a class="sourceLine" id="cb1661-2" data-line-number="2">lda.gibbs.control =<span class="st"> </span><span class="kw">list</span>( <span class="dt">burnin =</span> <span class="dv">100</span>,  <span class="dt">iter=</span><span class="dv">500</span>, <span class="dt">nstart=</span><span class="dv">1</span>,  </a>
<a class="sourceLine" id="cb1661-3" data-line-number="3">                          <span class="dt">verbose=</span><span class="dv">100</span>,  <span class="dt">seed=</span><span class="dv">2018</span>)</a>
<a class="sourceLine" id="cb1661-4" data-line-number="4">topic.model =<span class="st"> </span><span class="kw">LDA</span>(DTM, K, <span class="dt">method=</span><span class="st">&quot;Gibbs&quot;</span>, <span class="dt">control=</span>lda.gibbs.control)</a></code></pre></div>
<pre><code>## K = 4; V = 3152; M = 1966
## Sampling 600 iterations!
## Iteration 100 ...
## Iteration 200 ...
## Iteration 300 ...
## Iteration 400 ...
## Iteration 500 ...
## Iteration 600 ...
## Gibbs sampling completed!</code></pre>

<p>Let us output the list of latent topics generated using the <strong>Gibbs Sampling</strong> method, restricting to only the top 15 words per topic.</p>

<div class="sourceCode" id="cb1663"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1663-1" data-line-number="1">(<span class="dt">topic.matrix =</span> <span class="kw">terms</span>(topic.model, <span class="dv">15</span>))</a></code></pre></div>
<pre><code>##       Topic 1          Topic 2       Topic 3       
##  [1,] &quot;networks&quot;       &quot;models&quot;      &quot;stochastic&quot;  
##  [2,] &quot;neural&quot;         &quot;adversarial&quot; &quot;via&quot;         
##  [3,] &quot;optimization&quot;   &quot;inference&quot;   &quot;using&quot;       
##  [4,] &quot;bayesian&quot;       &quot;generative&quot;  &quot;data&quot;        
##  [5,] &quot;training&quot;       &quot;variational&quot; &quot;gradient&quot;    
##  [6,] &quot;network&quot;        &quot;descent&quot;     &quot;linear&quot;      
##  [7,] &quot;convolutional&quot;  &quot;gaussian&quot;    &quot;distributed&quot; 
##  [8,] &quot;recurrent&quot;      &quot;processes&quot;   &quot;sparse&quot;      
##  [9,] &quot;gradient&quot;       &quot;machine&quot;     &quot;graph&quot;       
## [10,] &quot;policy&quot;         &quot;learning&quot;    &quot;algorithms&quot;  
## [11,] &quot;adaptive&quot;       &quot;synthesis&quot;   &quot;efficient&quot;   
## [12,] &quot;matrix&quot;         &quot;information&quot; &quot;estimation&quot;  
## [13,] &quot;efficient&quot;      &quot;clustering&quot;  &quot;unsupervised&quot;
## [14,] &quot;representation&quot; &quot;local&quot;       &quot;image&quot;       
## [15,] &quot;classification&quot; &quot;search&quot;      &quot;approach&quot;    
##       Topic 4          
##  [1,] &quot;learning&quot;       
##  [2,] &quot;deep&quot;           
##  [3,] &quot;reinforcement&quot;  
##  [4,] &quot;model&quot;          
##  [5,] &quot;structured&quot;     
##  [6,] &quot;via&quot;            
##  [7,] &quot;sampling&quot;       
##  [8,] &quot;transfer&quot;       
##  [9,] &quot;representations&quot;
## [10,] &quot;graphs&quot;         
## [11,] &quot;knowledge&quot;      
## [12,] &quot;language&quot;       
## [13,] &quot;method&quot;         
## [14,] &quot;selection&quot;      
## [15,] &quot;active&quot;</code></pre>

<p>Note that we leave the choice of methods to the readers. The success of topic modeling comes with proper pre-processing of terms and proper tune-up of each method, among many other considerations.</p>
<p>For the computed <strong>per-document topic proportion</strong>, let us display only the first ten documents and their corresponding topic distribution. Notice that the summation of topic proportion per document is always 1.</p>

<div class="sourceCode" id="cb1665"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1665-1" data-line-number="1">doc.topic =<span class="st"> </span><span class="kw">posterior</span>(topic.model)<span class="op">$</span>topics[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,]</a>
<a class="sourceLine" id="cb1665-2" data-line-number="2"><span class="kw">colnames</span>(doc.topic) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;T&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>, K))</a>
<a class="sourceLine" id="cb1665-3" data-line-number="3">doc.topic</a></code></pre></div>
<pre><code>##         T1     T2     T3     T4
## D1  0.2273 0.2818 0.2273 0.2636
## D2  0.2500 0.2500 0.2500 0.2500
## D3  0.2768 0.2232 0.2232 0.2768
## D4  0.2232 0.2232 0.2411 0.3125
## D5  0.2368 0.2193 0.2368 0.3070
## D6  0.2455 0.2818 0.2455 0.2273
## D7  0.3070 0.2368 0.2368 0.2193
## D8  0.2547 0.2547 0.2358 0.2547
## D9  0.2368 0.2368 0.2368 0.2895
## D10 0.2845 0.2328 0.2328 0.2500</code></pre>

<p>For the computed <strong>per-topic word distribution</strong>, let us display only three topics and their corresponding word distribution.</p>

<div class="sourceCode" id="cb1667"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1667-1" data-line-number="1">topic.terms =<span class="st"> </span><span class="kw">posterior</span>(topic.model)<span class="op">$</span>terms</a>
<a class="sourceLine" id="cb1667-2" data-line-number="2">ttd =<span class="st"> </span><span class="ot">NULL</span>; most =<span class="st"> </span><span class="dv">250</span>; topic.term.distribution =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1667-3" data-line-number="3"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(K<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb1667-4" data-line-number="4">  topic.term.distribution[[k]] =<span class="st">  </span><span class="kw">sort</span>(topic.terms[k, ], <span class="dt">decreasing=</span><span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb1667-5" data-line-number="5">  topics =<span class="st"> </span><span class="kw">as.data.frame</span>(topic.term.distribution[[k]][<span class="dv">1</span><span class="op">:</span>most], </a>
<a class="sourceLine" id="cb1667-6" data-line-number="6">                         <span class="dt">stringsAsFactors=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1667-7" data-line-number="7">  t.names =<span class="st"> </span><span class="kw">rownames</span>(topics)</a>
<a class="sourceLine" id="cb1667-8" data-line-number="8">  p =<span class="st"> </span><span class="kw">cbind</span>(   t.names,  <span class="kw">round</span>(topics,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb1667-9" data-line-number="9">  <span class="kw">colnames</span>(p) =<span class="st"> </span><span class="kw">c</span>(<span class="kw">paste0</span>(<span class="st">&quot;T&quot;</span>, k), <span class="kw">paste0</span>(<span class="st">&quot;Probs&quot;</span>, k))</a>
<a class="sourceLine" id="cb1667-10" data-line-number="10">  ttd =<span class="st"> </span><span class="kw">cbind</span>(ttd, <span class="kw">as.matrix</span>(p))</a>
<a class="sourceLine" id="cb1667-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb1667-12" data-line-number="12"><span class="kw">rownames</span>(ttd) =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1667-13" data-line-number="13"><span class="co"># topic.term.distribution, display 1st 15 rows</span></a>
<a class="sourceLine" id="cb1667-14" data-line-number="14">(<span class="dt">ttd =</span> <span class="kw">as.data.frame</span>(ttd, <span class="dt">stringsAsFactors=</span><span class="ot">FALSE</span>))[<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>,] </a></code></pre></div>
<pre><code>##                T1 Probs1          T2 Probs2
## 1        networks  0.073      models  0.035
## 2          neural  0.067 adversarial  0.027
## 3    optimization  0.032   inference  0.020
## 4        bayesian  0.020  generative  0.020
## 5        training  0.018 variational  0.018
## 6         network  0.016     descent  0.013
## 7   convolutional  0.013    gaussian  0.010
## 8       recurrent  0.012   processes  0.009
## 9        gradient  0.012     machine  0.008
## 10         policy  0.010    learning  0.008
## 11       adaptive  0.009   synthesis  0.007
## 12         matrix  0.009 information  0.007
## 13      efficient  0.009  clustering  0.007
## 14 representation  0.007       local  0.007
## 15 classification  0.007      search  0.007
##              T3 Probs3
## 1    stochastic  0.025
## 2           via  0.024
## 3         using  0.024
## 4          data  0.021
## 5      gradient  0.014
## 6        linear  0.013
## 7   distributed  0.013
## 8        sparse  0.012
## 9         graph  0.012
## 10   algorithms  0.012
## 11    efficient  0.010
## 12   estimation  0.010
## 13 unsupervised  0.009
## 14        image  0.009
## 15     approach  0.009</code></pre>

<p>Let us display <strong>wordcloud</strong> for the top terms in the first topic (See Figure <a href="machinelearning3.html#fig:wordcloudtopic">11.16</a>).</p>

<div class="sourceCode" id="cb1669"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1669-1" data-line-number="1"><span class="kw">library</span>(wordcloud)    <span class="co"># generates wordcloud.</span></a>
<a class="sourceLine" id="cb1669-2" data-line-number="2"><span class="kw">library</span>(RColorBrewer) <span class="co"># renders colored texts.</span></a>
<a class="sourceLine" id="cb1669-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1669-4" data-line-number="4">terms =<span class="st"> </span>ttd[,<span class="kw">c</span>(<span class="st">&quot;T1&quot;</span>)]; probabilities =<span class="st"> </span><span class="kw">as.numeric</span>(ttd[,<span class="kw">c</span>(<span class="st">&quot;Probs1&quot;</span>)])</a>
<a class="sourceLine" id="cb1669-5" data-line-number="5"><span class="kw">wordcloud</span>(<span class="dt">words =</span> terms, <span class="dt">freq =</span> probabilities,  <span class="dt">max.words=</span><span class="dv">150</span> , </a>
<a class="sourceLine" id="cb1669-6" data-line-number="6">          <span class="dt">random.order=</span><span class="ot">FALSE</span>,  <span class="dt">rot.per =</span> <span class="fl">0.35</span>, <span class="dt">min.freq =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb1669-7" data-line-number="7">          <span class="dt">colors=</span><span class="kw">brewer.pal</span>(<span class="dv">8</span>, <span class="st">&quot;Dark2&quot;</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:wordcloudtopic"></span>
<img src="DS_files/figure-html/wordcloudtopic-1.png" alt="Topic Model (WordCloud)" width="90%" />
<p class="caption">
Figure 11.16: Topic Model (WordCloud)
</p>
</div>

<p>We can readily assume that the first topic suggests a context around neural networks.</p>
<p>There are many other considerations when dealing with <strong>Topic Modeling</strong>. Such considerations cannot all be discussed in one book. However, one such important consideration is choosing a suitable number (<strong>K</strong>) of topics. There are methods proposed, and the <strong>LDA(.)</strong> function offers a select number of metrics. We leave readers to experiment on the <strong>FindtopicsNumber(.)</strong> function from the <strong>ldatuning</strong> R package for the metrics named after Griffiths 2004, CaoJuan 2009, Arun 2010, and Deveaud 2014. We also encourage exploring the <strong>Rate of Perplexity Change (RPC)</strong>, which helps to measure and cross-validate a suitable number (<strong>K</strong>) of topics to use.</p>
<p><strong>Explicit Semantic Analysis (ESA)</strong>  </p>
<p>Another <strong>Semantic Analysis</strong> method proposed is <strong>Explicit Semantic Analysis (ESA)</strong>. This method may be regarded as an enhancement to the concept of <strong>LSA</strong> and <strong>LDA</strong> in that, instead of topics being <strong>latent</strong>, it introduces the use of <strong>topics</strong> that are explicitly pre-determined. In particular, <strong>topics</strong> are generated from <strong>Wikipedia</strong> or other sources, e.g. from <strong>Encyclopedia</strong>. We leave readers to investigate <strong>ESA</strong>.</p>
</div>
<div id="named-entity-recognition-ner" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.7</span> Named Entity Recognition (NER)  <a href="machinelearning3.html#named-entity-recognition-ner" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Entity identification or extraction</strong> is an essential component of <strong>NLP</strong> and is a sub-part of text extraction. It helps to recognize <strong>Named Entities</strong> to add more context to topic modeling. A <strong>Named Entity</strong> is a real-world proper name given to objects, people, places, dates, events, organizations, and currency. For example, there is only one <strong>Mount Everest</strong> in the world. Someone’s street address is a <strong>named entity</strong>. <strong>Neil Armstrong</strong> is the first man on the moon. <strong>Bitcoin</strong> is a digital currency. Lastly, <strong>Mcdonald’s and Coca-Cola</strong> are two world-famous names.</p>
<p>Additionally, <strong>Named Entities</strong> can be categorized accordingly. For example, <strong>January 1st</strong> is categorized as <strong>Date</strong>. Likewise, <strong>New Year’s Eve</strong> is categorized as one of the dates/times.</p>
<p><strong>NER</strong> becomes apparent for applications around <strong>Customer Support</strong>, <strong>Human Resource</strong>, <strong>Health Care</strong>, and many others.</p>
</div>
<div id="sentiment-and-opinion-analysis" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.8</span> Sentiment and Opinion Analysis  <a href="machinelearning3.html#sentiment-and-opinion-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to <strong>NER</strong>, reading and recognizing sentiments and not just <strong>named entities</strong> is just as important. It helps to map text to sentiments to interpret and predict sentiments. For this, one of the common <strong>Lexicon</strong> resources that maps terms to sentiments is called <strong>SentiWordNet</strong>. There are other resources such as <strong>WordNet-Affect</strong>, <strong>MPQA</strong>, and <strong>SenticNet</strong> to compare <span class="citation">(Musto C., Semeraro G., Polignano M., <a href="bibliography.html#ref-ref940c">n.d.</a>)</span>.</p>
<p>Each of the different resources addresses the following sentimental representations:</p>
<ul>
<li><p><strong>Common Polarity</strong> - texts are represented as <strong>Positive</strong>, <strong>Negative</strong>, or <strong>Neutral</strong>. For example, the word <strong>good</strong> is positive, <strong>bad</strong> is negative, and <strong>ok</strong> may be interpreted as <strong>Neutral</strong>.</p></li>
<li><p><strong>Intensitiy</strong> - this adds granularity in the form of <strong>Strong</strong>, <strong>Medium</strong>, <strong>Weak</strong> or in the form <strong>positive</strong> and <strong>negative</strong> scores. For example <strong>very good</strong> instead of just <strong>good</strong>.</p></li>
<li><p><strong>Subjectivity</strong> - texts are represented based on human senses, emotions, or opinions. For example, the word <strong>happy</strong> has a higher positive score, and <strong>sad</strong> has a lower negative score. A text that says <strong>this art looks good</strong> has a higher positive score than one that says <strong>this art looks terrible</strong>.</p></li>
<li><p><strong>Objectivity</strong> - texts are represented based on facts. For example, <strong>Earth is round</strong>.</p></li>
</ul>
<p>To illustrate, let us use a 3rd-party library called <strong>sentimentr</strong> developed by <strong>Tyler Rinker</strong> that uses <strong>SentiWordNet</strong>.</p>
<p>We feed a sentence to the function <strong>sentiment_by(.)</strong> that suggests positive sentiment, and we get the following positive score:</p>

<div class="sourceCode" id="cb1670"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1670-1" data-line-number="1"><span class="kw">library</span>(sentimentr)</a>
<a class="sourceLine" id="cb1670-2" data-line-number="2"><span class="kw">sentiment</span>(<span class="st">&#39;I am very good&#39;</span>)</a></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          4     0.675</code></pre>

<p>We then feed a sentence that suggests negative sentiment, and we get a positive negative score.</p>

<div class="sourceCode" id="cb1672"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1672-1" data-line-number="1"><span class="kw">sentiment</span>(<span class="st">&#39;I am not very good&#39;</span>)</a></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          5  -0.06708</code></pre>

<p>Let us feed random positive words and see how the score looks like:</p>

<div class="sourceCode" id="cb1674"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1674-1" data-line-number="1">terms =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;happy&quot;</span>, <span class="st">&quot;care&quot;</span>, <span class="st">&quot;great&quot;</span>, <span class="st">&quot;light&quot;</span>, <span class="st">&quot;cheerful&quot;</span> )</a>
<a class="sourceLine" id="cb1674-2" data-line-number="2"><span class="kw">sentiment</span>(terms)</a></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          1      0.75
## 2:          2           1          1      1.00
## 3:          3           1          1      0.50
## 4:          4           1          1      0.00
## 5:          5           1          1      0.75</code></pre>

<p>Now, let us try with random negative words:</p>

<div class="sourceCode" id="cb1676"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1676-1" data-line-number="1">terms =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;sad&quot;</span>, <span class="st">&quot;careless&quot;</span>, <span class="st">&quot;worse&quot;</span>, <span class="st">&quot;dark&quot;</span>)</a>
<a class="sourceLine" id="cb1676-2" data-line-number="2"><span class="kw">sentiment</span>(terms)</a></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          1     -0.50
## 2:          2           1          1     -0.50
## 3:          3           1          1     -0.75
## 4:          4           1          1     -0.60</code></pre>

<p>In terms of application, we may find <strong>sentiment analysis</strong> to be useful as part of a <strong>Recommender System</strong>, which we discuss further in a section.</p>
</div>
</div>
<div id="time-series-forecasting" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.4</span> Time-Series Forecasting <a href="machinelearning3.html#time-series-forecasting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now switch context to discuss <strong>Time-Series Forecasting</strong> - an area that is also equally important. In this context, a probable typical case here is forecasting sales.</p>
<p>Note that most - if not all - of our previous discussions underscored static data - a snapshot of an incident. Here, we discuss data that carry changes over time - in a <strong>Time-Series</strong> fashion. Our goal is to identify patterns (e.g., Trend, Seasonality, among others) that may allow us to forecast accurately.</p>
<p>As we tackle other concepts, it is good to be familiar with terms used in <strong>Forecasting</strong>. Note that when plotting our data, some terms apply to the <strong>Time-Series</strong> behavior along the <strong>Y-axis</strong>, e.g. <strong>trend</strong>, <strong>drift</strong>, and <strong>stationary</strong> terms; while the <strong>X-axis</strong> reflects fix time interval.</p>
<ul>
<li><p><strong>Period</strong> - refers to the duration of one full cycle.</p></li>
<li><p><strong>Frequency</strong> - refers to the number of complete cycles.</p></li>
<li><p><strong>Time Lag</strong> - refers to the elapsed time between two related events.</p></li>
<li><p><strong>Trend</strong> - A series with a trend that follows an increasing or decreasing pattern. The <strong>trajectory</strong> remains constant, meaning there is no shift in the intercept.</p></li>
<li><p><strong>Drift</strong> - A series in which the <strong>trajectory</strong> shifts (affecting the intercept).</p></li>
<li><p><strong>Seasonality</strong> - A series has Seasonality if it follows a pattern that repeats, such as <strong>weekly</strong>, <strong>monthly</strong>, <strong>quarterly</strong>, and <strong>yearly</strong>.</p></li>
<li><p><strong>Stationary</strong> - A stationary series follows a pattern in which the mean (<span class="math inline">\(\mu\)</span>) and variance (<span class="math inline">\(\sigma^2\)</span>) along the <strong>Y</strong> axis are constant over time. This property may be regarded as <strong>strongly stationary</strong>. If there is an observed change in variance (<span class="math inline">\(\sigma^2\)</span>) over time while the mean (<span class="math inline">\(\mu\)</span>) remains constant, this may be regarded as <strong>weakly stationary</strong>. In the context of <strong>seasonality</strong> and <strong>trend</strong>, let us describe <strong>stationary</strong> to mean a series with no <strong>trend</strong> nor <strong>seasonality</strong> patterns.  </p></li>
<li><p><strong>Spike</strong> / <strong>Peak</strong> - follows sudden jumps - almost treated as <strong>outliers</strong>.</p></li>
<li><p><strong>Differencing</strong> - this method transforms a series from non-stationary to stationary. It also means removing trend and seasonality patterns. </p></li>
<li><p><strong>Correlation</strong> - this is a method of correlating a series between differing time lags. It can serve to identify Stationary vs non-stationary series. If the outcome suggests a non-stationary series, then the series may have a trend, a pattern of Seasonality, or even both.</p></li>
</ul>
<p>With such terms being described, a good starting point is to construct our data beginning with the idea that our data comes from some random sampling - by that, we mean data with noise.</p>

<div class="sourceCode" id="cb1678"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1678-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1678-2" data-line-number="2"><span class="co"># simulate seasonality  - 3 cycles (3 years)</span></a>
<a class="sourceLine" id="cb1678-3" data-line-number="3">(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">12</span> <span class="op">*</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>))  </a></code></pre></div>
<pre><code>##  [1] 0.45127 0.78378 0.70968 0.38174 0.63632 0.70135
##  [7] 0.64044 0.26668 0.81542 0.98299 0.02727 0.83749
## [13] 0.60324 0.56745 0.82005 0.25157 0.50549 0.86754
## [19] 0.95818 0.54570 0.13958 0.95534 0.39249 0.26849
## [25] 0.57221 0.91214 0.93429 0.88049 0.94569 0.81499
## [31] 0.03278 0.94271 0.94774 0.90209 0.55227 0.22489</code></pre>

<p>To simulate the idea of <strong>running total</strong>, let us use the function <strong>cumsum(.)</strong> to generate our <strong>cumulative sum</strong>.</p>

<div class="sourceCode" id="cb1680"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1680-1" data-line-number="1">(<span class="dt">x =</span> <span class="kw">cumsum</span>(x))</a></code></pre></div>
<pre><code>##  [1]  0.4513  1.2350  1.9447  2.3265  2.9628  3.6641
##  [7]  4.3046  4.5713  5.3867  6.3697  6.3969  7.2344
## [13]  7.8377  8.4051  9.2252  9.4767  9.9822 10.8498
## [19] 11.8080 12.3537 12.4932 13.4486 13.8411 14.1096
## [25] 14.6818 15.5939 16.5282 17.4087 18.3544 19.1694
## [31] 19.2021 20.1449 21.0926 21.9947 22.5470 22.7718</code></pre>

<p>Here, we need to use 3rd-party libraries as listed below to be able to illustrate the idea:</p>
<div class="sourceCode" id="cb1682"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1682-1" data-line-number="1"><span class="kw">library</span>(lubridate)</a>
<a class="sourceLine" id="cb1682-2" data-line-number="2"><span class="kw">library</span>(tseries)</a>
<a class="sourceLine" id="cb1682-3" data-line-number="3"><span class="kw">library</span>(forecast)</a>
<a class="sourceLine" id="cb1682-4" data-line-number="4"><span class="kw">library</span>(fpp)</a>
<a class="sourceLine" id="cb1682-5" data-line-number="5"><span class="kw">library</span>(caret)</a></code></pre></div>
<p>Our next step is to convert our <strong>Cumulative sum</strong> into a <strong>Time-Series</strong> object using the <strong>st(.)</strong> function, specifying the total frequency that leads to the accumulation.</p>

<div class="sourceCode" id="cb1683"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1683-1" data-line-number="1">(<span class="dt">TS =</span> <span class="kw">ts</span>(x, <span class="dt">frequency =</span> <span class="dv">12</span>, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">2018</span>, <span class="dv">1</span>)))</a></code></pre></div>
<pre><code>##          Jan     Feb     Mar     Apr     May     Jun
## 2018  0.4513  1.2350  1.9447  2.3265  2.9628  3.6641
## 2019  7.8377  8.4051  9.2252  9.4767  9.9822 10.8498
## 2020 14.6818 15.5939 16.5282 17.4087 18.3544 19.1694
##          Jul     Aug     Sep     Oct     Nov     Dec
## 2018  4.3046  4.5713  5.3867  6.3697  6.3969  7.2344
## 2019 11.8080 12.3537 12.4932 13.4486 13.8411 14.1096
## 2020 19.2021 20.1449 21.0926 21.9947 22.5470 22.7718</code></pre>

<p>There are two considerations when dealing with <strong>Time-Series</strong> data, among many others. One consideration is to be able to fill the gap when there is <strong>missing</strong> data. Another consideration is to perform <strong>smoothing</strong>.</p>
<p>When it comes to <strong>missing</strong> data, recall our discussion on <strong>Missingness and Imputation</strong> covered under <strong>Exploratory Data Analysis</strong> section in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>). We also discuss <strong>Kalman Filter</strong> under the <strong>Bayesian Models</strong> section in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>). We leave readers to review the mentioned sections.</p>
<p>For <strong>smoothing</strong>, e.g., <strong>interpolation</strong>, the next section introduces the use of <strong>STL</strong>.</p>
<div id="seasonal-trend-decomposition-using-loess-stl" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.1</span> Seasonal Trend Decomposition using LOESS (STL)  <a href="machinelearning3.html#seasonal-trend-decomposition-using-loess-stl" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now look into <strong>smoothing methods</strong> using <strong>STL</strong>. Given the <strong>Time-Series</strong> object we generated in the previous section, our goal is to see <strong>trends</strong> and <strong>seasonality</strong>, including performing <strong>differencing</strong> to see the delta (or lag) between periods.</p>
<p>To illustrate, let us use <strong>stl(.)</strong> function to plot not just <strong>trend</strong> and <strong>season</strong>, but also including the <strong>remainder</strong> (see Figure <a href="machinelearning3.html#fig:tsplot1">11.17</a>).</p>

<div class="sourceCode" id="cb1685"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1685-1" data-line-number="1"><span class="kw">stl</span>(TS, <span class="st">&quot;periodic&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tsplot1"></span>
<img src="DS_files/figure-html/tsplot1-1.png" alt="Time-Series Plot" width="70%" />
<p class="caption">
Figure 11.17: Time-Series Plot
</p>
</div>

<p>Notice in the figure that our data shows a constantly increasing trend. The <strong>seasonal</strong> trend also shows an increase per season — ignore the drops for a moment. If this were a sales trend and we were an investor, we would like what we are seeing.</p>
<p>Let us change our random data by introducing a range between -2 and 1. Now, let us review the plot (see Figure <a href="machinelearning3.html#fig:tsplot2">11.18</a>).</p>

<div class="sourceCode" id="cb1686"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1686-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1686-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1686-3" data-line-number="3">x =<span class="st"> </span><span class="kw">cumsum</span>(<span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">12</span> <span class="op">*</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">min=</span><span class="op">-</span><span class="dv">2</span>, <span class="dt">max=</span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1686-4" data-line-number="4">(<span class="dt">TS =</span> <span class="kw">ts</span>(x, <span class="dt">frequency =</span> <span class="dv">12</span>))</a></code></pre></div>
<pre><code>##       Jan     Feb     Mar     Apr     May     Jun     Jul     Aug
## 1 -0.6462 -0.2949 -0.1658 -1.0206 -1.1116 -1.0076 -1.0863 -2.2862
## 2 -2.4870 -2.7846 -2.3245 -3.5698 -4.0533 -3.4507 -2.5761 -2.9390
## 3 -5.9547 -5.2183 -4.4154 -3.7739 -2.9369 -2.4919 -4.3936 -3.5654
##       Sep     Oct     Nov     Dec
## 1 -1.8399 -0.8910 -2.8092 -2.2967
## 2 -4.5203 -3.6543 -4.4768 -5.6713
## 3 -2.7222 -2.0159 -2.3591 -3.6845</code></pre>
<div class="sourceCode" id="cb1688"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1688-1" data-line-number="1"><span class="kw">stl</span>(TS, <span class="st">&quot;periodic&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tsplot2"></span>
<img src="DS_files/figure-html/tsplot2-1.png" alt="Time-Series Plot" width="70%" />
<p class="caption">
Figure 11.18: Time-Series Plot
</p>
</div>

<p>By introducing a higher probability of generating negative values, we see a decreasing trend (e.g., more loss in sales). We may not prefer this sales trend.</p>
<p>Finally, let us introduce a more significant gap in range and review (see Figure <a href="machinelearning3.html#fig:tsplot3">11.19</a>).</p>

<div class="sourceCode" id="cb1689"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1689-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1689-2" data-line-number="2">x =<span class="st"> </span><span class="kw">cumsum</span>(<span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">12</span> <span class="op">*</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">min=</span><span class="op">-</span><span class="dv">10</span>, <span class="dt">max=</span><span class="dv">10</span>))</a>
<a class="sourceLine" id="cb1689-3" data-line-number="3">(<span class="dt">TS =</span> <span class="kw">ts</span>(x, <span class="dt">frequency =</span> <span class="dv">12</span>))</a></code></pre></div>
<pre><code>##        Jan      Feb      Mar      Apr      May      Jun      Jul
## 1  -0.9747   4.7009   8.8946   6.5295   9.2559  13.2829  16.0916
## 2  26.7534  28.1024  34.5035  29.5349  29.6448  36.9956  46.1592
## 3  43.6353  51.8781  60.5640  68.1737  77.0876  83.3874  74.0429
##        Aug      Sep      Oct      Nov      Dec
## 1  11.4252  17.7337  27.3934  17.9388  24.6886
## 2  47.0732  39.8648  48.9716  46.8214  42.1911
## 3  82.8971  91.8519  99.8937 100.9390  95.4367</code></pre>
<div class="sourceCode" id="cb1691"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1691-1" data-line-number="1"><span class="kw">stl</span>(TS, <span class="st">&quot;periodic&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tsplot3"></span>
<img src="DS_files/figure-html/tsplot3-1.png" alt="Time-Series Plot" width="70%" />
<p class="caption">
Figure 11.19: Time-Series Plot
</p>
</div>

<p>While the trend may look similar to the first plot, notice that the <strong>remainder</strong> section of our plot shows some more significant <strong>swings</strong> or <strong>fluctuations</strong>.</p>
<p>The three plots we reviewed all come from results using the <strong>stl(.)</strong> function. <strong>STL</strong> stands for <strong>Seasonal Trend Decomposition using LOESS</strong>. The idea behind <strong>STL</strong> is to be able to <strong>decompose</strong> our raw data using <strong>smoothing</strong> methods and <strong>translate</strong> into four meaningful insights, accounting for <strong>rolling total</strong>, <strong>trend</strong>, <strong>seasonality</strong> and <strong>remainder</strong>. Also, recall our discussion on <strong>Scatterplot Smoothing</strong>, introducing <strong>LOESS</strong> for smoothing, in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>).</p>
</div>
<div id="forecasting-models" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.2</span> Forecasting Models <a href="machinelearning3.html#forecasting-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us cover a few simple classic <strong>Forecasting</strong> methods we can use, namely:</p>
<ul>
<li><p><strong>Naïve Forecasting</strong> - this is the simplest forecasting in which the forecast is based on the previous (or final) value. For example, based on the recent data in just the previous section, the final sales for Dec 2020 is 95.4367. This value becomes the forecast for the next value for Jan of 2021. </p></li>
<li><p><strong>Seasonal Naïve Forecasting</strong> - this method follows the same as <strong>Naïve Forecasting</strong>; however, the forecast for Dec 2021 is based on the value taken from the same previous season. Therefore, the forecast for Jan 2021 will follow based on the season value from Jan 2020, which is 43.6353.</p></li>
<li><p><strong>Linear Trend Projection</strong> - this method follows the <strong>Least-Squares</strong> model in which a straight <strong>trend-line</strong> stretches in increasing or decreasing fashion fitting through data. The forecast follows the next value that falls on the line. See <strong>Least-Squares</strong> discussion in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>). </p></li>
<li><p><strong>Moving Average</strong> - this method is mainly used in trading. There are three types of moving average: </p>
<ul>
<li><p><strong>Simple Moving Average (SMA)</strong> - this provides the average of N-periods: <span class="math inline">\(SMA = \frac{1}{N}\sum_{i=1}^N A_i\)</span> where <strong>A</strong> is the average for a period.</p></li>
<li><p><strong>Weighted Moving Average(WMA)</strong> - this provides the average based on the formula: <span class="math inline">\(WMA = \frac{1}{N}\sum_{i=1}^N A_i \times W_i\)</span> where <strong>W</strong> is a given (or arbitrary) weighting factor. </p></li>
<li><p><strong>Exponential Moving Average (EMA)</strong> - this provides the average based on the formula: <span class="math inline">\(\text{EMA}^{T+1} = (\text{P} - \text{EMA}^T) \times \left(\frac{2}{N + 1}\right) + \text{EMA}^T\)</span> where <strong>P</strong> is previous EMA and <strong>P</strong> is today’s price.</p></li>
</ul></li>
</ul>
<p>The next few sections cover a couple of advanced methods dealing with <strong>Time-Series</strong> forecasting using our generated <strong>Time-Series</strong> object.</p>
</div>
<div id="time-series-linear-model-tslm" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.3</span> Time-Series Linear Model (TSLM)  <a href="machinelearning3.html#time-series-linear-model-tslm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With the given <strong>Time-Series</strong> object generated using <strong>stl(.)</strong>, let us model our forecast using linear regression. Note here that the <strong>tslm(.)</strong> function is equivalent to <strong>lm(.)</strong> for <strong>linear regression</strong> modeling in which we fit a line. Here, fitting a line considers trends and seasonality in a <strong>Time-Series</strong> fashion. Moreover, we use the <strong>forecast(.)</strong> function to forecast, determining future values.</p>
<p>There are four ways to fit a model to our data and forecast based on the model.</p>
<p><strong>Fitting TS Model with the linear trend only</strong></p>

<div class="sourceCode" id="cb1692"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1692-1" data-line-number="1">h =<span class="st"> </span><span class="dv">3</span> <span class="co"># forecast 3 periods ahead  in the horizon</span></a>
<a class="sourceLine" id="cb1692-2" data-line-number="2">tslm.model =<span class="st"> </span><span class="kw">tslm</span>(TS <span class="op">~</span><span class="st">  </span>trend)</a>
<a class="sourceLine" id="cb1692-3" data-line-number="3">(<span class="dt">forecasts1 =</span> forecast<span class="op">::</span><span class="kw">forecast</span>(tslm.model, <span class="dt">h=</span>h))</a></code></pre></div>
<pre><code>##       Point Forecast Lo 80 Hi 80 Lo 95 Hi 95
## Jan 4          93.13 82.69 103.6 76.89 109.4
## Feb 4          95.85 85.36 106.3 79.54 112.2
## Mar 4          98.57 88.03 109.1 82.18 115.0</code></pre>

<p><strong>TS Model with seasonality only</strong></p>

<div class="sourceCode" id="cb1694"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1694-1" data-line-number="1">tslm.model =<span class="st"> </span><span class="kw">tslm</span>(TS <span class="op">~</span><span class="st">  </span>season)</a>
<a class="sourceLine" id="cb1694-2" data-line-number="2">(<span class="dt">forecasts2 =</span> forecast<span class="op">::</span><span class="kw">forecast</span>(tslm.model, <span class="dt">h=</span>h))</a></code></pre></div>
<pre><code>##       Point Forecast  Lo 80 Hi 80  Lo 95 Hi 95
## Jan 4          23.14 -27.46 73.74 -56.11 102.4
## Feb 4          28.23 -22.37 78.83 -51.02 107.5
## Mar 4          34.65 -15.95 85.25 -44.59 113.9</code></pre>

<p><strong>TS Linear Model with both linear trend and seasonality</strong></p>

<div class="sourceCode" id="cb1696"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1696-1" data-line-number="1">tslm.model =<span class="st"> </span><span class="kw">tslm</span>(TS <span class="op">~</span><span class="st">  </span>trend <span class="op">+</span><span class="st"> </span>season)</a>
<a class="sourceLine" id="cb1696-2" data-line-number="2">(<span class="dt">forecasts3 =</span> forecast<span class="op">::</span><span class="kw">forecast</span>(tslm.model, <span class="dt">h=</span>h))</a></code></pre></div>
<pre><code>##       Point Forecast Lo 80 Hi 80 Lo 95 Hi 95
## Jan 4          87.54 73.85 101.2 66.08 109.0
## Feb 4          92.63 78.94 106.3 71.17 114.1
## Mar 4          99.06 85.37 112.7 77.60 120.5</code></pre>

<p><strong>TS Linear Model using fourier series, where K = max. order of fourier terms.</strong></p>

<div class="sourceCode" id="cb1698"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1698-1" data-line-number="1">tslm.model =<span class="st"> </span><span class="kw">tslm</span>(TS <span class="op">~</span><span class="st">  </span>forecast<span class="op">::</span><span class="kw">fourier</span>(TS, <span class="dt">K=</span><span class="dv">4</span>))</a>
<a class="sourceLine" id="cb1698-2" data-line-number="2">(<span class="dt">forecasts4 =</span> forecast<span class="op">::</span><span class="kw">forecast</span>(tslm.model, </a>
<a class="sourceLine" id="cb1698-3" data-line-number="3">              <span class="kw">data.frame</span>(forecast<span class="op">::</span><span class="kw">fourier</span>(TS, <span class="dt">K=</span><span class="dv">4</span>, <span class="dt">h=</span>h))))</a></code></pre></div>
<pre><code>##       Point Forecast  Lo 80 Hi 80  Lo 95  Hi 95
## Jan 4          27.10 -19.16 73.37 -45.16  99.36
## Feb 4          25.46 -20.80 71.73 -46.80  97.72
## Mar 4          35.94 -10.32 82.21 -36.32 108.20</code></pre>

<p>We then plot our forecast corresponding to each model (see Figures  and  ).</p>

<div class="sourceCode" id="cb1700"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1700-1" data-line-number="1"><span class="kw">library</span>(patchwork)</a>
<a class="sourceLine" id="cb1700-2" data-line-number="2">p1 =<span class="st"> </span><span class="kw">autoplot</span>(forecasts1, <span class="dt">ylab=</span><span class="st">&quot;Forecast&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Trend&quot;</span>) </a>
<a class="sourceLine" id="cb1700-3" data-line-number="3">p2 =<span class="st"> </span><span class="kw">autoplot</span>(forecasts2, <span class="dt">ylab=</span><span class="st">&quot;Forecast&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Season&quot;</span>)</a>
<a class="sourceLine" id="cb1700-4" data-line-number="4">p1 <span class="op">/</span><span class="st"> </span>p2  <span class="co"># display two plots vertically</span></a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tslmplot1"></span>
<img src="DS_files/figure-html/tslmplot1-1.png" alt="Forecast (using STLM)" width="70%" />
<p class="caption">
Figure 11.20: Forecast (using STLM)
</p>
</div>
<div class="sourceCode" id="cb1701"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1701-1" data-line-number="1">p3 =<span class="st"> </span><span class="kw">autoplot</span>(forecasts3, <span class="dt">ylab=</span><span class="st">&quot;Forecast&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Trend + Season&quot;</span>)</a>
<a class="sourceLine" id="cb1701-2" data-line-number="2">p4 =<span class="st"> </span><span class="kw">autoplot</span>(forecasts4, <span class="dt">ylab=</span><span class="st">&quot;Forecast&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Fourier(K=4)&quot;</span>)</a>
<a class="sourceLine" id="cb1701-3" data-line-number="3">p3 <span class="op">/</span><span class="st"> </span>p4 <span class="co"># display two plots vertically </span></a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tslmplot2"></span>
<img src="DS_files/figure-html/tslmplot2-1.png" alt="Forecast (using STLM)" width="70%" />
<p class="caption">
Figure 11.21: Forecast (using STLM)
</p>
</div>

<p>All the plots show our forecast around the fourth period, estimated with confidence levels.</p>
</div>
<div id="autoregressive-integrated-moving-average-arima" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.4</span> AutoRegressive Integrated Moving Average (ARIMA)  <a href="machinelearning3.html#autoregressive-integrated-moving-average-arima" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>ARIMA</strong> is also called the <strong>Box-Jenkins</strong> method formulated by George Box and Gwilym Jenkins in 1976, which allows us to <strong>model</strong> stationary time-series. <strong>ARIMA</strong> uses three parameters: </p>
<ul>
<li><strong>p</strong> is non-seasonal parameter for AR order</li>
<li><strong>d</strong> is non-seasonal parameter for differencing (de-trending)</li>
<li><strong>q</strong> is non-seasonal parameter for MA order</li>
</ul>
<p>Below are <strong>ARIMA</strong> model examples that can represent different time series:</p>
<p><span class="math display">\[\begin{align}
ARIMA(1, 0, 0) &amp;\rightarrow &amp;Y_t = \beta_1  Y_{t-1} + \mathcal{E}_t\\
ARIMA(2, 0, 0) &amp;\rightarrow &amp;Y_t = \beta_1  Y_{t-1} + \beta_2  Y_{t-2} + \mathcal{E}_t\\
ARIMA(1, 1, 0) &amp;\rightarrow &amp;Y_t = \beta_1 \Delta Y_{t-1}  + \mathcal{E}_t
               \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ where\ \Delta Y_{t-1} = Y_t - Y_{t-1}\\
ARIMA(1, 1, 2) &amp;\rightarrow &amp;Y_t = \beta_1 \Delta Y_{t-1} + \theta_1 \mathcal{E}_{t-1} + \theta_2 \mathcal{E}_{t-2} + \mathcal{E}_t
\end{align}\]</span></p>
<p>To understand <strong>ARIMA</strong>, we need to break it down into three methods, namely <strong>Autoregression (AR)</strong>, <strong>Integration (I)</strong>, and <strong>Moving Average (MA)</strong>.</p>
<p><strong>First</strong>, the <strong>Autoregression (AR)</strong> method allows us to forecast (or predict) the future in reference to the past. Our goal is to achieve a future state that regresses the previous state. We can model that statement based on the following linear equation with an assumed intercept in the form of a mean (<span class="math inline">\(\mu\)</span>) and a white noise denoted by <span class="math inline">\(\mathcal{E}_t\)</span>:</p>
<p><span class="math display">\[\begin{align}
AR(p) = \mu + \underbrace{\left(\sum_{i=1}^p \beta_i y_{t-i}\right)}_\text{AR(p)} + \mathcal{E}_t = \mu + \beta_1y_{t-1} + \beta_2y_{t-2} \ + ... +\ \beta_p y_{t-p}  + \mathcal{E}_t  
\end{align}\]</span></p>
<p>where <span class="math inline">\(y_{t - p}\)</span> is the state value at <strong>(t-p)</strong> time-period (or the <strong>lag</strong>) in the past and the mean (<span class="math inline">\(\mu\)</span>) is the overall mean. The parameter <strong>p</strong> is the order of terms to use.</p>
<p>To illustrate, let us use <strong>first-order</strong> using the following equation:</p>
<p><span class="math display">\[\begin{align}
AR(p=1) = \mu + \beta_1 y_{t-1} + \mathcal{E}_t
\end{align}\]</span></p>
<p>Here, for a simple example, assume that our previous state corresponds to a value of 25. Assume that to be more conservative, we weight that value by using a multiplying factor of 0.5 — the <span class="math inline">\(\beta_1\)</span> coefficient for the <strong>first-order</strong> term. In addition, the overall average value is 20, which is our starting point or baseline.</p>
<p><span class="math display">\[
y_{t - 1} = 25\ \ \ \ \ \ \ \beta_1 = 0.5\ \ \ \ \ \ \mu = 20 
\]</span></p>
<p>Therefore, our predicted value <span class="math inline">\(\hat{S}\)</span> becomes:</p>
<p><span class="math display">\[\begin{align}
\hat{S}(1) = y_t &amp;= \mu + \beta_1 y_{t-1} \\
&amp;=20 + 0.5\times 25   \nonumber \\
&amp;=32.5 \nonumber
\end{align}\]</span></p>
<p>If only we know the actual state at the period (t), then we should be able to determine the amount of hidden information (whether white noise or any confounding factors) not covered by our prediction. For example, assume that the error is <span class="math inline">\(\mathcal{E}_t = 2\)</span>, then the actual state value computes to about the following:</p>
<p><span class="math display">\[\begin{align}
S(1) = y_t &amp;= \mu + \beta_1 y_{t-1}  + \mathcal{E}_t\\
&amp;=20 + 0.5\times 25  + 2  \nonumber\\
&amp;=34.5   \nonumber
\end{align}\]</span></p>
<p>In this particular example, we can say that we have modeled our forecast using only one period in the past. We can denote that as <strong>AR(1)</strong>, meaning that our autoregression model is based on the <strong>first-order</strong> term. Moreover, if we have to consider two periods (or lags) in the past, we can denote that as <strong>AR(2)</strong> so that the <strong>second-order</strong> formula then becomes:</p>
<p><span class="math display">\[\begin{align}
AR(p=2) =  = \mu + \beta_1 y_{t-1} + \beta_2 y_{t-2}  + \mathcal{E}_t 
\end{align}\]</span></p>
<p>We can determine the proper order (<strong>p</strong>) to use for <strong>AR(p)</strong> by using <strong>PACF</strong>, which will be covered later.</p>
<p><strong>Second</strong>, the <strong>Moving Average (MA)</strong> method allows us to deal with previous state errors instead of state values, as covered in our first step above for <strong>AR(p)</strong>. Our <strong>MA</strong> equation then looks like so:</p>
<p><span class="math display">\[\begin{align}
MA(q) = \mu + \underbrace{\left(\sum_{i=1}^q \theta _i\mathcal{E}_{t-i}\right)}_\text{MA(q)} + \mathcal{E}_t = \mu + \mathcal{E}_t + \theta_1 \mathcal{E}_{t-1} +  \theta_2 \mathcal{E}_{t-2}\ + ... +\ 
 \theta_q \mathcal{E}_{t-q}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathcal{E}_{t - q}\)</span> is the error at <strong>(t-q)</strong> time-period (or the <strong>lag</strong>) in the past, and the mean (<span class="math inline">\(\mu\)</span>) is the overall mean. The parameter <strong>q</strong> is the order of terms to use.</p>
<p>To illustrate, let us use <strong>first-order</strong> using the following equation:</p>
<p><span class="math display">\[\begin{align}
MA(q = 1) = \mu + \theta_1 \mathcal{E}_{t-1} 
\end{align}\]</span></p>
<p>Here, we continue to use our previous example in which we have a previous state corresponding to a value of 25. However, given that our overall average value is 20, we know that there is a 5-point error (or difference) in our previous state. Moreover, we still assume that to be more conservative, we weigh our error (not the state value) by using a multiplying factor of 2.5.</p>
<p><span class="math display">\[
y_{t - 1} = 25\ \ \ \ \ \ \ \theta_1 = 2.5\ \ \ \ \ \ \mu= 20\ \ \ \ \ \ \ \ \mathcal{E}_{t-1} = 5 
\]</span></p>
<p>Therefore, our predicted value <span class="math inline">\(\hat{S}\)</span> becomes:</p>
<p><span class="math display">\[\begin{align}
\hat{S}(1) = y_t &amp;= \mu + \theta_1 \mathcal{E}_{t-1} \\ 
&amp;=20 + 2.5\times 5  \nonumber \\
&amp;=32.5 \nonumber
\end{align}\]</span></p>
<p>A similar situation applies should we know the true value. Assume an error difference (white noise) of 2 at period (<strong>t</strong>); therefore, our true value becomes:</p>
<p><span class="math display">\[\begin{align}
S(1) = y_t&amp;= \mu+ \mathcal{E}_t + \theta_1 \mathcal{E}_{t-1}    \\
&amp;=20 + 2  + 2.5\times 5  \nonumber \\
&amp;=34.5 \nonumber
\end{align}\]</span></p>
<p>Also, in this particular example, we can say that we have modeled our forecast using only one period in the past. We can denote that as <strong>MA(1)</strong>, meaning that our moving average model is based on the <strong>first-order</strong>. Moreover, if we have to consider two periods in the past, we can denote that as <strong>MA(2)</strong> so that the <strong>second-order</strong> formula then becomes:</p>
<p><span class="math display">\[\begin{align}
MA(q = 2) = \mu + \mathcal{E}_t + \theta_1 \mathcal{E}_{t-1} +  \theta_2 \mathcal{E}_{t-2}
\end{align}\]</span></p>
<p>We can determine the proper order (<strong>q</strong>) for <strong>MA(q)</strong> by using <strong>ACF</strong>, which is discussed later.</p>
<p><strong>Third</strong>, the <strong>Integrated (I)</strong> method allows us to convert our <strong>Time-Series</strong> data from non-stationary to stationary series by a simple method called <strong>Differencing</strong>. Here, we denote <span class="math inline">\(\mathbf{I(d)}\)</span> as a function that performs differencing for a given <strong>Time-Series</strong> such that the parameter <strong>d</strong> indicates the degree of differencing (or how many iterations to take to difference the series).</p>
<p><span class="math display">\[\begin{align}
I(d) = \{Y_{(t+1)} - Y_t\}_{t=1}^{(T-1)}
\end{align}\]</span></p>
<p>To illustrate, the <strong>Time-Series</strong> data below can transform into the following new vector by <strong>first-degree</strong> differencing, e.g., <strong>I(1)</strong>:</p>
<p><span class="math display">\[
y = (10,9,5,7,6,4)\ \ \ \ \ \rightarrow\ \ \ \ 
z^{(1)} = (-1,-4, 2, -1,-2)
\]</span></p>
<p>If it is essential to perform <strong>second-degree</strong> differencing, e.g. <strong>I(2)</strong>, then we can iterate again:</p>
<p><span class="math display">\[
y = (10,9,5,7,6,4)\ \ \ \ \ \rightarrow\ \ \ \ 
z^{(1)} = (-1,-4, 2, -1,-2)\ \ \ \ \ \rightarrow\ \ \ \ 
z^{(2)} = (-3,6, -3, -1)
\]</span>
We can use any <strong>degree</strong> of differencing, e.g., <strong>I(d)</strong>, as we see necessary.</p>
<p><strong>Fourth</strong>, we can combine all three methods to form <strong>ARIMA(p,d,q)</strong>with the three given methods. The parameters allow us to fine-tune our formulae for an accurate forecast.</p>
<p><span class="math display">\[\begin{align}
\text{ARIMA}(p, d, q) =  \mu + 
\underbrace{\left(\sum_{i=1}^p \beta_i y_{t-i}\right)}_\text{AR(p)} + 
\underbrace{\left(\sum_{i=1}^q \theta _i\mathcal{E}_{t-i}\right)}_\text{MA(q)} + \mathcal{E}_t
\end{align}\]</span></p>
<p>Note that if we exclude the <strong>differencing</strong>, e.g., <strong>I(d)</strong>, our model becomes the <strong>ARMA(p,q)</strong> model. This model assumes that our <strong>Time-Series</strong> data is already stationary.</p>
<p><strong>Finally</strong>, to determine the number of orders to use for <strong>AR(p)</strong> and <strong>MA(q)</strong>, we have to rely on <strong>Autocorrelation</strong>, which allows us to compare the correlation of a time-series data between two time periods. Here, we introduce <strong>AutoCorrelation Function (ACF)</strong> and <strong>Partial AutoCorrelation Function (PACF)</strong>.</p>
<p><strong>AutoCorrelation Function (ACF)</strong>  </p>
<p>To start, we first determine the value of the <strong>q</strong> parameter for <strong>MA</strong>. In this case, we use <strong>ACF</strong>. We can illustrate this using the equation below:</p>
<p><span class="math display">\[\begin{align}
ACF(k) = \frac{\sum_{t=k+1}^T \left(y_t - \bar{y}\right)\left(y_{t -k} - \bar{y}\right)}
   {\sum_{t=1}^T \left(y_t - \bar{y}\right)^2}
\end{align}\]</span></p>
<p>Our example implementation of the <strong>ACF</strong> equation is as follows:</p>

<div class="sourceCode" id="cb1702"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1702-1" data-line-number="1">my.acf &lt;-<span class="st"> </span><span class="cf">function</span>(y) {</a>
<a class="sourceLine" id="cb1702-2" data-line-number="2">   N =<span class="st"> </span><span class="kw">length</span>(y)  </a>
<a class="sourceLine" id="cb1702-3" data-line-number="3">   y.mean =<span class="st"> </span><span class="kw">mean</span>(y)</a>
<a class="sourceLine" id="cb1702-4" data-line-number="4">   acf =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N   )</a>
<a class="sourceLine" id="cb1702-5" data-line-number="5">   <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="st"> </span>(N<span class="dv">-1</span>) ) {</a>
<a class="sourceLine" id="cb1702-6" data-line-number="6">     numer =<span class="st"> </span><span class="dv">0</span>; denom =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1702-7" data-line-number="7">     <span class="cf">for</span> (t <span class="cf">in</span> (k<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1702-8" data-line-number="8">        numer =<span class="st"> </span>numer <span class="op">+</span><span class="st"> </span>(y[t] <span class="op">-</span><span class="st"> </span>y.mean) <span class="op">*</span><span class="st"> </span>(y[t<span class="op">-</span>k] <span class="op">-</span><span class="st"> </span>y.mean)</a>
<a class="sourceLine" id="cb1702-9" data-line-number="9">     }</a>
<a class="sourceLine" id="cb1702-10" data-line-number="10">     <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1702-11" data-line-number="11">        denom =<span class="st"> </span>denom <span class="op">+</span><span class="st"> </span>(y[t] <span class="op">-</span><span class="st"> </span>y.mean)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1702-12" data-line-number="12">     }</a>
<a class="sourceLine" id="cb1702-13" data-line-number="13">     acf[k <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span>numer <span class="op">/</span><span class="st"> </span>denom</a>
<a class="sourceLine" id="cb1702-14" data-line-number="14">   }</a>
<a class="sourceLine" id="cb1702-15" data-line-number="15">   acf</a>
<a class="sourceLine" id="cb1702-16" data-line-number="16">}</a></code></pre></div>

<p>To use the function, let us generate some random time-series and plot (see Figure <a href="machinelearning3.html#fig:tsdata">11.22</a>).</p>

<div class="sourceCode" id="cb1703"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1703-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1703-2" data-line-number="2">N =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb1703-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1703-4" data-line-number="4">y =<span class="st">  </span><span class="kw">cos</span>( <span class="kw">seq</span>(<span class="dv">0</span>, N, <span class="dt">length.out =</span> N)) <span class="op">+</span><span class="st"> </span>e</a>
<a class="sourceLine" id="cb1703-5" data-line-number="5">y =<span class="st"> </span><span class="kw">ts</span> (<span class="dt">data =</span> y, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">2018</span>, <span class="dv">1</span>),  <span class="dt">frequency=</span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb1703-6" data-line-number="6">y =<span class="st"> </span><span class="kw">as.vector</span>(y)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tsdata"></span>
<img src="DS_files/figure-html/tsdata-1.png" alt="Stationary (Gaussian Noise)" width="70%" />
<p class="caption">
Figure 11.22: Stationary (Gaussian Noise)
</p>
</div>

<p>Using our own implementation of <strong>ACF</strong>, we get the following:</p>

<div class="sourceCode" id="cb1704"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1704-1" data-line-number="1">acf1 =<span class="st"> </span><span class="kw">my.acf</span>(y );  <span class="kw">head</span>(acf1, <span class="dt">n=</span><span class="dv">30</span>) <span class="co"># display only first 30 values</span></a></code></pre></div>
<pre><code>##  [1]  1.00000  0.21814 -0.27968 -0.31197 -0.18164  0.12728  0.18185
##  [8]  0.17614 -0.05514 -0.34622 -0.20759 -0.01356  0.15699  0.14616
## [15]  0.12170 -0.11613 -0.23486 -0.06625  0.18138  0.22938 -0.02141
## [22] -0.14976 -0.15499 -0.12128  0.16216  0.17166  0.06235 -0.09066
## [29] -0.16201 -0.05920</code></pre>

<p>We can validate with <strong>acf(.)</strong> function from the built-in <strong>stats</strong> R package:</p>

<div class="sourceCode" id="cb1706"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1706-1" data-line-number="1">p =<span class="st"> </span>stats<span class="op">::</span><span class="kw">acf</span>(y, <span class="dt">type=</span><span class="st">&quot;correlation&quot;</span> , <span class="dt">plot=</span><span class="ot">FALSE</span>, <span class="dt">lag.max=</span>N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1706-2" data-line-number="2">acf2 =<span class="st"> </span><span class="kw">as.vector</span>(p<span class="op">$</span>acf);  <span class="kw">head</span>(acf1, <span class="dt">n=</span><span class="dv">30</span>) <span class="co"># display only first 30 values</span></a></code></pre></div>
<pre><code>##  [1]  1.00000  0.21814 -0.27968 -0.31197 -0.18164  0.12728  0.18185
##  [8]  0.17614 -0.05514 -0.34622 -0.20759 -0.01356  0.15699  0.14616
## [15]  0.12170 -0.11613 -0.23486 -0.06625  0.18138  0.22938 -0.02141
## [22] -0.14976 -0.15499 -0.12128  0.16216  0.17166  0.06235 -0.09066
## [29] -0.16201 -0.05920</code></pre>

<p>Now to compare if both are equal:</p>

<div class="sourceCode" id="cb1708"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1708-1" data-line-number="1"><span class="kw">all.equal</span>(acf1 , acf2 )</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>

<p>Additionally, we also accompany <strong>ACF</strong> with an estimation of our confidence level to help with our <strong>cut-off</strong>. However, first, let us recall the equation and corresponding implementation:</p>
<p><span class="math display">\[\begin{align}
\pm z \times \left(\sigma / \sqrt{T}\right)
\ \ \ \ \ \ \text{where z-score} = 1.96 \text{ and }\sigma = 1
\end{align}\]</span></p>

<div class="sourceCode" id="cb1710"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1710-1" data-line-number="1">conf.interval &lt;-<span class="st"> </span><span class="cf">function</span>(y, alpha) {</a>
<a class="sourceLine" id="cb1710-2" data-line-number="2">  N =<span class="st"> </span><span class="kw">length</span>(y); z =<span class="st"> </span>alpha; sigma =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1710-3" data-line-number="3">  <span class="kw">c</span>(sigma,<span class="op">-</span>sigma) <span class="op">*</span><span class="st"> </span>z<span class="op">/</span><span class="kw">sqrt</span>(N) </a>
<a class="sourceLine" id="cb1710-4" data-line-number="4">}</a></code></pre></div>

<p>A confidence level of 95% has a z=score of 1.96. This maps to the following ACF boundaries.</p>

<div class="sourceCode" id="cb1711"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1711-1" data-line-number="1">(<span class="dt">cl =</span> <span class="kw">conf.interval</span>(y, <span class="fl">1.96</span>))</a></code></pre></div>
<pre><code>## [1]  0.196 -0.196</code></pre>

<p>We now plot <strong>ACF</strong> - see Figure <a href="machinelearning3.html#fig:acfplot">11.23</a>. Notice that the correlation <strong>tails off</strong> for one with seasonality.</p>

<div class="sourceCode" id="cb1713"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1713-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(acf1))</a>
<a class="sourceLine" id="cb1713-2" data-line-number="2"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),  <span class="dt">xaxt=</span><span class="st">&quot;n&quot;</span>,</a>
<a class="sourceLine" id="cb1713-3" data-line-number="3">      <span class="dt">xlab=</span><span class="st">&quot;Lag&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;ACF&quot;</span>, </a>
<a class="sourceLine" id="cb1713-4" data-line-number="4">     <span class="dt">main=</span><span class="st">&quot;ACF Plot&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1713-5" data-line-number="5"><span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">1</span>, <span class="dt">at =</span> <span class="kw">seq</span>(<span class="dv">0</span>, N), <span class="dt">tcl =</span> <span class="fl">-0.3</span>, <span class="dt">labels =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1713-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1713-7" data-line-number="7"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1713-8" data-line-number="8"><span class="kw">lines</span>(x, acf1, <span class="dt">type=</span><span class="st">&#39;h&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1713-9" data-line-number="9"><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">conf.interval</span>(y, <span class="fl">1.96</span>), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:acfplot"></span>
<img src="DS_files/figure-html/acfplot-1.png" alt="ACF Plot" width="70%" />
<p class="caption">
Figure 11.23: ACF Plot
</p>
</div>

<p>With the confidence level drawn horizontally in the figure, any <strong>ACF</strong> value between <span class="math inline">\(\pm\)</span> (0.196) is considered as having zero correlation for a <strong>lag</strong>; and thus gets discarded.</p>
<p>Therefore, to finally obtain the order for our <strong>q</strong> parameter for <strong>MA</strong>, we get:</p>

<div class="sourceCode" id="cb1714"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1714-1" data-line-number="1">(<span class="dt">MA.q =</span> <span class="kw">which</span>(<span class="op">!</span><span class="kw">between</span>(acf1, cl[<span class="dv">2</span>], cl[<span class="dv">1</span>])))</a></code></pre></div>
<pre><code>##  [1]  1  2  3  4 10 11 17 20 47 56</code></pre>

<p><strong>Partial AutoCorrelation Function (PACF)</strong>  </p>
<p>Next, we use <strong>PACF</strong> to determine the value for the <strong>p</strong> parameter for <strong>AR</strong>. The basic idea of <strong>PACF</strong> starts with the equation below in which the value for <span class="math inline">\(Y_t\)</span> can be explained directly by the <strong>pth</strong> term, namely <span class="math inline">\(Y_{t-k}\)</span>. In other words, we suggest that there is some autocorrelation between <span class="math inline">\(Y_t\)</span> and <span class="math inline">\(Y_{t-k}\)</span>.</p>
<p><span class="math display">\[\begin{align}
Y_t = \beta_0 + \beta_1 Y_{t-1}  + \beta_2 Y_{t-2}  + \beta_3 Y_{t-3}\ + ... +\ \beta_k Y_{t-k}
\end{align}\]</span></p>
<p>Therefore, we can discard <span class="math inline">\(Y_{t-1},\  Y_{t-2},\ Y_{t-3}, ...,\ Y_{t-k+1}\)</span> for a <strong>Time-Series</strong> at lag <strong>k</strong>. What we are interested is the coefficient at lag <strong>k</strong>, namely <span class="math inline">\(\beta_k\)</span> from the term <span class="math inline">\(\beta_k Y_{t-k}\)</span>.</p>
<p>The value for <strong>PACF</strong> is obtained from the coefficient of the terminating term of the following equation.</p>
<p><span class="math display">\[\begin{align}
Y_t = \beta_0 + \beta_1 Y_{t-1}  + \beta_2 Y_{t-2}  + \beta_3 Y_{t-3}\ + ... +\ \beta_k Y_{t-k}
\end{align}\]</span></p>
<p>The terminating term of the equation above is based on a given <strong>Kth</strong> order. For example, <strong>PACF(K=2)</strong> equals the <span class="math inline">\(\beta_2\)</span> coefficient from the following equation:</p>
<p><span class="math display">\[\begin{align}
Y_t = \beta_0 + \beta_1 Y_{t-1}  + \beta_2 Y_{t-2}
\end{align}\]</span></p>
<p><strong>PACF(K=4)</strong> equals the <span class="math inline">\(\beta_4\)</span> coefficient from the following equation:</p>
<p><span class="math display">\[\begin{align}
Y_t = \beta_0 + \beta_1 Y_{t-1}  + \beta_2 Y_{t-2} + \beta_3 Y_{t-3}  + \beta_4 Y_{t-4}
\end{align}\]</span></p>
<p>Now, let us introduce three ways to calculate <strong>PACF</strong> for a system of equations. Each equation is generated for each value of K, namely <span class="math inline">\(k = 1,2,3,...,N\)</span>.</p>
<p>The first way is to use symmetric invertible <strong>Toeplitz matrix</strong>, which is simply a list of systems of equations — <strong>Yule-Walker</strong> equations. A generic <strong>Toeplitz matrix</strong> is shown below: </p>
<p><span class="math display">\[\begin{align}
\underbrace{ 
\left[ 
\begin{array}{lllllll}
1 &amp; \rho_{1} &amp; \rho_{2 } &amp; \cdots &amp; \rho_{k-2} &amp; \rho_{k-1} \\
 \rho_{1}  &amp; 1 &amp; \rho_{1 } &amp; \cdots &amp; \rho_{k-3} &amp; \rho_{k-2} \\
 \rho_{2} &amp; \rho_{1} &amp; 1 &amp; \cdots &amp; \rho_{k-4} &amp; \rho_{k-3} \\
\vdots &amp;  \vdots &amp; \vdots&amp; \ddots &amp;\vdots &amp; \vdots\\
\rho_{k-2} &amp; \rho_{k-3}  &amp; \rho_{k-4} &amp; \cdots &amp; 1 &amp; \rho_{1} \\
\rho_{k-1}  &amp; \rho_{k-2} &amp; \rho_{k-3}&amp; \cdots &amp; \rho_{1} &amp; 1 \\
\end{array}
\right]
}_{\text{R}}
\underbrace{
\left[
\begin{array}{l}
\beta_1 \\
\beta_2 \\
\beta_3 \\
\vdots \\
\beta_{k-1} \\
\beta_k \\
\end{array}
\right]}_{\beta}
=
\underbrace{
\left[
\begin{array}{l}
\rho_1 \\
\rho_2 \\
\rho_3 \\
\vdots \\
\rho_{k-1} \\
\rho_k \\
\end{array}
\right]}_{\rho} \label{eqn:eqnnumber503}
\end{align}\]</span></p>
<p>Without going through derivations, we compute for the coefficients like so:</p>
<p><span class="math display">\[\begin{align}
\beta = R^{-1}\rho
\end{align}\]</span></p>
<p>To illustrate, given <strong>k=5</strong>, we only consider the <span class="math inline">\(k \times x\)</span> dimension of <strong>R</strong> and the kth dimension of <span class="math inline">\(\rho\)</span> to solve for the <strong>PACF</strong> value at lag 5.</p>
<p><span class="math display">\[\begin{align}
\underbrace{\beta}_{(1:k)} = \underbrace{R^{-1}}_{(1:k,1:k)}\underbrace{\rho}_{(1:k)}
\end{align}\]</span></p>
<p>Let us use the vector of <strong>ACF</strong> we generated previously to create a <strong>Toeplitz matrix</strong> given <strong>k=5</strong>:</p>

<div class="sourceCode" id="cb1716"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1716-1" data-line-number="1">k=<span class="dv">5</span></a>
<a class="sourceLine" id="cb1716-2" data-line-number="2">R   =<span class="st"> </span><span class="kw">toeplitz</span>(acf1) </a>
<a class="sourceLine" id="cb1716-3" data-line-number="3">(<span class="dt">R.kk =</span> R[<span class="dv">1</span><span class="op">:</span>k, <span class="dv">1</span><span class="op">:</span>k])  <span class="co"># display only 5x5 toeplitz matrix for illustration</span></a></code></pre></div>
<pre><code>##         [,1]    [,2]    [,3]    [,4]    [,5]
## [1,]  1.0000  0.2181 -0.2797 -0.3120 -0.1816
## [2,]  0.2181  1.0000  0.2181 -0.2797 -0.3120
## [3,] -0.2797  0.2181  1.0000  0.2181 -0.2797
## [4,] -0.3120 -0.2797  0.2181  1.0000  0.2181
## [5,] -0.1816 -0.3120 -0.2797  0.2181  1.0000</code></pre>

<p>Then we use <strong>solve(.)</strong> to invert the matrix and solve for the coefficients.</p>

<div class="sourceCode" id="cb1718"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1718-1" data-line-number="1">beta.kk =<span class="st"> </span>acf1[<span class="dv">2</span><span class="op">:</span>(k <span class="op">+</span><span class="st"> </span><span class="dv">1</span> )]</a>
<a class="sourceLine" id="cb1718-2" data-line-number="2">(<span class="dt">coeffs =</span> <span class="kw">as.vector</span>( <span class="kw">solve</span>(R.kk) <span class="op">%*%</span><span class="st"> </span>beta.kk))</a></code></pre></div>
<pre><code>## [1]  0.20877 -0.33304 -0.11718 -0.19982  0.07212</code></pre>

<p>Out of the obtained coefficients, namely <span class="math inline">\(\beta_1\)</span> = 0.2088, <span class="math inline">\(\beta_2\)</span> = -0.333, <span class="math inline">\(\beta_3\)</span> = -0.1172, <span class="math inline">\(\beta_4\)</span> = -0.1998, and <span class="math inline">\(\beta_5\)</span> = 0.0721, we consider only PACF(k=5) = <span class="math inline">\(\beta_5\)</span> = 0.0721.</p>
<p>For an example implementation of <strong>PACF</strong>, we have the following:</p>

<div class="sourceCode" id="cb1720"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1720-1" data-line-number="1">my.pacf &lt;-<span class="st"> </span><span class="cf">function</span>(R, rho) {</a>
<a class="sourceLine" id="cb1720-2" data-line-number="2">  N =<span class="st"> </span><span class="kw">length</span>(rho) </a>
<a class="sourceLine" id="cb1720-3" data-line-number="3">  pacf =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1720-4" data-line-number="4">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1720-5" data-line-number="5">    beta =<span class="st"> </span><span class="kw">solve</span>(R[<span class="dv">1</span><span class="op">:</span>k, <span class="dv">1</span><span class="op">:</span>k]) <span class="op">%*%</span><span class="st"> </span>rho[<span class="dv">1</span><span class="op">:</span>k]</a>
<a class="sourceLine" id="cb1720-6" data-line-number="6">    pacf =<span class="st"> </span><span class="kw">c</span>(pacf, beta[k])</a>
<a class="sourceLine" id="cb1720-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb1720-8" data-line-number="8">  pacf</a>
<a class="sourceLine" id="cb1720-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb1720-10" data-line-number="10">P   =<span class="st"> </span><span class="kw">length</span>(acf1) </a>
<a class="sourceLine" id="cb1720-11" data-line-number="11">rho =<span class="st"> </span>acf1[<span class="dv">2</span><span class="op">:</span>P]</a>
<a class="sourceLine" id="cb1720-12" data-line-number="12"><span class="kw">head</span>(<span class="kw">my.pacf</span>(R, rho), <span class="dt">n=</span><span class="dv">30</span>)  <span class="co"># for k=1,2,...,N</span></a></code></pre></div>
<pre><code>##  [1]  0.218137 -0.343610 -0.184589 -0.185733  0.072116 -0.017177
##  [7]  0.153925 -0.078614 -0.223876 -0.086100 -0.138184 -0.046556
## [13] -0.062338  0.154348 -0.134410 -0.033726 -0.048870  0.076038
## [19] -0.006173 -0.113236 -0.042865 -0.095876 -0.077892  0.083693
## [25] -0.041574 -0.001806 -0.005386 -0.024463 -0.066546 -0.008387</code></pre>

<p><strong>Alternatively</strong>, to solve for <strong>PACF</strong> without using <strong>Toeplitz matrix</strong>, we can use the <strong>Levinson-Durbin</strong> Algorithm. The Algorithm comes in two forms, one of which obtains the backward prediction coefficients <span class="citation">(Borchers B. <a href="bibliography.html#ref-ref1048b">2001</a>)</span>. </p>
<p>Step 1: <span class="math inline">\(\beta_{1} = \frac{\rho_1}{\rho_0},\ \ \ P_1 = \beta_{1}\)</span></p>
<p>Step 2: For <span class="math inline">\(n = 2,3,...,p\)</span>, compute for the updates:</p>
<p><span class="math display">\[\begin{align}
\beta_{nn} &amp;= 
   \frac{\rho_n - \sum_{k=1}^{n-1} \beta_{n-1,k} \rho_{n-k}}
         {1 - \sum_{k=1}^{n-1} \beta_{n-1,k} \rho_k}\\
\beta_{nk} &amp;= \beta_{n-1} - \beta_{nn}\ \beta_{n-1, n-k},\ \ \ \  \ where\ \ \ \ \ {k=1,2,...,n-1} \\
P_{n} &amp;= \beta_{nn}
\end{align}\]</span></p>
<p>We leave readers to investigate the algorithm to obtain the forward prediction coefficients as an exercise.</p>
<p>Below is an example implementation of the <strong>Levinson-Durbin</strong> algorithm (motivated by the original R script from Ross Ihaka’s Statistical 726 Course <span class="citation">(<a href="bibliography.html#ref-ref1023r">n.d.</a>)</span> with modification following the algorithm above):</p>

<div class="sourceCode" id="cb1722"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1722-1" data-line-number="1">my.Levinson.Durbin =<span class="st"> </span><span class="cf">function</span>(rho, <span class="dt">lag.max =</span> <span class="dv">2</span>) {</a>
<a class="sourceLine" id="cb1722-2" data-line-number="2">    N =<span class="st"> </span><span class="kw">length</span>(rho)</a>
<a class="sourceLine" id="cb1722-3" data-line-number="3">    beta =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1722-4" data-line-number="4">    pacf =<span class="st"> </span>beta[<span class="dv">1</span>] =<span class="st"> </span>rho[<span class="dv">2</span>] <span class="op">/</span><span class="st"> </span>rho[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1722-5" data-line-number="5">    rho =<span class="st"> </span>rho[<span class="dv">2</span><span class="op">:</span>(lag.max<span class="op">+</span><span class="dv">1</span>)]</a>
<a class="sourceLine" id="cb1722-6" data-line-number="6">    pacf =<span class="st"> </span><span class="kw">c</span>(beta[<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb1722-7" data-line-number="7">    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>lag.max) {</a>
<a class="sourceLine" id="cb1722-8" data-line-number="8">        beta.n   =<span class="st"> </span>beta[<span class="dv">1</span><span class="op">:</span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)]</a>
<a class="sourceLine" id="cb1722-9" data-line-number="9">        rho.n   =<span class="st"> </span>rho[<span class="dv">1</span><span class="op">:</span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)]  </a>
<a class="sourceLine" id="cb1722-10" data-line-number="10">        beta[n]  =<span class="st"> </span>( rho[n] <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>( beta.n <span class="op">*</span><span class="st"> </span><span class="kw">rev</span>(rho.n) )) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1722-11" data-line-number="11"><span class="st">                     </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>( beta.n <span class="op">*</span><span class="st">  </span>rho.n))</a>
<a class="sourceLine" id="cb1722-12" data-line-number="12">        beta[<span class="dv">1</span><span class="op">:</span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)] =<span class="st"> </span>beta.n <span class="op">-</span><span class="st"> </span>beta[n] <span class="op">*</span><span class="st"> </span><span class="kw">rev</span>(beta.n)</a>
<a class="sourceLine" id="cb1722-13" data-line-number="13">        pacf =<span class="st"> </span><span class="kw">c</span>(pacf, beta[n])</a>
<a class="sourceLine" id="cb1722-14" data-line-number="14">    }</a>
<a class="sourceLine" id="cb1722-15" data-line-number="15">    pacf </a>
<a class="sourceLine" id="cb1722-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb1722-17" data-line-number="17">pacf1 =<span class="st"> </span><span class="kw">my.Levinson.Durbin</span>(acf1, <span class="dt">lag.max =</span> (P<span class="dv">-1</span>))</a>
<a class="sourceLine" id="cb1722-18" data-line-number="18"><span class="kw">head</span>(pacf1, <span class="dt">n=</span><span class="dv">30</span>)</a></code></pre></div>
<pre><code>##  [1]  0.218137 -0.343610 -0.184589 -0.185733  0.072116 -0.017177
##  [7]  0.153925 -0.078614 -0.223876 -0.086100 -0.138184 -0.046556
## [13] -0.062338  0.154348 -0.134410 -0.033726 -0.048870  0.076038
## [19] -0.006173 -0.113236 -0.042865 -0.095876 -0.077892  0.083693
## [25] -0.041574 -0.001806 -0.005386 -0.024463 -0.066546 -0.008387</code></pre>

<p>Alternatively, we can still use <strong>pacf(.)</strong> function with type equal to <strong>partial</strong>:</p>

<div class="sourceCode" id="cb1724"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1724-1" data-line-number="1">pacf2 =<span class="st"> </span><span class="kw">as.vector</span>( <span class="kw">pacf</span>(y, <span class="dt">type=</span><span class="st">&quot;partial&quot;</span>, <span class="dt">lag.max=</span>(P<span class="dv">-1</span>), <span class="dt">plot=</span><span class="ot">FALSE</span>)<span class="op">$</span>acf)</a>
<a class="sourceLine" id="cb1724-2" data-line-number="2"><span class="kw">head</span>(pacf2, <span class="dt">n=</span><span class="dv">30</span>)</a></code></pre></div>
<pre><code>##  [1]  0.218137 -0.343610 -0.184589 -0.185733  0.072116 -0.017177
##  [7]  0.153925 -0.078614 -0.223876 -0.086100 -0.138184 -0.046556
## [13] -0.062338  0.154348 -0.134410 -0.033726 -0.048870  0.076038
## [19] -0.006173 -0.113236 -0.042865 -0.095876 -0.077892  0.083693
## [25] -0.041574 -0.001806 -0.005386 -0.024463 -0.066546 -0.008387</code></pre>

<p>Let us validate:</p>

<div class="sourceCode" id="cb1726"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1726-1" data-line-number="1"><span class="kw">all.equal</span>(pacf1, pacf2)</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>

<p>We now plot <strong>PACF</strong> - see Figure <a href="machinelearning3.html#fig:pacfplot">11.24</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pacfplot"></span>
<img src="DS_files/figure-html/pacfplot-1.png" alt="PACF Plot" width="70%" />
<p class="caption">
Figure 11.24: PACF Plot
</p>
</div>

<p>Notice that both <strong>ACF</strong> and <strong>PACF</strong> plots show some behavior in which the first couple of lags are beyond the confidence levels, indicating a strong correlation of the <strong>first lag</strong> lags at least with the current time, e.g., <span class="math inline">\(Y_t\)</span>. Then the rest of the lags <strong>tail off</strong> — meaning the correlation diminishes gradually. On the other hand, there are cases in which the plots show a strong correlation at the beginning, then suddenly <strong>cuts off</strong>. For example, in the <strong>ACF</strong> plot, if we see a strong correlation at the beginning of the lag, e.g., lag 1, and <strong>cuts off</strong> for the subsequent lags, then we can interpret that to mean that <strong>AR(1)</strong> or <strong>MA(1)</strong> is most likely the configuration we seek in which <strong>p</strong> for <strong>AR</strong> or <strong>q</strong> for <strong>MA</strong> follows a parameter value of 1.</p>
<p>Other literature explains signs of <strong>Seasonality</strong> or <strong>Trend</strong> reflected in the plots. As an exercise, we leave readers to investigate other examples of <strong>ACF</strong> and <strong>PACF</strong> plots and review their interpretations.</p>
</div>
<div id="multiplicative-seasonal-arima-sarima" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.5</span> Multiplicative Seasonal ARIMA (SARIMA) <a href="machinelearning3.html#multiplicative-seasonal-arima-sarima" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>ARIMA</strong> models are mainly used for <strong>stationary</strong> time series. To address <strong>non-stationary</strong> time-series, we can use <strong>SARIMA</strong>, which is written in the following expression:</p>
<p><span class="math display">\[\begin{align}
\text{ARIMA}\underbrace{(p,d,q)}_{\text{non-seasonal}} \times \underbrace{(P,D,Q)}_{\text{seasonal}} m
\end{align}\]</span>
where:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\mathbf{p}\ \text{is non-seasonal param for AR order} &amp; 
\mathbf{P}\ \text{is seasonal param for AR order} \\
\mathbf{d}\ \text{is non-seasonal param for differencing} &amp; 
\mathbf{D}\ \text{is seasonal param for differencing} \\
\mathbf{q}\ \text{is non-seasonal param for MA order} &amp; 
\mathbf{Q}\ \text{is seasonal param for MA order}
\end{array}\\
\mathbf{m}\ \text{number of periods to cover for a given frequency or season}
\end{align*}\]</span></p>
<p>To understand <strong>SARIMA</strong>, let us introduce the <strong>Backshift operator</strong> denoted simply as <strong>B</strong>, which shifts a given period one period back. Below are examples of backshifting a given period:</p>
<p><span class="math display">\[\begin{align}
B (Y_t) = Y_{t - 1}\ \ \ \ \ \ \ \ \ \
B (Y_{t-1}) = Y_{t - 2}\ \ \ \ \ \ \ \ \ \ \
B^2 (Y_t) = B (Y_{t-1}) = Y_{t - 2}\ \ \ \ \ \ 
\end{align}\]</span></p>
<p>Another example is to use the same notation for differencing:</p>
<p><span class="math display">\[\begin{align}
\underbrace{Y_t - Y_{t - 1} = ( 1 - B) y_t}_{\text{1st difference}}\ \ \ \ \ \ \ \ 
\underbrace{Y_t - Y_{t - 1} - Y_{t - 2} = ( 1 - B)^2 y_t}_{\text{2nd difference}}
\end{align}\]</span></p>
<p>The last example is to use the same notation to backshift a season to <strong>m</strong> periods:</p>
<p><span class="math display">\[\begin{align}
(1 - B^m)y_t
\end{align}\]</span></p>
<p>where <strong>m</strong> is the number of periods to backshift in a given season.</p>
<p><strong>Backshift operator</strong> follows both the algebraic and polynomial rules so that given AR(1) and AR(2), respectively, as an example, we start with the usual equations:</p>
<p><span class="math display">\[\begin{align}
y_t = \mu + \beta_1 y_{t-1}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
y_t = \mu + \beta_1 y_{t-1} + \beta_2 y_{t-2}
\end{align}\]</span></p>
<p>We then re-arrange our equation</p>
<p><span class="math display">\[\begin{align}
\begin{array}{ll}
y_t - \beta_1 y_{t-1}   &amp;= \mu \\
(1 - \beta_1 B  )y_t &amp;= \mu 
\end{array}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}{ll}
y_t - \beta_1 y_{t-1} - \beta_2 y_{t-2} &amp;= \mu \\
(1 - \beta_1 B - \beta_2 B^2)y_t &amp;= \mu 
\end{array} \label{eqn:eqnnumber504}
\end{align}\]</span></p>
<p>The corresponding polynomial for the <strong>backshift operation</strong> becomes:</p>
<p><span class="math display">\[\begin{align}
\beta (B) = (1 - \beta_1 B)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\beta (B) = (1 - \beta_1 B - \beta_2 B^2)
\end{align}\]</span></p>
<p>so then we get the final polynomial notation (which applies for both AR(1) and AR(2) generically):</p>
<p><span class="math display">\[
\beta(B) y_t = \mu
\]</span></p>
<p>In terms of <strong>SARIMA</strong> modeling, we start with a notation as expressed below:</p>
<p><span class="math display">\[\mathbf{\text{SARIMA}(2,1,1)(1,2,1)_{4}}\]</span>.</p>
<p>Here, we break the notation into the following:</p>
<p><span class="math display">\[\begin{align}
\underbrace{
\begin{array}{rllll} 
AR(2)\ &amp;\rightarrow\ y_t &amp;= \beta_1 y_{t-1} + \beta_2 y_{t-2}\\
I(1)\ &amp;\rightarrow\ \Delta y_t &amp;= y_t -  y_{t-1}\\
MA(1)\ &amp;\rightarrow\ \mathcal{E}_t &amp;= \theta_1 \mathcal{E}_{t-1}\\
\end{array}}_{\text{non-seasonal}} \left|
\underbrace{
\begin{array}{rllll}
AR(1)\ &amp;\rightarrow\ y_t &amp;= \alpha_1 y_{t-1} \\
I(2)\ &amp;\rightarrow\ \Delta y_t &amp;= y_t -  y_{t-1} - y_{t-2}\\
MA(2)\ &amp;\rightarrow\ \mathcal{E}_t &amp;= \phi_1 \mathcal{E}_{t-1}  \\
\end{array}}_{\text{seasonal}}
\right. \label{eqn:eqnnumber505}
\end{align}\]</span></p>
<p>Using the <strong>backshift notation</strong>, we write the following:</p>
<p><span class="math display">\[\begin{align}
\underbrace{
\begin{array}{rll}
AR(2)\ &amp;\rightarrow\ (1 -\beta_1 B - \beta_2 B^2) y_t \\
I(1)\ &amp;\rightarrow\ (1 - B)y_t\\
MA(1)\ &amp;\rightarrow\ (1 - \theta_1 B )\mathcal{E}_t   \\
\end{array}}_{\text{non-seasonal}} 
\left|
\underbrace{
\begin{array}{rll}
AR(1)\ &amp;\rightarrow\ (1 - \alpha B^4)y_t  \\
I(2)\ &amp;\rightarrow\ (1 - B^4)^2y_t\\
MA(2)\ &amp;\rightarrow\ (1 -\phi_1 B^4) \mathcal{E}_t \\
\end{array}}_{\text{seasonal}}
\right. \label{eqn:eqnnumber506}
\end{align}\]</span></p>
<p>Therefore, the final <strong>SARIMA</strong> model, namely <span class="math inline">\(\text{SARIMA}(2,1,1)(1,2,1)_{\mathbf{4}}\)</span> corresponds to the following operation.</p>
<p><span class="math display">\[\begin{align}
(1 -\beta_1 B - \beta_2 B^2) (1 - B)(1 - \alpha B^4)(1 - B^4)^2y_t = (1 - \theta_1 B )(1 -\phi_1 B^4) \mathcal{E}_t
\end{align}\]</span></p>
</div>
<div id="time-series-decomposition" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.6</span> Time-Series Decomposition <a href="machinelearning3.html#time-series-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Chapter <strong>4</strong> (<strong>Numerical Calculus</strong>)}, we covered <strong>Fourier Series and Transformation</strong>. We expressed the idea that a large wave can be represented as a convolution of smaller waves such that we can, as an example, mathematically decompose the large wave equation into a series of sinusoidal terms.</p>
<p>In this section, the same idea applies. However, the opposite applies in that combining terms is equivalent to applying the effect of <strong>smoothing</strong> the series.</p>
<p>We can use smoothing methods such as splines and local regression to decompose trend, seasonality, and random noise.</p>
<p><span class="math display">\[\begin{align}
yT = f(sT, tT, rT) = sT + tT + rT
\end{align}\]</span></p>
<p>When components are multiplicative, then use <span class="math inline">\(\log_e(.)\)</span>.</p>
<p><span class="math display">\[\begin{align}
yT = f(sT, tT, rT) = log(sT) + log(tT) + log(rT)
\end{align}\]</span></p>
<p>To illustrate, let us concoct a dataset that covers 24 periods:</p>

<div class="sourceCode" id="cb1728"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1728-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1728-2" data-line-number="2">N =<span class="st"> </span><span class="dv">24</span></a>
<a class="sourceLine" id="cb1728-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1728-4" data-line-number="4">(<span class="dt">y =</span>  <span class="kw">cos</span>( <span class="kw">seq</span>(<span class="dv">0</span>, N, <span class="dt">length.out =</span> N)) <span class="op">+</span><span class="st"> </span>e )</a></code></pre></div>
<pre><code>##  [1]  1.3770  0.8048 -1.5916 -2.1303 -3.3094  1.2044  1.9389  0.2930
##  [9]  1.2851 -0.8821 -1.3850  1.3734  2.1954  0.1697 -0.5775  0.8016
## [17]  1.1534 -2.5945 -1.2912  0.6182  1.7401  0.1012 -0.2509  0.3510</code></pre>

<p>For a monthly period, we can change the <strong>frequency</strong> to 12. Since our data set has a stretch of 24 periods, then we cover two years:</p>

<div class="sourceCode" id="cb1730"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1730-1" data-line-number="1">(<span class="dt">ts.data =</span> <span class="kw">ts</span> (<span class="dt">data =</span> y, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">2018</span>, <span class="dv">1</span>),  <span class="dt">frequency=</span><span class="dv">12</span>))</a></code></pre></div>
<pre><code>##          Jan     Feb     Mar     Apr     May     Jun     Jul     Aug
## 2018  1.3770  0.8048 -1.5916 -2.1303 -3.3094  1.2044  1.9389  0.2930
## 2019  2.1954  0.1697 -0.5775  0.8016  1.1534 -2.5945 -1.2912  0.6182
##          Sep     Oct     Nov     Dec
## 2018  1.2851 -0.8821 -1.3850  1.3734
## 2019  1.7401  0.1012 -0.2509  0.3510</code></pre>

<p>Furthermore, for a quarterly period, we can change the <strong>frequency</strong> to 4. Similarly, since our data stretches to 24 periods, then we cover six years:</p>

<div class="sourceCode" id="cb1732"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1732-1" data-line-number="1">(<span class="dt">ts.data =</span> <span class="kw">ts</span> (<span class="dt">data =</span> y, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">2018</span>, <span class="dv">1</span>),  <span class="dt">frequency=</span><span class="dv">4</span>))</a></code></pre></div>
<pre><code>##         Qtr1    Qtr2    Qtr3    Qtr4
## 2018  1.3770  0.8048 -1.5916 -2.1303
## 2019 -3.3094  1.2044  1.9389  0.2930
## 2020  1.2851 -0.8821 -1.3850  1.3734
## 2021  2.1954  0.1697 -0.5775  0.8016
## 2022  1.1534 -2.5945 -1.2912  0.6182
## 2023  1.7401  0.1012 -0.2509  0.3510</code></pre>

<p>Let us now decompose by <strong>additive</strong> method:</p>

<div class="sourceCode" id="cb1734"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1734-1" data-line-number="1">(<span class="dt">ts.decomposed =</span> <span class="kw">decompose</span>( ts.data , <span class="st">&quot;additive&quot;</span>))</a></code></pre></div>
<pre><code>## $x
##         Qtr1    Qtr2    Qtr3    Qtr4
## 2018  1.3770  0.8048 -1.5916 -2.1303
## 2019 -3.3094  1.2044  1.9389  0.2930
## 2020  1.2851 -0.8821 -1.3850  1.3734
## 2021  2.1954  0.1697 -0.5775  0.8016
## 2022  1.1534 -2.5945 -1.2912  0.6182
## 2023  1.7401  0.1012 -0.2509  0.3510
## 
## $seasonal
##         Qtr1    Qtr2    Qtr3    Qtr4
## 2018  0.6753 -0.4334 -0.5115  0.2695
## 2019  0.6753 -0.4334 -0.5115  0.2695
## 2020  0.6753 -0.4334 -0.5115  0.2695
## 2021  0.6753 -0.4334 -0.5115  0.2695
## 2022  0.6753 -0.4334 -0.5115  0.2695
## 2023  0.6753 -0.4334 -0.5115  0.2695
## 
## $trend
##          Qtr1     Qtr2     Qtr3     Qtr4
## 2018       NA       NA -0.97084 -1.50667
## 2019 -1.01542 -0.27120  0.60603  0.91954
## 2020  0.24325 -0.03718  0.21165  0.45691
## 2021  0.68931  0.71877  0.51704  0.04127
## 2022 -0.39347 -0.50561 -0.45519 -0.04488
## 2023  0.42212  0.51877       NA       NA
## 
## $random
##          Qtr1     Qtr2     Qtr3     Qtr4
## 2018       NA       NA -0.10928 -0.89318
## 2019 -2.96928  1.90897  1.84429 -0.89605
## 2020  0.36654 -0.41150 -1.08518  0.64700
## 2021  0.83073 -0.11568 -0.58310  0.49082
## 2022  0.87151 -1.65549 -0.32457  0.39357
## 2023  0.64265  0.01586       NA       NA
## 
## $figure
## [1]  0.6753 -0.4334 -0.5115  0.2695
## 
## $type
## [1] &quot;additive&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;decomposed.ts&quot;</code></pre>

<p>Now, let us try to reconstruct by adding:</p>

<div class="sourceCode" id="cb1736"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1736-1" data-line-number="1">(ts.decomposed<span class="op">$</span>seasonal <span class="op">+</span><span class="st"> </span>ts.decomposed<span class="op">$</span>trend <span class="op">+</span><span class="st"> </span>ts.decomposed<span class="op">$</span>random )</a></code></pre></div>
<pre><code>##         Qtr1    Qtr2    Qtr3    Qtr4
## 2018      NA      NA -1.5916 -2.1303
## 2019 -3.3094  1.2044  1.9389  0.2930
## 2020  1.2851 -0.8821 -1.3850  1.3734
## 2021  2.1954  0.1697 -0.5775  0.8016
## 2022  1.1534 -2.5945 -1.2912  0.6182
## 2023  1.7401  0.1012      NA      NA</code></pre>

<p>Let us also decompose by a <strong>multiplicative</strong> method and try to reconstruct by multiplying. We should see the same time-series data.</p>

<div class="sourceCode" id="cb1738"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1738-1" data-line-number="1">ts.decomposed =<span class="st"> </span><span class="kw">decompose</span>( ts.data , <span class="st">&quot;multiplicative&quot;</span>)</a>
<a class="sourceLine" id="cb1738-2" data-line-number="2">(ts.decomposed<span class="op">$</span>seasonal <span class="op">*</span><span class="st"> </span>ts.decomposed<span class="op">$</span>trend <span class="op">*</span><span class="st"> </span>ts.decomposed<span class="op">$</span>random )</a></code></pre></div>
<pre><code>##         Qtr1    Qtr2    Qtr3    Qtr4
## 2018      NA      NA -1.5916 -2.1303
## 2019 -3.3094  1.2044  1.9389  0.2930
## 2020  1.2851 -0.8821 -1.3850  1.3734
## 2021  2.1954  0.1697 -0.5775  0.8016
## 2022  1.1534 -2.5945 -1.2912  0.6182
## 2023  1.7401  0.1012      NA      NA</code></pre>

<p>We can then plot and see what it looks like. See Figure <a href="machinelearning3.html#fig:tsdecompose">11.25</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tsdecompose"></span>
<img src="DS_files/figure-html/tsdecompose-1.png" alt="Time-Series Decomposition" width="70%" />
<p class="caption">
Figure 11.25: Time-Series Decomposition
</p>
</div>

<p>In the next section, let us perform prediction using <strong>ARIMA/SARIMA</strong>.</p>
</div>
<div id="stl-with-aicbic" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.7</span> STL with AIC/BIC<a href="machinelearning3.html#stl-with-aicbic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In our recent discussion on <strong>Arima and Sarima</strong>, we have introduced models in the form of linear equations. Such linear equations, in a <strong>Linear Model</strong>, as discussed in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>) under <strong>AIC</strong> and <strong>BIC</strong>, can be transformed such that the <strong>coefficients</strong> are evaluated for <strong>relevance</strong>.</p>
<p>To illustrate, let us use <strong>stlf(.)</strong> to apply <strong>ARIMA</strong> method and <strong>AIC</strong> for <strong>information criteria</strong> to our recent <strong>ts.data</strong> time series in the previous section. Here, we try to forecast what it looks like for the first two quarters in the year following the time series, namely 2024.</p>

<div class="sourceCode" id="cb1740"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1740-1" data-line-number="1"><span class="kw">library</span>(forecast)</a>
<a class="sourceLine" id="cb1740-2" data-line-number="2"><span class="kw">stlf</span>(ts.data, <span class="dt">h=</span><span class="dv">2</span>,  <span class="dt">method=</span><span class="st">&#39;arima&#39;</span>, <span class="dt">ic=</span><span class="st">&#39;bic&#39;</span>)</a></code></pre></div>
<pre><code>##         Point Forecast   Lo 80 Hi 80  Lo 95 Hi 95
## 2024 Q1         0.7659 -0.9617 2.493 -1.876 3.408
## 2024 Q2        -0.4565 -2.1840 1.271 -3.099 2.186</code></pre>

<p>Another method of interest is <strong>Error Trend Seasonality (ETS)</strong>. Again, let us use <strong>AIC</strong> this time with <strong>MAE</strong> for optimization criteria.</p>

<div class="sourceCode" id="cb1742"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1742-1" data-line-number="1"><span class="kw">stlf</span>(ts.data, <span class="dt">h=</span><span class="dv">2</span>,  <span class="dt">method=</span><span class="st">&#39;ets&#39;</span>, <span class="dt">ic=</span><span class="st">&#39;aic&#39;</span>, <span class="dt">opt.crit=</span><span class="st">&#39;mae&#39;</span>)</a></code></pre></div>
<pre><code>##         Point Forecast   Lo 80 Hi 80  Lo 95 Hi 95
## 2024 Q1        1.20850 -0.6654 3.082 -1.657 4.074
## 2024 Q2       -0.01387 -1.8878 1.860 -2.880 2.852</code></pre>

<p>We leave readers to investigate <strong>ETS</strong>.</p>
</div>
<div id="multivariate-time-series" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.8</span> Multivariate Time-Series<a href="machinelearning3.html#multivariate-time-series" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In real-world scenarios, time series can be complex. This section is to account for multivariate features in a time series which we can base on the following linear equation with two independent variables and one dependent variable:</p>
<p><span class="math display">\[\begin{align}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\end{align}\]</span></p>
<p>We can concoct a simple multivariate data point (using sine with noise) like so:</p>

<div class="sourceCode" id="cb1744"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1744-1" data-line-number="1">N     =<span class="st"> </span><span class="dv">12</span></a>
<a class="sourceLine" id="cb1744-2" data-line-number="2">x1    =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,N) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="dv">1</span>)     </a>
<a class="sourceLine" id="cb1744-3" data-line-number="3">x2    =<span class="st"> </span><span class="kw">rep</span>( <span class="kw">sin</span>(<span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">4</span>)), <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dv">0</span>, <span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1744-4" data-line-number="4">beta0 =<span class="st"> </span><span class="fl">0.5</span>; beta1 =<span class="st"> </span><span class="fl">0.7</span>; beta2 =<span class="st"> </span><span class="fl">1.7</span></a>
<a class="sourceLine" id="cb1744-5" data-line-number="5">y     =<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>beta2 <span class="op">*</span><span class="st"> </span>x2</a>
<a class="sourceLine" id="cb1744-6" data-line-number="6">(<span class="dt">m    =</span> <span class="kw">as.matrix</span>(<span class="kw">cbind</span>(x1,x2, y)))</a></code></pre></div>
<pre><code>##           x1      x2      y
##  [1,]  1.834  0.5559  2.729
##  [2,]  2.199  0.9853  3.714
##  [3,]  4.298 -0.4192  2.796
##  [4,]  4.937 -0.3096  3.429
##  [5,]  4.853  1.7500  6.872
##  [6,]  6.110  0.4042  5.465
##  [7,]  6.187 -0.1599  4.559
##  [8,]  7.256 -1.4828  3.059
##  [9,] 10.095 -0.3386  6.991
## [10,] 12.435  1.1624 11.181
## [11,] 11.388 -0.2296  8.081
## [12,] 12.291 -0.7346  7.855</code></pre>

<p>Then converting to time-series, our <strong>time-series</strong> starts from the year 2018 and stretches through 2020 (on a quarterly basis):</p>

<div class="sourceCode" id="cb1746"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1746-1" data-line-number="1">(<span class="dt">ts.multiv =</span> <span class="kw">ts</span>(m, <span class="dt">start =</span><span class="kw">c</span>(<span class="dv">2018</span>,<span class="dv">1</span>), <span class="dt">frequency=</span><span class="dv">4</span>))</a></code></pre></div>
<pre><code>##             x1      x2      y
## 2018 Q1  1.834  0.5559  2.729
## 2018 Q2  2.199  0.9853  3.714
## 2018 Q3  4.298 -0.4192  2.796
## 2018 Q4  4.937 -0.3096  3.429
## 2019 Q1  4.853  1.7500  6.872
## 2019 Q2  6.110  0.4042  5.465
## 2019 Q3  6.187 -0.1599  4.559
## 2019 Q4  7.256 -1.4828  3.059
## 2020 Q1 10.095 -0.3386  6.991
## 2020 Q2 12.435  1.1624 11.181
## 2020 Q3 11.388 -0.2296  8.081
## 2020 Q4 12.291 -0.7346  7.855</code></pre>

<p>We can then plot and see what it looks like. See Figure <a href="machinelearning3.html#fig:multivseries">11.26</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:multivseries"></span>
<img src="DS_files/figure-html/multivseries-1.png" alt="Multivariate Time-Series" width="70%" />
<p class="caption">
Figure 11.26: Multivariate Time-Series
</p>
</div>

<p>Let us fit our model to data and predict (forecast what it looks like in the year 2021, covering the first two quarters):</p>

<div class="sourceCode" id="cb1748"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1748-1" data-line-number="1">ts.fit =<span class="st"> </span><span class="kw">tslm</span>(ts.multiv <span class="op">~</span><span class="st"> </span>trend <span class="op">+</span><span class="st"> </span>season)    </a>
<a class="sourceLine" id="cb1748-2" data-line-number="2">(<span class="dt">f  =</span> <span class="kw">forecast</span>(ts.fit, <span class="dt">h=</span><span class="dv">2</span>))   </a></code></pre></div>
<pre><code>## x1
##         Point Forecast Lo 80 Hi 80 Lo 95 Hi 95
## 2021 Q1          13.83 11.79 15.86 10.43 17.23
## 2021 Q2          15.15 13.12 17.19 11.75 18.55
## 
## x2
##         Point Forecast   Lo 80 Hi 80  Lo 95 Hi 95
## 2021 Q1         0.4175 -0.8653 1.700 -1.726 2.561
## 2021 Q2         0.6124 -0.6704 1.895 -1.532 2.756
## 
## y
##         Point Forecast Lo 80 Hi 80 Lo 95 Hi 95
## 2021 Q1          10.89 8.225 13.56 6.437 15.34
## 2021 Q2          12.15 9.481 14.81 7.693 16.60</code></pre>

<p>Note that <strong>Multivariate Time-Series</strong> is different from <strong>Multiple Time-Series</strong>. The former pertains to dependent variables, namely <strong>X</strong>, while the latter pertains to independent variables, namely <strong>Y</strong>. Additionally, the latter considers different time series in different state spaces, depending on different factors and conditions; hence <strong>multiple series</strong>. We cover that in the next section.</p>
</div>
<div id="forecasting-considerations" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.9</span> Forecasting Considerations<a href="machinelearning3.html#forecasting-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are many considerations to make when dealing with time-series data and forecasting. Our time-series data is mixed with noise and many other factors in real-world scenarios. Our goal is to perform some level of pre-processing to clean our data.</p>
<p><strong>Denoising Time-series</strong></p>
<p>One of the essential preparations to make is to reduce the amount of noise from our data. There are many methods of denoising time series. Each method may be more specific and relative to certain domains. In general, however, we are listing below a few suggested methods:</p>
<ul>
<li><strong>Rolling and Expanding Window</strong> method</li>
<li><strong>Wavelet Transform</strong> method</li>
<li><strong>Spline Smoothing</strong> method</li>
<li><strong>Convolution</strong> method</li>
</ul>
<p>We leave readers to investigate each of the mentioned denoising methods.</p>
<p><strong>Sampling Time-Series</strong></p>
<p>Another important step to make is when sampling our data. Take <strong>market trading</strong> as an example. Certain period is affected by <strong>Job Employment status</strong>, <strong>10-year notes status</strong>, <strong>market cycles</strong>, <strong>industry cycle</strong>, <strong>fiscal reports</strong>, and now including the recent <strong>Covid-19 Pandemic</strong>. When sampling for data, do we need to skip the year 2020, considering the unique condition compared to any other periods? For this reason, there are cases when we need to sample and model our forecast based on different conditions, <strong>multiple series</strong> in state space.</p>
<p><strong>Lag features</strong></p>
<p><strong>Lags</strong> in time series can be helpful in some cases with which we can generate new features. Using our original multivariate time series, we can use the <strong>Lags</strong> from the target variable, namely <strong>y</strong>, by shifting one lag and using the result as a new feature, namely <strong>x3</strong>.</p>

<div class="sourceCode" id="cb1750"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1750-1" data-line-number="1">m</a></code></pre></div>
<pre><code>##           x1      x2      y
##  [1,]  1.834  0.5559  2.729
##  [2,]  2.199  0.9853  3.714
##  [3,]  4.298 -0.4192  2.796
##  [4,]  4.937 -0.3096  3.429
##  [5,]  4.853  1.7500  6.872
##  [6,]  6.110  0.4042  5.465
##  [7,]  6.187 -0.1599  4.559
##  [8,]  7.256 -1.4828  3.059
##  [9,] 10.095 -0.3386  6.991
## [10,] 12.435  1.1624 11.181
## [11,] 11.388 -0.2296  8.081
## [12,] 12.291 -0.7346  7.855</code></pre>

<p>Below is an example of shifting <strong>lag</strong>:</p>

<div class="sourceCode" id="cb1752"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1752-1" data-line-number="1"><span class="kw">library</span>(data.table)</a>
<a class="sourceLine" id="cb1752-2" data-line-number="2">(<span class="dt">x3 =</span> <span class="kw">shift</span>(m[,<span class="dv">3</span>], <span class="dt">n=</span><span class="dv">1</span>, <span class="dt">fill=</span><span class="ot">NA</span>, <span class="dt">type=</span><span class="st">&quot;lag&quot;</span>) )</a></code></pre></div>
<pre><code>##  [1]     NA  2.729  3.714  2.796  3.429  6.872  5.465  4.559  3.059
## [10]  6.991 11.181  8.081</code></pre>

<p>We then incorporate the shifted series back into the matrix in the form of a new feature.</p>

<div class="sourceCode" id="cb1754"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1754-1" data-line-number="1"><span class="kw">as.matrix</span>(<span class="kw">data.frame</span>(<span class="dt">x1 =</span> m[,<span class="dv">1</span>], <span class="dt">x2 =</span> m[,<span class="dv">2</span>], <span class="dt">x3 =</span> x3, <span class="dt">y =</span> m[,<span class="dv">3</span>]))</a></code></pre></div>
<pre><code>##           x1      x2     x3      y
##  [1,]  1.834  0.5559     NA  2.729
##  [2,]  2.199  0.9853  2.729  3.714
##  [3,]  4.298 -0.4192  3.714  2.796
##  [4,]  4.937 -0.3096  2.796  3.429
##  [5,]  4.853  1.7500  3.429  6.872
##  [6,]  6.110  0.4042  6.872  5.465
##  [7,]  6.187 -0.1599  5.465  4.559
##  [8,]  7.256 -1.4828  4.559  3.059
##  [9,] 10.095 -0.3386  3.059  6.991
## [10,] 12.435  1.1624  6.991 11.181
## [11,] 11.388 -0.2296 11.181  8.081
## [12,] 12.291 -0.7346  8.081  7.855</code></pre>

<p>From there, we can perform forecasting.</p>
<p>The number of shifts can be determined perhaps based on cycles or frequencies:</p>

<div class="sourceCode" id="cb1756"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1756-1" data-line-number="1"><span class="kw">cycle</span>(ts.multiv)</a></code></pre></div>
<pre><code>##      Qtr1 Qtr2 Qtr3 Qtr4
## 2018    1    2    3    4
## 2019    1    2    3    4
## 2020    1    2    3    4</code></pre>
<div class="sourceCode" id="cb1758"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1758-1" data-line-number="1"><span class="kw">frequency</span>(ts.multiv)</a></code></pre></div>
<pre><code>## [1] 4</code></pre>

<p><strong>Panel data</strong> and <strong>Pooled data</strong></p>
<p>We end our discussion of <strong>Time-Series</strong> in this section by introducing <strong>Panel data</strong>, also called <strong>Longitudinal data</strong>, borrowed from <strong>Statistics and Econometrics</strong> theory. This type of data combines both <strong>Time-Series</strong> data and <strong>Cross-Sectional</strong> data. To explain the difference between the two, as an example for the former, <strong>Time-Series</strong> data is a collection or group of observations for a single variable, e.g., market stock price, over time. As for the latter, we have a collection of observations for multiple variables, e.g., companies and job employment status, over the same period.</p>
<p>In some cases, <strong>Pooled data</strong> is interpreted as <strong>Panel data</strong>. However, to be more concrete, when we have a collection of <strong>cross-sectional</strong> data over time, we refer to it as <strong>Pooled data</strong>. If we have a <strong>cross-sectional</strong> data sampled multiple times, we regard it as <strong>Panel data</strong>. For example, it becomes <strong>Pooled data</strong> for cross-sectional data collected over two years. It then becomes <strong>Panel data</strong> for cross-sectional data collected every five years - e.g., collecting various factors affecting climate change every five years.</p>
<p>Also, in some cases, we find terms such as <strong>Stacked Time-Series</strong>, <strong>Stacked Cross-Section</strong>, and <strong>Pooled data</strong>. As the term implies, multiple <strong>Time-Series</strong> data can be stacked on top of each other (if visually plotted). Similarly, the same applies to multiple <strong>Cross-Sectional</strong> data <strong>Stacked Time-Series</strong>.</p>
<p><strong>Fixed Effect Model vs. Random Effect Model</strong></p>
<p>It helps to be familiar when dealing with <strong>Time-Series</strong>, with two models mainly introduced in <strong>Statistics</strong> and <strong>Econometrics</strong>. The first is <strong>Fixed Effects</strong> model, which describes modeling data that are fixed in nature. Examples of such data that are constant and not changing are the sex and nationality of an individual. On the other hand, <strong>Random Effect Model</strong> changes over time, such as market price.</p>
<p>For date utilities that we can use for forecasting, please see the Appendix for the <strong>lubridate</strong> package.</p>
</div>
</div>
<div id="recommender-systems" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.5</span> Recommender Systems <a href="machinelearning3.html#recommender-systems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recommender Systems</strong> are systems that filter and suggest the most relevant information to users. When it comes to relevant information, barring <strong>privacy and confidentiality</strong>, we mine as much valuable and relevant information as we can, such as the following few examples:</p>
<ul>
<li><strong>Customer Information or Profile</strong></li>
<li><strong>Customer Choices and Interactions</strong></li>
<li><strong>Product Properties</strong></li>
<li><strong>Events and Activities</strong></li>
<li><strong>Relationships of Information and Interactions</strong></li>
<li><strong>Information that signifies popularity, frequency, and trend</strong></li>
</ul>
<p>There are two types of <strong>Filtering</strong> of information:</p>
<ul>
<li><p><strong>Collaborative-based Filtering</strong> - this Filtering accounts for customer collaboration in reactions. Reactions can be measured based on two types of interaction:</p>
<ul>
<li><p><strong>Explicit</strong> interaction - A user may directly rank, rate, or vote for a product.</p></li>
<li><p><strong>Implicit</strong> interaction - A user may repeatedly visit a product page.</p></li>
</ul></li>
<li><p><strong>Content-based Filtering</strong> - this Filtering accounts for product information in customer feedback, reviews, opinions, and suggestions. Through customer feedback, we can determine similar interests in a particular product. In addition, we can compare top product features that overlap with user interests.</p></li>
</ul>
<p>To illustrate further, Figure <a href="machinelearning3.html#fig:recommender">11.27</a> shows the two types of <strong>Information Filtering</strong>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:recommender"></span>
<img src="recommender.png" alt="Information Filtering" width="60%" />
<p class="caption">
Figure 11.27: Information Filtering
</p>
</div>

<p>The first type in the figure shows a table of customer ratings over movies. Here, users seem to have ranked <strong>Movie 1</strong> higher than the others. The second type in the figure shows two tables - the first table records movies and corresponding features. The other table records users and their corresponding preferences. Here, two users prefer <strong>Drama</strong> movies filmed at least since 2005.</p>
<p>In real-world scenarios, we deal with a much larger matrix that is highly sparsed, meaning there is a large number of missing data. Below, we simulate a sparsed <strong>utility</strong> matrix. We use 25% sparsity to simulate missing data.</p>

<div class="sourceCode" id="cb1760"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1760-1" data-line-number="1"><span class="kw">library</span>(Matrix)</a>
<a class="sourceLine" id="cb1760-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1760-3" data-line-number="3">max.rating =<span class="st"> </span><span class="dv">5</span>; items =<span class="st"> </span><span class="dv">5</span>; users =<span class="st"> </span><span class="dv">10</span>;  sparsity =<span class="st"> </span><span class="fl">0.25</span>; N =<span class="st"> </span>items <span class="op">*</span><span class="st"> </span>users; </a>
<a class="sourceLine" id="cb1760-4" data-line-number="4">m.size =<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>sparsity) <span class="op">*</span><span class="st"> </span>N; X =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb1760-5" data-line-number="5">cells =<span class="st"> </span><span class="kw">sample</span>(N, <span class="dt">size=</span>m.size, <span class="dt">replace =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1760-6" data-line-number="6">X[cells] =<span class="st"> </span><span class="kw">round</span>(<span class="kw">runif</span>(m.size, <span class="dt">min=</span><span class="dv">1</span>, <span class="dt">max=</span>max.rating), <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1760-7" data-line-number="7">A =<span class="st"> </span><span class="kw">matrix</span>(X, <span class="dt">nrow=</span>users, <span class="dt">ncol=</span>items, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1760-8" data-line-number="8"><span class="kw">colnames</span>(A) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;Item&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,items))</a>
<a class="sourceLine" id="cb1760-9" data-line-number="9"><span class="kw">rownames</span>(A) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;User&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,users))</a>
<a class="sourceLine" id="cb1760-10" data-line-number="10">A</a></code></pre></div>
<pre><code>##        Item1 Item2 Item3 Item4 Item5
## User1      3     4     3     4     2
## User2      5     2     0     3     3
## User3      0     5     0     0     2
## User4      3     3     1     5     3
## User5      0     4     5     0     0
## User6      3     2     3     0     2
## User7      4     2     5     0     0
## User8      0     3     4     3     3
## User9      2     5     3     2     0
## User10     0     3     4     3     3</code></pre>

<p>Now, let us walk through how a simple <strong>Recommender System</strong> works.</p>
<p><strong>First</strong>, given the raw <strong>utility matrix</strong> above, let us perform <strong>normalization</strong> to eliminate extreme ratings, e.g., a user giving a rating of five on all items.</p>
<p><span class="math display">\[\begin{align}
\mathbf{b}_{ij} = \mu  + \mathbf{b}_{i} + \mathbf{b}_j
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{u}\)</span> - global average</li>
<li><span class="math inline">\(\mathbf{b}_i\)</span> - average rating of a user</li>
<li><span class="math inline">\(\mathbf{b}_j\)</span> - average rating of an item</li>
</ul>
<p>Let us now normalize our matrix and scale it back to the rating range of [0, 5].</p>

<div class="sourceCode" id="cb1762"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1762-1" data-line-number="1">normalize &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb1762-2" data-line-number="2">  mu =<span class="st"> </span><span class="kw">mean</span>(A)</a>
<a class="sourceLine" id="cb1762-3" data-line-number="3">  I  =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb1762-4" data-line-number="4">  J  =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb1762-5" data-line-number="5">  AA =<span class="st">  </span><span class="kw">matrix</span>(<span class="dv">0</span>, I, J, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1762-6" data-line-number="6">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>I) {</a>
<a class="sourceLine" id="cb1762-7" data-line-number="7">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>J) {</a>
<a class="sourceLine" id="cb1762-8" data-line-number="8">      <span class="cf">if</span> (A[i,j] <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1762-9" data-line-number="9">        bi =<span class="st"> </span>A[i,]; bi =<span class="st"> </span><span class="kw">mean</span>( bi[ <span class="kw">which</span>( bi <span class="op">!=</span><span class="st"> </span><span class="dv">0</span> ) ] )</a>
<a class="sourceLine" id="cb1762-10" data-line-number="10">        bj =<span class="st"> </span>A[,j]; bj =<span class="st"> </span><span class="kw">mean</span>( bj[ <span class="kw">which</span>( bj <span class="op">!=</span><span class="st"> </span><span class="dv">0</span> ) ] )</a>
<a class="sourceLine" id="cb1762-11" data-line-number="11">        AA[i,j] =<span class="st"> </span><span class="kw">round</span>( mu <span class="op">+</span><span class="st"> </span>bi <span class="op">+</span><span class="st"> </span>bj, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1762-12" data-line-number="12">      }</a>
<a class="sourceLine" id="cb1762-13" data-line-number="13">    }</a>
<a class="sourceLine" id="cb1762-14" data-line-number="14">  }</a>
<a class="sourceLine" id="cb1762-15" data-line-number="15">  min.A =<span class="st"> </span><span class="kw">min</span>(AA)</a>
<a class="sourceLine" id="cb1762-16" data-line-number="16">  max.A =<span class="st"> </span><span class="kw">max</span>(AA)</a>
<a class="sourceLine" id="cb1762-17" data-line-number="17">  AA =<span class="st"> </span>(<span class="dv">5</span> <span class="op">-</span><span class="st"> </span><span class="dv">0</span>) <span class="op">*</span><span class="st"> </span>(AA <span class="op">-</span><span class="st"> </span>min.A) <span class="op">/</span><span class="st"> </span>(max.A <span class="op">-</span><span class="st"> </span>min.A) <span class="op">+</span><span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1762-18" data-line-number="18">  <span class="kw">colnames</span>(AA) =<span class="st"> </span><span class="kw">colnames</span>(A)</a>
<a class="sourceLine" id="cb1762-19" data-line-number="19">  <span class="kw">rownames</span>(AA) =<span class="st"> </span><span class="kw">rownames</span>(A)</a>
<a class="sourceLine" id="cb1762-20" data-line-number="20">  AA</a>
<a class="sourceLine" id="cb1762-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb1762-22" data-line-number="22">(<span class="dt">A =</span> <span class="kw">round</span>( <span class="kw">normalize</span>(A), <span class="dv">1</span>))</a></code></pre></div>
<pre><code>##        Item1 Item2 Item3 Item4 Item5
## User1    4.3   4.3   4.4   4.3   3.9
## User2    4.3   4.3   0.0   4.3   3.9
## User3    0.0   4.4   0.0   0.0   4.1
## User4    4.2   4.2   4.3   4.2   3.8
## User5    0.0   4.9   5.0   0.0   0.0
## User6    3.9   3.9   4.0   0.0   3.6
## User7    4.5   4.5   4.6   0.0   0.0
## User8    0.0   4.3   4.4   4.3   3.9
## User9    4.2   4.2   4.3   4.2   0.0
## User10   0.0   4.3   4.4   4.3   3.9</code></pre>

<p><strong>Second</strong>, our next goal is to deal with sparsity. Note that a high sparse matrix may not provide correct and accurate recommendations. For this reason, more information may be required, such as relying on <strong>hybrid</strong> or <strong>content-based</strong> information filtering. In any case, let us assume some decent amount of sparsity. Here, we need to approximate the values of the missing data. The easiest way is to use <strong>SVD</strong>. Please refer to our discussion of <strong>SVD</strong> under <strong>Matrix Factorization</strong> in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra I</strong>), our discussion of <strong>PCA</strong> under <strong>Feature Engineering</strong> in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>), and our discussion of <strong>Topic Modeling</strong> under <strong>NLP</strong> in Chapter <strong>11</strong> (<strong>Computational Learning III</strong>).</p>
<p>Let us now run <strong>svd(.)</strong> to obtain <strong>U</strong>, <strong>D</strong>, and <strong>V</strong>. Here, <strong>U</strong> corresponds to <strong>Users</strong> and their relationships with the <strong>latent factors</strong>, namely <strong>D</strong>. Similarly, <strong>V</strong> corresponds to <strong>Items</strong> and their similarities with the <strong>latent factors</strong>. The <strong>D</strong> is the singular eigenvalue ordered in increasing order that indicates the strength of the **latent factor. See below:</p>

<div class="sourceCode" id="cb1764"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1764-1" data-line-number="1">(<span class="dt">M =</span> <span class="kw">svd</span>(A))</a></code></pre></div>
<pre><code>## $d
## [1] 23.207  7.225  6.333  5.271  2.936
## 
## $u
##          [,1]     [,2]     [,3]     [,4]     [,5]
##  [1,] -0.4031  0.07690  0.18355  0.09401  0.26989
##  [2,] -0.3093  0.40194  0.45605 -0.15448 -0.44574
##  [3,] -0.1749  0.30866 -0.25647 -0.63324 -0.32890
##  [4,] -0.3936  0.07411  0.17932  0.09270  0.26242
##  [5,] -0.2277 -0.43402 -0.50546 -0.05542 -0.32551
##  [6,] -0.3017 -0.17395  0.05309 -0.47428  0.52959
##  [7,] -0.2808 -0.57111  0.14391 -0.21386 -0.09417
##  [8,] -0.3348  0.24117 -0.39801  0.25017  0.07550
##  [9,] -0.3323 -0.26578  0.25407  0.39848 -0.38046
## [10,] -0.3348  0.24117 -0.39801  0.25017  0.07550
## 
## $v
##         [,1]     [,2]    [,3]    [,4]    [,5]
## [1,] -0.3685 -0.27603  0.8566 -0.1914  0.1327
## [2,] -0.5738 -0.09535 -0.2531 -0.3634 -0.6823
## [3,] -0.4946 -0.53375 -0.3922  0.2977  0.4775
## [4,] -0.3874  0.46061  0.1812  0.7502 -0.2053
## [5,] -0.3744  0.64627 -0.1246 -0.4242  0.4967</code></pre>

<p>If we regard this from the perspective of <strong>PCA</strong>, we are looking to see which eigenvalues are relevant based on the <strong>latent factors</strong>. For example, using <strong>K=2</strong>, let us reconstruct our matrix by reducing only the first two columns of <strong>U</strong>, <strong>D</strong>, and <strong>V</strong> as so:</p>

<div class="sourceCode" id="cb1766"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1766-1" data-line-number="1">k =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1766-2" data-line-number="2">A.hat =<span class="st"> </span>M<span class="op">$</span>u[,<span class="dv">1</span><span class="op">:</span>k ] <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(M<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span>k ]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(M<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>k ])</a>
<a class="sourceLine" id="cb1766-3" data-line-number="3"><span class="kw">rownames</span>(A.hat )  =<span class="st"> </span><span class="kw">rownames</span>(A)</a>
<a class="sourceLine" id="cb1766-4" data-line-number="4"><span class="kw">colnames</span>(A.hat )  =<span class="st"> </span><span class="kw">colnames</span>(A)</a>
<a class="sourceLine" id="cb1766-5" data-line-number="5"><span class="kw">round</span>(A.hat ,<span class="dv">0</span>)</a></code></pre></div>
<pre><code>##        Item1 Item2 Item3 Item4 Item5
## User1      3     5     4     4     4
## User2      2     4     2     4     5
## User3      1     2     1     3     3
## User4      3     5     4     4     4
## User5      3     3     4     1     0
## User6      3     4     4     2     2
## User7      4     4     5     1     0
## User8      2     4     3     4     4
## User9      3     5     5     2     2
## User10     2     4     3     4     4</code></pre>

<p>By choosing <strong>K=2</strong>, let us see if we closely match the original matrix. Here, we use <strong>SSE</strong> for our <strong>loss function</strong>. Other monotonically equivalent measures can be used, such as <strong>MSE</strong> or <strong>RMSE</strong> Additionally, regularization such as <strong>L1</strong> and <strong>L2</strong> as discussed in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>) can be added to the <strong>loss function</strong>:</p>
<p><span class="math display">\[\begin{align}
SSE =  \sum_{ij \in A} \left(A_{ij} - \hat{A}_{ij}\right)^2 
\end{align}\]</span></p>
<p>For example:</p>

<div class="sourceCode" id="cb1768"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1768-1" data-line-number="1">(<span class="dt">SSE =</span> <span class="kw">sum</span>((A <span class="op">-</span><span class="st"> </span>A.hat)<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 76.52</code></pre>

<p>If we choose to use <strong>K=3</strong>, we obtain the following SSE:</p>

<div class="sourceCode" id="cb1770"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1770-1" data-line-number="1">k =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb1770-2" data-line-number="2">A.hat =<span class="st"> </span>M<span class="op">$</span>u[,<span class="dv">1</span><span class="op">:</span>k ] <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(M<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span>k ]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(M<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>k ])</a>
<a class="sourceLine" id="cb1770-3" data-line-number="3">(<span class="dt">SSE =</span> <span class="kw">sum</span>((A <span class="op">-</span><span class="st"> </span>A.hat)<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 36.41</code></pre>

<p>Notice that the <strong>sum squared error (SSE)</strong> is lesser, making it appropriate to choose <strong>K=3</strong>.</p>
<p>Below is an example implementation of our cross-validation to determine which <strong>K</strong> to use based on <strong>SSE</strong>:</p>

<div class="sourceCode" id="cb1772"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1772-1" data-line-number="1">xvalidate &lt;-<span class="st"> </span><span class="cf">function</span>(A, M) {</a>
<a class="sourceLine" id="cb1772-2" data-line-number="2">  N =<span class="st"> </span><span class="kw">length</span>(M<span class="op">$</span>d)</a>
<a class="sourceLine" id="cb1772-3" data-line-number="3">  SSE =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">Inf</span>, N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1772-4" data-line-number="4">  K   =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1772-5" data-line-number="5">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1772-6" data-line-number="6">    A.hat =<span class="st"> </span>M<span class="op">$</span>u[,<span class="dv">1</span><span class="op">:</span>k ] <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(M<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span>k ]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(M<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>k ])</a>
<a class="sourceLine" id="cb1772-7" data-line-number="7">    SSE[k <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span><span class="kw">sum</span>((A <span class="op">-</span><span class="st"> </span>A.hat)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1772-8" data-line-number="8">    K[ k <span class="op">-</span><span class="st"> </span><span class="dv">1</span>]  =<span class="st"> </span>k</a>
<a class="sourceLine" id="cb1772-9" data-line-number="9">  }</a>
<a class="sourceLine" id="cb1772-10" data-line-number="10">  <span class="kw">list</span>(<span class="st">&quot;SSE&quot;</span> =<span class="st"> </span>SSE, <span class="st">&quot;K&quot;</span> =<span class="st"> </span>K)</a>
<a class="sourceLine" id="cb1772-11" data-line-number="11">}</a></code></pre></div>

<p>Now, let us plot (See Figure <a href="machinelearning3.html#fig:sserecommend">11.28</a>):</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sserecommend"></span>
<img src="DS_files/figure-html/sserecommend-1.png" alt="Cross-Validation for (SVD)" width="70%" />
<p class="caption">
Figure 11.28: Cross-Validation for (SVD)
</p>
</div>

<p>Excluding <strong>zeroes</strong>, the plot clearly shows that <strong>K=4</strong> has the least <strong>SSE</strong>. The reconstructed matrix shows as:</p>

<div class="sourceCode" id="cb1773"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1773-1" data-line-number="1">k =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb1773-2" data-line-number="2">A.hat =<span class="st"> </span>M<span class="op">$</span>u[,<span class="dv">1</span><span class="op">:</span>k ] <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(M<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span>k ]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(M<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>k ])</a>
<a class="sourceLine" id="cb1773-3" data-line-number="3"><span class="kw">rownames</span>(A.hat )  =<span class="st"> </span><span class="kw">rownames</span>(A)</a>
<a class="sourceLine" id="cb1773-4" data-line-number="4"><span class="kw">colnames</span>(A.hat )  =<span class="st"> </span><span class="kw">colnames</span>(A)</a>
<a class="sourceLine" id="cb1773-5" data-line-number="5">(<span class="dt">A.hat =</span> <span class="kw">round</span>(A.hat, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>##        Item1 Item2 Item3 Item4 Item5
## User1   4.19  4.84  4.02  4.46  3.51
## User2   4.47  3.41  0.62  4.03  4.55
## User3   0.13  3.74  0.46 -0.20  4.58
## User4   4.10  4.73  3.93  4.36  3.42
## User5   0.13  4.25  5.46 -0.20  0.47
## User6   3.69  4.96  3.26  0.32  2.83
## User7   4.54  4.31  4.73 -0.06  0.14
## User8  -0.03  4.45  4.29  4.35  3.79
## User9   4.35  3.44  4.83  3.97  0.55
## User10 -0.03  4.45  4.29  4.35  3.79</code></pre>

<p>Now in terms of recommending a list of items, suppose we have a new user with the following rating:</p>
<div class="sourceCode" id="cb1775"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1775-1" data-line-number="1">new.user =<span class="st">  </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">0</span>) </a></code></pre></div>
<p>Two common measures are used in <strong>Recommender Systems</strong>, namely <strong>Pearson Correlation</strong> and <strong>Cosine Similarity</strong>. In our case, we use <strong>Cosine Similarity</strong> to illustrate (recalling the equation below). Here, we compare the similarity of user <strong>i</strong> and user <strong>j</strong> for all items.</p>
<p><span class="math display">\[\begin{align}
sim(\vec{r_i}, \vec{r_j}) = cos(\vec{r_i}, \vec{r_j}) = \frac{\vec{r_i} \cdot \vec{r_j}}{\|\vec{r_i}\|_2 \times \|\vec{r_j}\|_2}
\end{align}\]</span></p>

<div class="sourceCode" id="cb1776"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1776-1" data-line-number="1">sim &lt;-<span class="st"> </span><span class="cf">function</span>(ri, rj) {</a>
<a class="sourceLine" id="cb1776-2" data-line-number="2">    <span class="kw">sum</span>( ri <span class="op">*</span><span class="st"> </span>rj ) <span class="op">/</span><span class="st"> </span>(  <span class="kw">sqrt</span>( <span class="kw">sum</span>(ri<span class="op">^</span><span class="dv">2</span>) ) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>( <span class="kw">sum</span>(rj<span class="op">^</span><span class="dv">2</span>) ) )</a>
<a class="sourceLine" id="cb1776-3" data-line-number="3">}</a></code></pre></div>

<p>Let us use our reconstructed matrix to determine if the first user and new user have similar interests based on the result of the <strong>Cosine Similarity</strong> like so:</p>

<div class="sourceCode" id="cb1777"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1777-1" data-line-number="1">ri =<span class="st"> </span>first.user =<span class="st"> </span>A.hat[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb1777-2" data-line-number="2">rj =<span class="st"> </span>new.user</a>
<a class="sourceLine" id="cb1777-3" data-line-number="3">(<span class="dt">cos.sim =</span> <span class="kw">sim</span>(ri, rj))</a></code></pre></div>
<pre><code>## [1] 0.6628</code></pre>

<p>We obtain a cosine similarity of 0.6628. A value closer to 1 means strong similarity, a value closer to -1 means opposite similarity, and a zero value indicates no similarity.</p>
<p>Now, let us compare all other users against the new user and see which user is likely to have the same ratings as the new user.</p>

<div class="sourceCode" id="cb1779"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1779-1" data-line-number="1">N =<span class="st"> </span><span class="kw">nrow</span>(A.hat)</a>
<a class="sourceLine" id="cb1779-2" data-line-number="2">similar =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb1779-3" data-line-number="3"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1779-4" data-line-number="4">  similar[i] =<span class="st"> </span><span class="kw">sim</span>(A.hat[i,], rj)</a>
<a class="sourceLine" id="cb1779-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1779-6" data-line-number="6"><span class="kw">names</span>(similar) =<span class="st"> </span><span class="kw">rownames</span>(A.hat)</a>
<a class="sourceLine" id="cb1779-7" data-line-number="7">sorted.users =<span class="st"> </span><span class="kw">sort</span>(similar, <span class="dt">decreasing=</span><span class="ot">TRUE</span>, <span class="dt">index.return=</span><span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb1779-8" data-line-number="8">sorted.users<span class="op">$</span>x</a></code></pre></div>
<pre><code>##  User5  User7  User6  User8 User10  User9  User4  User1  User3  User2 
## 0.9895 0.8146 0.7702 0.7309 0.7309 0.6982 0.6629 0.6628 0.5003 0.3432</code></pre>

<p>The result shows user (User5) as the top user with a cosine similarity of 0.9895 that has a seemingly similar interest as the new user. What could be the possible movies rated by the user?</p>

<div class="sourceCode" id="cb1781"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1781-1" data-line-number="1">user.index =<span class="st"> </span>sorted.users<span class="op">$</span>ix[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1781-2" data-line-number="2"><span class="kw">sort</span>(A.hat[user.index,], <span class="dt">decreasing =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## Item3 Item2 Item5 Item1 Item4 
##  5.46  4.25  0.47  0.13 -0.20</code></pre>

<p>The result shows the items rated by the user (User5) ordered by the highest rating. We can recommend those items to the new user. Note that those ratings come from the reconstructed matrix (A.hat).</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="machinelearning2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="deeplearning1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
