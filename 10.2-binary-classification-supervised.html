<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10.2 Binary Classification (Supervised) | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="10.2 Binary Classification (Supervised) | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10.2 Binary Classification (Supervised) | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="10.1-regression.html"/>
<link rel="next" href="10.3-multi-class-classification-supervised.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i>Caveat</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="15-appendix.html"><a href="15-appendix.html"><i class="fa fa-check"></i><b>15</b> Appendix</a><ul>
<li class="chapter" data-level="15.1" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html"><i class="fa fa-check"></i><b>15.1</b> Appendix A</a><ul>
<li class="chapter" data-level="15.1.1" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html#trigonometry"><i class="fa fa-check"></i><b>15.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="15.1.2" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html#logarithms"><i class="fa fa-check"></i><b>15.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="15.1.3" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html#category-theory"><i class="fa fa-check"></i><b>15.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html"><i class="fa fa-check"></i><b>15.2</b> Appendix B</a><ul>
<li class="chapter" data-level="15.2.1" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-random-chances"><i class="fa fa-check"></i><b>15.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="15.2.2" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-replacements"><i class="fa fa-check"></i><b>15.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="15.2.3" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>15.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="15.2.4" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>15.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="15.2.5" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>15.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="15.2.6" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>15.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="15.2.7" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>15.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="15.3-appendix-c.html"><a href="15.3-appendix-c.html"><i class="fa fa-check"></i><b>15.3</b> Appendix C</a></li>
<li class="chapter" data-level="15.4" data-path="15.4-appendix-d.html"><a href="15.4-appendix-d.html"><i class="fa fa-check"></i><b>15.4</b> Appendix D</a><ul>
<li class="chapter" data-level="15.4.1" data-path="15.4-appendix-d.html"><a href="15.4-appendix-d.html#lubridate-library"><i class="fa fa-check"></i><b>15.4.1</b> Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="binary-classification-supervised" class="section level2 hasAnchor">
<h2><span class="header-section-number">10.2</span> Binary Classification (Supervised)<a href="10.2-binary-classification-supervised.html#binary-classification-supervised" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For <strong>Regression</strong> - specifically <strong>Linear Regression</strong>, we tend to use <strong>sum squared error (SSE)</strong> to minimize our loss function and use <strong>Mean Square Error (MSE)</strong> or <strong>Root Mean Square Error (RMSE)</strong> to evaluate performance for which the response variable is continuous.</p>
<p>For <strong>Classification</strong>, we minimize our loss (or gain) function based on measures derived from <strong>Information Theory</strong> such as <strong>Gini Index (GI)</strong> and <strong>Entropy (H)</strong> and use <strong>confusion matrix</strong> as a reference to evaluate performance for which the response variable is categorical. We score the performance based on <strong>accuracy</strong>, <strong>sensitivity</strong>, <strong>specificity</strong>, and <strong>F1 score</strong>.</p>
<div id="linear-svm-sgdpegasos" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.2.1</span> Linear SVM (SGD/PEGASOS)  <a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Support Vector Machine (SVM)</strong> is a classic <strong>Computational Learning</strong> technique used for <strong>classification</strong>. As long as data points are <strong>linearly separable</strong>, we can use a simple <strong>linear SVM (LSVM)</strong> to perform <strong>classification</strong>. If that is not the case, then perhaps there are other ways. Figure <a href="10.2-binary-classification-supervised.html#fig:svm2">10.16</a> shows example techniques of how to separate data points.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:svm2"></span>
<img src="svm2.png" alt="Support Vector Machine" width="90%" />
<p class="caption">
Figure 10.16: Support Vector Machine
</p>
</div>
<p>Modulus transformation is a good illustration of how one can use even basic mathematics to separate data points. For example, in separating even numbers from odd numbers, we use modulus operations. Additionally, Figure <a href="10.2-binary-classification-supervised.html#fig:svm1">10.17</a> shows SVM techniques in separating data points.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:svm1"></span>
<img src="svm1.png" alt="Support Vector Machine" width="90%" />
<p class="caption">
Figure 10.17: Support Vector Machine
</p>
</div>
<p>In this section, let us discuss <strong>classification</strong> using <strong>linear SVM</strong>. Note that <strong>LSVM</strong> is about modeling an optimal <strong>decision boundary</strong> and not about modeling probability in the same way we do with <strong>General Linear Models</strong>. It is different from <strong>linear regression</strong> in which we measure the <strong>minimum</strong> horizontal distance - the residual - from the data point to the line. It is also far different from the <strong>Principal Component Analysis (PCA)</strong> in which we measure the <strong>maximum</strong> distance - the variance - from the projected data point to the point of origin along the <strong>Principal Component</strong> axis (see Figure <a href="9.6-featureengineering.html#fig:olspca">9.49</a>). In <strong>linear SVM</strong>, we measure the <strong>minimum</strong> orthogonal distance - a normal line - from the data point to a hyperplane. See Figure <a href="10.2-binary-classification-supervised.html#fig:linearsvm">10.18</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linearsvm"></span>
<img src="linearsvm.png" alt="Linear SVM" width="80%" />
<p class="caption">
Figure 10.18: Linear SVM
</p>
</div>
<p>Note that <strong>LSVM</strong> relies on concepts we cover in <strong>linear algebra</strong> around vector calculation. Understanding <strong>Euclidian norm</strong>, <strong>Dot Product</strong>, <strong>Magnitude</strong>, <strong>Direction</strong>, <strong>Projection</strong>, and many others are essential in building our intuition on <strong>SVM</strong>.</p>
<p>To understand the concept, we start with the standard <strong>linear equation</strong> (where <strong>m</strong> is the slope and <strong>b</strong> is the intercept) with a slight modification (where <span class="math inline">\(\mathbf{x_2}\)</span> takes the place of <strong>y</strong>):</p>
<p><span class="math display" id="eq:equate1120051">\[\begin{align}
y = mx + b\ \ \ \ \ \rightarrow\ \ \ \ \ \ \ x_2 =  m x_1 + b \tag{10.55} 
\end{align}\]</span></p>
<p>In an alternative format, we can re-arrange the equation by moving <span class="math inline">\(\mathbf{x_2}\)</span> to the right side and, at the same time, show a generalized equation for the line like so:</p>
<p><span class="math display" id="eq:equate1120052">\[\begin{align}
0 = mx_1 - x_2 + b \ \ \ \ \ \ \rightarrow\ \ \ \ \ \ \ \ \ \
\underbrace{\mathbf{w}^T\mathbf{x} + b = 0}_{\text{1-D hyperplane}} \tag{10.56} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{w}\)</span> - <strong>norm</strong> to the hyperplane - and <span class="math inline">\(\mathbf{x}\)</span> are vectors such that:</p>
<p><span class="math display">\[
\mathbf{w} = 
\left[\begin{array}{r}
m \\
-1
\end{array}\right]
\ \ \ \ 
\text{and}
\ \ \ \ 
\mathbf{x} = 
\left[\begin{array}{r}
x_1 \\
x_2
\end{array}\right].
\]</span>
The <strong>b</strong> term becomes a <strong>bias</strong> coefficient that displaces the <strong>hyperplane</strong> from point of origin , namely <strong>O = </strong> <span class="math inline">\(\mathbf{[0,0]}^T\)</span>, if <strong>b &gt; 0</strong>.</p>
<p>Assume <strong>b=0</strong> and the slope is <strong>m=2</strong> based on <strong>rise=2</strong> and <strong>run=1</strong>. We then have the following:</p>

<div class="sourceCode" id="cb1449"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1449-1" data-line-number="1">b =<span class="st"> </span><span class="dv">0</span>; m =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1449-2" data-line-number="2">x1 =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>, <span class="dt">length.out=</span><span class="dv">10</span>)  <span class="co"># some continuous obs for feature x1</span></a>
<a class="sourceLine" id="cb1449-3" data-line-number="3">x2  =<span class="st"> </span>m <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>b   </a></code></pre></div>

<p>The equation should equal to zero: <span class="math inline">\(m x1 - x2 = \mathbf{w}^T\mathbf{x} = 0\)</span></p>

<div class="sourceCode" id="cb1450"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1450-1" data-line-number="1">w =<span class="st"> </span><span class="kw">c</span>(m, <span class="dv">-1</span>);  x =<span class="st"> </span><span class="kw">rbind</span>(x1, x2)</a>
<a class="sourceLine" id="cb1450-2" data-line-number="2">v1 =<span class="st"> </span>m <span class="op">*</span><span class="st"> </span>x1 <span class="op">-</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>b</a>
<a class="sourceLine" id="cb1450-3" data-line-number="3">v2 =<span class="st"> </span><span class="kw">c</span>( <span class="kw">t</span>(w) <span class="op">%*%</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>b )</a>
<a class="sourceLine" id="cb1450-4" data-line-number="4"><span class="kw">data.frame</span>(<span class="st">&quot;sum v1&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(v1), <span class="st">&quot;sum v2&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(v2), <span class="st">&quot;equal&quot;</span> =<span class="st"> </span><span class="kw">all.equal</span>(v1, v2))</a></code></pre></div>
<pre><code>##   sum.v1 sum.v2 equal
## 1      0      0  TRUE</code></pre>

<p>Based on the equation above, we can see the plotted hyperplane in Figure <a href="10.2-binary-classification-supervised.html#fig:hyperplane">10.19</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hyperplane"></span>
<img src="DS_files/figure-html/hyperplane-1.png" alt="Hyperplane" width="70%" />
<p class="caption">
Figure 10.19: Hyperplane
</p>
</div>
<p>As we can see, the vector <strong>w</strong> points in the <strong>direction</strong> denoted by (<span class="math inline">\(w_1\)</span>,<span class="math inline">\(w_2\)</span>) = (2, -1). To derive the size (magnitude) of <strong>w</strong>, we use the following euclidean formula (recall <strong>euclidean norm</strong>):</p>
<p><span class="math display" id="eq:equate1120054" id="eq:equate1120053">\[\begin{align}
\|\mathbf{w}\| = \sqrt{w_1^2 + w_2^2 + w^2_3 +\ ...\ + w^2_n}  \tag{10.57} \\
= \sqrt{(2)^2 +(-1)^2} = \sqrt{5} \tag{10.58} 
\end{align}\]</span></p>
<p>Given a vector, namely <strong>p (4,2)</strong>, let us compute the magnitude of the projection of <strong>p</strong> into the subspace <strong>w</strong>. Let us denote the projection of <strong>p</strong> as <strong>q</strong>. Here, we use the formula:</p>
<p><span class="math display" id="eq:eqnnumber406">\[\begin{align}
c_{(scale)} = \frac{\mathbf{w} \cdot \mathbf{p}}{\| \mathbf{w} \|^2} =
\frac{
\left[\begin{array}{r} 2\\-1 \end{array}\right] \cdot
\left[\begin{array}{r} 4\\2 \end{array}\right] \tag{10.59}
}
{(\sqrt{5})^2} = \frac{2\times4 + (-1)\times 2}{5} = 1.2
\end{align}\]</span></p>
<p><span class="math display" id="eq:eqnnumber407">\[\begin{align}
\mathbf{q}_{(proj)} = c_{(scale)} \times \mathbf{w} = 
1.2 \times 
\left[\begin{array}{r} 2\\-1 \end{array}\right] = 
\left[\begin{array}{r} 2.4\\-1.2 \end{array}\right]  \tag{10.60}
\end{align}\]</span></p>
<div class="sourceCode" id="cb1452"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1452-1" data-line-number="1">proj &lt;-<span class="st"> </span><span class="cf">function</span>(p, v) {</a>
<a class="sourceLine" id="cb1452-2" data-line-number="2">  v.size =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(v<span class="op">^</span><span class="dv">2</span>))  <span class="co"># magnitude</span></a>
<a class="sourceLine" id="cb1452-3" data-line-number="3">  c.scale =<span class="st"> </span>(<span class="kw">t</span>(v) <span class="op">%*%</span><span class="st"> </span>p) <span class="op">/</span><span class="st"> </span>v.size<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1452-4" data-line-number="4">  <span class="kw">c</span>(c.scale) <span class="op">*</span><span class="st"> </span>v</a>
<a class="sourceLine" id="cb1452-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1452-6" data-line-number="6">p =<span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">2</span>); (<span class="dt">q =</span> <span class="kw">proj</span>(p, w))</a></code></pre></div>
<pre><code>## [1]  2.4 -1.2</code></pre>
<p>Using similar computation, we can obtain the projection of <strong>r</strong>, denoted as <strong>s</strong>, like so:</p>
<p><span class="math display">\[
\mathbf{s}_{(proj)} = \left[\begin{array}{r} -0.8\\ 0.4 \end{array}\right] 
\]</span></p>
<div class="sourceCode" id="cb1454"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1454-1" data-line-number="1">r =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>); (<span class="dt">s =</span> <span class="kw">proj</span>(r, w))</a></code></pre></div>
<pre><code>## [1] -0.8  0.4</code></pre>
<p>Let us see the plot in Figure <a href="10.2-binary-classification-supervised.html#fig:hyperplane1">10.20</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hyperplane1"></span>
<img src="DS_files/figure-html/hyperplane1-1.png" alt="Projections and Magnitude" width="70%" />
<p class="caption">
Figure 10.20: Projections and Magnitude
</p>
</div>
<p>The magnitude of the projection <strong>(q)</strong> of <strong>p</strong> and project <strong>(s)</strong> of <strong>r</strong> are:</p>
<p><span class="math display">\[\begin{align*}
\|\mathbf{q}\| = \sqrt{2.4^2 + (-1.2)^2} = 2.683282\ \ \ \ \ \ \ 
\|\mathbf{s}\| = \sqrt{(-0.8)^2 + (0.4)^2} = 0.8944272
\end{align*}\]</span></p>
<p>In other words, the orthogonal distance from point <strong>p</strong> to the hyperplane equals <span class="math inline">\(\|\mathbf{q}\|\)</span>. Similarly, the orthogonal distance from point <strong>r</strong> to the hyperplane equals <span class="math inline">\(\|\mathbf{s}\|\)</span>. Therefore, to note, the distance from any random point to the hyperplane can be computed based on its projection to <strong>w</strong>. Equivalently, <span class="math inline">\(\|\mathbf{q}\|\)</span> = <span class="math inline">\(\|\mathbf{p}\|\)</span> and <span class="math inline">\(\|\mathbf{s}\|\)</span> = <span class="math inline">\(\|\mathbf{r}\|\)</span>.</p>
<p>We can calculate the <strong>margin</strong> of each point across a hyperplane. For example:</p>
<p><span class="math display">\[\begin{align*}
\text{M}_{p} = 2 \| \mathbf{p}\| = 5.366563\ \ \ \ \ \
\text{M}_{r} = 2 \| \mathbf{r}\| = 1.788854\ \ \ \ \ \
 where: \text{M = margin}
\end{align*}\]</span></p>
<p>See Figure <a href="10.2-binary-classification-supervised.html#fig:hyperplane2">10.21</a> for the plot of the margins.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hyperplane2"></span>
<img src="DS_files/figure-html/hyperplane2-1.png" alt="Margins and Hyperplanes" width="70%" />
<p class="caption">
Figure 10.21: Margins and Hyperplanes
</p>
</div>
<p>Given that we can calculate <strong>distance</strong> and <strong>margin</strong>, our next goal for <strong>LSVM</strong> is to find the optimal <strong>decision boundary</strong> - the <span class="math inline">\(H_0\)</span> hyperplane. It can be achieved based on the following steps:</p>
<p><strong>First</strong>, data points are classified based on the following formula:</p>
<p><span class="math display" id="eq:equate1120055">\[\begin{align}
g(x) = \mathbf{w}^Tx + b \tag{10.61} 
\end{align}\]</span></p>
<p><span class="math display">\[
where: g(x) = \begin{cases}
\ge +1 &amp; \text{(+) data points on or above the }H_{(+)} \\
0  &amp; \text{the }H_0 \text{ hyperplane} \\
\le-1 &amp; \text{(-) data points on or below the }H_{(-)}
\end{cases}
\]</span></p>
<p>Any data point classified as positive (+) respects the following equation:</p>
<p><span class="math display" id="eq:equate1120056">\[\begin{align}
\mathbf{w}^Tx + b \ge +1 \tag{10.62} 
\end{align}\]</span></p>
<p>Any data point classified as negative (-) respects the following equation:</p>
<p><span class="math display" id="eq:equate1120057">\[\begin{align}
\mathbf{w}^Tx + b \le -1 \tag{10.63} 
\end{align}\]</span></p>
<p>For mathematical convenience, we combine the two equations into one <strong>constraining</strong> equation by introducing an extra variable, namely <strong>y</strong> where <span class="math inline">\(y \in (-1,+1)\)</span> like so:</p>
<p><span class="math display" id="eq:equate1120058">\[\begin{align}
y_i (\mathbf{w}^T\mathbf{x_i} + b)  \ge 1\ \ \  \equiv\ \ \  y_i (\mathbf{w}^T\mathbf{x_i} + b) -1 \ge 0 \tag{10.64} 
\end{align}\]</span></p>
<p>The equation above imposes a <strong>hard margin</strong> constraint such that it allows us to classify any <strong>x</strong> data points so that if <span class="math inline">\(\mathbf{w}^T\mathbf{x} + b\)</span> is positive and <span class="math inline">\(y = +1\)</span>, then <strong>x</strong> is correctly classified. Similarly if <span class="math inline">\(\mathbf{w}^T\mathbf{x} + b\)</span> is negative and <span class="math inline">\(y = -1\)</span>, then <strong>x</strong> is correctly classified.</p>
<p>However, there may be cases when <strong>x</strong> data points do not respect the equation above and thus may render misclassification, which happens when such data points fall between the <span class="math inline">\(H_{(+)}\)</span> and <span class="math inline">\(H_{(-)}\)</span> hyperplanes. For example:</p>
<p><span class="math display" id="eq:equate1120059">\[\begin{align}
y_i (\mathbf{w}^T\mathbf{x_i} + b)  &lt; 1\ \ \ \ \leftarrow\ \ \ \text{mis-classified} \tag{10.65} 
\end{align}\]</span></p>
<p>Such data points may cause the entire dataset not to be <strong>linearly separable</strong>. To mitigate the situation, we can relax the constraint by <strong>softening</strong> the margin allowing a few data points to <strong>cross over</strong> to the other side of the <span class="math inline">\(H_{(+)}\)</span> and <span class="math inline">\(H_{(-)}\)</span> boundaries. For example, positive (+) data points can cross over to the other side of the <span class="math inline">\(H_{(+)}\)</span> hyperplane in the same way that negative (-) data points can cross over to the other side of the <span class="math inline">\(H_{(-)}\)</span> hyperplane. In doing so, we end up with a <strong>soft margin</strong> expressed like so:</p>
<p><span class="math display" id="eq:equate1120060">\[\begin{align}
y_i (\mathbf{w}^T\mathbf{x_i} + b)  \ge 1 - \xi_i \ \ \  \equiv\ \ \  y_i (\mathbf{w}^T\mathbf{x} + b) -(1 - \xi_i) \ge 0 \tag{10.66} 
\end{align}\]</span></p>
<p>We introduce a <strong>slack variable</strong> denoted by the <strong>xi</strong> (<span class="math inline">\(\xi\)</span>) symbol. The <strong>slack variable</strong> serves as a measure of <strong>crossover error</strong> - not necessarily a <strong>misclassification error</strong> because, for example, positive (+) data points may still find themselves between the <strong>decision boundary</strong> and the <span class="math inline">\(H_{(+)}\)</span> boundary in that they are therefore still correctly classified. However, as data points get farther away from the borders and deeper to the other side, the error progresses to a <strong>misclassification error</strong>. Under such conditions, each data point that crosses over <strong>incurs some loss</strong> - calculated through <strong>Hinge Loss</strong>. We continue this discussion further in step four. </p>
<p><strong>Second</strong>, data points that are located along the <span class="math inline">\(H_{(+)}\)</span> and <span class="math inline">\(H_{(-)}\)</span> hyperplanes are called the <strong>support vectors</strong> because they <strong>support</strong> the <strong>decision boundary</strong> and they help to dictate the separability distance between two classes. The idea, therefore, is to identify those <strong>support vectors</strong>. To do that, we need to rely on distance measurement, which requires us to review three equations derived from the original equation, namely <span class="math inline">\(g(x) =\mathbf{w}^T\mathbf{x} + b\)</span>. The three equations are expressed as such:</p>
<p><span class="math display" id="eq:equate1120061">\[\begin{align}
\underbrace{\mathbf{w}^Tx + b = +1}_{\text{equation for the }H_{(+)}\text{ hyperplane}}\ \ \ \ \ \ \ \ \ \ 
\underbrace{\mathbf{w}^Tx + b = -1}_{\text{equation for the }H_{(-)}\text{ hyperplane}} \tag{10.67} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1120062">\[\begin{align}
\underbrace{\mathbf{w}^Tx + b = 0}_{\text{equation for the }H_{0}\text{ hyperplane}} \tag{10.68} 
\end{align}\]</span></p>
<p>We have shown earlier how to project a vector to <strong>w</strong> and derive the magnitude of the vector. Alternatively, we also can use the following equation (based on <strong>Pythagoras</strong>) to compute the distance of any point to a hyperplane directly.</p>
<p><span class="math display" id="eq:equate1120063">\[\begin{align}
D(x, w, b) = \frac{|\mathbf{w}^T\mathbf{x} + b|}{\|\mathbf{w}\|} \ \ \ \ \leftarrow \text{based on}\ \ \ \ \ \
\underbrace{\frac{|ax +bx + c|}{\sqrt{a^2 + b^2}}}_{\text{euclidean distance}} \tag{10.69} 
\end{align}\]</span></p>
<p>For example, we can solve for the distance of both <span class="math inline">\(\|\mathbf{p}\|\)</span> and <span class="math inline">\(\|\mathbf{r}\|\)</span> like so:</p>
<div class="sourceCode" id="cb1456"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1456-1" data-line-number="1">Distance &lt;-<span class="st"> </span><span class="cf">function</span>(x, w, b) {</a>
<a class="sourceLine" id="cb1456-2" data-line-number="2">  numer =<span class="st"> </span><span class="kw">abs</span>( <span class="kw">t</span>(w) <span class="op">%*%</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>b )</a>
<a class="sourceLine" id="cb1456-3" data-line-number="3">  denom =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(w<span class="op">^</span><span class="dv">2</span>))  </a>
<a class="sourceLine" id="cb1456-4" data-line-number="4">  numer <span class="op">/</span><span class="st"> </span>denom</a>
<a class="sourceLine" id="cb1456-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1456-6" data-line-number="6"><span class="kw">c</span>(<span class="st">&quot;dist(p)&quot;</span> =<span class="st"> </span><span class="kw">Distance</span>(p, w, b), <span class="st">&quot;dist(r)&quot;</span> =<span class="st"> </span><span class="kw">Distance</span>(r, w, b))</a></code></pre></div>
<pre><code>## dist(p) dist(r) 
##  2.6833  0.8944</code></pre>
<p>Now, using the same distance equation, we can determine the distance of both <span class="math inline">\(H_{(+)}\)</span> and <span class="math inline">\(H_{(-)}\)</span> hyperplanes to the <span class="math inline">\(H_0\)</span> hyperplane.</p>
<p>For <span class="math inline">\(H_{(+)}\)</span> hyperplane, we have the following distance from which our (+) support vectors are found:</p>
<p><span class="math display" id="eq:equate1120064">\[\begin{align}
D(x, w, b) = \frac{|\mathbf{w}^T\mathbf{x} + b|}{\|\mathbf{w}\|}
= \frac{|+1|}{\|\mathbf{w}\|}
\ \ \ \ \ \leftarrow\ \ \ \ \underbrace{\mathbf{w}^Tx + b = +1}_{\text{equation for the }H_{(+)}\text{ hyperplane}} \tag{10.70} 
\end{align}\]</span></p>
<p>For <span class="math inline">\(H_{(-)}\)</span> hyperplane, we have the following distance from which our (-) support vectors are found:</p>
<p><span class="math display" id="eq:equate1120065">\[\begin{align}
D(x, w, b) = \frac{|\mathbf{w}^T\mathbf{x} + b|}{\|\mathbf{w}\|}
= \frac{|-1|}{\|\mathbf{w}\|}
\ \ \ \ \ \leftarrow\ \ \ \ \underbrace{\mathbf{w}^Tx + b = -1}_{\text{equation for the }H_{(-)}\text{ hyperplane}} \tag{10.71} 
\end{align}\]</span></p>
<p>From here, it is easy to prove that the <strong>margin</strong> - the distance between <span class="math inline">\(H_{(+)}\)</span> and <span class="math inline">\(H_{(-)}\)</span> hyperplanes - of our hyperplane is equal to the following:</p>
<p><span class="math display" id="eq:equate1120066">\[\begin{align}
\text{margin} = D_{H_{(+)}} + D_{H_{(-)}} = \frac{|+1|}{\|\mathbf{w}\|} +  \frac{|-1|}{\|\mathbf{w}\|} = \frac{2}{\|\mathbf{w}\|} \tag{10.72} 
\end{align}\]</span></p>
<p>In other words, our (+) <strong>support vectors</strong> are a <strong>margin</strong> distant-away from (-) <strong>support vectors</strong>.</p>
<p>Alternatively, the width of the margin is calculated based on subtracting the closest opposing support vectors. See Figure <a href="10.2-binary-classification-supervised.html#fig:optimizingsvm">10.22</a>.</p>
<p><span class="math display" id="eq:equate1120067">\[\begin{align}
\text{width}_{\left(v_{(+)} - v_{(-)}\right)} = \frac{\mathbf{w}}{\|\mathbf{w}\|} \cdot \left(v_{(+)} - v_{(-)}\right) = \frac{2}{\|\mathbf{w}\|}  \tag{10.73} 
\end{align}\]</span></p>
<p>For example:</p>
<div class="sourceCode" id="cb1458"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1458-1" data-line-number="1">width &lt;-<span class="st"> </span><span class="cf">function</span>(v.pos, v.neg, w) {</a>
<a class="sourceLine" id="cb1458-2" data-line-number="2">  u =<span class="st">  </span>w <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(w<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1458-3" data-line-number="3">  <span class="kw">abs</span>(<span class="kw">c</span>(u <span class="op">%*%</span><span class="st"> </span>( v.pos <span class="op">-</span><span class="st"> </span>v.neg )))</a>
<a class="sourceLine" id="cb1458-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb1458-5" data-line-number="5">w.norm =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">-1</span>); v.pos =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>); v.neg =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1458-6" data-line-number="6"><span class="kw">all.equal</span>( <span class="kw">width</span>(v.pos, v.neg, w.norm),  <span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(w.norm<span class="op">^</span><span class="dv">2</span>)) )</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Also, notice in the equation above that if we minimize <strong>w</strong>, the <strong>margin</strong> is effectively maximized. It is our optimization problem, expressed like so:</p>
<p><span class="math display" id="eq:equate1120068">\[\begin{align}
\text{max}\ \frac{2}{\|\mathbf{w} \|} \equiv \text{min}\  \|\mathbf{w} \| \tag{10.74} 
\end{align}\]</span></p>
<p><strong>Third</strong>, we need to find the optimal <strong>decision-boundary</strong>. To do that, all we need to do is perform the optimization equation below (with a caveat such that minimizing the <strong>w</strong> is subject to a constraint):</p>
<p><span class="math display" id="eq:equate1120069">\[\begin{align}
\text{min}\  \|\mathbf{w}\|\ \ \ \ \leftarrow\ \ \ \ \ \ \ \text{subject to:}\ \ \ \ y_i (\mathbf{w}^T\mathbf{x_i} + b)  \ge 1 \tag{10.75} 
\end{align}\]</span></p>
<p>For mathematical convenience, we can perform mathematical manipulation using other variants of the minimization term, keeping the proportionality intact.</p>
<p><span class="math display" id="eq:equate1120070">\[\begin{align}
\text{min} \left\{\|\mathbf{w}\| \propto  \|\mathbf{w}\|^2 \propto  \frac{1}{2}\|\mathbf{w}\|^2 \right\} \equiv \text{min} \left\{ \frac{1}{2}\|\mathbf{w}\|^2 \right\} \tag{10.76} 
\end{align}\]</span></p>
<p>Here, without losing proportionality, we transform the equation into a <strong>convex function</strong> expressed as the square of <span class="math inline">\(\|\mathbf{w}\|\)</span>, divided by 2. This transformation allows us an easy way to work with <strong>lagrangian multiplier</strong> optimization, which we will cover later.</p>
<p>Also, we can adjust the constraint to address <strong>scale variant</strong> concerns by dividing it by the <strong>w</strong> norm so that we have the following:</p>
<p><span class="math display" id="eq:equate1120071">\[\begin{align}
y_i \left[D(x_i, w, b)\right] \ge 1
\ \ \ \ \ \leftarrow\ \ \ \ \ 
y_i \left(\frac{\mathbf{w}^T\mathbf{x_i} + b}{\|\mathbf{w}\|}\right)  \ge 1
\ \ \ \ \ \leftarrow\ \ \ \ \ y_i (\mathbf{w}^T\mathbf{x_i} + b)  \ge 1 \tag{10.77} 
\end{align}\]</span></p>
<p>With all that said, our minimization formula for a perfectly separable dataset becomes:</p>
<p><span class="math display" id="eq:equate1120072">\[\begin{align}
 \text{arg}\ \underset{\mathbf{w},b}{\text{min}} \left\{ \frac{1}{2}\|\mathbf{w}\|^2 \right\}\ \ \ \ \mathbf{s.t.}\ \ \ y_i 
 \left[D(x_i, w, b)\right] \ge 1,\ \ \ \ \ where\ \forall i = 1,..,n. \tag{10.78} 
\end{align}\]</span></p>
<p>Note that the distance formula depends upon the values of <strong>w</strong> and <strong>b</strong> - they are the parameters of our <span class="math inline">\(H_0\)</span> hyperplane. As long as we know that the data points are <strong>linearly separable</strong>, we can generate a list of arbitrary <span class="math inline">\(H_0\)</span> hyperplanes described by their corresponding set of <strong>w</strong> and <strong>b</strong> values. However, only one of the <span class="math inline">\(H_0\)</span> hyperplanes can be a qualified candidate based upon the <strong>margin</strong> that ensures the maximum separability of the (+) and (-) support vectors.</p>
<p>Here, by minimizing the value of <strong>w</strong> (and <strong>b</strong>), we maximize the <strong>margin</strong>.</p>
<p>See Figure <a href="10.2-binary-classification-supervised.html#fig:optimizingsvm">10.22</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:optimizingsvm"></span>
<img src="optimizingsvm.png" alt="Optimal Hyperplane" width="60%" />
<p class="caption">
Figure 10.22: Optimal Hyperplane
</p>
</div>
<p><strong>Fourth</strong>, recall in step one that if our dataset is not <strong>linearly separable</strong>, perhaps we can relax the <strong>hard margin</strong> a bit; instead, we can impose a <strong>softer margin</strong> to permit data points to cross over the <strong>decision boundary</strong>. Consequently, however, by crossing over to the other side, the data points are forced to <strong>incur</strong> some loss, represented by a <strong>slack variable</strong> (<span class="math inline">\(\xi\)</span>).</p>
<p>Now, a <strong>slack variable</strong> (<span class="math inline">\(\xi_i\)</span>) acts as a <strong>loss function</strong> - specifically, the <strong>Hinge Loss</strong> function - that allows a <strong>cross-over</strong> violation such that the deeper the cross-over to the wrong side (meaning, the larger the violation), the more significant the loss.</p>
<p>For a good intuition, it helps to show two <strong>loss functions</strong>, namely <strong>logistic loss</strong> function and <strong>hinge loss</strong> function. See Figure <a href="10.2-binary-classification-supervised.html#fig:hingeloss">10.23</a>. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hingeloss"></span>
<img src="hingeloss.png" alt="Loss Functions" width="90%" />
<p class="caption">
Figure 10.23: Loss Functions
</p>
</div>
<p>Here, we use <strong>Hinge Loss</strong> for <strong>SVM</strong>, expressed bellow, to measure the amount of <strong>loss</strong> for data points.</p>
<p><span class="math display" id="eq:equate1120073">\[\begin{align}
\xi_i = 
\underbrace{\text{max} \left\{0, 1 - y_i (\mathbf{w}^T\mathbf{x_i} + b) \right\}}_{\text{hinge loss function}}
\ \ \ \ \text{based on}\ \ \ y_i (\mathbf{w}^T\mathbf{x_i} + b) &lt; 1\ \ \leftarrow\  \text{(mis-classified)} \tag{10.79} 
\end{align}\]</span></p>
<p>A simple intuition behind this is to equate (<span class="math inline">\(\mathbf{w}^T\mathbf{x_i} + b\)</span>) with a y-hat (<span class="math inline">\(\mathbf{\hat{y}}\)</span>) outcome such that we can then formulate the outcome this way:</p>
<p><span class="math display" id="eq:eqnnumber408">\[\begin{align}
y_i (\hat{y_i}) = 
\begin{cases}
\ge +1 &amp; \text{if}\ (y_i = +1, \hat{y_i} \ge +1) \ or\ (y_i=-1,\hat{y_i} \le -1) \\
\le -1 &amp; \text{if}\ (y_i = -1, \hat{y_i} \ge +1) \ or\ (y_i=+1,\hat{y_i} \le -1) \\
\end{cases} \tag{10.80}
\end{align}\]</span></p>
<p>Therefore:</p>
<p><span class="math display" id="eq:eqnnumber409">\[\begin{align}
\xi_i = 
\text{max} \left\{0, 1 - y_i (\hat{y}_i) \right\} 
\begin{cases}
=0 &amp; \text{max} \left\{0, 1 - (\ge+1) \right\}\ \ \leftarrow \text{(zero loss)}\\
&gt;0 &amp; \text{max} \left\{0, 1 - (\le-1) \right\}\ \ \leftarrow \text{(with loss)}\\
\end{cases} \tag{10.81}
\end{align}\]</span></p>
<p>We now go back to our optimization equation and plug in the <strong>slack variable</strong> (<span class="math inline">\(\xi_i\)</span>), which gets minimized also. See below:</p>
<p><span class="math display" id="eq:eqnnumber410">\[\begin{align}
\text{arg}\ \underset{\mathbf{w},b, \xi_i}{\text{min}} \left\{ \frac{1}{2}\|\mathbf{w}\|^2  +  \sum_{i=1}^n\xi_i^p\right\}
\ \ \ \mathbf{\text{s.t}}\ \ \text{const.}
\begin{cases}
y_i\left[D(x_i, w, b)\right] \ge 1 -  \xi_i\\
\xi_i \ge 0
\end{cases}
\ \ \ \ \forall i = 1,..,n \tag{10.82}
\end{align}\]</span></p>
<p>Notice the inclusion of an exponent <strong>p</strong> to the <strong>Hinge Loss</strong>. It takes care of outliers by increasing the <strong>loss</strong> exponentially, thus pushing them further away from the border.</p>
<p>Now, because we use a <strong>softer margin</strong>, in the process of maximizing the <strong>margin</strong>, there may be chances that more data points may forcibly get trapped between the borders, accumulating a larger number of misclassified data points than necessary. A way to avoid that is to have a <strong>trade-off</strong> between how much misclassification we can take versus how large our margin should be. We apply a <strong>cost</strong> as a penalty represented by <strong>C</strong> or by the <strong>lambda</strong> <span class="math inline">\(\lambda\)</span> parameter to regulate the trade-off. We have two choices like so.</p>
<p><span class="math display" id="eq:equate1120074">\[\begin{align}
\underbrace{\text{arg}\ \underset{\mathbf{w},b, \xi_i}{\text{min}} \left\{ \frac{1}{2}\|\mathbf{w}\|^2  +  C\sum_{i=1}^n\xi_i^p\right\}}_{\text{1st choice}}
\ \ \ \ \ \ \ \ \ \ \ \ \ 
\underbrace{\text{arg}\ \underset{\mathbf{w},b, \xi_i}{\text{min}} \left\{ \frac{\lambda}{2}\|\mathbf{w}\|^2  +  \sum_{i=1}^n\xi_i^p\right\}}_{\text{2nd choice}} \tag{10.83} 
\end{align}\]</span></p>
<p>Here, we choose to add <strong>lambda</strong> (<span class="math inline">\(\lambda\)</span>) to our equation, still subject to the aforementioned constraints.</p>
<p>The final equation for our <strong>objective (cost) function</strong> below is what we call a solution to the <strong>primal optimization problem</strong>. By <strong>primal</strong>, it means that the original variables (e.g. <strong>w</strong> and <strong>b</strong>) continue to be used for optimization. Also, recall <strong>Ridge Regression</strong> under the <strong>General Modeling</strong> section in relation to the equation below:</p>
<p><span class="math display" id="eq:eqnnumber411">\[\begin{align}
\mathcal{J}(\mathbf{w}) = 
\text{arg}\ \underset{\mathbf{w},b, \xi_i}{\text{min}} \left\{ \underbrace{\frac{\lambda}{2}\|\mathbf{w}\|^2}_{
\begin{array}{l}
    \text{ridge-like} \\
    \text{regularizer}
\end{array}
    }  +
\underbrace{\sum_{i=1}^n
\overbrace{\text{max} \left\{0, 1 - y_i (\mathbf{w}^T\mathbf{x_i} + b) \right\}^p}^{
\text{hinge }(\xi_i)}}_{\text{loss}}
\right\} \tag{10.84}
\end{align}\]</span></p>
<p><span class="math display" id="eq:eqnnumber412">\[\begin{align}
\ \ \ \mathbf{\text{subject to }}\ \ 
\begin{cases}
y_i\left[D(x_i, w, b)\right] \ge 1 -  \xi_i\\
\xi_i \ge 0
\end{cases}
\ \ \ \ \forall i = 1,..,n \tag{10.85}
\end{align}\]</span></p>
<p>On the other hand, we can rewrite the same problem into another form called the <strong>dual optimization formulation</strong>. We use such <strong>formulation</strong> in line with the optimization method using <strong>Lagrange multiplier</strong> (See Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>). This topic will be more discussed when dealing with <strong>kernel non-linear SVM</strong>.</p>
<p><strong>Fifth</strong>, for <strong>primal optimization problem</strong>, we can use <strong>Gradient Descent</strong> or <strong>Stochastic Gradient Descent (SGD)</strong>, or both. Given the <strong>non-differentiable</strong> nature of the <strong>Hinge Loss</strong>, we need to use <strong>sub-gradient</strong> instead. We use one called <strong>PEGASOS</strong>,, an acronym for <strong>Primal Estimated sub-GrAdient SOlver for SVM</strong>. </p>
<p>For the <strong>update rule</strong> using the <strong>PEGASOS</strong> algorithm, we have the following:</p>

<p><span class="math display" id="eq:equate1120076" id="eq:equate1120075">\[\begin{align}
\mathbf{w}^{(t+1)} &amp;= \mathbf{w}^{(t)} - \eta^{(t)} \nabla_{\mathbf{w}}  \mathcal{J}(\mathbf{w}^{(t)})  \tag{10.86} \\
&amp;= (1  - \eta^{(t)} \lambda)\mathbf{w}^{(t)} + \eta^{(t)}\ \mathbf{1}[\ y^{(t)}_i \mathbf{w}^T \mathbf{x}^{(t)}_i]\ y^{(t)}_i x^{(t)}_i \tag{10.87} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align*}
\text{where: }\\
\ \ \ \eta \text{ is learning rate }\\
\ \ \ \lambda \text{ is a regularization parameter }\\
\end{align*}\]</span>
</p>
<p>with an <strong>indicator function</strong> being:</p>

<p><span class="math display" id="eq:eqnnumber413">\[\begin{align}
\mathbf{1}[\ y^{(t)}_i \mathbf{w}^T \mathbf{x}^{(t)}_i]= 
\begin{cases}
1 &amp;  y^{(t)}_i \mathbf{w}^T \mathbf{x}^{(t)}_i &lt; 1\\
0   &amp; \text{otherwise}
\end{cases} \tag{10.88}
\end{align}\]</span>
</p>
<p>Here, we take the <strong>gradient</strong> (<span class="math inline">\(\nabla\)</span>) of the cost function <span class="math inline">\(\mathcal{J}(\mathbf{w})\)</span> with respect to <strong>w</strong>. Then we multiply the gradient by the learning rate (<span class="math inline">\(\eta\)</span>). We then update <span class="math inline">\(\mathbf{w}\)</span> and repeat the process until convergence. The algorithm eventually convergences because our objective function is <strong>convex</strong>.</p>
<p>Below are two juxtaposed algorithms for review. One is with a generic gradient descent algorithm, and the other is the <strong>PEGASOS</strong> algorithm introduced by Shalev-Shwartz S. et al. <span class="citation">(<a href="bibliography.html#ref-ref694s">2007</a>)</span>.</p>

<p><span class="math display">\[
\begin{array}{lll}
\mathbf{\text{Gradient Descent Algorithm}} \\
\\
\text{Initialize } \mathbf{w}_0\ \text{ and } \eta \\
\text{loop}\ t\ in\ 1:\ \text{T} \\
\ \ \ \text{Calculate}\ \mathbf{gradient } (\nabla_\mathbf{w})\\
\ \ \ \ \ \ \ of\ \mathcal{J}(\mathbf{w}^{(t)}) \\
\ \ \ \text{Update}\ \mathbf{w}\\
\ \ \ \ \ \ \  e.g.\\
\ \ \ \ \ \ \ \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \nabla_{\mathbf{w}} \mathcal{J}(\mathbf{w}^{(t)})\\
\text{end loop} \\
\text{Ouput}\ \mathbf{w}^{(T)} \\ {} \\ {} \\ {} 
\end{array} 
\left|
\begin{array}{ll}
\mathbf{\text{PEGASOS Algorithm}} \\
\text{(Shai Shalev-Shwartz et al, 2007)}\\
\\
\text{Input:}\ \ S, \lambda, \text{T} \\
\text{Set}\ \mathbf{w}_1 = 0\\
\text{loop}\ t\ in\ 1,2,...,\text{T}  \\
\ \ \ \ \text{Choose}\ i^{(t)}\ \in \{1,...,|S|\}\ \ \text{(unif. random)} \\  
\ \ \ \ \text{Set}\ \eta^{(t)} = \frac{1}{t\lambda}\\
\ \ \ \ \text{if}\ \ y_{i_t} (\mathbf{w}^T x_i^{(t)}) &lt; 1\ \text{then} \\
\ \ \ \ \ \ \ \ \mathbf{w}^{(t+1)} = (1 - \eta^{(t)} \lambda) \mathbf{w}^{(t)} + \eta^{(t)} y_i^{(t)} x_i^{(t)}\\
\ \ \ \ \text{else}\\
\ \ \ \ \ \ \ \ \mathbf{w}^{(t+1)} = (1 - \eta^{(t)} \lambda) \mathbf{w}^{(t)} \\
\ \ \ \ \text{end if}\\
\text{end loop} \\
\text{Ouput}\ \mathbf{w}^{(T+1)} 
\end{array}
\right.
\]</span>
</p>
<p><strong>PEGASOS</strong> performs <strong>sub-gradient descent stochastically</strong> as it randomly selects an observation from the dataset. Here, we use <strong>sample.int(.)</strong> to simulate random sampling of the index (<span class="math inline">\(i_t\)</span>).</p>
<p>Below is an example implementation of <strong>PEGASOS</strong> for <strong>SVM</strong>:</p>

<div class="sourceCode" id="cb1460"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1460-1" data-line-number="1">my.pegasos.svm &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">lambda=</span><span class="fl">0.01</span>, <span class="dt">limit=</span><span class="dv">15000</span>) {</a>
<a class="sourceLine" id="cb1460-2" data-line-number="2">  x   =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, data[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)])  </a>
<a class="sourceLine" id="cb1460-3" data-line-number="3">  y   =<span class="st"> </span>data[,<span class="kw">c</span>(<span class="dv">3</span>)]</a>
<a class="sourceLine" id="cb1460-4" data-line-number="4">  n   =<span class="st"> </span><span class="kw">nrow</span>(x); p =<span class="st"> </span><span class="kw">ncol</span>(x)</a>
<a class="sourceLine" id="cb1460-5" data-line-number="5">  w   =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, p) </a>
<a class="sourceLine" id="cb1460-6" data-line-number="6">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb1460-7" data-line-number="7">      i  =<span class="st"> </span><span class="kw">sample.int</span>(n, <span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1460-8" data-line-number="8">      xi =<span class="st"> </span>x[i,]; yi =<span class="st"> </span>y[i]</a>
<a class="sourceLine" id="cb1460-9" data-line-number="9">      eta =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(t <span class="op">*</span><span class="st"> </span>lambda)      <span class="co"># learning rate</span></a>
<a class="sourceLine" id="cb1460-10" data-line-number="10">      <span class="cf">if</span> ( yi <span class="op">*</span><span class="st"> </span>(<span class="kw">t</span>(w) <span class="op">%*%</span><span class="st"> </span>xi) <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>) { <span class="co"># s.t. (1 - yi * (t(w) %*% xi) &gt;= 0)</span></a>
<a class="sourceLine" id="cb1460-11" data-line-number="11">        w =<span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>lambda) <span class="op">*</span><span class="st"> </span>w  <span class="op">+</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>yi <span class="op">*</span><span class="st"> </span>xi</a>
<a class="sourceLine" id="cb1460-12" data-line-number="12">      } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1460-13" data-line-number="13">        w =<span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>lambda) <span class="op">*</span><span class="st"> </span>w </a>
<a class="sourceLine" id="cb1460-14" data-line-number="14">      }</a>
<a class="sourceLine" id="cb1460-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb1460-16" data-line-number="16">  w  </a>
<a class="sourceLine" id="cb1460-17" data-line-number="17">}</a></code></pre></div>

<p>With the optimized <strong>w</strong> coefficients (or weights), we can implement the hyperplanes like so:</p>

<div class="sourceCode" id="cb1461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1461-1" data-line-number="1">svm.hyperplanes &lt;-<span class="st"> </span><span class="cf">function</span>(svm.model) {</a>
<a class="sourceLine" id="cb1461-2" data-line-number="2"> b      =<span class="st">  </span>svm.model[<span class="dv">1</span>]; rise =<span class="st"> </span>svm.model[<span class="dv">2</span>]; run =<span class="st"> </span>svm.model[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb1461-3" data-line-number="3"> H0.int =<span class="st">  </span><span class="op">-</span>b <span class="op">/</span><span class="st"> </span>run    </a>
<a class="sourceLine" id="cb1461-4" data-line-number="4"> Hp.int =<span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>b) <span class="op">/</span><span class="st"> </span>run </a>
<a class="sourceLine" id="cb1461-5" data-line-number="5"> Hn.int =<span class="st"> </span><span class="op">-</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>b) <span class="op">/</span><span class="st"> </span>run</a>
<a class="sourceLine" id="cb1461-6" data-line-number="6"> <span class="kw">list</span>(<span class="st">&quot;slope&quot;</span> =<span class="st"> </span><span class="op">-</span>rise  <span class="op">/</span><span class="st"> </span>run, </a>
<a class="sourceLine" id="cb1461-7" data-line-number="7">      <span class="st">&quot;H0.int&quot;</span> =<span class="st"> </span>H0.int, <span class="st">&quot;Hp.int&quot;</span> =<span class="st"> </span>Hp.int, <span class="st">&quot;Hn.int&quot;</span> =<span class="st"> </span>Hn.int)</a>
<a class="sourceLine" id="cb1461-8" data-line-number="8">}</a></code></pre></div>

<p>Let us use the <strong>implementation</strong> to learn the classifier. Below is a sample dataset to use:</p>

<div class="sourceCode" id="cb1462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1462-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1462-2" data-line-number="2">N         =<span class="st"> </span><span class="dv">20</span>; v =<span class="st"> </span><span class="dv">1</span> <span class="co"># variance</span></a>
<a class="sourceLine" id="cb1462-3" data-line-number="3">x1.blue   =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); x2.blue =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); y1 =<span class="st"> </span><span class="kw">rep</span>( <span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1462-4" data-line-number="4">x1.red    =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); x2.red  =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); y2 =<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1462-5" data-line-number="5">x         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(x1.blue, x1.red), <span class="kw">c</span>(x2.blue, x2.red)) </a>
<a class="sourceLine" id="cb1462-6" data-line-number="6">y         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(y1, y2))</a>
<a class="sourceLine" id="cb1462-7" data-line-number="7">data      =<span class="st"> </span><span class="kw">cbind</span>(x, y)</a>
<a class="sourceLine" id="cb1462-8" data-line-number="8">svm.pegasos.model =<span class="st"> </span><span class="kw">my.pegasos.svm</span>(data) </a>
<a class="sourceLine" id="cb1462-9" data-line-number="9">hplanes   =<span class="st"> </span><span class="kw">svm.hyperplanes</span>(svm.pegasos.model)</a></code></pre></div>

<p>We now plot and see Figure <a href="10.2-binary-classification-supervised.html#fig:gdsvm">10.24</a>.</p>
<div class="sourceCode" id="cb1463"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1463-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x[,<span class="dv">1</span>]), <span class="dt">ylim=</span><span class="kw">range</span>(x[,<span class="dv">2</span>]),</a>
<a class="sourceLine" id="cb1463-2" data-line-number="2">     <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">main=</span><span class="st">&quot;LSVM (Sub-Gradient Descent)&quot;</span>)</a>
<a class="sourceLine" id="cb1463-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1463-4" data-line-number="4"><span class="kw">points</span>(x, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span><span class="dv">-1</span>, <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>))</a>
<a class="sourceLine" id="cb1463-5" data-line-number="5"><span class="kw">abline</span>(<span class="dt">a=</span>hplanes<span class="op">$</span>H0.int, <span class="dt">b=</span>hplanes<span class="op">$</span>slope, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1463-6" data-line-number="6"><span class="kw">abline</span>(<span class="dt">a=</span>hplanes<span class="op">$</span>Hp.int, <span class="dt">b=</span>hplanes<span class="op">$</span>slope, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a>
<a class="sourceLine" id="cb1463-7" data-line-number="7"><span class="kw">abline</span>(<span class="dt">a=</span>hplanes<span class="op">$</span>Hn.int, <span class="dt">b=</span>hplanes<span class="op">$</span>slope, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gdsvm"></span>
<img src="DS_files/figure-html/gdsvm-1.png" alt="LSVM (Sub-Gradient Descent)" width="70%" />
<p class="caption">
Figure 10.24: LSVM (Sub-Gradient Descent)
</p>
</div>
<p><strong>Sixth</strong>, for prediction, we can use the model to classify any new or missing data.</p>
<p><span class="math display" id="eq:eqnnumber414">\[\begin{align}
h(x; \mathbf{w}) = \text{sign}(g(x)) = \text{sign}(\mathbf{w}^T \mathbf{x} + b)
\ \ \ \ \leftarrow\ \ \ \
\begin{cases}
+1 &amp; \mathbf{w}^T\mathbf{x} \ge 0\\ 
-1 &amp; \mathbf{w}^T\mathbf{x} &lt;0
\end{cases} \tag{10.89}
\end{align}\]</span></p>
<p>Below is an example of predicting the class of a given set of data.</p>

<div class="sourceCode" id="cb1464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1464-1" data-line-number="1">x.new =<span class="st"> </span><span class="kw">rbind</span>( <span class="kw">c</span>(<span class="st">&quot;intercept&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;x1&quot;</span> =<span class="st"> </span><span class="dv">-2</span>, <span class="st">&quot;x2&quot;</span> =<span class="st">  </span><span class="dv">2</span>), </a>
<a class="sourceLine" id="cb1464-2" data-line-number="2">               <span class="kw">c</span>(<span class="st">&quot;intercept&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;x1&quot;</span> =<span class="st">  </span><span class="dv">2</span>, <span class="st">&quot;x2&quot;</span> =<span class="st"> </span><span class="dv">-2</span>))</a>
<a class="sourceLine" id="cb1464-3" data-line-number="3">my.svm.prediction &lt;-<span class="st"> </span><span class="cf">function</span>(w, x) {</a>
<a class="sourceLine" id="cb1464-4" data-line-number="4">    <span class="kw">c</span>(<span class="kw">sign</span>(w <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x)))</a>
<a class="sourceLine" id="cb1464-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1464-6" data-line-number="6">h =<span class="st"> </span><span class="kw">my.svm.prediction</span>(svm.pegasos.model, x.new)</a>
<a class="sourceLine" id="cb1464-7" data-line-number="7">pred =<span class="st"> </span><span class="kw">ifelse</span>(h <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;positive&quot;</span>, <span class="st">&quot;negative&quot;</span>)</a>
<a class="sourceLine" id="cb1464-8" data-line-number="8"><span class="kw">print</span>(<span class="kw">cbind</span>(x.new, pred), <span class="dt">quote=</span><span class="ot">FALSE</span>, <span class="dt">right=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##      intercept x1 x2     pred
## [1,]         1 -2  2 positive
## [2,]         1  2 -2 negative</code></pre>

<p><strong>Finally</strong>, to evaluate performance, we can also use <strong>Chi-squared test</strong> in the same way we did with <strong>Poisson Regression</strong>. See the previous section for the method of evaluation.</p>
</div>
<div id="kernel-svm-smo" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.2.2</span> Kernel SVM (SMO)  <a href="10.2-binary-classification-supervised.html#kernel-svm-smo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This section (<strong>SMO-based SVM</strong>) and the next section (<strong>SDCA-based SVM</strong>) may require an understanding of the <strong>theory of duality</strong> starting with <strong>simplex method</strong>, <strong>dual simplex</strong>, and <strong>primal and dual formulations</strong> from Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) under Subsections <strong>Simplex Method</strong>, <strong>Dual Method</strong>, and <strong>Primal Dual</strong> respectively under <strong>Polynomial Regression</strong> Section.</p>
<p>In this section, we take the case in which our <strong>optimization problem</strong> is written into a different formulation - the <strong>dual optimization formulation</strong>. Here, we cover two complementing <strong>algorithms</strong> to solve for <strong>SVM</strong>, namely <strong>Lagrangian Multiplier method</strong> and <strong>Sequential Minimal Optimization (SMO)</strong>.</p>
<p><strong>First</strong>, we begin our discussion by introducing the <strong>Lagrangian duality</strong>, a variant of the <strong>Fenchel duality</strong> for optimization problems.  </p>
<p>Based on <strong>Duality Theory</strong> and <strong>Lagrange Multiplier</strong> in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>), we obtain the following equation: </p>
<p><span class="math display" id="eq:equate1120077">\[\begin{align}
Lik(X, \alpha) = f(X) - \sum_{i=1}^n \alpha_i \times (g_i(X) - c_i) \tag{10.90} 
\end{align}\]</span></p>
<p>Now, if only the dataset is perfectly separable with a <strong>hard margin</strong>, then our equation from the <strong>primal formulation</strong> in the previous section equates to the following <strong>primal Lagrangian formulation</strong>:</p>
<p><span class="math display" id="eq:equate1120078">\[\begin{align}
Lik(w, b, \alpha_i) =
\underbrace{\frac{1}{2} \|\mathbf{w}\|^2}_{f(\mathbf{w},b)} 
- \sum_{i=1}^n \alpha_i 
\underbrace{\left(y_i \left( \mathbf{w}^T\mathbf{\vec{x}_i} + b\right) - 1\right)}_{g(w,b) - c_i} \tag{10.91} 
\end{align}\]</span></p>
<p><strong>Second</strong>, if we then relax the <strong>hard margin</strong> by using a penalty (<span class="math inline">\(\xi_i\)</span>) where <strong>p=1</strong>, then our <strong>primal Lagrangian formulation</strong> becomes:</p>

<p><span class="math display" id="eq:equate1120079">\[\begin{align}
Lik(w, b, \xi_i, \alpha_i) = \left[\frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \right] - \sum_{i=1}^n\left[ \alpha_i \left\{  y_i \left( \mathbf{w}^T\mathbf{\vec{x}_i} + b\right) - (1 - \xi_i )\right\} \right] - \sum_{i=1}^n \alpha_i \left( \xi_i \right)  \tag{10.92} 
\end{align}\]</span>
</p>
<p>Here, we use <strong>C</strong> as our regularization parameter. In this formulation, the <strong>alpha</strong> (<span class="math inline">\(\alpha\)</span>) symbol represents a set of <strong>Lagrange multipliers</strong>.</p>
<p>We can then take the <strong>partial derivative</strong> of the function with respect to <strong>w</strong>, <strong>b</strong>, and <span class="math inline">\(\xi_i\)</span> respectively:</p>
<p><span class="math display" id="eq:equate1120081" id="eq:equate1120080">\[\begin{align}
{}&amp;\nabla_w Lik(w, b, \xi_i, \alpha_i) = \frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} -  \sum_{i=1}^n \alpha_i  y_i \mathbf{\vec{x}_i}   = 0
\ \ \ \ \rightarrow\ \ \ \ \ \mathbf{w} = \sum_{i=1}^n \alpha_i y_i \mathbf{\vec{x}_i}  \tag{10.93} \\
\nonumber \\
&amp;\nabla_b Lik(w, b, \xi_i, \alpha_i) = \frac{\partial L}{\partial b} = - \sum_{i=1}^n \alpha_i  y_i = 0\ \ \ \ \rightarrow\ \ \ \ \ \sum_{i=1}^n \alpha_i  y_i =  0
\nonumber \\
&amp;\nabla_{\xi_i} Lik(w, b, \xi_i, \alpha_i) = \frac{\partial L}{\partial \xi_i} = C - \alpha_i = 0\ \ \ \ \rightarrow\ \ \ \ \ C  =  \alpha_i \tag{10.94} 
\end{align}\]</span></p>
<p>By substituting the derived <strong>w</strong> to the <strong>Lagrangian formulation</strong> (dropping constants) where <strong>w</strong>=<span class="math inline">\(\left(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i\right)\)</span>, we get the following:</p>
<p><span class="math display" id="eq:equate1120083" id="eq:equate1120082">\[\begin{align}
Lik(w, b, \alpha_i) 
{}&amp;= \frac{1}{2} \left(\sum_{i=1} \alpha_i y_i \mathbf{\vec{x}_i}\right) \cdot \left(\sum_{j=1} \alpha_j y_j \mathbf{\vec{x}_j}\right)  \tag{10.95} \\
&amp;- \left(\sum_{i=1} \alpha_i y_i \mathbf{\vec{x}_i}\right) \cdot \left(\sum_{j=1} \alpha_j y_j \mathbf{\vec{x}_j}\right) - \sum \alpha_i y_i b + \sum \alpha_i. \tag{10.96} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\sum \alpha_i y_i b = 0\)</span></p>
<p>Simplifying the equation, we end up with the <strong>Dual Lagrangian formulation</strong>. By <strong>Dual</strong>, we disregard the use of the <strong>original variables</strong> (e.g., <strong>w</strong> and <strong>b</strong>) as minimized in the <strong>primal optimization formulation</strong> and use a <strong>dot product of two vectors</strong> for optimization instead in which we have the following:</p>
<p><span class="math display" id="eq:equate1120084">\[\begin{align}
Lik(w, b, \alpha_i)  = \sum \alpha_i - \frac{1}{2} \sum_{i=1} \sum_{j=1} \alpha_i \alpha_j y_i y_j (\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j})  \tag{10.97} 
\end{align}\]</span></p>
<p>conditioned on the following <strong>Karush-Khun-Tucker (KKT)</strong> constraints where <strong>p=1</strong>:  </p>
<p><span class="math display" id="eq:equate1120093" id="eq:equate1120092" id="eq:equate1120091" id="eq:equate1120090" id="eq:equate1120089" id="eq:equate1120088" id="eq:equate1120087" id="eq:equate1120086" id="eq:equate1120085">\[\begin{align}
\text{* Optimality (Stationarity) } &amp; \nabla_w Lik(w, b, \xi_i, \alpha_i)  = 0, &amp; i=1,...,n \tag{10.98} \\
 &amp; \nabla_b Lik(w, b, \xi_i, \alpha_i)  = 0, &amp; i=1,...,n \tag{10.99} \\
  &amp; \nabla_{\xi_i} Lik(w, b, \xi_i, \alpha_i)  = 0, &amp; i=1,...,n \tag{10.100} \\
\text{* Primal Feasibility } &amp; y_i(\mathbf{w}^T \mathbf{\vec{x}_i} + b)  \ge 1,  &amp; i=1,...,n  \tag{10.101} \\
  &amp;  \xi_i \ge 0, &amp; i=1,...,n  \tag{10.102} \\
\text{* Dual Feasibility } &amp; \alpha_i \ge 0, &amp; i=1,...,n  \tag{10.103} \\
  &amp;  C - \alpha_i \ge 0, &amp; i=1,...,n  \tag{10.104} \\
\text{* Complementary Slackness } &amp; \alpha_i\left(y_i(\mathbf{w}^T \mathbf{\vec{x}_i} + b) - 1 + \xi_i\right) = 0, &amp; i=1,...,n  \tag{10.105} \\
  &amp;  (C - \alpha_i)\ \xi_i = 0, &amp; i=1,...,n  \tag{10.106} 
\end{align}\]</span></p>
<p><strong>Third</strong>, with all that, the final equation for our <strong>objective (cost) function</strong> below is what we call solution to the <strong>dual optimization problem</strong>. The <strong>dual formulation</strong> tells us to maximize the <strong>dot product of the vectors</strong> instead, namely <span class="math inline">\((\mathbf{x_i} \cdot \mathbf{x_j})\)</span>: </p>
<p><span class="math display" id="eq:equate1120094">\[\begin{align}
\mathcal{J}(\alpha) = 
\text{arg}\ \underset{\alpha_i \ge 0}{\text{max}} 
\left\{\sum_{i=1}^n \alpha_i- \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \left(\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j}\right) \right\}  \tag{10.107} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align*}
\ \ \mathbf{subject\ to }\ \
\begin{cases}
0 \le \alpha_i \le C \\
\sum_{i=1}^n \alpha_i y_i = 0
\end{cases}
,\ \forall i=1,...,n
\end{align*}\]</span></p>
<p>For classification, we can use the below <strong>prediction function</strong>.</p>
<p><span class="math display" id="eq:equate1120095">\[\begin{align}
h\left(\mathbf{x}\right) = 
\underbrace{\sum_{i=1}^n \alpha_i y_i \left(\mathbf{x}  \cdot \mathbf{\vec{x}_i}  \right) + b}_{\text{dual}}
\ \ \ \ \leftarrow\ \ \ \ \ \
\underbrace{\text{sign}(\mathbf{w}^T \mathbf{x} + b)}_{\text{primal }} \tag{10.108} 
\end{align}\]</span></p>
<p><strong>Fourth</strong>, we get to the point in which we modify the objective function and prediction function such that they support <strong>Kernel functions</strong>. The transformation of the equation is indeed straightforward. We pass the dot-product of the two vectors to a <strong>Kernel function</strong> like so:</p>
<p><span class="math display" id="eq:equate1120096">\[\begin{align}
\mathcal{J}(\alpha) = 
\text{arg}\ \underset{\alpha_i \ge 0}{\text{max}} 
\left\{\sum_{i=1}^n \alpha_i- \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \ \mathbf{K}\left(\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j}\right) \right\}  \tag{10.109} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1120097">\[\begin{align}
h\left(\mathbf{x}\right)  = 
\underbrace{\text{sign}\left(\sum_{i=1}^n \alpha_i y_i\ \mathbf{K}\left( \mathbf{x} \cdot \mathbf{\vec{x}_i}   \right) + b\right)}_{\text{dual}}
\ \ \ \ \leftarrow\ \ \ \ \ \
\underbrace{\text{sign}(\mathbf{w}^T \mathbf{K}(\mathbf{x}) + b)}_{\text{primal }} \tag{10.110} 
\end{align}\]</span></p>
<p>The choice of <strong>Kernel function</strong> is based on the nature of the dataset. If the data follows some linear characteristics, perhaps a <strong>Linear Kernel</strong> is appropriate. Below are three <strong>Kernels</strong> used for <strong>SVM</strong>:</p>
<p><strong>Linear Kernel</strong> - if data linearly demonstrates separability, then we use the kernel below: </p>
<p><span class="math display" id="eq:equate1120098">\[\begin{align}
\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j}\right) = \left(\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j} \right)   \tag{10.111} 
\end{align}\]</span></p>
<p><strong>Polynomimal Kernel</strong> - if data demonstrates separability in a polynomial fashion (given <strong>d</strong> degrees), then we use the kernel below: </p>
<p><span class="math display" id="eq:equate1120099">\[\begin{align}
\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j}\right) = \left(\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j} + c \right)^d
\ \ \ \ \ \text{where }\ \mathbf{c}\ \text{is constant} \tag{10.112} 
\end{align}\]</span></p>
<p><strong>Radial Basis Kernel (RBF)</strong> - if data demonstrates separability in a gaussian fashion, then we can use the kernel below:  </p>
<p><span class="math display" id="eq:equate1120100">\[\begin{align}
\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j}\right) = exp\left( - \gamma \| \mathbf{\vec{x}_i} - \mathbf{\vec{x}_j}\|^2 \right)
\ \ \ \ \ where\ \ \gamma = \frac{1}{2\sigma^2}\ and\ \sigma\ \text{is variance} \tag{10.113} 
\end{align}\]</span></p>
<p><strong>Laplacian Kernel</strong> - this kernel is a variant of <strong>RBF</strong> kernel. </p>
<p><span class="math display" id="eq:equate1120101">\[\begin{align}
\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j}\right) = exp\left( - \gamma \| \mathbf{\vec{x}_i} - \mathbf{\vec{x}_j}\| \right)
\ \ \ \ \ where\ \ \gamma = \frac{1}{\sigma}\ and\ \sigma\ \text{is variance} \tag{10.114} 
\end{align}\]</span></p>
<p><strong>Exponential Kernel</strong> - this kernel is another variant of <strong>RBF</strong> kernel. </p>
<p><span class="math display" id="eq:equate1120102">\[\begin{align}
\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j}\right) = exp\left( - \gamma \| \mathbf{\vec{x}_i} - \mathbf{\vec{x}_j}\| \right)
\ \ \ \ \ where\ \ \gamma = \frac{1}{2\sigma^2}\ and\ \sigma\ \text{is variance} \tag{10.115} 
\end{align}\]</span></p>
<p><strong>Fifth</strong>, we now come down to the <strong>algorithm</strong>. Here, let us discuss an algorithm called <strong>Sequential Minimal Optimization (SMO)</strong> introduced by John C. Platt <span class="citation">(<a href="bibliography.html#ref-ref718j">1998</a>)</span>. A simplified variant of <strong>SMO</strong> is found in a Stanford CS229 course <span class="citation">(<a href="bibliography.html#ref-ref729s">2009</a>)</span>.</p>
<p>We start with the objective function <span class="math inline">\(\mathcal{J}(\lambda)\)</span> in the <strong>fourth</strong> step and the two constraints in the <strong>third</strong> step. The intent is to solve for the <strong>lagrangian multipliers</strong>, e.g. (<span class="math inline">\(\alpha_i, \alpha_j)\)</span>, so that given <span class="math inline">\(\alpha = (\alpha_1, \alpha_2, \alpha_3,\ ...\ \alpha_n)\)</span>, the <strong>objective function</strong> allows us to solve only one pair of elements at a time; meaning, the goal is to heuristically pick an arbitrary pair of <span class="math inline">\(\alpha\)</span> elements, e.g. <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(\alpha_j\)</span>, such that in solving for their values using <strong>SMO</strong>, we respect the following <strong>KKT</strong> constraints:</p>
<p><span class="math display" id="eq:equate1120105" id="eq:equate1120104" id="eq:equate1120103">\[\begin{align}
\alpha_i = 0\ \ \ \ &amp;\rightarrow y_i f(\mathbf{\vec{x}_i}) \ge 1  \tag{10.116} \\
0 &lt; \alpha_i &lt; C\ \ \ \ &amp;\rightarrow y_i f(\mathbf{\vec{x}_i}) = 1  \tag{10.117} \\
\alpha_i = C\ \ \ \ &amp;\rightarrow y_i f(\mathbf{\vec{x}_i}) \le 1  \tag{10.118} 
\end{align}\]</span></p>
<p>To illustrate, we use Figure <a href="10.2-binary-classification-supervised.html#fig:smo">10.25</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:smo"></span>
<img src="smo.png" alt="Dual Constraint" width="90%" />
<p class="caption">
Figure 10.25: Dual Constraint
</p>
</div>
<p>The first constraint (inequality constraint), namely <span class="math inline">\(0 \le \alpha_i \le C\)</span> and <span class="math inline">\(0 \le \alpha_j \le C\)</span>, bounds the diagonal lines within the boxes shown in the figure. The second constraint (linear equality constraint), namely <span class="math inline">\(\sum_{i=1}^n \alpha_i y_i = 0\)</span>, limits us along the diagonal lines in the figure.</p>
<p>To enforce the first constraint, we start with the following expression:</p>
<p><span class="math display" id="eq:equate1120106">\[\begin{align}
\gamma = \alpha_i + s\ \alpha_j \tag{10.119} 
\end{align}\]</span></p>
<p>Here, let <span class="math inline">\(s = y_i y_j\)</span> and let the labels <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> to have values between (-1,1) respectively. Therefore, <span class="math inline">\(s = -1\)</span> if <span class="math inline">\(y_i \ne y_j\)</span> and <span class="math inline">\(s = +1\)</span> if <span class="math inline">\(y_i = y_j\)</span>. So that:</p>
<p><span class="math display" id="eq:equate1120110" id="eq:equate1120109" id="eq:equate1120108" id="eq:equate1120107">\[\begin{align}
\text{if}\ s = -1 \text{ and }  \gamma &gt; 0, &amp;\text{then} &amp;min(\alpha_j) = 0, &amp;max(\alpha_j) = C - \gamma  \tag{10.120} \\
\text{if}\ s = -1 \text{ and }  \gamma &lt; 0, &amp;\text{then} &amp;min(\alpha_j) = -\gamma, &amp;max(\alpha_j) = C \tag{10.121} \\
\text{if}\ s = +1 \text{ and } \gamma &lt; C, &amp;\text{then} &amp;min(\alpha_j) = 0, &amp;max(\alpha_j) = \gamma \tag{10.122} \\
\text{if}\ s = +1 \text{ and }  \gamma &gt; C, &amp;\text{then} &amp;min(\alpha_j) = \gamma - C, &amp;max(\alpha_j) = C \tag{10.123} 
\end{align}\]</span></p>
<p>Combining the conditions above, we obtain the following upper (H) and lower (L) bounds:</p>
<p><span class="math display" id="eq:equate1120112" id="eq:equate1120111">\[\begin{align}
If\ y_i \ne y_j,&amp; L = max(0, \alpha_j^{(t)} - \alpha_i^{(t)}),&amp; H=min(C, C+ \alpha_j^{(t)} - \alpha_1^{(t)}) \tag{10.124} \\
If\ y_i = y_j,&amp; L = max(0, \alpha_i^{(t)} + \alpha_j^{(t)} -C),&amp; H=min(C, \alpha_i^{(t)} + \alpha_j^{(t)}) \tag{10.125} 
\end{align}\]</span></p>
<p>To enforce the second constraint along the diagonal line, we update the second alpha (<span class="math inline">\(\alpha_j\)</span>) using the following formula:</p>
<p><span class="math display" id="eq:equate1120113">\[\begin{align}
\alpha_j^{(t+1)} = \alpha_j^{(t)} - \frac{y_j\left(E_i  - E_j\right)}{\eta}  \tag{10.126} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display" id="eq:equate1120115" id="eq:equate1120114">\[\begin{align}
E_i &amp;= f(\mathbf{\vec{x}}_i) - y_i  \tag{10.127} \\
\eta &amp;= 2\ \mathbf{K}\left( \mathbf{\vec{x}}_i, \mathbf{\vec{x}}_j   \right) - \mathbf{K}\left(  \mathbf{\vec{x}}_i, \mathbf{\vec{x}}_i \right) - \mathbf{K}\left(  \mathbf{\vec{x}}_j, \mathbf{\vec{x}}_j  \right) \tag{10.128} 
\end{align}\]</span></p>
<p>Note that the <strong>eta</strong> (<span class="math inline">\(\eta\)</span>) symbol represents second derivative of the objective function along the diagonal line <span class="citation">(Platt J.C. <a href="bibliography.html#ref-ref718j">1998</a>)</span>. Here, we incorporate the use of <strong>Kernel function</strong>. Also, note that <span class="math inline">\(\mathbf{E_i}\)</span> is the error between the true label and the estimated output on the ith observation. See <strong>prediction function</strong> discussed above, namely <span class="math inline">\(h(\mathbf{x})\)</span> for <span class="math inline">\(f(\mathbf{\vec{x}}_i)\)</span>.</p>
<p>The next step is to get the new value of the second alpha (<span class="math inline">\(\alpha_j\)</span>) by clipping it within the range [L, H].</p>
<p><span class="math display" id="eq:eqnnumber415">\[\begin{align}
\alpha_j^{(t+1)} = \begin{cases}
H &amp; if\ \alpha_j^{(t+1)} &gt; H\\
\alpha_j^{(t+1)} &amp; if\ L \le \alpha_j^{(t+1)} \le H\\ 
L &amp;\ if\ \alpha_j^{(t+1)} &lt; L
\end{cases}  \tag{10.129}
\end{align}\]</span></p>
<p>Equivalently, to get the new value of the first alpha (<span class="math inline">\(\alpha_i\)</span>), we use the <strong>y</strong> labels along with the second alpha (<span class="math inline">\(\alpha_i\)</span>):</p>
<p><span class="math display" id="eq:equate1120116">\[\begin{align}
\alpha_i^{(t+1)} = \alpha_i^{(t)} + s\left(a_j^{(t)} - a_j^{(t+1)}\right)
\ \ \ \ \ \ where\ \ \ \ \ s = y_i y_j \tag{10.130} 
\end{align}\]</span></p>
<p>Now, to classify labels, we also need to calculate the intercept <strong>b</strong>. Here, we first compute for the <span class="math inline">\(\mathbf{b}_1\)</span> and <span class="math inline">\(\mathbf{b}_2\)</span> thresholds.</p>
<p>Let the following be:</p>
<p><span class="math display" id="eq:equate1120117">\[\begin{align}
\mathbf{w}_i = y_i\left(\alpha_i^{(t+1)} - \alpha_i^{(t)}\right)
\ \ \ \ \ \ \ \ \ \ 
\mathbf{w}_j = y_j\left(\alpha_j^{(t+1)} - \alpha_j^{(t)}\right) \tag{10.131} 
\end{align}\]</span></p>
<p>therefore:</p>
<p><span class="math display" id="eq:equate1120118">\[\begin{align}
b_1 =  E_i + 
\mathbf{w}_i \mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_i} \right) + 
\mathbf{w}_j \mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j} \right) + b^{(t)} \tag{10.132} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1120119">\[\begin{align}
b_2 = E_j +
\mathbf{w}_i\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j} \right) +
\mathbf{w}_j \mathbf{K}\left( \mathbf{\vec{x}_j}, \mathbf{\vec{x}_j} \right) + b^{(t)} \tag{10.133} 
\end{align}\]</span></p>
<p>then, we clip our intercept <strong>b</strong> like so:</p>
<p><span class="math display" id="eq:eqnnumber416">\[\begin{align}
b^{(t+1)} = \begin{cases}
b_1 &amp; if\ 0\ &lt; \alpha_i &lt; C \\
b_2 &amp; if\ 0\ &lt; \alpha_j &lt; C \\
(b_1 + b_2)/2 &amp; otherwise
\end{cases} \tag{10.134}
\end{align}\]</span></p>
<p>Finally, we also need to handle our <strong>error cache</strong> required during enforcing the second constraint above, taking care of non-bound multipliers (<span class="math inline">\(\alpha_k\)</span>) such that <span class="math inline">\(k \ne i\)</span> and <span class="math inline">\(k \ne j\)</span>.</p>
<p><span class="math display" id="eq:equate1120120">\[\begin{align}
E^{(t+1)}_k = E^{(t)}_k  + 
\mathbf{w}_i\mathbf{K}(\mathbf{\vec{x}}_i, \mathbf{\vec{x}}_k) + 
\mathbf{w}_j\mathbf{K}(\mathbf{\vec{x}}_j, \mathbf{\vec{x}}_k) + b^{(t)} - b^{(t+1)} \tag{10.135} 
\end{align}\]</span></p>
<p>Below is our example implementation of <strong>SMO</strong> in R code following the <strong>pseudo-code</strong> in Platt J.C. <span class="citation">(<a href="bibliography.html#ref-ref718j">1998</a>)</span> and motivated by a python code from jonchar.net with modification. The implementation has three procedures, namely <strong>take.step(.)</strong>, <strong>examine.example(.)</strong>, and a main function - in our case, we have <strong>my.smo.svm(.)</strong> for main.</p>
<p>Our implementation starts with a helper function, namely <strong>roll(.)</strong>, for the random starting point of a vector.</p>
<div class="sourceCode" id="cb1466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1466-1" data-line-number="1">roll &lt;-<span class="st"> </span><span class="cf">function</span>(x, start) {</a>
<a class="sourceLine" id="cb1466-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb1466-3" data-line-number="3">  <span class="cf">if</span> (start <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>start <span class="op">&gt;=</span><span class="st"> </span>n) <span class="kw">return</span>(x)</a>
<a class="sourceLine" id="cb1466-4" data-line-number="4">  x.e =<span class="st"> </span>x[(n <span class="op">-</span><span class="st"> </span>start <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>n]</a>
<a class="sourceLine" id="cb1466-5" data-line-number="5">  x.b =<span class="st"> </span>x[<span class="dv">1</span><span class="op">:</span>(n <span class="op">-</span><span class="st"> </span>start)]</a>
<a class="sourceLine" id="cb1466-6" data-line-number="6">  <span class="kw">c</span>(x.e, x.b)</a>
<a class="sourceLine" id="cb1466-7" data-line-number="7">}</a></code></pre></div>
<p>Then, we have two <strong>basic</strong> kernel functions - a linear kernel and a Gaussian kernel, respectively.</p>

<div class="sourceCode" id="cb1467"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1467-1" data-line-number="1">linear.kernel &lt;-<span class="st"> </span><span class="cf">function</span>(x1, x2, <span class="dt">b =</span> <span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1467-2" data-line-number="2">    x1 <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x2)  <span class="op">+</span><span class="st"> </span>b</a>
<a class="sourceLine" id="cb1467-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb1467-4" data-line-number="4">radial.kernel &lt;-<span class="st"> </span><span class="cf">function</span>(x1, x2, <span class="dt">sigma=</span><span class="dv">1</span>) { <span class="co"># Gaussian Kernel</span></a>
<a class="sourceLine" id="cb1467-5" data-line-number="5">    <span class="cf">if</span> (<span class="kw">is.null</span>(<span class="kw">nrow</span>(x1)) <span class="op">||</span><span class="st"> </span><span class="kw">dim</span>(x1) <span class="op">==</span><span class="st"> </span><span class="dv">1</span> ) {</a>
<a class="sourceLine" id="cb1467-6" data-line-number="6">        z =<span class="st"> </span><span class="op">-</span><span class="kw">sweep</span>(x2, <span class="dv">2</span>, x1, <span class="st">&#39;-&#39;</span>)</a>
<a class="sourceLine" id="cb1467-7" data-line-number="7">        x.norm =<span class="st"> </span><span class="kw">apply</span>( z , <span class="dv">1</span>, <span class="cf">function</span>(x) { <span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>)) })  </a>
<a class="sourceLine" id="cb1467-8" data-line-number="8">    } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1467-9" data-line-number="9">    <span class="cf">if</span> (<span class="kw">is.null</span>(<span class="kw">nrow</span>(x2)) <span class="op">||</span><span class="st"> </span><span class="kw">dim</span>(x2) <span class="op">==</span><span class="st"> </span><span class="dv">1</span> ) {</a>
<a class="sourceLine" id="cb1467-10" data-line-number="10">        z =<span class="st"> </span><span class="op">-</span><span class="kw">sweep</span>(x1, <span class="dv">2</span>, x2, <span class="st">&#39;-&#39;</span>)</a>
<a class="sourceLine" id="cb1467-11" data-line-number="11">        x.norm =<span class="st"> </span><span class="kw">apply</span>( z , <span class="dv">1</span>, <span class="cf">function</span>(x) { <span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>)) }) </a>
<a class="sourceLine" id="cb1467-12" data-line-number="12">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1467-13" data-line-number="13">        pu =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb1467-14" data-line-number="14">        N =<span class="st"> </span><span class="kw">nrow</span>(x1)</a>
<a class="sourceLine" id="cb1467-15" data-line-number="15">        <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1467-16" data-line-number="16">             u  =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(x1[i,],N),N, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1467-17" data-line-number="17">             a  =<span class="st">  </span><span class="kw">array</span>( <span class="kw">as.matrix</span>(u  <span class="op">-</span><span class="st"> </span>x2), <span class="kw">c</span>(N,<span class="dv">2</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1467-18" data-line-number="18">             pu =<span class="st"> </span><span class="kw">c</span>(pu, a)</a>
<a class="sourceLine" id="cb1467-19" data-line-number="19">        }</a>
<a class="sourceLine" id="cb1467-20" data-line-number="20">        pu     =<span class="st"> </span><span class="kw">array</span>(pu, <span class="kw">c</span>(N,<span class="dv">2</span>,N))</a>
<a class="sourceLine" id="cb1467-21" data-line-number="21">        x.norm =<span class="st"> </span><span class="kw">apply</span>(pu, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">1</span>), <span class="cf">function</span>(x) { <span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>)) })</a>
<a class="sourceLine" id="cb1467-22" data-line-number="22">    }</a>
<a class="sourceLine" id="cb1467-23" data-line-number="23">    <span class="kw">exp</span>( <span class="op">-</span><span class="st"> </span>(x.norm<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>sigma<span class="op">^</span><span class="dv">2</span>) )</a>
<a class="sourceLine" id="cb1467-24" data-line-number="24">}</a></code></pre></div>

<p>More importantly, we implement our objective function and prediction (decision) functions:</p>

<div class="sourceCode" id="cb1468"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1468-1" data-line-number="1">J =<span class="st"> </span>objective.function &lt;-<span class="st"> </span><span class="cf">function</span>(alphas, kernel, x, y) {</a>
<a class="sourceLine" id="cb1468-2" data-line-number="2">  a =<span class="st"> </span>alphas; <span class="kw">sum</span>(a) <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>((a<span class="op">*</span>a) <span class="op">*</span><span class="st"> </span>(y<span class="op">*</span>y) <span class="op">*</span><span class="st"> </span><span class="kw">kernel</span>(x,x))</a>
<a class="sourceLine" id="cb1468-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb1468-4" data-line-number="4">h =<span class="st"> </span>prediction.function &lt;-<span class="st"> </span><span class="cf">function</span>(alphas, kernel, x1, x2, y, b) {</a>
<a class="sourceLine" id="cb1468-5" data-line-number="5">  (alphas <span class="op">*</span><span class="st"> </span>y) <span class="op">%*%</span><span class="st"> </span><span class="kw">kernel</span>( x1, x2)  <span class="op">-</span><span class="st"> </span>b</a>
<a class="sourceLine" id="cb1468-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb1468-7" data-line-number="7">f =<span class="st"> </span>evaluate &lt;-<span class="st"> </span><span class="cf">function</span>(i) {</a>
<a class="sourceLine" id="cb1468-8" data-line-number="8">    <span class="kw">h</span>(alphas, kernel, x, <span class="kw">t</span>(<span class="kw">c</span>(x[i,])), y, b)</a>
<a class="sourceLine" id="cb1468-9" data-line-number="9">}</a></code></pre></div>

<p>The <strong>take.step(.)</strong> procedure implements the <strong>update rules</strong> for <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(\alpha_j\)</span>, including the <strong>b</strong> threshold and <strong>error cache</strong>.</p>

<div class="sourceCode" id="cb1469"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1469-1" data-line-number="1">take.step &lt;-<span class="st"> </span><span class="cf">function</span>(i1, i2, E2) {</a>
<a class="sourceLine" id="cb1469-2" data-line-number="2">  <span class="cf">if</span> (i1 <span class="op">==</span><span class="st"> </span>i2) { <span class="kw">return</span>(<span class="dv">0</span>) }</a>
<a class="sourceLine" id="cb1469-3" data-line-number="3">  alph1 =<span class="st"> </span>alphas[i1]; alph2 =<span class="st"> </span>alphas[i2]</a>
<a class="sourceLine" id="cb1469-4" data-line-number="4">  y1    =<span class="st"> </span>y[i1];      y2    =<span class="st"> </span>y[i2]</a>
<a class="sourceLine" id="cb1469-5" data-line-number="5">  s     =<span class="st"> </span>y1 <span class="op">*</span><span class="st"> </span>y2</a>
<a class="sourceLine" id="cb1469-6" data-line-number="6">  <span class="co"># Compute L, H</span></a>
<a class="sourceLine" id="cb1469-7" data-line-number="7">  <span class="cf">if</span> (y1 <span class="op">!=</span><span class="st"> </span>y2) {</a>
<a class="sourceLine" id="cb1469-8" data-line-number="8">    L =<span class="st"> </span><span class="kw">max</span>(<span class="dv">0</span>, alph2 <span class="op">-</span><span class="st"> </span>alph1); H =<span class="st"> </span><span class="kw">min</span>(C, C <span class="op">+</span><span class="st"> </span>alph2 <span class="op">-</span><span class="st"> </span>alph1)</a>
<a class="sourceLine" id="cb1469-9" data-line-number="9">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1469-10" data-line-number="10">  <span class="cf">if</span> (y1 <span class="op">==</span><span class="st"> </span>y2) {</a>
<a class="sourceLine" id="cb1469-11" data-line-number="11">    L =<span class="st"> </span><span class="kw">max</span>(<span class="dv">0</span>, alph1 <span class="op">+</span><span class="st"> </span>alph2 <span class="op">-</span><span class="st"> </span>C); H =<span class="st"> </span><span class="kw">min</span>(C, alph1 <span class="op">+</span><span class="st"> </span>alph2)</a>
<a class="sourceLine" id="cb1469-12" data-line-number="12">  } </a>
<a class="sourceLine" id="cb1469-13" data-line-number="13">  <span class="cf">if</span> (L <span class="op">==</span><span class="st"> </span>H) {</a>
<a class="sourceLine" id="cb1469-14" data-line-number="14">    <span class="kw">return</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1469-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb1469-16" data-line-number="16">  <span class="cf">if</span> (eps <span class="op">&lt;</span><span class="st"> </span>alph1 <span class="op">&amp;&amp;</span><span class="st"> </span>alph1 <span class="op">&lt;</span><span class="st"> </span>C <span class="op">-</span><span class="st"> </span>eps) {</a>
<a class="sourceLine" id="cb1469-17" data-line-number="17">      E1 =<span class="st"> </span>errors[i1]</a>
<a class="sourceLine" id="cb1469-18" data-line-number="18">  } <span class="cf">else</span> { </a>
<a class="sourceLine" id="cb1469-19" data-line-number="19">      E1 =<span class="st"> </span><span class="kw">f</span>(i1) <span class="op">-</span><span class="st"> </span>y1  </a>
<a class="sourceLine" id="cb1469-20" data-line-number="20">  }</a>
<a class="sourceLine" id="cb1469-21" data-line-number="21">  <span class="co"># Computer kernels</span></a>
<a class="sourceLine" id="cb1469-22" data-line-number="22">  k11 =<span class="st"> </span><span class="kw">kernel</span>(<span class="kw">t</span>(x[i1,]), <span class="kw">t</span>(x[i1,]))</a>
<a class="sourceLine" id="cb1469-23" data-line-number="23">  k12 =<span class="st"> </span><span class="kw">kernel</span>(<span class="kw">t</span>(x[i1,]), <span class="kw">t</span>(x[i2,]))</a>
<a class="sourceLine" id="cb1469-24" data-line-number="24">  k22 =<span class="st"> </span><span class="kw">kernel</span>(<span class="kw">t</span>(x[i2,]), <span class="kw">t</span>(x[i2,]))</a>
<a class="sourceLine" id="cb1469-25" data-line-number="25">  eta =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k12 <span class="op">-</span><span class="st"> </span>k11 <span class="op">-</span><span class="st"> </span>k22</a>
<a class="sourceLine" id="cb1469-26" data-line-number="26">  <span class="cf">if</span> (eta <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1469-27" data-line-number="27">    a2 =<span class="st"> </span>alph2 <span class="op">-</span><span class="st"> </span>y2 <span class="op">*</span><span class="st"> </span>(E1 <span class="op">-</span><span class="st"> </span>E2) <span class="op">/</span><span class="st"> </span>eta</a>
<a class="sourceLine" id="cb1469-28" data-line-number="28">    <span class="cf">if</span> (a2 <span class="op">&lt;</span><span class="st"> </span>L) { a2 =<span class="st"> </span>L } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1469-29" data-line-number="29">    <span class="cf">if</span> (a2 <span class="op">&gt;</span><span class="st"> </span>H) { a2 =<span class="st"> </span>H }</a>
<a class="sourceLine" id="cb1469-30" data-line-number="30">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1469-31" data-line-number="31">    adj.alphas =<span class="st"> </span>alphas</a>
<a class="sourceLine" id="cb1469-32" data-line-number="32">    adj.alphas[i2] =<span class="st"> </span>L</a>
<a class="sourceLine" id="cb1469-33" data-line-number="33">    L.obj =<span class="st"> </span><span class="kw">J</span>(adj.alphas, kernel, x, y)</a>
<a class="sourceLine" id="cb1469-34" data-line-number="34">    adj.alphas[i2] =<span class="st"> </span>H</a>
<a class="sourceLine" id="cb1469-35" data-line-number="35">    H.obj =<span class="st"> </span><span class="kw">J</span>(adj.alphas, kernel, x, y)</a>
<a class="sourceLine" id="cb1469-36" data-line-number="36">    <span class="cf">if</span> (L.obj <span class="op">&gt;</span><span class="st"> </span>H.obj <span class="op">+</span><span class="st"> </span>eps) { a2 =<span class="st"> </span>L } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1469-37" data-line-number="37">    <span class="cf">if</span> (L.obj <span class="op">&lt;</span><span class="st"> </span>H.obj <span class="op">-</span><span class="st"> </span>eps) { a2 =<span class="st"> </span>H } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1469-38" data-line-number="38">      { a2 =<span class="st"> </span>alph2 }</a>
<a class="sourceLine" id="cb1469-39" data-line-number="39">  }</a>
<a class="sourceLine" id="cb1469-40" data-line-number="40">  <span class="cf">if</span> (a2 <span class="op">&lt;</span><span class="st"> </span><span class="fl">1e-8</span>)     { a2 =<span class="st"> </span><span class="dv">0</span> } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1469-41" data-line-number="41">  <span class="cf">if</span> (a2 <span class="op">&gt;</span><span class="st"> </span>C <span class="op">-</span><span class="st"> </span><span class="fl">1e-8</span>) { a2 =<span class="st"> </span>C }</a>
<a class="sourceLine" id="cb1469-42" data-line-number="42">  <span class="cf">if</span> (<span class="kw">abs</span>(a2 <span class="op">-</span><span class="st"> </span>alph2) <span class="op">&lt;</span><span class="st"> </span>eps <span class="op">*</span><span class="st"> </span>(a2 <span class="op">+</span><span class="st"> </span>alph2 <span class="op">+</span><span class="st"> </span>eps)) { <span class="kw">return</span>(<span class="dv">0</span>) }</a>
<a class="sourceLine" id="cb1469-43" data-line-number="43">  <span class="co"># update alpha_i</span></a>
<a class="sourceLine" id="cb1469-44" data-line-number="44">  a2 =<span class="st"> </span><span class="kw">c</span>(a2)</a>
<a class="sourceLine" id="cb1469-45" data-line-number="45">  a1 =<span class="st"> </span><span class="kw">c</span>(alph1 <span class="op">+</span><span class="st"> </span>s <span class="op">*</span><span class="st"> </span>(alph2 <span class="op">-</span><span class="st"> </span>a2))</a>
<a class="sourceLine" id="cb1469-46" data-line-number="46">  <span class="co"># update new b threshold</span></a>
<a class="sourceLine" id="cb1469-47" data-line-number="47">  w1 =<span class="st"> </span><span class="kw">c</span>(y1 <span class="op">*</span><span class="st"> </span>(a1 <span class="op">-</span><span class="st"> </span>alph1));  w2 =<span class="st"> </span><span class="kw">c</span>(y2 <span class="op">*</span><span class="st"> </span>(a2 <span class="op">-</span><span class="st"> </span>alph2))</a>
<a class="sourceLine" id="cb1469-48" data-line-number="48">  b1 =<span class="st"> </span><span class="kw">c</span>(E1 <span class="op">+</span><span class="st"> </span>w1 <span class="op">*</span><span class="st"> </span>k11 <span class="op">+</span><span class="st"> </span>w2 <span class="op">*</span><span class="st"> </span>k12 <span class="op">+</span><span class="st"> </span>b)</a>
<a class="sourceLine" id="cb1469-49" data-line-number="49">  b2 =<span class="st"> </span><span class="kw">c</span>(E2 <span class="op">+</span><span class="st"> </span>w1 <span class="op">*</span><span class="st"> </span>k12 <span class="op">+</span><span class="st"> </span>w2 <span class="op">*</span><span class="st"> </span>k22 <span class="op">+</span><span class="st"> </span>b)</a>
<a class="sourceLine" id="cb1469-50" data-line-number="50">  <span class="cf">if</span> ( <span class="dv">0</span> <span class="op">&lt;</span><span class="st"> </span>a1 <span class="op">&amp;&amp;</span><span class="st"> </span>a1 <span class="op">&lt;</span><span class="st"> </span>C) {</a>
<a class="sourceLine" id="cb1469-51" data-line-number="51">     b.new =<span class="st"> </span>b1</a>
<a class="sourceLine" id="cb1469-52" data-line-number="52">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1469-53" data-line-number="53">  <span class="cf">if</span> ( <span class="dv">0</span> <span class="op">&lt;</span><span class="st"> </span>a2 <span class="op">&amp;&amp;</span><span class="st"> </span>a2 <span class="op">&lt;</span><span class="st"> </span>C) {</a>
<a class="sourceLine" id="cb1469-54" data-line-number="54">     b.new =<span class="st"> </span>b2</a>
<a class="sourceLine" id="cb1469-55" data-line-number="55">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1469-56" data-line-number="56">    b.new =<span class="st"> </span>(b1 <span class="op">+</span><span class="st"> </span>b2) <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span></a>
<a class="sourceLine" id="cb1469-57" data-line-number="57">  }</a>
<a class="sourceLine" id="cb1469-58" data-line-number="58">  <span class="co"># update alphas</span></a>
<a class="sourceLine" id="cb1469-59" data-line-number="59">  alphas[i1] &lt;&lt;-<span class="st"> </span>a1</a>
<a class="sourceLine" id="cb1469-60" data-line-number="60">  alphas[i2] &lt;&lt;-<span class="st"> </span>a2</a>
<a class="sourceLine" id="cb1469-61" data-line-number="61">  <span class="co"># update error cache</span></a>
<a class="sourceLine" id="cb1469-62" data-line-number="62">  <span class="cf">if</span> (<span class="dv">0</span> <span class="op">&lt;</span><span class="st"> </span>a1 <span class="op">&amp;&amp;</span><span class="st"> </span>a1 <span class="op">&lt;</span><span class="st"> </span>C) { errors[i1] &lt;&lt;-<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb1469-63" data-line-number="63">  <span class="cf">if</span> (<span class="dv">0</span> <span class="op">&lt;</span><span class="st"> </span>a2 <span class="op">&amp;&amp;</span><span class="st"> </span>a2 <span class="op">&lt;</span><span class="st"> </span>C) { errors[i2] &lt;&lt;-<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb1469-64" data-line-number="64">  alph.indices =<span class="st"> </span><span class="kw">which</span>( <span class="op">!</span>(<span class="kw">seq</span>(n) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(i1,i2)) )</a>
<a class="sourceLine" id="cb1469-65" data-line-number="65">  errors[alph.indices] &lt;&lt;-<span class="st"> </span>errors[alph.indices] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1469-66" data-line-number="66"><span class="st">        </span>w1 <span class="op">*</span><span class="st"> </span><span class="kw">kernel</span>(<span class="kw">t</span>(x[i1,]), x[alph.indices,]) <span class="op">+</span></a>
<a class="sourceLine" id="cb1469-67" data-line-number="67"><span class="st">        </span>w2 <span class="op">*</span><span class="st"> </span><span class="kw">kernel</span>(<span class="kw">t</span>(x[i2,]), x[alph.indices,]) <span class="op">+</span><span class="st"> </span>b <span class="op">-</span><span class="st"> </span>b.new</a>
<a class="sourceLine" id="cb1469-68" data-line-number="68">  <span class="co"># update b threshold  </span></a>
<a class="sourceLine" id="cb1469-69" data-line-number="69">  b &lt;&lt;-<span class="st"> </span><span class="kw">c</span>(b.new)</a>
<a class="sourceLine" id="cb1469-70" data-line-number="70">  <span class="kw">return</span> (<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1469-71" data-line-number="71">}</a></code></pre></div>

<p>As the functionâs name implies, we examine a given observation (or example) based on the second index.</p>

<div class="sourceCode" id="cb1470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1470-1" data-line-number="1">examine.example &lt;-<span class="st"> </span><span class="cf">function</span>(i2) {</a>
<a class="sourceLine" id="cb1470-2" data-line-number="2">  y2     =<span class="st"> </span>y[i2]</a>
<a class="sourceLine" id="cb1470-3" data-line-number="3">  alph2  =<span class="st"> </span>alphas[i2] </a>
<a class="sourceLine" id="cb1470-4" data-line-number="4">  <span class="cf">if</span> (eps <span class="op">&lt;</span><span class="st"> </span>alph2  <span class="op">&amp;&amp;</span><span class="st"> </span>alph2 <span class="op">&lt;</span><span class="st"> </span>C <span class="op">-</span><span class="st"> </span>eps) {</a>
<a class="sourceLine" id="cb1470-5" data-line-number="5">      E2 =<span class="st"> </span>errors[i2]</a>
<a class="sourceLine" id="cb1470-6" data-line-number="6">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1470-7" data-line-number="7">      E2 =<span class="st"> </span><span class="kw">f</span>(i2) <span class="op">-</span><span class="st"> </span>y2</a>
<a class="sourceLine" id="cb1470-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb1470-9" data-line-number="9">  r2     =<span class="st"> </span>E2 <span class="op">*</span><span class="st"> </span>y2</a>
<a class="sourceLine" id="cb1470-10" data-line-number="10">  <span class="cf">if</span> ((r2 <span class="op">&lt;</span><span class="st"> </span><span class="op">-</span>tol <span class="op">&amp;&amp;</span><span class="st"> </span>alph2 <span class="op">&lt;</span><span class="st"> </span>C) <span class="op">||</span><span class="st"> </span>(r2 <span class="op">&gt;</span><span class="st"> </span>tol <span class="op">&amp;&amp;</span><span class="st"> </span>alph2 <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)) {</a>
<a class="sourceLine" id="cb1470-11" data-line-number="11">    i1.indices =<span class="st"> </span><span class="kw">which</span>( alphas <span class="op">!=</span><span class="st"> </span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>alphas <span class="op">!=</span><span class="st"> </span>C )</a>
<a class="sourceLine" id="cb1470-12" data-line-number="12">    m =<span class="st"> </span><span class="kw">length</span>(i1.indices)</a>
<a class="sourceLine" id="cb1470-13" data-line-number="13">    <span class="co"># Consider heuristics in terms of max error deltas</span></a>
<a class="sourceLine" id="cb1470-14" data-line-number="14">    <span class="cf">if</span> (m <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1470-15" data-line-number="15">      <span class="cf">if</span> (E2 <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1470-16" data-line-number="16">        i1 =<span class="st"> </span><span class="kw">which.min</span>(errors)</a>
<a class="sourceLine" id="cb1470-17" data-line-number="17">      } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1470-18" data-line-number="18">      <span class="cf">if</span> (E2 <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1470-19" data-line-number="19">        i1 =<span class="st"> </span><span class="kw">which.max</span>(errors)</a>
<a class="sourceLine" id="cb1470-20" data-line-number="20">      }</a>
<a class="sourceLine" id="cb1470-21" data-line-number="21">      <span class="cf">if</span> (<span class="kw">take.step</span>(i1, i2, E2)) { <span class="kw">return</span>(<span class="dv">1</span>) }</a>
<a class="sourceLine" id="cb1470-22" data-line-number="22">    }</a>
<a class="sourceLine" id="cb1470-23" data-line-number="23">    <span class="co"># Consider heuristics in terms of non-zero and non-C alphas</span></a>
<a class="sourceLine" id="cb1470-24" data-line-number="24">    <span class="cf">if</span> (m <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1470-25" data-line-number="25">      rand.i =<span class="st"> </span><span class="kw">sample.int</span>(m, <span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1470-26" data-line-number="26">      rolled.indices =<span class="st"> </span><span class="kw">roll</span>(i1.indices, rand.i)</a>
<a class="sourceLine" id="cb1470-27" data-line-number="27">      <span class="cf">for</span> (i1 <span class="cf">in</span> rolled.indices) {</a>
<a class="sourceLine" id="cb1470-28" data-line-number="28">        <span class="cf">if</span> (<span class="kw">take.step</span>(i1, i2, E2)) { <span class="kw">return</span>(<span class="dv">1</span>) }</a>
<a class="sourceLine" id="cb1470-29" data-line-number="29">      }</a>
<a class="sourceLine" id="cb1470-30" data-line-number="30">    }</a>
<a class="sourceLine" id="cb1470-31" data-line-number="31">    <span class="co"># Otherwise, consider all observations</span></a>
<a class="sourceLine" id="cb1470-32" data-line-number="32">    rand.i =<span class="st"> </span><span class="kw">sample.int</span>(n, <span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1470-33" data-line-number="33">    <span class="cf">for</span> (i1 <span class="cf">in</span> <span class="kw">roll</span>(<span class="kw">seq</span>(n), rand.i)) {</a>
<a class="sourceLine" id="cb1470-34" data-line-number="34">        <span class="cf">if</span> (<span class="kw">take.step</span>(i1, i2, E2)) { <span class="kw">return</span>(<span class="dv">1</span>) }</a>
<a class="sourceLine" id="cb1470-35" data-line-number="35">    }</a>
<a class="sourceLine" id="cb1470-36" data-line-number="36">  }</a>
<a class="sourceLine" id="cb1470-37" data-line-number="37">  <span class="kw">return</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1470-38" data-line-number="38">}</a></code></pre></div>

<p>Our training function is <strong>my.smo.svm(.)</strong> and it learns data for classification.</p>

<div class="sourceCode" id="cb1471"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1471-1" data-line-number="1">train =<span class="st"> </span>my.smo.svm &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">C=</span><span class="dv">1</span>, <span class="dt">tol=</span><span class="fl">10e-3</span>, <span class="dt">eps=</span><span class="fl">10e-3</span>) {</a>
<a class="sourceLine" id="cb1471-2" data-line-number="2">  n           =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1471-3" data-line-number="3">  num.changed =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1471-4" data-line-number="4">  examine.all =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1471-5" data-line-number="5">  <span class="cf">while</span> (num.changed <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>examine.all ) {</a>
<a class="sourceLine" id="cb1471-6" data-line-number="6">    num.changed =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1471-7" data-line-number="7">    <span class="cf">if</span> (examine.all) {</a>
<a class="sourceLine" id="cb1471-8" data-line-number="8">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1471-9" data-line-number="9">        num.changed =<span class="st"> </span>num.changed <span class="op">+</span><span class="st"> </span><span class="kw">examine.example</span>(i)</a>
<a class="sourceLine" id="cb1471-10" data-line-number="10">      }</a>
<a class="sourceLine" id="cb1471-11" data-line-number="11">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1471-12" data-line-number="12">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">which</span>( alphas <span class="op">!=</span><span class="st"> </span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>alphas <span class="op">!=</span><span class="st"> </span>C )) {</a>
<a class="sourceLine" id="cb1471-13" data-line-number="13">        num.changed =<span class="st"> </span>num.changed <span class="op">+</span><span class="st"> </span><span class="kw">examine.example</span>(i)</a>
<a class="sourceLine" id="cb1471-14" data-line-number="14">      }</a>
<a class="sourceLine" id="cb1471-15" data-line-number="15">    }</a>
<a class="sourceLine" id="cb1471-16" data-line-number="16">    <span class="cf">if</span> (examine.all <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1471-17" data-line-number="17">      examine.all =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1471-18" data-line-number="18">    } <span class="cf">else</span> <span class="cf">if</span> (num.changed <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1471-19" data-line-number="19">      examine.all =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1471-20" data-line-number="20">    }</a>
<a class="sourceLine" id="cb1471-21" data-line-number="21">  }</a>
<a class="sourceLine" id="cb1471-22" data-line-number="22">  <span class="kw">list</span>(<span class="st">&quot;alphas&quot;</span> =<span class="st"> </span>alphas, <span class="st">&quot;kernel&quot;</span> =<span class="st"> </span>kernel, <span class="st">&quot;x&quot;</span> =<span class="st"> </span>x, <span class="st">&quot;y&quot;</span> =<span class="st"> </span>y, <span class="st">&quot;b&quot;</span> =<span class="st"> </span>b)</a>
<a class="sourceLine" id="cb1471-23" data-line-number="23">}</a></code></pre></div>

<p>Then, we have a function to plot the result of our <strong>SVM</strong>:</p>

<div class="sourceCode" id="cb1472"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1472-1" data-line-number="1">plot.svm &lt;-<span class="st"> </span><span class="cf">function</span>(title, model) {</a>
<a class="sourceLine" id="cb1472-2" data-line-number="2">  alphas =<span class="st"> </span>model<span class="op">$</span>alphas; kernel =<span class="st"> </span>model<span class="op">$</span>kernel</a>
<a class="sourceLine" id="cb1472-3" data-line-number="3">  x =<span class="st"> </span>model<span class="op">$</span>x; y =<span class="st"> </span>model<span class="op">$</span>y; b =<span class="st"> </span>model<span class="op">$</span>b</a>
<a class="sourceLine" id="cb1472-4" data-line-number="4">  x.test =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(x[,<span class="dv">1</span>]), <span class="kw">max</span>(x[,<span class="dv">1</span>]), <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb1472-5" data-line-number="5">  y.test =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(x[,<span class="dv">2</span>]), <span class="kw">max</span>(x[,<span class="dv">2</span>]), <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb1472-6" data-line-number="6">  grid =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">100</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>), <span class="dv">100</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1472-7" data-line-number="7">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x.test)) {</a>
<a class="sourceLine" id="cb1472-8" data-line-number="8">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(y.test)) {</a>
<a class="sourceLine" id="cb1472-9" data-line-number="9">        grid[j,i] =<span class="st"> </span><span class="kw">c</span>(<span class="kw">h</span>(alphas, kernel, x, <span class="kw">t</span>(<span class="kw">c</span>(x.test[j],y.test[i])), y, b))</a>
<a class="sourceLine" id="cb1472-10" data-line-number="10">      }</a>
<a class="sourceLine" id="cb1472-11" data-line-number="11">  }</a>
<a class="sourceLine" id="cb1472-12" data-line-number="12">  <span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x[,<span class="dv">1</span>]), <span class="dt">ylim=</span><span class="kw">range</span>(x[,<span class="dv">2</span>]),</a>
<a class="sourceLine" id="cb1472-13" data-line-number="13">     <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">main=</span>title)</a>
<a class="sourceLine" id="cb1472-14" data-line-number="14">  <span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1472-15" data-line-number="15">  <span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">v=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1472-16" data-line-number="16">  <span class="kw">points</span>(x[,<span class="dv">1</span>],x[,<span class="dv">2</span>], <span class="dt">pch=</span><span class="dv">20</span>,  <span class="dt">col=</span><span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span><span class="dv">-1</span>, </a>
<a class="sourceLine" id="cb1472-17" data-line-number="17">                                          <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>))</a>
<a class="sourceLine" id="cb1472-18" data-line-number="18">  no_opt =<span class="st"> </span><span class="kw">which</span>(<span class="kw">abs</span>(alphas) <span class="op">&gt;</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb1472-19" data-line-number="19">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(no_opt)) {</a>
<a class="sourceLine" id="cb1472-20" data-line-number="20">      <span class="kw">points</span>(x[no_opt[i],<span class="dv">1</span>], x[no_opt[i],<span class="dv">2</span>], <span class="dt">pch=</span><span class="dv">1</span>, <span class="dt">cex=</span><span class="fl">1.2</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb1472-21" data-line-number="21">  }</a>
<a class="sourceLine" id="cb1472-22" data-line-number="22">  <span class="kw">contour</span>(<span class="dt">x =</span> x.test, <span class="dt">y =</span> y.test, <span class="dt">method =</span> <span class="st">&quot;edge&quot;</span>, <span class="dt">lwd=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb1472-23" data-line-number="23">        grid, <span class="dt">levels=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>), </a>
<a class="sourceLine" id="cb1472-24" data-line-number="24">        <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>),</a>
<a class="sourceLine" id="cb1472-25" data-line-number="25">        <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1472-26" data-line-number="26">}</a></code></pre></div>

<p>Also, for fix parameters, let us have the following global variables:</p>

<div class="sourceCode" id="cb1473"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1473-1" data-line-number="1">tol    =<span class="st"> </span><span class="fl">10e-3</span> <span class="co"># tolerance</span></a>
<a class="sourceLine" id="cb1473-2" data-line-number="2">eps    =<span class="st"> </span><span class="fl">10e-3</span> <span class="co"># constraint limit</span></a></code></pre></div>

<p>Now, to use our implementation, let us use the following dataset:</p>

<div class="sourceCode" id="cb1474"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1474-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1474-2" data-line-number="2">N         =<span class="st"> </span><span class="dv">100</span>; v =<span class="st"> </span><span class="dv">1</span> <span class="co"># variance</span></a>
<a class="sourceLine" id="cb1474-3" data-line-number="3">x1.blue   =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); x2.blue =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); y1 =<span class="st"> </span><span class="kw">rep</span>( <span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1474-4" data-line-number="4">x1.red    =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); x2.red  =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); y2 =<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1474-5" data-line-number="5">x         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(x1.blue, x1.red), <span class="kw">c</span>(x2.blue, x2.red)) </a>
<a class="sourceLine" id="cb1474-6" data-line-number="6">y         =<span class="st"> </span><span class="kw">c</span>(<span class="kw">cbind</span>(<span class="kw">c</span>(y1, y2)))</a></code></pre></div>

<p>Finally, let us run our implementation and plot. See Figure <a href="10.2-binary-classification-supervised.html#fig:smo1">10.26</a>.</p>

<div class="sourceCode" id="cb1475"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1475-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1475-2" data-line-number="2">n      =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1475-3" data-line-number="3">kernel =<span class="st"> </span>linear.kernel</a>
<a class="sourceLine" id="cb1475-4" data-line-number="4">alphas =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1475-5" data-line-number="5">b      =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1475-6" data-line-number="6">C      =<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb1475-7" data-line-number="7">errors =<span class="st"> </span><span class="kw">h</span>(alphas, kernel, x, x, y, b) <span class="op">-</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb1475-8" data-line-number="8">my.smo.model =<span class="st"> </span><span class="kw">my.smo.svm</span>(x)</a>
<a class="sourceLine" id="cb1475-9" data-line-number="9"><span class="kw">plot.svm</span>(<span class="st">&quot;SMO (linear)&quot;</span>, my.smo.model)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:smo1"></span>
<img src="DS_files/figure-html/smo1-1.png" alt="SMO (Linear)" width="70%" />
<p class="caption">
Figure 10.26: SMO (Linear)
</p>
</div>

<p>Below, we show learning <strong>SVM-SMO</strong> using radial kernel. See Figure <a href="10.2-binary-classification-supervised.html#fig:smo2">10.27</a>.</p>

<div class="sourceCode" id="cb1476"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1476-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1476-2" data-line-number="2">n =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb1476-3" data-line-number="3">pi.set =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi, <span class="dt">length.out=</span>n)</a>
<a class="sourceLine" id="cb1476-4" data-line-number="4">x1.outer.rand =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb1476-5" data-line-number="5">x2.outer.rand =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>) </a>
<a class="sourceLine" id="cb1476-6" data-line-number="6">x1.inner.rand =<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb1476-7" data-line-number="7">x2.inner.rand =<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>) </a>
<a class="sourceLine" id="cb1476-8" data-line-number="8">x1.outer.line =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set)  </a>
<a class="sourceLine" id="cb1476-9" data-line-number="9">x2.outer.line =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) </a>
<a class="sourceLine" id="cb1476-10" data-line-number="10">x1.inner.line =<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set)  </a>
<a class="sourceLine" id="cb1476-11" data-line-number="11">x2.inner.line =<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) </a>
<a class="sourceLine" id="cb1476-12" data-line-number="12">x.line    =<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand), </a>
<a class="sourceLine" id="cb1476-13" data-line-number="13">                  <span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand)) </a>
<a class="sourceLine" id="cb1476-14" data-line-number="14">x         =<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">cbind</span>(x1.outer.rand, x2.outer.rand), </a>
<a class="sourceLine" id="cb1476-15" data-line-number="15">                  <span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand)) </a>
<a class="sourceLine" id="cb1476-16" data-line-number="16">y         =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), <span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, n))</a></code></pre></div>
<div class="sourceCode" id="cb1477"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1477-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1477-2" data-line-number="2">n      =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1477-3" data-line-number="3">kernel =<span class="st"> </span>radial.kernel</a>
<a class="sourceLine" id="cb1477-4" data-line-number="4">alphas =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1477-5" data-line-number="5">b      =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1477-6" data-line-number="6">C      =<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb1477-7" data-line-number="7">errors =<span class="st"> </span><span class="kw">h</span>(alphas, kernel, x, x, y, b) <span class="op">-</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb1477-8" data-line-number="8">my.smo.model =<span class="st"> </span><span class="kw">my.smo.svm</span>(x)</a>
<a class="sourceLine" id="cb1477-9" data-line-number="9"><span class="kw">plot.svm</span>(<span class="st">&quot;SMO (Radial)&quot;</span>, my.smo.model)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:smo2"></span>
<img src="DS_files/figure-html/smo2-1.png" alt="SMO (Radial)" width="70%" />
<p class="caption">
Figure 10.27: SMO (Radial)
</p>
</div>

<p>Below, we show another <strong>SVM-SMO</strong> learning using radial kernel. See Figure <a href="10.2-binary-classification-supervised.html#fig:smo3">10.28</a>.</p>

<div class="sourceCode" id="cb1478"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1478-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1478-2" data-line-number="2">n =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb1478-3" data-line-number="3">pi.set =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,  pi, <span class="dt">length.out=</span>n)</a>
<a class="sourceLine" id="cb1478-4" data-line-number="4">x1.outer.rand =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb1478-5" data-line-number="5">x2.outer.rand =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>) </a>
<a class="sourceLine" id="cb1478-6" data-line-number="6">x1.inner.rand =<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>) <span class="op">+</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1478-7" data-line-number="7">x2.inner.rand =<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>) <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1478-8" data-line-number="8">x1.outer.line =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set)</a>
<a class="sourceLine" id="cb1478-9" data-line-number="9">x2.outer.line =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) </a>
<a class="sourceLine" id="cb1478-10" data-line-number="10">x1.inner.line =<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1478-11" data-line-number="11">x2.inner.line =<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1478-12" data-line-number="12">x.line    =<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand), </a>
<a class="sourceLine" id="cb1478-13" data-line-number="13">                  <span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand)) </a>
<a class="sourceLine" id="cb1478-14" data-line-number="14">x         =<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">cbind</span>(x1.outer.rand, x2.outer.rand), </a>
<a class="sourceLine" id="cb1478-15" data-line-number="15">                  <span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand)) </a>
<a class="sourceLine" id="cb1478-16" data-line-number="16">y         =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), <span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, n))</a></code></pre></div>
<div class="sourceCode" id="cb1479"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1479-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1479-2" data-line-number="2">n      =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1479-3" data-line-number="3">kernel =<span class="st"> </span>radial.kernel</a>
<a class="sourceLine" id="cb1479-4" data-line-number="4">alphas =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1479-5" data-line-number="5">b      =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1479-6" data-line-number="6">C      =<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb1479-7" data-line-number="7">errors =<span class="st"> </span><span class="kw">h</span>(alphas, kernel, x, x, y, b) <span class="op">-</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb1479-8" data-line-number="8">my.smo.model =<span class="st"> </span><span class="kw">my.smo.svm</span>(x)</a>
<a class="sourceLine" id="cb1479-9" data-line-number="9"><span class="kw">plot.svm</span>(<span class="st">&quot;SMO (Radial)&quot;</span>, my.smo.model)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:smo3"></span>
<img src="DS_files/figure-html/smo3-1.png" alt="SMO (Radial)" width="70%" />
<p class="caption">
Figure 10.28: SMO (Radial)
</p>
</div>

<p>We leave readers to adjust the random seed, number of samples, and the <strong>C</strong> parameter as an exercise.</p>
</div>
<div id="sdca-based-svm" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.2.3</span> SDCA-based SVM <a href="10.2-binary-classification-supervised.html#sdca-based-svm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another algorithm to mention for <strong>dual optimization</strong> is the <strong>Stochastic Dual Coordinate Ascent (SDCA)</strong> analyzed by Shai Shalev-Shwartz et al. <span class="citation">(<a href="bibliography.html#ref-ref745c">2008</a>)</span> along with reference to <strong>DCA</strong> as proposed by Hsieh C. et al. <span class="citation">(<a href="bibliography.html#ref-ref745c">2008</a>)</span>.</p>
<p>There are variants of <strong>SDCA</strong>, all intended to enhance the algorithm in parallel processing, distributed processing, and using mini-batch.  </p>
<p>In relation to the <strong>Dual Lagrangian formulation</strong>, we rewrite the second term of the objective function in the <strong>fourth</strong> step of <strong>SMO</strong>: </p>
<p><span class="math display" id="eq:equate1120121">\[\begin{align}
\mathcal{J}(\alpha) = 
\text{arg}\ \underset{\alpha_i \ge 0}{\text{max}} 
\left\{\sum_{i=1}^n \alpha_i- \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \left(\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j}\right) \right\}  \tag{10.136} 
\end{align}\]</span></p>
<p>such that we have the following:</p>
<p><span class="math display" id="eq:equate1120124" id="eq:equate1120123" id="eq:equate1120122">\[\begin{align}
 \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j (\mathbf{x_i} \cdot \mathbf{x_j}) 
&amp;=  \frac{1}{2} \left(\sum_{i=1}^n  \alpha_i y_i \mathbf{x_i} \right)
 \left( \sum_{j=1}^n \alpha_j y_j \mathbf{x_j} \right) \tag{10.137} \\
&amp;=  \frac{1}{2} \mathbf{w} \cdot \mathbf{w} \tag{10.138} \\ 
&amp;= \frac{1}{2}\|\mathbf{w}\|^2_2   \tag{10.139} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\frac{1}{2} \left(\sum_{i=1}^n \alpha_i y_i \mathbf{x_i} \right)\)</span> is based on result of the partial derivative of the <strong>dual form</strong> with respect to <strong>w</strong>.</p>
<p>Our objective function is therefore rewritten this way (note here that we are using the <span class="math inline">\(\lambda &gt; 0\)</span> as our regularizer):</p>
<p><span class="math display" id="eq:equate1120125">\[\begin{align}
\mathcal{J}(\alpha) = 
\text{arg}\ \underset{\alpha_i \ge 0}{\text{max}} 
\left\{\sum_{i=1}^n \alpha_i- \frac{\lambda}{2} \|\mathbf{w}\|^2 \right\}  \tag{10.140} 
\end{align}\]</span></p>
<p>Now consider the delta of <span class="math inline">\(\alpha_i\)</span> denoted as <span class="math inline">\(\Delta^{(\alpha)}_i\)</span> so that <span class="math inline">\(\alpha^{(t+1)}_i = \alpha_i^{(t)} + \Delta\alpha_i\)</span>, we then have the dual form for the ith observation: <span class="math inline">\(\mathbf{D(\alpha_i + \Delta^{(\alpha)}_i)}\)</span>. Equivalently, our primal form is updated with the following delta <span class="math inline">\((\lambda n)^{(-1)}\Delta^{(\alpha)} x_i\)</span> so that we therefore have <span class="math inline">\(\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} + (\lambda n)^{(-1)}\Delta^{(\alpha)} x_i\)</span>.</p>
<p>We then update our objective function:</p>
<p><span class="math display" id="eq:equate1120126">\[\begin{align}
\mathcal{J}(\alpha) = 
\text{arg}\ \underset{\alpha_i \ge 0}{\text{max}} 
\left\{\sum_{i=1}^n \alpha_i + \Delta^{(\alpha)}) - \frac{\lambda}{2} \|\mathbf{w} + (\lambda n)^{-1} \Delta^{(\alpha)} x_i\|^2 \right\}  \tag{10.141} 
\end{align}\]</span></p>
<p>With all that, we now have our <strong>update rules</strong> for <strong>SDCA</strong> like so:</p>
<p><span class="math display" id="eq:equate1120129" id="eq:equate1120128" id="eq:equate1120127">\[\begin{align}
\Delta_i^{(\alpha)} {}&amp;=  y_i\ \underbrace{
           \text{max} \left(0, \text{min} \left(1, \frac{1 - 
             y_i x_i^T \mathbf{w}^{(t-1)}}{\|x_i\|^2_2/(\lambda n)}\right) + y_i \alpha_i^{(t-1)} \right)
}_{\text{hinge-loss in closed form}}
             - \alpha_i^{(t-1)} \tag{10.142} \\
\alpha_{i}^{(t+1)} &amp;= \alpha_i^{(t)} + \Delta_i^{(\alpha)}\ \ \ \ \ \ \ \leftarrow \text{(dual update)}  \tag{10.143} \\
\mathbf{w}^{(t+1)} &amp;= \mathbf{w}^{(t)} + \frac{\Delta_i^{(\alpha)}}{\lambda n} x_i\ \ \ \leftarrow \text{(primal update)}   \tag{10.144} 
\end{align}\]</span></p>
<p>For the algorithm, we focus on the <strong>SDCA-Perm</strong> variant from Shai Shalev-Shwatz and Tong Zhang <span class="citation">(<a href="bibliography.html#ref-ref710s">2013</a>)</span> in solving the <strong>dual problem</strong> of <strong>SVM</strong> using the <strong>hinge-loss</strong> and <strong>random option</strong>. Below is the algorithm for review.</p>
<p><span class="math display">\[
 \begin{array}{ll}
\mathbf{\text{SDCA Algorithm}} \\
\text{Shalev-Schwartz et al. (arxiv 1209.1873v2 2013)}\\
\\
\text{Input:}\ \ S, \lambda, \text{T} \\
\text{Set}\ \mathbf{w}_0 = z + \frac{1}{\lambda n} \sum_{i=1}^n \alpha^{0} x_i\\
\text{loop}\ t\ in\ 1,2,...,\text{T}  \\
\ \ \ \ \text{Choose}\ i_t\ \in \{1,...,|S|\}\ \ \leftarrow \ \ \text{(uniformly random)}\\  
\ \ \ \ \Delta_i^{(\alpha)} = y_i\ \text{max} \left(0, \text{min} \left(1, \frac{1 - 
             y_i x_i^T \mathbf{w}^{(t-1)}}{\|x_i\|^2_2/(\lambda n)}\right) + y_i \alpha_i^{(t-1)} \right) - \alpha_i^{(t-1)}\\
\ \ \ \ \alpha_i^{(t+1)} = \alpha_i^{(t)} + \Delta_i^{(\alpha)}\ \ \ \ \ \ \ \ \ \ \leftarrow \text{(dual update)} \\
\ \ \ \ \mathbf{w}^{(t+1)} =  \mathbf{w}^{(t)} + \frac{\Delta_i^{(\alpha)}}{\lambda n} x_i\ \ \ \ \ \ \leftarrow \text{(primal update)} \\
\text{end loop} \\
\text{Ouput}\ \mathbf{w}^{(T+1)} 
\end{array}
\]</span></p>
<p>Below is an example implementation of <strong>SDCA</strong> for <strong>SVM</strong>. Here, we use <strong>sample.int(.)</strong> to simulate random sampling of the index (<span class="math inline">\(i^{(t)}\)</span>).</p>

<div class="sourceCode" id="cb1480"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1480-1" data-line-number="1">my.sdca.svm &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, <span class="dt">lambda=</span><span class="fl">0.01</span>, <span class="dt">limit=</span><span class="dv">15000</span>) {</a>
<a class="sourceLine" id="cb1480-2" data-line-number="2">  n     =<span class="st"> </span><span class="kw">nrow</span>(x); </a>
<a class="sourceLine" id="cb1480-3" data-line-number="3">  x =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x)</a>
<a class="sourceLine" id="cb1480-4" data-line-number="4">  p =<span class="st"> </span><span class="kw">ncol</span>(x)</a>
<a class="sourceLine" id="cb1480-5" data-line-number="5">  w     =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, p)</a>
<a class="sourceLine" id="cb1480-6" data-line-number="6">  alphas =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1480-7" data-line-number="7">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb1480-8" data-line-number="8">      i  =<span class="st"> </span><span class="kw">sample.int</span>(n, <span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1480-9" data-line-number="9">      xi =<span class="st"> </span>x[i,]; yi =<span class="st"> </span>y[i]</a>
<a class="sourceLine" id="cb1480-10" data-line-number="10">      norm_x  =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(xi<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1480-11" data-line-number="11">      delta.alpha_i =<span class="st"> </span>yi <span class="op">*</span><span class="st"> </span><span class="kw">max</span>(<span class="dv">0</span>, <span class="kw">min</span>(<span class="dv">1</span>, ( (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>yi <span class="op">*</span><span class="st"> </span><span class="kw">t</span>(xi) <span class="op">%*%</span><span class="st"> </span>w) <span class="op">*</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1480-12" data-line-number="12"><span class="st">                </span>lambda <span class="op">*</span><span class="st"> </span>n<span class="op">/</span><span class="st"> </span>norm_x )) <span class="op">+</span><span class="st"> </span>yi <span class="op">*</span><span class="st"> </span>alphas[i]) <span class="op">-</span><span class="st"> </span>alphas[i]</a>
<a class="sourceLine" id="cb1480-13" data-line-number="13">      alphas[i] =<span class="st"> </span>alphas[i] <span class="op">+</span><span class="st"> </span>delta.alpha_i</a>
<a class="sourceLine" id="cb1480-14" data-line-number="14">      w       =<span class="st"> </span>w <span class="op">+</span><span class="st"> </span>( delta.alpha_i <span class="op">*</span><span class="st"> </span>xi) <span class="op">/</span><span class="st"> </span>(lambda <span class="op">*</span><span class="st"> </span>n )</a>
<a class="sourceLine" id="cb1480-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb1480-16" data-line-number="16">  <span class="kw">list</span>(<span class="st">&quot;alphas&quot;</span> =<span class="st"> </span>alphas, <span class="st">&quot;w&quot;</span> =<span class="st"> </span>w)</a>
<a class="sourceLine" id="cb1480-17" data-line-number="17">}</a></code></pre></div>

<p>Let us use the same linear dataset as we did in <strong>SMO</strong> previously to use our implementation.</p>

<div class="sourceCode" id="cb1481"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1481-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1481-2" data-line-number="2">eps =<span class="st"> </span><span class="fl">10e-3</span></a>
<a class="sourceLine" id="cb1481-3" data-line-number="3">N         =<span class="st"> </span><span class="dv">20</span>; v =<span class="st"> </span><span class="dv">1</span> <span class="co"># variance</span></a>
<a class="sourceLine" id="cb1481-4" data-line-number="4">x1.blue   =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); x2.blue =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); y1 =<span class="st"> </span><span class="kw">rep</span>( <span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1481-5" data-line-number="5">x1.red    =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); x2.red  =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); y2 =<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1481-6" data-line-number="6">x         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(x1.blue, x1.red), <span class="kw">c</span>(x2.blue, x2.red)) </a>
<a class="sourceLine" id="cb1481-7" data-line-number="7">y         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(y1, y2))</a></code></pre></div>

<p>Moreover, because our algorithm is stochastic, let us use a random seed for our implementation.</p>

<div class="sourceCode" id="cb1482"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1482-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1482-2" data-line-number="2">svm.sdca.model =<span class="st"> </span><span class="kw">my.sdca.svm</span>(x,y)</a>
<a class="sourceLine" id="cb1482-3" data-line-number="3">hplanes        =<span class="st"> </span><span class="kw">svm.hyperplanes</span>(svm.sdca.model<span class="op">$</span>w)</a></code></pre></div>

<p>We then plot. See Figure <a href="10.2-binary-classification-supervised.html#fig:sdcasvm">10.29</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sdcasvm"></span>
<img src="DS_files/figure-html/sdcasvm-1.png" alt="SVM (Stochastic Dual Coordinate Ascent)" width="70%" />
<p class="caption">
Figure 10.29: SVM (Stochastic Dual Coordinate Ascent)
</p>
</div>

<p><strong>Finally</strong>, for prediction, we can use the model to classify any new or missing data.</p>
<p><span class="math display" id="eq:eqnnumber417">\[\begin{align}
h(x; \mathbf{w}) = \text{sign}(g(x)) = \text{sign}(\mathbf{w}^T \mathbf{x} + b)\ \ \ \leftarrow\ \ \ \
\begin{cases}
+1 &amp; \mathbf{w}^T \mathbf{x} \ge 0\\
-1 &amp; \mathbf{w}^T \mathbf{x} &lt; 0\\
\end{cases} \tag{10.145}
\end{align}\]</span></p>
<div class="sourceCode" id="cb1483"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1483-1" data-line-number="1">x.new =<span class="st"> </span><span class="kw">rbind</span>( <span class="kw">c</span>(<span class="st">&quot;intercept&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;x1&quot;</span> =<span class="st"> </span><span class="dv">-2</span>, <span class="st">&quot;x2&quot;</span> =<span class="st">  </span><span class="dv">2</span>), </a>
<a class="sourceLine" id="cb1483-2" data-line-number="2">               <span class="kw">c</span>(<span class="st">&quot;intercept&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;x1&quot;</span> =<span class="st">  </span><span class="dv">2</span>, <span class="st">&quot;x2&quot;</span> =<span class="st"> </span><span class="dv">-2</span>))</a>
<a class="sourceLine" id="cb1483-3" data-line-number="3">my.sdca.prediction &lt;-<span class="st"> </span><span class="cf">function</span>(w, x) {</a>
<a class="sourceLine" id="cb1483-4" data-line-number="4">    <span class="kw">c</span>(<span class="kw">sign</span>(w  <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x)))</a>
<a class="sourceLine" id="cb1483-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1483-6" data-line-number="6">h =<span class="st"> </span><span class="kw">my.sdca.prediction</span>(svm.sdca.model<span class="op">$</span>w, x.new)</a>
<a class="sourceLine" id="cb1483-7" data-line-number="7">pred =<span class="st"> </span><span class="kw">ifelse</span>(h <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;positive&quot;</span>, <span class="st">&quot;negative&quot;</span>)</a>
<a class="sourceLine" id="cb1483-8" data-line-number="8"><span class="kw">print</span>(<span class="kw">cbind</span>(x.new, pred), <span class="dt">quote=</span><span class="ot">FALSE</span>, <span class="dt">right=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##      intercept x1 x2     pred
## [1,]         1 -2  2 positive
## [2,]         1  2 -2 negative</code></pre>
<p>We showed a sample Kernel functions in our <strong>SMO-based SVM</strong>. We leave readers to investigate Kernel functions in use with <strong>SDCA-based SVM</strong> and as an exercise, modify our example implementation above. Also, there are variants of SDCA worth investigating with the introduction of <strong>mini-batching</strong>, along with properties that make the algorithm adaptive and distributed.</p>
<p>Also, for other variants of <strong>SVM</strong>, we leave readers to investigate <strong>Top-K multiclass SVM</strong> for a high accuracy multiclass <strong>SVM</strong> algorithm with <strong>top k</strong> error prioritization, and <strong>Ensemble SVM</strong> to support <strong>ensemble models</strong> for <strong>SVM models</strong>. Note that an example application for <strong>Top-K</strong> multi-classification is around <strong>image/object classification and image/object segmentation</strong>.</p>
<p>As for <strong>prediction performance</strong> for <strong>SVM</strong> methods, we can reference <strong>ConfusionMatrix</strong> for metrics such as <strong>specificity</strong>, <strong>sensitivity</strong>, <strong>accuracy</strong>, <strong>F1 score</strong>, and others.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="10.1-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="10.3-multi-class-classification-supervised.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
