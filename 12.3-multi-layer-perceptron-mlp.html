<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.3 Multi Layer Perceptron (MLP)  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="12.3 Multi Layer Perceptron (MLP)  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.3 Multi Layer Perceptron (MLP)  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2023-03-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="12.2-adaptive-linear-neuron-adaline.html"/>
<link rel="next" href="12.4-convolutional-neural-network-cnn.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat-and-disclaimer.html"><a href="caveat-and-disclaimer.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-poisson-equation"><i class="fa fa-check"></i><b>4.6.1</b> The Poisson Equation </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.4</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.6</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearson’s Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naïve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naïve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs. Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-learning-deep-rl.html"><a href="13.8-deep-reinforcement-learning-deep-rl.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Learning (Deep RL)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multi-layer-perceptron-mlp" class="section level2 hasAnchor">
<h2><span class="header-section-number">12.3</span> Multi Layer Perceptron (MLP)  <a href="12.3-multi-layer-perceptron-mlp.html#multi-layer-perceptron-mlp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now delve deep into <strong>MLP</strong>, one of the many essential topics in <strong>Deep Learning</strong>. The fundamental inner workings of <strong>MLP</strong> serve as a necessary precursor following other advanced <strong>neural networks</strong> such as <strong>Convolutional Neural Network</strong> and <strong>Recurrent Neural Network</strong>.</p>
<p>From a simple <strong>Perceptron</strong>, let us add a bit of complexity using Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:deltarule">12.8</a> to demonstrate a <strong>two-layer</strong> <strong>Neural Network</strong> in which each layer is a composite of <strong>Simple Perceptrons</strong>. Note that whereas <strong>Perceptrons</strong> use <strong>step functions</strong>, let us use <strong>sigmoid functions</strong>. Hereafter, if we use the acronym <strong>MLP</strong>, we refer to a vanilla <strong>Multi-Layer Neural Network</strong>.</p>
<p>There are two hidden layers in the network, each consisting of two perceptrons, followed by an output layer.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:deltarule"></span>
<img src="deltarule.png" alt="Multi-Layer Perceptron" width="80%" />
<p class="caption">
Figure 12.8: Multi-Layer Perceptron
</p>
</div>
<p>The schema demonstrates a <strong>Multi-Layered Perceptron</strong> that has the property of a <strong>complete Neural Network</strong>. That means that every feature input connects to every neuron in each layer. Moreover, every neuron in each layer connects to all other neurons of the next layer. Therefore, the number of parameters (or <span class="math inline">\(\omega\)</span> coefficients) of a complete <strong>Neural Network</strong> is calculated. For example, suppose we have four hundred input features, three <strong>hidden</strong> layers of five neurons each, and two outputs in the output layer ( granting we consider full bias - each input or hidden layer has a bias), then we are looking at the following computation:</p>
<p><span class="math display">\[
\begin{array}{ll}
\text{number of }&amp;\text{parameters (}\omega\ \text{coefficients)} \\
&amp;= \text{no. of features} \times \text{no. of neuron in HL1}  \\
&amp;+ \text{ no. of neuron in HL1} \times \text{no. of neuron in HL2}\\
&amp;+ \text{ no. of neuron in HL2} \times \text{no. of neuron in HL3}\\
&amp;+ \text{ no. of neuron in HL3} \times \text{no. of output}\\
&amp;= 400 \times 5 + 5 \times 5 + 5 \times 5 + 5 \times 2\\
&amp;= 2060\\
\\
\text{number of }&amp;\text{parameters (bias)} \\
&amp;= \text{no. of features} \times \text{no. of neuron in HL1}  \\
&amp;+ \text{ no. of neuron in HL1} \times \text{no. of neuron in HL2}\\
&amp;+ \text{ no. of neuron in HL2} \times \text{no. of neuron in HL3}\\
&amp;+ \text{ no. of neuron in HL3} \times \text{no. of output}\\
&amp;= 1\times 5 + 1 \times 5 + 1 \times 5 +1 \times 2\\
&amp;= 17
\end{array}
\]</span></p>
<p>The number of parameters becomes too large as we accommodate more input features, more neurons, and more hidden layers. As we recall in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>), we have gone through a review of <strong>Exploratory Data Analysis (EDA)</strong> that allows us to reduce the dimensionality of our input. Unfortunately, as we begin to go deeper into neural networks and work on images, feature extraction may not be that simple. Nonetheless, we cover <strong>Convolutional Neural Network (CNN)</strong> up ahead, which performs feature extraction. There are ways to reduce parameters in <strong>CNN</strong>, as we shall see later.</p>
<p>Using Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:deltarule">12.8</a>, let us demonstrate three major aspects of <strong>modeling</strong> an <strong>MLP</strong>.</p>
<ul>
<li><p><strong>The Forward Pass (or Forward Feed)</strong> - each input data passes through each layer toward the output layer.</p></li>
<li><p><strong>The BackPropagation</strong> - each connection that input data passes through carries an initial arbitrary <strong>weight</strong> that needs to be optimized. The choice of optimization method is a consideration to make. Here, we use the common <strong>Gradient Descent</strong>. The result produced by the output layer may not necessarily match the target immediately, and so it takes a sufficient iteration to meet a close match. This mismatch error needs to be propagated back through the network. We can use <strong>sum squared error</strong> for our <strong>loss function</strong> to calculate the error that we need to propagate. For this, we require the <strong>loss function</strong> to be differentiable with respect to each weight.</p></li>
<li><p><strong>The Backward Pass (or Backward Feed)</strong> - for <strong>Optimization</strong> to happen, we need to re-calculate the weights of all connections toward the input layer. This is where we implement our update rule.</p></li>
</ul>
<div id="forward-feed" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.1</span> Forward Feed <a href="12.3-multi-layer-perceptron-mlp.html#forward-feed" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the <strong>Forward Feed</strong> (also called <strong>Forward Pass</strong>) portion, our first hidden layer performs a dot product of the input <span class="math inline">\(x_{(i:p)}\)</span> and the weights <span class="math inline">\(\omega_{(i:p,1:2)}\)</span>, and then it feeds the result to a <strong>sigmoid function</strong>. Our choice of <strong>activation function</strong> is scaled between 0 and 1. Here, <strong>p</strong> represents the dimensionality of our model, e.g., the number of features.</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\hat{z}_1 = \sum_{i=0}^{p=2} \omega_{(i,1)} x_{i}\ \ \ \ \ z_1 = \text{sigmoid}\left( \hat{z}_1 \right)\\
\hat{z}_2 = \sum_{i=0}^{p=2} \omega_{(i,2)} x_{i}\ \ \ \ \ z_2 = \text{sigmoid}\left( \hat{z}_2 \right)\\
\end{array}
\ \ \ 
where\ \ x_0 = 1\ \ \text{(constant used for bias)}   
\end{align*}\]</span></p>
<p>Note that for the sake of notation, let us consider <span class="math inline">\(\hat{z}_1\)</span> and <span class="math inline">\(\hat{z}_2\)</span> to be the <strong>net input</strong> and <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> to be the <strong>activation output</strong>.</p>
<p>Also, note that for a given constant equal to 1, we have chosen to have a separate bias weight for each connection to a perceptron; otherwise, if we have a fixed weight for bias, we also can use the below formula:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\hat{z}_1 = \sum_{i=1}^{p=2} \omega_{(i,1)} x_{i} + \omega_0\ \ \ \ \ z_1 = \text{sigmoid}\left( \hat{z}_1 \right)\\
\hat{z}_2 = \sum_{i=1}^{p=2} \omega_{(i,2)} x_{i} + \omega_0\ \ \ \ \ z_2 = \text{sigmoid}\left( \hat{z}_2 \right)\\
\end{array}
\ \ \ 
where\ \ x_0 = 1\ \ \text{(constant for bias)}
\end{align*}\]</span></p>
<p>In this first hidden layer, we have six parameters to optimize.</p>
<p>Then, our second hidden layer performs a dot product of the output from the first hidden layer and the weights <span class="math inline">\(\alpha_{(i:p,1:2)}\)</span>, and then it feeds the result to a <strong>sigmoid function</strong>.</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\hat{h}_1 = \sum_{i=0}^{p=2} \alpha_{(i,1)} z_{i}\ \ \ \ \ h_1 = \text{sigmoid}\left( \hat{h}_1 \right)\\
\hat{h}_2 = \sum_{i=0}^{p=2} \alpha_{(i,2)} z_{i}\ \ \ \ \ h_2 = \text{sigmoid}\left( \hat{h}_2 \right)\\
\end{array}
\ \ 
where\ \ z_0 = 1\ \ \text{(constant used for bias)}
\end{align*}\]</span></p>
<p>Let us also consider <span class="math inline">\(\hat{h}_1\)</span> and <span class="math inline">\(\hat{h}_2\)</span> to be the <strong>net input</strong> and <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span> to be the <strong>activation output</strong>.</p>
<p>In this second hidden layer, we have six parameters to optimize.</p>
<p>Finally, our output layer performs a dot product of the output from the second hidden layer and the weights <span class="math inline">\(\varphi_{(i:p,1:2)}\)</span>, and then it feeds the result to a <strong>sigmoid function</strong>.</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\hat{o}_1 = \sum_{i=0}^{p=2} \varphi_{(i,1)} h_{i}\ \ \ \ \ o_1 = \text{sigmoid}\left( \hat{o}_1 \right)\\
\hat{o}_2 = \sum_{i=0}^{p=2} \varphi_{(i,2)} h_{i}\ \ \ \ \ o_2 = \text{sigmoid}\left( \hat{o}_2 \right)\\
\end{array} 
\ \  
where\ \ h_0 = 1\ \ \text{(constant used for bias)}
\end{align*}\]</span></p>
<p>In this output layer, we have six parameters to optimize.</p>
</div>
<div id="backward-feed" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.2</span> Backward Feed <a href="12.3-multi-layer-perceptron-mlp.html#backward-feed" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the <strong>Backward Feed</strong> (also called <strong>Backward Pass</strong>) portion, we formulate our <strong>update function</strong> for our weight parameters like so (where <span class="math inline">\(\eta\)</span> is the learning rate between 0 and 1):</p>
<p><span class="math display" id="eq:equate1140009">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)}  - \eta \nabla \omega  \mathcal{L}  \tag{12.12} 
\end{align}\]</span></p>
<p>That is true for all weights.</p>
<p>We then iterate multiple times until our <strong>loss function</strong> is minimized. For that to happen, we need to optimize a total of 18 parameters corresponding to the 18 neural connections:</p>
<p><span class="math display">\[
params = \underbrace{( 2 \times 2 + 2 \times 2 + 2 \times 2 )}_{\text{feature inputs}} + \underbrace{( 1 \times 2 + 1 \times 2 + 1 \times 2 )}_{\text{bias}} = 18 
\]</span></p>
<p>Before we provide an example of optimizing the parameters, let us first discuss <strong>BackPropagation</strong> in the next section.</p>
</div>
<div id="backpropagation" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.3</span> BackPropagation <a href="12.3-multi-layer-perceptron-mlp.html#backpropagation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To illustrate the concept of <strong>Backpropagation</strong>, we have to recall <strong>Partial Differentiation</strong> covered in Chapter <strong>4</strong> (<strong>Numerical Calculus</strong>) and the concept of <strong>Chain rule</strong> and <strong>Delta rule</strong> in the context of <strong>Backpropagation</strong>.</p>
<p><strong>Chain rule</strong> </p>
<p>Let us use Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:backprop">12.9</a> to discuss the <strong>Chain rule</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:backprop"></span>
<img src="backprop.png" alt="BackPropagation" width="90%" />
<p class="caption">
Figure 12.9: BackPropagation
</p>
</div>
<p>We start with the idea that each neuron in the <strong>Neural Network</strong> contributes an <strong>effect</strong> to another <strong>Neuron</strong> from one layer to the next.</p>
<p>The diagram shows how neuron <strong>a</strong> in <strong>layer H1</strong>, for example, contributes indirectly to neuron <strong>o</strong> in the <strong>Output Layer</strong>. Mathematically, the contribution or effect of <strong>a</strong> to <strong>o</strong> can be expressed as <span class="math inline">\(\frac{\partial o}{\partial a}\)</span>.</p>
<p>To determine the effect of <strong>a</strong> to <strong>o</strong>, we first need to know the effect of <strong>a</strong> to <strong>d</strong>, the effect of <strong>d</strong> to <strong>g</strong>, and the effect of <strong>g</strong> to <strong>o</strong>.</p>
<p><span class="math display">\[
a\rightarrow d\rightarrow g\rightarrow o
\]</span></p>
<p>Mathematically, it is written as:</p>
<p><span class="math display" id="eq:equate1140010">\[\begin{align}
\frac{\partial o}{\partial a} = 
\left(\frac{\partial o}{\partial g} \right)
\left(\frac{\partial g}{\partial d} \right)
\left(\frac{\partial d}{\partial a} \right) \tag{12.13} 
\end{align}\]</span></p>
<p>However, notice in Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:backprop">12.9</a> that there is <strong>no</strong> direct connection from <strong>a</strong> to <strong>d</strong> nor from <strong>d</strong> to <strong>g</strong>. To get to <strong>d</strong> from <strong>a</strong>, we can either go to <strong>b</strong> first or <strong>c</strong> first. Similarly, to get to <strong>g</strong> from <strong>d</strong>, we can either go to <strong>e</strong> or <strong>f</strong> first.</p>
<p><span class="math display" id="eq:equate1140011">\[\begin{align}
\left(\frac{\partial d}{\partial a} \right) = 
\left(\frac{\partial d}{\partial b} \right)
\left(\frac{\partial b}{\partial a} \right)
\ \ \ \ \ or
\ \ \ \ \ 
\left(\frac{\partial d}{\partial a} \right) = 
\left(\frac{\partial d}{\partial c} \right)
\left(\frac{\partial c}{\partial a} \right) \tag{12.14} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140012">\[\begin{align}
\left(\frac{\partial g}{\partial d} \right) = 
\left(\frac{\partial g}{\partial e} \right)
\left(\frac{\partial e}{\partial d} \right)
\ \ \ \ \ or
\ \ \ \ \ 
\left(\frac{\partial g}{\partial d} \right) = 
\left(\frac{\partial g}{\partial f} \right)
\left(\frac{\partial f}{\partial d} \right) \tag{12.15} 
\end{align}\]</span></p>
<p>Because we are interested in the effect of <strong>a</strong> to <strong>d</strong> and <strong>d</strong> to <strong>g</strong>, we can combine the effects of both paths like so:</p>
<p><span class="math display" id="eq:equate1140013">\[\begin{align}
\left(\frac{\partial d}{\partial a} \right) = 
\left(\frac{\partial d}{\partial b} \right)
\left(\frac{\partial b}{\partial a} \right)
+
\left(\frac{\partial d}{\partial c} \right)
\left(\frac{\partial c}{\partial a} \right)  \tag{12.16} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140014">\[\begin{align}
\left(\frac{\partial g}{\partial d} \right) = 
\left(\frac{\partial g}{\partial e} \right)
\left(\frac{\partial e}{\partial d} \right)
+
\left(\frac{\partial g}{\partial f} \right)
\left(\frac{\partial f}{\partial d} \right)  \tag{12.17} 
\end{align}\]</span></p>
<p>Therefore, to get from <strong>a</strong> to <strong>o</strong>, we have the following complete <strong>Chain rule</strong> equation:</p>
<p><span class="math display" id="eq:equate1140015">\[\begin{align}
\frac{\partial o}{\partial a} = 
\left(\frac{\partial o}{\partial g} \right)
\left(\frac{\partial g}{\partial d} \right)
\left(\frac{\partial d}{\partial a} \right) = 
\left(\frac{\partial o}{\partial g} \right)
\left(
\left[
\frac{\partial g}{\partial e}
\frac{\partial e}{\partial d}
\right]
 +  
\left[
\frac{\partial g}{\partial f}
\frac{\partial f}{\partial d}
\right]
 \right) 
\left(
\left[
\frac{\partial d}{\partial b}
\frac{\partial b}{\partial a}
\right]
 +  
\left[
\frac{\partial d}{\partial c}
\frac{\partial c}{\partial a}
\right]
 \right)  \tag{12.18} 
\end{align}\]</span></p>
<p><strong>Delta rule</strong> </p>
<p>Our ultimate goal in <strong>Backpropagation</strong> is to optimize the <strong>weights</strong> or <strong>parameters</strong> of a <strong>Neural Network</strong>. In Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:backprop">12.9</a>, the <strong>Neural Network</strong> in the diagram shows that there are 229 parameters to optimize. For optimization, we need to know first the effect of each parameter on the overall performance of the <strong>MLP model</strong>. This effect can be measured in terms of comparing the output of the model against the actual target using the following <strong>error</strong> formula denoted by the <strong>epsilon</strong> symbol (<span class="math inline">\(\epsilon\)</span>):</p>
<p><span class="math display" id="eq:equate1140016">\[\begin{align}
\epsilon = (\text{o - t}) = -(\text{t - o})\ \ \ \ \ where\ \ \ \mathbf{t} = target\ \ and\ \  \mathbf{o} = output \tag{12.19} 
\end{align}\]</span></p>
<p>From there, we can formulate our <strong>total loss function</strong>, which is required to be differentiable. Note that the <span class="math inline">\(\frac{1}{2}\)</span> is added for mathematical convenience (e.g., ease of differentiation).</p>
<p><span class="math display" id="eq:equate1140017">\[\begin{align}
\mathcal{L}_{(total)} = \frac{1}{2} \sum_{i=1}^m \left(t_i - o_i\right)^2 = 
\underbrace{\frac{1}{2}\left(t_1 - o_1\right)^2}_{\mathcal{L}_{o_1}} + 
\underbrace{\frac{1}{2}\left(t_2 - o_2\right)^2 }_{\mathcal{L}_{o_2}} +\ ...+\ 
\underbrace{\frac{1}{2}\left(t_m - o_m\right)^2 }_{\mathcal{L}_{o_m}} \tag{12.20} 
\end{align}\]</span></p>
<p>Ideally, we want the <strong>total loss function</strong> to produce zero error. That can only be achieved if we can optimize the effect of each <strong>weight</strong> to the <strong>loss function</strong>. To calculate the effect of a <strong>weight</strong> (<span class="math inline">\(\omega\)</span>) to the <strong>loss function</strong>, we use the following notation: <span class="math inline">\(\frac{\partial \mathcal{L}_{(total)}}{\partial \omega}\)</span>.</p>
<p>For example, to determine the effect of a <strong>weight</strong>, e.g., <span class="math inline">\(\omega_{10}\)</span> between <strong>layer H5</strong> and the <strong>Output layer</strong> to the <strong>loss function</strong>, we can form the following equation:</p>
<p><span class="math display" id="eq:equate1140018">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{10}} = 
 \sum_{o=i}
\frac{\partial \mathcal{L}_{o}}{\partial \omega_{10}}   \tag{12.21} 
\end{align}\]</span></p>
<p>To determine the effect to the <strong>loss function</strong> of a <strong>weight</strong>, e.g. <span class="math inline">\(\omega_{8}\)</span> between <strong>layer H4</strong> and the <strong>Output layer</strong>, we can form the following equation:</p>
<p><span class="math display" id="eq:equate1140019">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{8}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_5}
\frac{\partial H_5}{\partial \hat{H}_5}
\frac{\partial \hat{H}_5}{\partial \omega_{8}} \tag{12.22} 
\end{align}\]</span></p>
<p>Now, to determine the effect to the <strong>loss function</strong> with respect to a <strong>weight</strong> between the <strong>Input layer</strong> and the <strong>Output layer</strong>, we can form the following equation:</p>
<p><span class="math display" id="eq:equate1140020">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{1}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_5}
\frac{\partial H_5}{\partial \hat{H}_5}
\frac{\partial \hat{H}_5}{\partial H_4}
\frac{\partial H_4}{\partial \hat{H}_4}
\frac{\partial \hat{H}_4}{\partial H_3}
\frac{\partial H_3}{\partial \hat{H}_3}
\frac{\partial \hat{H}_3}{\partial H_2}
\frac{\partial H_2}{\partial \hat{H}_2}
\frac{\partial \hat{H}_2}{\partial H_1}
\frac{\partial H_1}{\partial \hat{H}_1}
\frac{\partial \hat{H}_{1}}{\partial \omega_{1}} \tag{12.23} 
\end{align}\]</span></p>
<p>As we can imagine, the equation gets longer as the number of layers increases. However, algorithmically, there is a better way to express the equation. It is where the <strong>Delta rule</strong> comes into the picture. In the equation, we notice three contributions.</p>
<p><strong>First</strong>, let us momentarily use Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:deltarule">12.8</a> to review the contribution of a <strong>net input</strong> to an <strong>activation output</strong>. Both <strong>net input</strong> and <strong>activation output</strong> are part of a neuron. </p>
<p>Each of the <strong>neuron</strong> in <strong>hidden layers</strong> comes with two variables, namely the <strong>net input</strong> and the <strong>activation output</strong>. In fact, the <strong>activation output</strong> is called the <strong>activation output</strong> obtained from the <strong>activation function</strong>. In our case, we are using the <strong>sigmoid function</strong>. </p>
<p>We can get the contribution of <span class="math inline">\(\mathbf{\hat{h}_2}\)</span> to <span class="math inline">\(\mathbf{h_2}\)</span> by differentiating the <strong>sigmoid function</strong>.</p>
<p><span class="math display" id="eq:equate1140021">\[\begin{align}
a(z) = sigmoid(f(x)) = \frac{1}{1 + exp(-z)}\ \ \ \rightarrow a&#39;(z) = a(z) ( 1 - a(z))
\ \  where\ z = f(x) \tag{12.24} 
\end{align}\]</span></p>
<p>Because <span class="math inline">\(\mathbf{h_2}\)</span> is the <strong>activation output</strong> of a <strong>sigmoid function</strong>, we can therefore obtain the derivative of the function with respect to the <strong>net input</strong>, namely <span class="math inline">\(\mathbf{\hat{h}_2}\)</span>:</p>
<p><span class="math display" id="eq:equate1140022">\[\begin{align}
f(x) = \hat{h}_2 = 1 \times \alpha_{0,2} +  z_1 \times \alpha_{1,2}  +  z_2 \times \alpha_{2,2}
\ \ \ \ \ \ \ \ \ \ \ 
\frac{\partial \mathbf{h_2}}{\partial \mathbf{\hat{h}_2}} = \mathbf{h_2} ( 1 - \mathbf{h_2} ) \tag{12.25} 
\end{align}\]</span></p>
<p>Note that the full derivation of the derivative is excluded.</p>
<p>Note also that other <strong>activation function</strong> of choice has different derivatives.</p>
<p><strong>Second</strong>, we also have the contribution of individual <strong>weights</strong> to the <strong>net input</strong>, e.g. <span class="math inline">\(\frac{\partial \hat{h_2}}{\partial \alpha_{2,2}}\)</span>.</p>
<p>Given <span class="math inline">\(f(x) = \hat{h}_2\)</span>, we can obtain the derivative of the <strong>net input</strong> with respect to a given <strong>weight</strong>. Similarly, we also can obtain the derivative of the <strong>weight</strong> given a <strong>net input</strong>):</p>
<p><span class="math display" id="eq:equate1140023">\[\begin{align}
\frac{\partial \hat{h_2}}{\partial \alpha_{2,2}} =  z_2 \times {\alpha_{2,2}}^{(1-1)} = \mathbf{z_2}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\frac{\partial \hat{h_2}}{\partial x_2} = {\mathbf{z_2}}^{(1-1)} \times {\mathbf{\alpha_{2,2}}} = \mathbf{\alpha_{2,2}} \tag{12.26} 
\end{align}\]</span></p>
<p><strong>Third</strong>, we have the contribution of the <strong>activation output</strong> to the <strong>total loss function</strong>, e.g. <span class="math inline">\(\frac{\partial \mathcal{L}_{(total)}}{\partial o}\)</span>.</p>
<p>We can then differentiate the <strong>total loss function</strong> with respect to a <strong>given output</strong> so that given the equation below:</p>
<p><span class="math display" id="eq:equate1140024">\[\begin{align}
\mathcal{L_{(total)}}  = 
2 \frac{1}{2} \left(t_1 - o_1\right)^{2-1}  +  
2 \frac{1}{2} \left(t_2 - o_2\right)^{2-1} +\ ...\ +\ 
2 \frac{1}{2} \left(t_m - o_m\right)^{2-1} \tag{12.27} 
\end{align}\]</span></p>
<p>we obtain the following derivatives:</p>
<p><span class="math display" id="eq:equate1140025">\[\begin{align}
\frac{ \partial \mathcal{L_{(total)}} } {\partial o_1}= -\underbrace{ \left(t_1 - o_1\right) }_{\partial \mathcal{L_1}} 
\ \ \ where\ \ \ \ \partial \mathcal{L}_1 = \epsilon_1  \tag{12.28} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140026">\[\begin{align}
\frac{ \partial \mathcal{L_{(total)}} } {\partial o_2}  = -\underbrace{ \left(t_2 - o_2\right) }_{\partial \mathcal{L_2}} 
\ \ \ where\ \ \ \ \partial \mathcal{L}_2 = \epsilon_2 \tag{12.29} 
\end{align}\]</span></p>
<p><strong>Fourth</strong>, the contribution of an <strong>activation output</strong> to the <strong>total loss function</strong>, e.g. <span class="math inline">\(\left(\frac{\partial L_{(total)}}{\partial h_2} \right)\)</span>, can be decomposed into the following:</p>
<p><span class="math display" id="eq:equate1140027">\[\begin{align}
\left(\frac{\partial L_{(total)}}{\partial h_2} \right) =
\left(\frac{\partial L_1}{\partial h_2} +  \frac{\partial L_2}{\partial h_2}\right)  \tag{12.30} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display" id="eq:equate1140028">\[\begin{align}
\frac{\partial L_1}{\partial h_2} = 
\left(\frac{\partial L_1}{\partial o_1}\right)
\left(\frac{\partial o_1}{\partial \hat{o}_1}\right)
\left(\frac{\partial \hat{o}_1}{\partial h_2}\right) = 
\underbrace{-(t_1 - o_1) \times o_1 ( 1 - o_1)}_{\delta_1\leftarrow \text{delta rule}} \times \varphi_{2,1} \tag{12.31} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140029">\[\begin{align}
\frac{\partial L_2}{\partial h_2} = 
\left(\frac{\partial L_2}{\partial o_2}\right)
\left(\frac{\partial o_2}{\partial \hat{o}_2}\right)
\left(\frac{\partial \hat{o}_2}{\partial h_2}\right) =
\underbrace{-(t_2 - o_2) \times o_2 ( 1 - o_2)}_{\delta_2\leftarrow \text{delta rule}} \times \varphi_{2,2} \tag{12.32} 
\end{align}\]</span></p>
<p>Therefore,</p>
<p><span class="math display" id="eq:equate1140031" id="eq:equate1140030">\[\begin{align}
\left(\frac{\partial L_{(total)}}{\partial h_2} \right) &amp;= 
\underbrace{-(t_1 - o_1) \times o_1 ( 1 - o_1)}_{\delta_1\leftarrow \text{delta rule}} \times \varphi_{2,1} + \underbrace{-(t_2 - o_2) \times o_2 ( 1 - o_2)}_{\delta_2\leftarrow \text{delta rule}} \times \varphi_{2,2} \tag{12.33} \\
&amp;= \sum_{y=1}^{Y=2}\delta_y \varphi_{2,y} \tag{12.34} 
\end{align}\]</span></p>
<p><strong>Lastly</strong>, if we then combine all three contributions (or derivatives), we end up with the following equation for the example in Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:deltarule">12.8</a>.</p>
<p><span class="math display" id="eq:equate1140032">\[\begin{align}
\frac{\partial L_{(total)}}{\partial \alpha_{2,2}} =  
\left(\frac{\partial L_{(total)}}{\partial h_2} \right)
\left(\frac{\partial h_2}{\partial \hat{h}_2} \right)
\left(\frac{\partial \hat{h}_2}{\partial \alpha_{2,2}} \right) =
\underbrace{\sum_{y=1}^{Y=2}\delta_y \varphi_{2,o} \times o_2 (1 - o_2)}_{\delta_h \leftarrow \text{delta rule}} \times z_2 = \delta_h z_2 \tag{12.35} 
\end{align}\]</span></p>
<p>Now, back to Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:backprop">12.9</a> as reference, having the <strong>Delta rules</strong> in mind, let us figure out how to solve for <span class="math inline">\(\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{i,q}}\)</span> where <strong>i</strong> is the ith feature (including constant) in the <strong>Input Layer</strong> and <strong>q</strong> is the qth neuron in the next layer.</p>
<p><strong>First</strong>, we deal with the effect of a <strong>neuron</strong> to the <strong>total loss function</strong>. Consider <strong>L=5</strong> such that <span class="math inline">\(H_5 = H_{L}\)</span>. We then have:</p>
<p><span class="math display" id="eq:equate1140033">\[\begin{align}
\left\{
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L,k)}} =  
\sum_{y=1}
\frac{\partial \mathcal{L}_y}{\partial H_{(L,k)}}  = 
\sum_{y=1} \delta_{y}\times \omega_{(L, k)}
\right\}_{k=1}^K \tag{12.36} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display" id="eq:equate1140034">\[\begin{align}
\frac{\partial \mathcal{L}_y}{\partial H_{(L,k)}}= 
\underbrace{ \frac{\partial \mathcal{L}_y}{\partial O_y} 
\frac{\partial O_y}{\partial \hat{O}_y} }_{\delta_y}
\frac{\partial \hat{O}_y}{\partial H_{(L,k)}} = 
\underbrace{-(t_y - o_y)\times o_y(1 - o_y)}_{\delta_{y}} \times \omega_{(L, k)} \tag{12.37} 
\end{align}\]</span></p>
<p>Here, <strong>L</strong> is the <strong>H5 layer</strong> index with <strong>K</strong> neurons. The indices are used such that <span class="math inline">\(\mathbf{H_{(5, 3)}}\)</span> refers to the third neuron in the <strong>H5 layer</strong> and <span class="math inline">\(\mathbf{\omega_{(5, 2)}}\)</span> refers to the second weight from <strong>H5 layer</strong>.</p>
<p><strong>Second</strong>, we deal with the effect of the <strong>activation output</strong> with respect to the <strong>net input</strong>.</p>
<p><span class="math display" id="eq:equate1140035">\[\begin{align}
\left\{
\frac{\partial H_{(L,k)}}{\partial \hat{H}_{(L,k)}} = H_{(L,k)} \left(1 - H_{(L,k)}\right)
\right\}_{k=1}^K \tag{12.38} 
\end{align}\]</span></p>
<p><strong>Third</strong>, we deal with the effect of <strong>neuron’s output</strong> to another <strong>neuron’s activation output</strong>:</p>
<p><span class="math display" id="eq:equate1140036">\[\begin{align}
\left\{
\frac{\partial \hat{H}_{(L,k)}}{\partial H_{(L-1,m)}} = 
\frac{\partial \left(\sum_{j=1} \omega_{(L-1, j)} H_{(L-1,j)}\right)}{\partial H_{(L-1,m)}} = \omega_{(L-1, m)}
\right\}_{m=1}^{M} \tag{12.39} 
\end{align}\]</span></p>
<p>Assume there are <strong>M</strong> neurons in <strong>H4 layer</strong> denoted by <span class="math inline">\(H_{(L-1, m)}\)</span>.</p>
<p><strong>Fourth</strong>, if we combine the first three factors to affect the <strong>total loss</strong> with respect to the weight (in <strong>H4 layer</strong>), we get the following:</p>
<p><span class="math display" id="eq:equate1140037">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{(L-1,m)}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L,k)}}
\frac{\partial H_{(L,k)}}{\partial \hat{H}_{(L,k)}}
\frac{\partial \hat{H}_{(L,k)}}{\partial \omega_{(L-1,m)}}  \tag{12.40} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140038">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{(L-1,m)}} = 
\underbrace{ \left[  \sum_{k=1}
\left(\sum_{y=1} \delta_{y}\times \omega_{(L, k)} \right) \times H_{(L,k)} (1 - H_{(L,k)})
\right] }_{\delta_{(L,k)}}  \times H_{(L-1,m)} \tag{12.41} 
\end{align}\]</span></p>
<p><strong>Fifth</strong>, similarly, if we combine the first three factors for the effect to the <strong>total loss</strong> with respect to the <strong>activation output</strong>, we get the following equation instead:</p>
<p><span class="math display" id="eq:equate1140039">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-1,m)}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L,k)}}
\frac{\partial H_{(L,k)}}{\partial \hat{H}_{(L,k)}}
\frac{\partial \hat{H}_{(L,k)}}{\partial \omega_{(L-1,m)}} \tag{12.42} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140040">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-1,m)}}  = 
\underbrace{ \left[  \sum_{k=1}
\left(\sum_{y=1} \delta_{y}\times \omega_{(L, k)} \right) \times H_{(L,k)} (1 - H_{(L,k)})
\right] }_{\delta_{(L,k)}}  \times \omega_{(L-1,m)} \tag{12.43} 
\end{align}\]</span></p>
<p><strong>Sixth</strong>, moving one more layer back (e.g. <strong>H3 layer</strong>) and using the equation below to expand further:</p>
<p><span class="math display" id="eq:equate1140041">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-1,m)}} = \delta_{(L,k)} \times \omega_{(L-1,m)} \tag{12.44} 
\end{align}\]</span></p>
<p>we get:</p>
<p><span class="math display" id="eq:equate1140042">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{(L-2,n)}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L,k)}}
\frac{\partial H_{(L,k)}}{\partial \hat{H}_{(L,k)}}
\frac{\partial \hat{H}_{(L,k)}}{\partial H_{(L-1,m)}}
\frac{\partial H_{(L-1,m)}}{\partial \hat{H}_{(L-1,m)}}
\frac{\partial \hat{H}_{(L-1,m)}}{\partial \omega_{(L-2,n)}} \tag{12.45} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140043">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-2,n)}} = 
\underbrace{ \left[  \sum_{m=1}
\left(\sum_{k=1} \delta_{_{(L,k)}}\times \omega_{(L-1, m)} \right) \times H_{(L-1,m)} (1 - H_{(L-1,m)})
\right] }_{\delta_{(L-1,m)}}  \times \omega_{(L-2,n)} \tag{12.46} 
\end{align}\]</span></p>
<p>Assume there are <strong>N</strong> neurons in <strong>H3 layer</strong> denoted by <span class="math inline">\(H_{(L-2, n)}\)</span>.</p>
<p><strong>Seventh</strong>, moving even further back, for another layer (e.g. <strong>H2 layer</strong>) in which:</p>
<p><span class="math display" id="eq:equate1140044">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{(L-3,o)}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L,k)}}
\frac{\partial H_{(L,k)}}{\partial \hat{H}_{(L,k)}}
\frac{\partial \hat{H}_{(L,k)}}{\partial H_{(L-1,m)}}
\frac{\partial H_{(L-1,m)}}{\partial \hat{H}_{(L-1,m)}}
\frac{\partial \hat{H}_{(L-1,m)}}{\partial H_{(L-2,n)}}
\frac{\partial H_{(L-2,n)}}{\partial \hat{H}_{(L-2,n)}}
\frac{\partial \hat{H}_{(L-2,n)}}{\partial H_{(L-3,o)}} \tag{12.47} 
\end{align}\]</span></p>
<p>we get:</p>
<p><span class="math display" id="eq:equate1140045">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-3,o)}} = 
\underbrace{ \left[  \sum_{n=1}
\left(\sum_{m=1} \delta_{_{(L-1,m)}}\times \omega_{(L-2, n)} \right) \times H_{(L-2,n)} (1 - H_{(L-2,n)})
\right] }_{\delta_{(L-2,n)}}  \times \omega_{(L-3,o)} \tag{12.48} 
\end{align}\]</span></p>
<p>Assume there are <strong>O</strong> neurons in <strong>H2 layer</strong> denoted by <span class="math inline">\(H_{(L-3, o)}\)</span>.</p>
<p><strong>Eight</strong>, and for the <strong>H1 layer</strong>, we get:</p>
<p><span class="math display" id="eq:equate1140046">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-4,p)}} = 
\underbrace{ \left[  \sum_{p=1}
\left(\sum_{n=1} \delta_{_{(L-2,n)}}\times \omega_{(L-3, o)} \right) \times H_{(L-3,o)} (1 - H_{(L-3,o)})
\right] }_{\delta_{(L-3,o)}}  \times \omega_{(L-4,p)} \tag{12.49} 
\end{align}\]</span></p>
<p>Assume there are <strong>P</strong> neurons in <strong>H1 layer</strong> denoted by <span class="math inline">\(H_{(L-4, p)}\)</span>.</p>
<p>Equivalently, we get:</p>
<p><span class="math display" id="eq:equate1140047">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{(L-4,p)}} = 
\underbrace{ \left[  \sum_{p=1}
\left(\sum_{n=1} \delta_{_{(L-2,n)}}\times \omega_{(L-3, o)} \right) \times H_{(L-3,o)} (1 - H_{(L-3,o)})
\right] }_{\delta_{(L-3,o)}}  \times H_{(L-4,p)} \tag{12.50} 
\end{align}\]</span></p>
<p>Simplifying the notation, we get:</p>
<p><span class="math display" id="eq:equate1140048">\[\begin{align}
\nabla \omega_{(L-4, p)} = \delta_{(L-3, o)} \times H_{(L-4,p)} \tag{12.51} 
\end{align}\]</span></p>
<p><strong>Finally</strong>, for the <strong>Input Layer</strong>, having gone through all the example equations above, we get the final effect of the <strong>Input Layer</strong> to the <strong>Output Layer</strong> like so:</p>
<p><span class="math display" id="eq:equate1140049">\[\begin{align}
\nabla \omega_{(I, q)} = \delta_{(L-4, p)} \times H_{(I,q)} \tag{12.52} 
\end{align}\]</span></p>
<p>Assume there are <strong>Q</strong> input features in the <strong>Input layer</strong> denoted by <span class="math inline">\(H_{(I, q)}\)</span>.</p>
<p>So far, our equations work only on a single sample. In the next section, we continue to discuss <strong>MLP</strong> further by example. Then, another section follows with the implementation, focusing on multiple samples.</p>
</div>
<div id="mlp-example" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.4</span> MLP Example<a href="12.3-multi-layer-perceptron-mlp.html#mlp-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we continue to use Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:deltarule">12.8</a>. Suppose we have the following data points, initial weights, and biases (see Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:mlpdatapoint">12.10</a>):</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpdatapoint"></span>
<img src="mlpdatapoint.png" alt="Multiple Perceptron (Data Points)" width="80%" />
<p class="caption">
Figure 12.10: Multiple Perceptron (Data Points)
</p>
</div>
<p>Note that our <strong>inputs</strong> in the table include the biases. Also, our bias constant connects to neurons in different layers with unique weights.</p>
<p>Our goal is to update the 18 (weight) parameters. Here, we show the steps in solving only three parameters as an example.</p>
<p><strong>First</strong>, for the <strong>Forward Pass</strong>, we calculate the <strong>net input and output</strong> of each layer. Starting with the <strong>Z layer</strong>, we have the <strong>net input</strong> as:</p>
<p><span class="math display">\[\begin{align*}
\hat{z} = \left[\begin{array}{rrr} 1 &amp; 0.12 &amp; 0.18 \end{array}\right]_{X} \cdot
   \left[\begin{array}{rr} 0.05 &amp; 0.40 \\ 0.21 &amp; 0.34 \\ 0.19 &amp; 0.67 \end{array}\right]_{\omega}
   = \left[\begin{array}{rr} 0.1094 &amp; 0.5614 \end{array}\right] 
\end{align*}\]</span></p>
<p>and <strong>activation output</strong> as:</p>
<p><span class="math display">\[\begin{align*}
z = sigmoid( \left[\begin{array}{rr} 0.1094 &amp; 0.5614 \end{array}\right]) = 
\left[\begin{array}{rr} 0.52732275 &amp; 0.63677641 \end{array}\right] 
\end{align*}\]</span></p>
<p>Note, hereafter, that we use a precision of 8 digits in our examples.</p>
<p>For <strong>H layer</strong>, we have the <strong>net input</strong> as:</p>
<p><span class="math display">\[\begin{align*}
\hat{h} &amp;= \left[\begin{array}{rrr} 1 &amp; 0.52732275 &amp;  0.63677641 \end{array}\right]_{Z} \cdot
   \left[\begin{array}{rr} 0.18 &amp; 0.27 \\ 0.09 &amp; 0.06 \\ 0.30 &amp; 0.15 \end{array}\right]_{\alpha}\\
   &amp;= \left[\begin{array}{rr} 0.41849197 &amp; 0.39715583 \end{array}\right] \nonumber
\end{align*}\]</span></p>
<p>and <strong>activation output</strong> as:</p>
<p><span class="math display">\[\begin{align*}
h = sigmoid( \left[\begin{array}{rr} 0.41849197 &amp; 0.39715583 \end{array}\right]) = 
\left[\begin{array}{rr} 0.60312234 &amp; 0.59800413 \end{array}\right]
\end{align*}\]</span></p>
<p>For <strong>Output layer</strong>, we have the <strong>net input</strong> as:</p>
<p><span class="math display">\[\begin{align*}
\hat{o} &amp;= \left[\begin{array}{rrr} 1 &amp; 0.60312234 &amp;  0.59800413 \end{array}\right]_{H} \cdot
   \left[\begin{array}{rr} 0.25 &amp; 0.05 \\ 0.03 &amp; 0.35 \\ 0.04 &amp; 0.40 \end{array}\right]_{\varphi}\\
   &amp;= \left[\begin{array}{rr} 0.29201384 &amp; 0.50029447 \end{array}\right]  \nonumber
\end{align*}\]</span></p>
<p>and <strong>activation output</strong> as:</p>
<p><span class="math display">\[\begin{align*}
o = sigmoid( \left[\begin{array}{rr} 0.29201384 &amp; 0.50029447  \end{array}\right]) = 
\left[\begin{array}{rr} 0.57248908 &amp; 0.62252853 \end{array}\right]
\end{align*}\]</span></p>
<p><strong>Second</strong>, for the <strong>Backpropagation</strong>, we calculate our <strong>loss function</strong>:</p>
<p><span class="math display" id="eq:equate1140051" id="eq:equate1140050">\[\begin{align}
\mathcal{L}_{(total)} &amp;= \frac{1}{2}\sum_{i=1}^m( t_i - o_i)^2  \tag{12.53} \\
&amp;= \frac{1}{2}( t_1 - o_1)^2 +  \frac{1}{2}( t_2 - o_2)^2 \tag{12.54} \\
&amp; = \frac{1}{2} (0.05 - 0.57248908)^2 + \frac{1}{2} (0.95 - 0.62252853)^2 \nonumber \\
&amp; = 0.1901162 \nonumber
\end{align}\]</span></p>
<p>Now, we calculate the gradients starting with the <strong>loss function</strong> with respect to the <strong>activation output</strong> of the <strong>O layer</strong>.</p>
<p><span class="math display" id="eq:equate1140053" id="eq:equate1140052">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial o_1} &amp;= \frac{\partial L_1}{\partial o_1}  = -(t_1 - o_1) = -(0.05 - 0.57248908) = 0.52248908 \tag{12.55} \\ 
\frac{\partial \mathcal{L}_{(total)}}{\partial o_2} &amp;= \frac{\partial L_2}{\partial o_2}  = -(t_2 - o_2) = -(0.95 - 0.62252853) = -0.32747147 \tag{12.56} 
\end{align}\]</span></p>
<p><strong>Third</strong>, we calculate the gradients of our <strong>activation output</strong>, <span class="math inline">\(\mathbf{o_1}\)</span>, (from our activation function) with respect to the <strong>net input</strong>, <span class="math inline">\(\mathbf{\hat{o}_1}\)</span>, of the <strong>O layer</strong> :</p>
<p><span class="math display" id="eq:equate1140055" id="eq:equate1140054">\[\begin{align}
\frac{\partial o_1}{\partial \hat{o}_1} &amp;= o_1 \left( 1 - o_1\right) =  0.57248908 ( 1 - 0.57248908) = 0.24474533 \tag{12.57} \\
\frac{\partial o_2}{\partial \hat{o}_2} &amp;= o_2 \left( 1 - o_2\right) = 0.62252853 ( 1 - 0.62252853) = 0.23498676 \tag{12.58} 
\end{align}\]</span></p>
<p><strong>Fourth</strong>, regarding the weights and biases for the <strong>O Layer</strong>, we first calculate the gradient of the <strong>total loss function</strong> with respect to the weights. Starting with (<span class="math inline">\(\varphi\)</span>) weights, in particular <span class="math inline">\(\varphi_{1,1}\)</span>, we perform the following derivatives:</p>
<p><span class="math display" id="eq:equate1140057" id="eq:equate1140056">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \varphi_{1,1}} &amp;= 
\left(\frac{\partial L_{(total)}}{\partial o_1} \right) 
\left(\frac{\partial o_1}{\partial \hat{o}_1} \right)
\left(\frac{\partial \hat{o}_1}{\partial \varphi_{1,1}} \right)  \tag{12.59} \\
&amp;=
\underbrace{\left(\frac{\partial L_1}{\partial  o_1} \right)}_{\text{1st factor}}\ \ 
\underbrace{\left(\frac{\partial o_1}{\partial \hat{o}_1} \right)}_{\text{2nd factor}}\ \ 
\underbrace{\left(\frac{\partial \hat{o}_1}{\partial \varphi_{1,1}} \right)}_{\text{3rd factor}} \tag{12.60} 
\end{align}\]</span></p>
<p>The first two factors, namely <span class="math inline">\(\left(\frac{\partial L_1}{\partial o_1} \right)\)</span> and <span class="math inline">\(\left(\frac{\partial o_1}{\partial \hat{o}_1} \right)\)</span> are already calculated for us above. Both equates to the <span class="math inline">\(\delta_1\)</span> from our <strong>delta.rule</strong>.</p>
<p><span class="math display" id="eq:equate1140058">\[\begin{align}
\delta_1 = \left(\frac{\partial L_{(total)}}{\partial o_1} \right) 
\left(\frac{\partial o_1}{\partial \hat{o}_1} \right) = (0.52248908)(0.24474533) = 0.12787676 \tag{12.61} 
\end{align}\]</span></p>
<p>Equivalently, for <span class="math inline">\(\delta_2\)</span>, we have:</p>
<p><span class="math display" id="eq:equate1140059">\[\begin{align}
\delta_2 = \left(\frac{\partial L_{(total)}}{\partial o_2} \right) 
\left(\frac{\partial o_2}{\partial \hat{o}_2} \right) = (-0.32747147)(0.23498676) =  -0.07695146 \tag{12.62} 
\end{align}\]</span></p>
<p>We now have to calculate the third factor, namely <span class="math inline">\(\left(\frac{\partial \hat{o}_1}{\partial \varphi_{1,1}} \right)\)</span>:</p>
<p><span class="math display" id="eq:equate1140060">\[\begin{align}
\frac{\partial \hat{o}_1} {\partial \varphi_{1,1}} = \frac{\partial \left(\varphi_{0,1} \times h_0 +  \varphi_{1,1} \times h_1 + \varphi_{2,1} \times h_2 \right) } {\partial \varphi_{1,1}}
= {\varphi_{1,1}}^{(1-1)} \times h_1 = h_1 =  0.60312234   \tag{12.63} 
\end{align}\]</span></p>
<p>Therefore, the gradient of the <strong>total loss function</strong> with respect to <span class="math inline">\(\varphi_{1,1}\)</span> is:
<span class="math display" id="eq:equate1140061">\[\begin{align}
\nabla \varphi_{1,1} \mathcal{L} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial \varphi_{1,1}} = \delta_1 h_1
= (0.12787676)(0.60312234)
 = 0.077125331 \tag{12.64} 
\end{align}\]</span></p>
<p>Calculation of gradient for other <strong>phi</strong> <span class="math inline">\((\varphi)\)</span> weights follow the same process.</p>
<p><strong>Fifth</strong>, in terms of the weights and biases for the <strong>H Layer</strong>, the gradient of the <strong>total loss function</strong> with respect to <span class="math inline">\(\alpha_{1,1}\)</span> is written as:</p>
<p><span class="math display" id="eq:equate1140062">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \alpha_{1,1}} = 
\underbrace{ \left(\frac{\partial L_{(total)}}{\partial h_1} \right) 
\left(\frac{\partial h_1}{\partial \hat{h}_1} \right)}_{\delta_{h_1}}
\left(\frac{\partial \hat{h}_1}{\partial \alpha_{1,1}} \right)  \tag{12.65} 
\end{align}\]</span></p>
<p>We can expand the first factor into the following:</p>
<p><span class="math display" id="eq:equate1140063">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \alpha_{1,1}} =
\underbrace{\left(\frac{\partial \mathcal{L}_1}{\partial h_1} +  \frac{\partial \mathcal{L}_2}{\partial h_1}\right) }_{\text{1st factor}}
\underbrace{\left(\frac{\partial h_1}{\partial \hat{h}_1} \right)}_{\text{2nd factor}}
\underbrace{\left(\frac{\partial \hat{h}_1}{\partial \alpha_{1,1}} \right)}_{\text{3rd factor}}  \tag{12.66} 
\end{align}\]</span></p>
<p>Decomposing the 1st factor into two terms, we have:</p>
<p><span class="math display" id="eq:equate1140064">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \alpha_{1,1}} = 
\underbrace{
\left(\overbrace{
      \underbrace{\frac{\partial \mathcal{L}_{1}}{\partial o_1}
       \frac{\partial o_1}{\partial \hat{o}_1}}_{\delta_{1}}
       \frac{\partial \hat{o}_1}{\partial h_1}}^{\partial \mathcal{L}_1 / \partial h_1} + 
       \overbrace{
       \underbrace{
       \frac{\partial \mathcal{L}_{2}}{\partial o_2}
       \frac{\partial o_2}{\partial \hat{o}_2} }_{\delta_{2}}
       \frac{\partial \hat{o}_2}{\partial h_1}}^{\partial \mathcal{L}_2 / \partial h_1}
       \right)
\frac{\partial h_1}{\partial \hat{h}_1}}_{\delta_{h_1}}
\frac{\partial \hat{h}_1}{\partial \alpha_{1,1}} \tag{12.67} 
\end{align}\]</span></p>
<p>Now, let us calculate the two terms:</p>
<p><span class="math display" id="eq:equate1140065">\[\begin{align}
\frac{\partial \mathcal{L}_1}{\partial h_1} = 
\underbrace{\left(\frac{\partial \mathcal{L}_1}{\partial o_1 }\right)
\left(\frac{\partial o_1}{\partial \hat{o}_1 }\right)}_{\delta_1}
\left(\frac{\partial  \hat{o}_1 }{\partial h_1}\right)
\ \ \ \ \ \ \ \ \ \
\frac{\partial \mathcal{L}_2}{\partial h_1} = 
\underbrace{
\left(\frac{\partial \mathcal{L}_2}{\partial o_2 }\right)
\left(\frac{\partial o_2}{\partial \hat{o}_2 }\right)}_{\delta_2}
\left(\frac{\partial  \hat{o}_2 }{\partial h_1}\right) \tag{12.68} 
\end{align}\]</span></p>
<p>The first two factors of the 1st term, namely <span class="math inline">\(\left(\frac{\partial \mathcal{L}_1}{\partial o_1 }\right)\)</span> and <span class="math inline">\(\left(\frac{\partial o_1 }{\partial \hat{o}_1}\right)\)</span> are already solved, denoted by <span class="math inline">\(\delta_1\)</span>. Equivalently, the first two factors of the 2nd term, namely <span class="math inline">\(\left(\frac{\partial \mathcal{L}_2}{\partial o_2 }\right)\)</span> and <span class="math inline">\(\left(\frac{\partial o_2 }{\partial \hat{o}_2}\right)\)</span> are also solved, denoted by <span class="math inline">\(\delta_2\)</span>.</p>
<p>The derivatives of the two <strong>net inputs</strong> with respect to <span class="math inline">\(h_1\)</span> are:</p>
<p><span class="math display" id="eq:equate1140066">\[\begin{align}
\frac{\partial  \hat{o}_1 }{\partial h_1} = 
\frac{\partial \left(\varphi_{0,1} \times h_0 +  \varphi_{1,1} \times h_1 + \varphi_{2,1} \times h_2 \right)}{\partial h_1} = \varphi_{1,1} \times {h_1}^{(1-1)} = \varphi_{1,1}  \tag{12.69} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140067">\[\begin{align}
\frac{\partial  \hat{o}_2 }{\partial h_1} = 
\frac{\partial \left(\varphi_{0,2} \times h_0 +  \varphi_{1,2} \times h_1 + \varphi_{2,2} \times h_2 \right)}{\partial h_1} = \varphi_{1,2} \times {h_1}^{(1-1)} = \varphi_{1,2}  \tag{12.70} 
\end{align}\]</span></p>
<p>That gives us the following:</p>
<p><span class="math display" id="eq:equate1140068">\[\begin{align}
\frac{\partial \mathcal{L}_1}{\partial h_1} = \delta_1 (\varphi_{1,1}) = (0.12787676)(0.03) = 0.0038363028  \tag{12.71} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140069">\[\begin{align}
 \frac{\partial \mathcal{L}_2}{\partial h_1} = \delta_2 \left( \varphi_{1,2}\right) = (-0.07695146)(0.35) =  -0.026933011 \tag{12.72} 
\end{align}\]</span></p>
<p>Therefore, the first factor is:</p>
<p><span class="math display" id="eq:equate1140070">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h_1}  = 
\left(\frac{\partial \mathcal{L}_1}{\partial h_1} +  \frac{\partial \mathcal{L}_2}{\partial h_1}\right) = 0.0038363028 + -0.026933011 = -0.023096708 \tag{12.73} 
\end{align}\]</span></p>
<p>Now, let us solve for the second factor (<strong>activation output</strong> from our <strong>sigmoid</strong> function).</p>
<p><span class="math display" id="eq:equate1140071">\[\begin{align}
\frac{\partial h_1}{\partial \hat{h}_1} = h_1( 1 - h_1) = 0.60312234 ( 1 - 0.60312234) = 
0.23936578 \tag{12.74} 
\end{align}\]</span></p>
<p>At this point, we do not need <span class="math inline">\(\left(\frac{\partial h_2}{\partial \hat{h}_2}\right)\)</span>.</p>
<p>And for the third factor, we have:</p>
<p><span class="math display" id="eq:equate1140072">\[\begin{align}
\frac{\partial \hat{h}_1}{\partial \alpha_{1,1}} = \frac{\partial \left(\alpha_{0,1} \times z_0 +  \alpha_{1,1} \times z_1 + \alpha_{2,1} \times z_2 \right) } {\partial \alpha_{1,1}}
= {\varphi_{1,1}}^{(1-1)} \times z_1 = z_1 =  0.52732275  \tag{12.75} 
\end{align}\]</span></p>
<p>As for the <strong>delta</strong>, e.g. <span class="math inline">\(\delta_{h_1}\)</span>, we have:</p>
<p><span class="math display" id="eq:equate1140073">\[\begin{align}
\delta_{h_1} =  \left(\frac{\partial L_{(total)}}{\partial h_1} \right) 
\left(\frac{\partial h_1}{\partial \hat{h}_1} \right) = (-0.023096708)(0.23936578) = -0.0055285615 \tag{12.76} 
\end{align}\]</span></p>
<p>Therefore, the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(\alpha_{1,1}\)</span> is:</p>
<p><span class="math display" id="eq:equate1140074">\[\begin{align}
\nabla \alpha_{1,1} \mathcal{L} =  
\frac{\partial \mathcal{L}_{(total)}}{\partial \alpha_{1,1}}  = \delta_{h_1} z_1
= (-0.0055285615)(0.52732275) = -0.0029153363  \tag{12.77} 
\end{align}\]</span></p>
<p>Calculation of gradient for other <strong>alpha</strong> <span class="math inline">\((\alpha)\)</span> weights follow the same process.</p>
<p><strong>Sixth</strong>, in terms of the weights and biases for the <strong>Z Layer</strong>, the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(\omega_{1,1}\)</span> is written as:</p>
<p><span class="math display" id="eq:equate1140075">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{1,1}} = 
\underbrace{ \left(\frac{\partial L_{(total)}}{\partial z_1} \right) 
\left(\frac{\partial z_1}{\partial \hat{z}_1} \right)}_{\delta_{z_1}}
\left(\frac{\partial \hat{z}_1}{\partial \omega_{1,1}} \right)  \tag{12.78} 
\end{align}\]</span></p>
<p>We can expand the first factor into the following:</p>
<p><span class="math display" id="eq:equate1140076">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{1,1}} =
\underbrace{\left(\frac{\partial \mathcal{L}_1}{\partial z_1} +  \frac{\partial \mathcal{L}_2}{\partial z_1}\right) }_{\text{1st factor}}
\underbrace{\left(\frac{\partial z_1}{\partial \hat{z}_1} \right)}_{\text{2nd factor}}
\underbrace{\left(\frac{\partial \hat{z}_1}{\partial \omega_{1,1}} \right)}_{\text{3rd factor}}  \tag{12.79} 
\end{align}\]</span></p>
<p>Decomposing the 1st factor into two terms, we have:</p>
<p><span class="math display" id="eq:equate1140077">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{1,1}} = 
\underbrace{
\left(\overbrace{
      \underbrace{\frac{\partial \mathcal{L}_{(total)}}{\partial h_1}
       \frac{\partial h_1}{\partial \hat{h}_1}}_{\delta_{h_1}}
       \frac{\partial \hat{h}_1}{\partial z_1}}^{\partial \mathcal{L}_1 / \partial z_1} + 
       \overbrace{
       \underbrace{
       \frac{\partial \mathcal{L}_{(total)}}{\partial h_2}
       \frac{\partial h_2}{\partial \hat{h}_2} }_{\delta_{h_2}}
       \frac{\partial \hat{h}_2}{\partial z_1}}^{\partial \mathcal{L}_2 / \partial z_1}
       \right)
\frac{\partial z_1}{\partial \hat{z}_1}}_{\delta_{z_1}}
\frac{\partial \hat{z}_1}{\partial \omega_{1,1}} \tag{12.80} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display" id="eq:equate1140078">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h_1} = 
\left(\frac{\partial \mathcal{L}_{1}}{\partial h_1} + 
       \frac{\partial \mathcal{L}_{2}}{\partial h_1}\right)
\ \ \ \ \ \ \ \ 
\frac{\partial \mathcal{L}_{(total)}}{\partial h_2} = 
\left(\frac{\partial \mathcal{L}_{1}}{\partial h_2} + 
       \frac{\partial \mathcal{L}_{2}}{\partial h_2}\right) \tag{12.81} 
\end{align}\]</span></p>
<p>Note that we have previously solved for the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(h_1\)</span>:</p>
<p><span class="math display" id="eq:equate1140079">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h_1} = -0.02309672 \tag{12.82} 
\end{align}\]</span></p>
<p>Here, we also have to solve for the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(h_2\)</span>:</p>
<p><span class="math display" id="eq:equate1140080">\[\begin{align}
\frac{\partial \mathcal{L}_{1}}{\partial h_2} = \delta_1
 \left(\frac{\partial \hat{o}_1}{\partial h_2}\right) = \delta_1 \varphi_{2,1} 
= (0.12787676)(0.04) = 0.0051150704  \tag{12.83} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140081">\[\begin{align}
\frac{\partial \mathcal{L}_{2}}{\partial h_2} = \delta_2
\left(\frac{\partial \hat{o}_2}{\partial h_2}\right) = \delta_2 \varphi_{2,2}
=  (-0.07695146)(0.40) = -0.030780584  \tag{12.84} 
\end{align}\]</span></p>
<p>where the derivatives of the two <strong>net inputs</strong> with respect to <span class="math inline">\(h_2\)</span> are:</p>
<p><span class="math display" id="eq:equate1140082">\[\begin{align}
\frac{\partial  \hat{o}_1 }{\partial h_2} = 
\frac{\partial \left(\varphi_{0,1} \times h_0 +  \varphi_{1,1} \times h_1 + \varphi_{2,1} \times h_2 \right)}{\partial h_2} = \varphi_{2,1} \times {h_2}^{(1-1)} = \varphi_{2,1} \tag{12.85} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140083">\[\begin{align}
\frac{\partial  \hat{o}_2 }{\partial h_2} = 
\frac{\partial \left(\varphi_{0,2} \times h_0 +  \varphi_{1,2} \times h_1 + \varphi_{2,2} \times h_2 \right)}{\partial h_2} = \varphi_{2,2} \times {h_2}^{(1-1)} = \varphi_{2,2} \tag{12.86} 
\end{align}\]</span></p>
<p>Therefore:</p>
<p><span class="math display" id="eq:equate1140084">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h_2} = 
\left(\frac{\partial \mathcal{L}_{1}}{\partial h_2}  +
\frac{\partial \mathcal{L}_{2}}{\partial h_2} \right) = 0.0051150704 + -0.030780584 = -0.025665514 \tag{12.87} 
\end{align}\]</span></p>
<p>Now, as for the <strong>H</strong> <strong>deltas</strong>, we already have solved for <span class="math inline">\(\delta_{h_1}\)</span> in the previous step. We should solve for the <span class="math inline">\(\delta_{h_2}\)</span> next. For that, we first need to solve for the derivatives of the <strong>activation outputs</strong> in <strong>H layer</strong> with respect to their corresponding <strong>net inputs</strong>. Note that <span class="math inline">\(\frac{\partial h_1}{\partial \hat{h}_1}\)</span> is already solved for us. We have to solve for <span class="math inline">\(\frac{\partial h_2}{\partial \hat{h}_2}\)</span>.</p>
<p><span class="math display" id="eq:equate1140085">\[\begin{align}
\frac{\partial h_2}{\partial \hat{h}_2} = h_2(1 - h_2) = 0.59800413 ( 1 - 0.59800413) = 0.24039519  \tag{12.88} 
\end{align}\]</span></p>
<p>Therefore, for <span class="math inline">\(\delta_{h_2}\)</span>, we get:</p>
<p><span class="math display" id="eq:equate1140086">\[\begin{align}
\delta_{h_2} = \left(\frac{\partial \mathcal{L}_{(total)}}{\partial h_2}\right)
\left(\frac{\partial h_2}{\partial \hat{h}_2}\right) = (-0.025665514)(0.24039519) = -0.0061698661 \tag{12.89} 
\end{align}\]</span></p>
<p>Next, we still have to solve for the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(\mathbf{z_1}\)</span>:</p>
<p><span class="math display" id="eq:equate1140087">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial z_1} = \left(\frac{\partial \mathcal{L}_{1}}{\partial z_1} + 
       \frac{\partial \mathcal{L}_{2}}{\partial z_1}\right)  \tag{12.90} 
\end{align}\]</span></p>
<p>The equation relies on solving for the derivative of <strong>net inputs</strong> ( <span class="math inline">\(\hat{h}_1\)</span> and <span class="math inline">\(\hat{h}_2\)</span>) with respect to the <strong>activation output</strong> (<span class="math inline">\(z_1\)</span>):</p>
<p><span class="math display" id="eq:equate1140088">\[\begin{align}
\frac{\partial \hat{h}_1}{\partial z_1} = 
\frac{\partial (\alpha_{0,1} \times z_0 + \alpha_{1,1} \times z_1 + \alpha_{2,1} \times z_2)} {\partial z_1} = \alpha_{1,1} = 0.09  \tag{12.91} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140089">\[\begin{align}
\frac{\partial \hat{h}_2}{\partial z_1} = 
\frac{\partial (\alpha_{0,2} \times z_0 + \alpha_{1,2} \times z_1 + \alpha_{2,2} \times z_2)} {\partial z_1} = \alpha_{1,2} = 0.06   \tag{12.92} 
\end{align}\]</span></p>
<p>Finally, we can solve for the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(z_1\)</span>:</p>
<p><span class="math display" id="eq:equate1140090">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial z_1} 
&amp;= (\delta_{h_1} \alpha_{1,1} + \delta_{h_2} \alpha_{1,2})  \tag{12.93} \\
&amp;= (-0.0055285615)(0.09) + (-0.0061698661)(0.06) \nonumber \\
&amp;= -0.00049757053 + -0.00037019197 \nonumber \\
&amp;= -0.0008677625 \nonumber
\end{align}\]</span></p>
<p>Next, we solve for the derivative of the <strong>activation output</strong> (<span class="math inline">\(z_1\)</span>) with respect to its <strong>net input</strong> (<span class="math inline">\(\hat{z}_1\)</span>) using <strong>sigmoid gradient</strong>:</p>
<p><span class="math display" id="eq:equate1140091">\[\begin{align}
\frac{\partial z_1}{\partial \hat{z}_1} = z_1(1 - z_1)  = 0.52732275(1 - 0.52732275) = 0.24925348 \tag{12.94} 
\end{align}\]</span></p>
<p>That should suffice to solve for <span class="math inline">\(\delta_{z_1}\)</span> <strong>delta</strong>:</p>
<p><span class="math display" id="eq:equate1140092">\[\begin{align}
\delta_{z_1} &amp;= \left[\left(\delta_{h_1} \alpha_{1,1} + \delta_{h_2} \alpha_{1,2}\right) \right] \left(\frac{\partial z_1}{\partial \hat{z}_1}\right) =  (-0.0008677625) (0.24925348)  \tag{12.95} \\
 &amp;=-0.00021629282 \nonumber
\end{align}\]</span></p>
<p>Lastly, we solve for the derivative of the <strong>net input</strong> (<span class="math inline">\(\hat{z}_1\)</span>) with respect to the <strong>weight</strong> (<span class="math inline">\(\omega_{1,1}\)</span>):</p>
<p><span class="math display" id="eq:equate1140093">\[\begin{align}
\frac{\partial \hat{z}_1}{\partial \omega_{1,1}} = \frac{\partial (\omega_{0,1} \times x_0 + \omega_{1,1} \times x_1 + \omega_{2,1} \times x_2)} {\partial \omega_{1,1}} =  x_1 = 0.12 \tag{12.96} 
\end{align}\]</span></p>
<p>Therefore:</p>
<p><span class="math display" id="eq:equate1140094">\[\begin{align}
\nabla \omega_{1,1} \mathcal{L} =  
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{1,1}} = \delta_{z_1} x_1 = 
(-0.00021629282)(0.12) = \text{-2.5955138e-5} \tag{12.97} 
\end{align}\]</span></p>
<p>The gradient calculation for other <strong>omega</strong> <span class="math inline">\((\omega)\)</span> weights follows the same process.</p>
<p><strong>Seventh</strong>, for <strong>Backward Pass</strong>, we now can perform update to our parameters. Here, we update the parameters we covered in our previous steps, namely <span class="math inline">\(\varphi_{1,1}\)</span>, <span class="math inline">\(\alpha_{1,1}\)</span>, and <span class="math inline">\(\omega_{1,1}\)</span>:</p>
<p><span class="math display" id="eq:equate1140095">\[\begin{align}
{\varphi_{1,1}}^{(t+1)} = {\varphi_{1,1}}^{(t)} - \eta \nabla \omega_{1,1} \mathcal{L}= 0.03 - 0.01 \times \text{0.077125331} = 0.029228747 \tag{12.98} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140096">\[\begin{align}
{\alpha_{1,1}}^{(t+1)} = {\alpha_{1,1}}^{(t)} - \eta \nabla \omega_{1,1} \mathcal{L}= 0.09 - 0.01 \times \text{-0.0029153363} = 0.090029153 \tag{12.99} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140097">\[\begin{align}
{\omega_{1,1}}^{(t+1)} = {\omega_{1,1}}^{(t)} - \eta \nabla \omega_{1,1} \mathcal{L}= 0.21 - 0.01 \times \text{-2.5955138e-5} = 0.21000026 \tag{12.100} 
\end{align}\]</span></p>
<p>The same <strong>update rule</strong> applies to all other parameters.</p>
<p>Note here that <strong>eta</strong> <span class="math inline">\((\eta)\)</span> symbol represents the learning rate set at 0.01 and <strong>t</strong> serves as an iteration index.</p>
<p><strong>Lastly</strong>, we iterate through the process until the <strong>cost</strong> minimizes into a tolerable threshold.</p>
<p>We emphasize that our entire <strong>MLP example</strong> uses <strong>sigmoid function</strong> and <strong>least square loss</strong>. However, while it may help showcase the operational and technical aspects of <strong>MLP</strong>, the choice of the <strong>activation function</strong> and <strong>loss function</strong> in our example may not necessarily render the expected result and interpretability. Later, we showcase <strong>Cross-Entropy loss</strong> as one of the alternative loss functions for <strong>Sigmoid</strong>. With that in mind, let us introduce alternative <strong>Activation Functions</strong> commonly used in <strong>Neural Networks</strong>.</p>
</div>
<div id="activation-function" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.5</span> Activation Function <a href="12.3-multi-layer-perceptron-mlp.html#activation-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>From the perspective of an <strong>activation function</strong>, most of our discussions center around <strong>sigmoid function</strong>. In this section, we outline other <strong>activation functions</strong>.</p>
<p>We use an activation function mainly for solving non-linear problems; thus, we may often read phrases such as adding nonlinearity to our network, which implies adding activation functions. Choosing an activation function depends on many factors, especially for our output layer. For example, if we are looking to solve a problem based on <strong>Linear or Logistic Regression</strong>, then perhaps <strong>RELU</strong> and its variations may apply. If it is based on <strong>Binomial Classification</strong>, then perhaps <strong>Sigmoid</strong> and its improved counterparts may apply. Otherwise, perhaps <strong>Softmax</strong> may apply, generalizing for <strong>Multinomial Classification</strong>.</p>
<p>The list below enumerates just a few of the common activation functions, among many others. We also introduce <strong>Swish</strong> <span class="citation">(Prajit Ramachandran et al. <a href="bibliography.html#ref-ref962p">2017</a>)</span> and <strong>Mish</strong> <span class="citation">(Diganta Misra <a href="bibliography.html#ref-ref972d">2019</a>)</span>.   </p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Sigmoid} &amp;= \frac{1}{1 + \text{exp}(-x)} \\ {} \\
\mathbf{RELU} &amp;= \text{max}(0, x) \\ {} \\
\mathbf{\text{Softmax}} &amp;= \frac{exp(x_i - max(x))}{\sum_j exp(x_j - max(x))} \\ {} \\
\mathbf{Mish} &amp;=  x \times \text{TanH}(\text{softplus}(x ))
\end{array}
\left|
\begin{array}{ll}
 \mathbf{TanH} &amp;= \frac{\text{exp}(x) - \text{exp}(-x)}{\text{exp}(x) + \text{exp}(-x)} \\ {} \\
 \mathbf{Leaky\ RELU} &amp;= \begin{array}{l}\text{max}(\alpha x, x),\\ \alpha=0.01 - 0.3\end{array}\\ {} \\
 \mathbf{Swish}  &amp;= x \times \text{sigmoid}(x)\\ {} \\
 \mathbf{Softplus} &amp;= \log_e( 1 + \text{exp}(x)) 
\end{array}
\right.
\]</span></p>
<p>Other variants of <strong>Swish</strong> are:</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{\text{Switch (RELU)}} &amp;= x \times \text{RELU}(x) 
\end{array}
\left| 
\begin{array}{ll}
\mathbf{\text{Switch (TanH)}} &amp;= x \times \text{TanH}(x)
\end{array}
\right.
\]</span></p>
<p>Note that when handling large numbers, a <strong>trick</strong> used for <strong>softmax</strong> - and perhaps for other formulas which rely on <strong>exp(.)</strong> for that matter - is to subtract the maximum x value from each x value to achieve some level of numerical stability.</p>
<p>Here is the implementation of each <strong>activation function</strong>:</p>

<div class="sourceCode" id="cb1798"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1798-1" data-line-number="1">linear          &lt;-<span class="st"> </span><span class="cf">function</span>(x) { x }</a>
<a class="sourceLine" id="cb1798-2" data-line-number="2">binary.step     &lt;-<span class="st"> </span><span class="cf">function</span>(x) { idx =<span class="st"> </span><span class="kw">which</span>(x <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> ); x[idx] =<span class="st"> </span><span class="dv">0</span>;  </a>
<a class="sourceLine" id="cb1798-3" data-line-number="3">                                  x[<span class="op">-</span>idx] =<span class="st"> </span><span class="dv">1</span>; x }</a>
<a class="sourceLine" id="cb1798-4" data-line-number="4">sigmoid         &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) }</a>
<a class="sourceLine" id="cb1798-5" data-line-number="5">tan.h           &lt;-<span class="st"> </span><span class="cf">function</span>(x) { (<span class="kw">exp</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) <span class="op">/</span><span class="st"> </span>( <span class="kw">exp</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) }</a>
<a class="sourceLine" id="cb1798-6" data-line-number="6">relu            &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">pmax</span>(x, <span class="dv">0</span>)  }</a>
<a class="sourceLine" id="cb1798-7" data-line-number="7">leaky.relu      &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">a=</span><span class="fl">0.01</span>) { <span class="kw">pmax</span>( x, a<span class="op">*</span>x) }</a>
<a class="sourceLine" id="cb1798-8" data-line-number="8">softplus        &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">log</span> ( <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x), <span class="kw">exp</span>(<span class="dv">1</span>)) }</a>
<a class="sourceLine" id="cb1798-9" data-line-number="9">swish.sigmoid   &lt;-<span class="st"> </span><span class="cf">function</span>(x) { x <span class="op">*</span><span class="st"> </span><span class="kw">sigmoid</span>(x) }</a>
<a class="sourceLine" id="cb1798-10" data-line-number="10">swish.relu      &lt;-<span class="st"> </span><span class="cf">function</span>(x) { x <span class="op">*</span><span class="st"> </span><span class="kw">relu</span>(x) }</a>
<a class="sourceLine" id="cb1798-11" data-line-number="11">swish.tanh      &lt;-<span class="st"> </span><span class="cf">function</span>(x) { x <span class="op">*</span><span class="st"> </span><span class="kw">tan.h</span>(x) }</a>
<a class="sourceLine" id="cb1798-12" data-line-number="12">mish.tan.h      &lt;-<span class="st"> </span><span class="cf">function</span>(x) { x <span class="op">*</span><span class="st"> </span><span class="kw">tan.h</span>(<span class="kw">softplus</span>(x))}</a>
<a class="sourceLine" id="cb1798-13" data-line-number="13">softmax         &lt;-<span class="st"> </span><span class="cf">function</span>(x) { p =<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">1</span>, max); x =<span class="st"> </span>x <span class="op">-</span><span class="st"> </span>p; p =<span class="st"> </span><span class="kw">exp</span>(x) </a>
<a class="sourceLine" id="cb1798-14" data-line-number="14">                                 s =<span class="st"> </span><span class="kw">apply</span>(p, <span class="dv">1</span>, sum); <span class="kw">sweep</span>(p, <span class="dv">1</span>, s, <span class="st">&quot;/&quot;</span>) </a>
<a class="sourceLine" id="cb1798-15" data-line-number="15">                               }</a></code></pre></div>

<p>Note that there are many other variations of <strong>RELU</strong> such as <strong>Shifted RELU (SRELU)</strong>, <strong>Parameterized RELU (PRELU)</strong>, <strong>Scaled Exponential Linear Unit (SELU)</strong>, and others. We leave readers to investigate the variations.</p>
<p>Now, for every <strong>activation function</strong> we use, there is a corresponding <strong>gradient function</strong> and corresponding <strong>loss function</strong> required for <strong>backpropagation</strong>. Let us enumerate the gradient functions of a few of the <strong>activation functions</strong> where <span class="math inline">\(\frac{\partial\ a(x)}{\partial\ x} = a&#39;(x)\)</span> (no derivations included):</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{\nabla\ Sigmoid} = \sigma(x)(1 - \sigma(x)) \\ {} \\
\mathbf{\nabla\ RELU} = \begin{cases}
0 &amp; if\ x \le 0\\
1 &amp; if\ x &gt; 0
\end{cases} \\ {} \\
\mathbf{\nabla\  Softmax} = a(x_i)( \delta_{ij} - a(x_j)) \\ {} \\
\mathbf{\nabla\  Mish} =  \frac{exp(x) \times \omega}{\delta^2} 
\end{array}
\left|
\begin{array}{ll}
\nabla\ \mathbf{TanH} = 1 - a(x)^2\\ {} \\
\mathbf{\nabla\ Leaky\ RELU} = \begin{array}{l}
\begin{cases}
\alpha &amp; if\ x\le 0,\\ 
1 &amp; if\ x &gt; 0
\end{cases}\\ 
e.g. \alpha=0.01 \end{array}\\ {} \\
\mathbf{\nabla\  Swish} = a(x) + \text{sigmoid}(x)( 1 - a(x)) \\ {} \\
\mathbf{\nabla\  Softplus} =  \frac{1}{1 + exp(-x)}\ \leftarrow \text{(sigmoid)} 
\end{array}
\right.
\]</span></p>
<p>Note that <strong>activation functions</strong> have to be differentiable. For <strong>RELU</strong> and <strong>Leaky RELU</strong>, we may not have to be strictly mathematical; instead, we can produce a <strong>quasi-derivative</strong> for the respective functions like so:</p>
<p><span class="math display" id="eq:eqnnumber603">\[\begin{align}
a&#39;(x) = \begin{cases}
0 &amp; if\ x &lt;= 0\\
1 &amp; if\ x &gt; 0
\end{cases}
\ \ \ \ \ \ \ \ \ \ \ \ \ 
a&#39;(x) = \begin{cases}
\alpha &amp; if\ x &lt;= 0\\
1 &amp; if\ x &gt; 0
\end{cases} \tag{12.101}
\end{align}\]</span></p>
<p>For <strong>Mish</strong> <span class="citation">(Diganta M., <a href="bibliography.html#ref-ref972d">2019</a>)</span>, we have the following parameters for its derivatives:</p>
<p><span class="math display" id="eq:equate1140098">\[\begin{align}
\omega = 4(x + 1) + 4e^{2x} + e^{3x} + e^x(4x + 6)
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\delta = 2e^x + e^{2x} + 2  \tag{12.102} 
\end{align}\]</span></p>
<p>For <strong>Swish</strong> <span class="citation">(Prajit R. et. al, <a href="bibliography.html#ref-ref962p">2017</a>)</span>, the gradient is <span class="math inline">\(a(x) = x \times \text{sigmoid}(x)\)</span>. Additionally, <strong>Swish</strong> also can use an extra parameter, namely <span class="math inline">\(\beta\)</span>. For example:</p>
<p><span class="math display" id="eq:equate1140099">\[\begin{align}
\text{Swish (Sigmoid)} = x \times \text{sigmoid}(\beta X) \tag{12.103} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140100">\[\begin{align}
\nabla \text{Swish (Sigmoid)} = \beta a(\beta x) + \text{sigmoid}(\beta x) \times ( 1 - \beta a(\beta x))  \tag{12.104} 
\end{align}\]</span></p>
<p>For <strong>Softmax</strong> (Ludwig Boltzmann, 1868), the derivative optionally uses <strong>Kronecker Delta</strong>, denoted by the symbol (<span class="math inline">\(\delta_{ij}\)</span>). Below is an example of applying the <strong>Kronecker Delta</strong> to a matrix that produces an <strong>Identity matrix</strong>:</p>
<p><span class="math display">\[
\left[
\begin{array}{lll}
1 &amp; 2 &amp; 3\\
4 &amp; 5 &amp; 6\\
7 &amp; 8 &amp; 9
\end{array}
\right]_X
\times
\left[
\begin{array}{lll}
1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 1
\end{array}
\right]_\delta = 
\left[
\begin{array}{lll}
1 &amp; 0 &amp; 0\\
0 &amp; 5 &amp; 0\\
0 &amp; 0 &amp; 9
\end{array}
\right]
\ \ \ \ where\ \ \ \
\delta_{jk} = 
\begin{cases}
1 &amp; \text{if j = k}\\
0 &amp; \text{if j }\ne\text{ k}
\end{cases}
\]</span></p>
<p>More discussion of <strong>Softmax</strong> to follow later ahead.</p>
<p>Below is a list of the implementation of the gradients:</p>

<div class="sourceCode" id="cb1799"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1799-1" data-line-number="1">gradient.sigmoid    &lt;-<span class="st"> </span><span class="cf">function</span>(o) {  o <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>o) } </a>
<a class="sourceLine" id="cb1799-2" data-line-number="2">gradient.tan.h      &lt;-<span class="st"> </span><span class="cf">function</span>(o) { <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>o<span class="op">^</span><span class="dv">2</span> }       </a>
<a class="sourceLine" id="cb1799-3" data-line-number="3">gradient.relu       &lt;-<span class="st"> </span><span class="cf">function</span>(o) { <span class="kw">pmax</span>(<span class="kw">sign</span>(o), <span class="dv">0</span>) }  </a>
<a class="sourceLine" id="cb1799-4" data-line-number="4">gradient.leaky.relu &lt;-<span class="st"> </span><span class="cf">function</span>(o, <span class="dt">a=</span><span class="fl">0.01</span>) { <span class="kw">pmax</span>(<span class="kw">sign</span>(o), a) }</a>
<a class="sourceLine" id="cb1799-5" data-line-number="5">gradient.softplus   &lt;-<span class="st"> </span><span class="cf">function</span>(o) { <span class="kw">sigmoid</span>(o) }  </a>
<a class="sourceLine" id="cb1799-6" data-line-number="6">gradient.swish.sigmoid  &lt;-<span class="st"> </span><span class="cf">function</span>(o) { a =<span class="st"> </span>o <span class="op">*</span><span class="st"> </span><span class="kw">sigmoid</span>(o);</a>
<a class="sourceLine" id="cb1799-7" data-line-number="7">                                         a <span class="op">+</span><span class="st"> </span><span class="kw">sigmoid</span>(o) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>a) }</a>
<a class="sourceLine" id="cb1799-8" data-line-number="8">gradient.mish.tan.h     &lt;-<span class="st"> </span><span class="cf">function</span>(o) { </a>
<a class="sourceLine" id="cb1799-9" data-line-number="9">                            w =<span class="st"> </span><span class="dv">4</span><span class="op">*</span>(o <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="dv">4</span><span class="op">*</span><span class="kw">exp</span>(<span class="dv">2</span><span class="op">*</span>o) <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="dv">3</span><span class="op">*</span>o) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1799-10" data-line-number="10"><span class="st">                            </span><span class="kw">exp</span>(o) <span class="op">*</span><span class="st"> </span>(<span class="dv">4</span><span class="op">*</span>o <span class="op">+</span><span class="st"> </span><span class="dv">6</span>)</a>
<a class="sourceLine" id="cb1799-11" data-line-number="11">                            d =<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">exp</span>(o) <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="dv">2</span><span class="op">*</span>o) <span class="op">+</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1799-12" data-line-number="12">                            (<span class="kw">exp</span>(o) <span class="op">*</span><span class="st"> </span>w) <span class="op">/</span><span class="st"> </span>d<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb1799-13" data-line-number="13">gradient.softmax   &lt;-<span class="st"> </span><span class="cf">function</span>(o) { J =<span class="st"> </span><span class="kw">list</span>();  N =<span class="st"> </span><span class="kw">nrow</span>(o)</a>
<a class="sourceLine" id="cb1799-14" data-line-number="14">                            <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1799-15" data-line-number="15">                                J[[i]] =<span class="st"> </span><span class="kw">diag</span>(o[i,]) <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1799-16" data-line-number="16"><span class="st">                                         </span><span class="kw">kronecker</span>(<span class="kw">t</span>(o[i,]), o[i,])</a>
<a class="sourceLine" id="cb1799-17" data-line-number="17">                            }; J }</a></code></pre></div>

<p>For a plot of the activation functions and corresponding derivatives, see Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:activationfunction">12.11</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:activationfunction"></span>
<img src="activationfunction.png" alt="Activation Function" width="100%" />
<p class="caption">
Figure 12.11: Activation Function
</p>
</div>
<p>It is possible to use a different <strong>activation function</strong> for hidden layers and the output layer. For example, for <strong>Logistic Regression</strong>, it may be common to use <strong>Rectified Linear Unit (RELU)</strong> as an <strong>activation functions</strong> for all <strong>hidden layers</strong> and opt to use <strong>sigmoid</strong> in the output layer. And for <strong>Multi Classification</strong>, we can use <strong>softmax function</strong> in the output layer.</p>
<p>Certain properties of an activation function can be reviewed to see if such a function qualifies to fit one’s purposes: range, monotonicity, boundary (e.g., bounded below, unbounded above), and smoothness. Apart from the usual <strong>performance</strong> and <strong>accuracy</strong> goals, some of the activation functions, in certain conditions, may help to avoid the <strong>vanishing and exploding gradient</strong> conditions; thus, such properties are essential. The former condition, e.g., <strong>vanishing gradient</strong>, may manifest if the <strong>neural network</strong> has too many layers. For example, notice in Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:activationfunction">12.11</a> that the gradients of both <strong>Sigmoid</strong> and <strong>TanH</strong> reduce the effect of gradients as gradients approach zero and as their net inputs stretch outwards, e.g., <span class="math inline">\(\pm \infty\)</span>. In the figure, all other activation functions (apart from the linear and binary step) have a lower bound (e.g., zero) towards the negative direction and an infinite bound towards the positive direction. The latter condition, e.g., <strong>exploding gradient</strong>, may manifest if the <strong>neural network</strong> has connections that accumulate large weights due to large gradients. Whereas <strong>vanishing gradients</strong> tend towards <strong>-infinity</strong> (<span class="math inline">\(+\infty\)</span>), <strong>exploding gradients</strong> tend towards <strong>+infinity</strong> (<span class="math inline">\(+\infty\)</span>).</p>
<p>In terms of performance measures, each <strong>activation function</strong> may work best with its own <strong>loss function</strong>. For example, for both <strong>Relu</strong> and <strong>Leaky Relu</strong>, we have the following <strong>Least Squared Error</strong> formula:</p>
<p><span class="math display" id="eq:equate1140101">\[\begin{align}
\mathcal{L} =  \frac{1}{N} \sum_{i=1}^N \left(t - o\right)^2 \tag{12.105} 
\end{align}\]</span></p>
<p>For both <strong>Sigmoid</strong> and <strong>Softmax</strong>, we can use <strong>Cross-Entropy (CE) Loss</strong>. Generally, we see the following equation, discussed in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>).</p>
<p><span class="math display" id="eq:equate1140102">\[\begin{align}
\mathcal{L}^{(CE)} =  
\underbrace{-\sum_x^X P_x \log_e \mathcal{Q}_x }_\text{Cross-Entropy Loss} 
= - \sum_x^X \left[  t_{x} \log_e \sigma(o_{(x)})\right] \tag{12.106} 
\end{align}\]</span></p>
<p>Note that the idea of <strong>Cross-Entropy</strong> is about measuring the distance between two distributions - See Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>).</p>
<p>In essence, for <strong>Sigmoid</strong>, a <strong>Binary CE loss</strong> can be translated into a <strong>Binomial Logistic Loss</strong> like so:</p>
<p><span class="math display" id="eq:eqnnumber604">\[\begin{align}
\mathcal{L}^{(CE)} = 
\underbrace{ - \frac{1}{N} \sum_{i=1}^N\left[t_i \log_e(o_i) + (1 - t_i)\log_e(1 - o_i) \right]}_\text{Binomial Logistic Loss}
= 
\begin{cases}
-\log_e(o_i) &amp; \text{if }t_i\text{ = 1} \\
-\log_e(1 - o_i) &amp; \text{if }t_i\text{ = 0} \\
\end{cases} \tag{12.107}
\end{align}\]</span></p>
<p>where <strong>N</strong> is the number of samples.</p>
<p>For <strong>Softmax</strong>, we may use a <strong>Multinomial Logistic Loss</strong> or <strong>Multiple Cross-Entropy Loss</strong> formula for <strong>Multi Classification</strong>:</p>
<p><span class="math display" id="eq:equate1140104" id="eq:equate1140103">\[\begin{align}
\mathcal{L}^{(CE)} &amp;= \underbrace{ - \frac{1}{N} \sum_{i=1}^N\sum_{k=1}^K\left[t_{ik} \log_e(o_{ik}) + (1 - t_{ik})\log_e(1 - o_{ik}) \right]}_\text{Multinomial Logistic Loss}  \tag{12.108} \\
&amp;= -\frac{1}{N}\sum_{N=1}^N\sum_{k=1}^Kt_{(ik)} \log_e(o_{(ik)})   \tag{12.109} 
\end{align}\]</span></p>
<p>where <strong>N</strong> is the number of samples and <strong>K</strong> is the number of classes. A K-value of 2 makes the loss a <strong>Binary CE loss</strong>.</p>
<p>Note that the <strong>target</strong> (t) is a <strong>one-hot vector</strong>, and the <strong>output</strong> (o) is a vector of probabilities that sums up to 1.</p>
<p>Let us give a special look into <strong>Softmax</strong> in the context of backpropagation. Our discussion focuses on the derivative of <strong>CE loss</strong> with respect to an <strong>activation output</strong> - also called <strong>activation output</strong> in our own context.</p>
<p><strong>Sigmoid (Logistic) Loss</strong></p>
<p>It helps to start with the more simple <strong>Sigmoid</strong> activation function for <strong>Binomial Classification</strong>. Then we can generalize into <strong>Softmax</strong> for <strong>Multi Classification</strong>.</p>
<p>Using the following <strong>sigmoid</strong> function, we should be able to get the derivative of the <strong>sigmoid</strong> function with respect to the raw input (the <strong>logits</strong>).</p>
<p><span class="math display" id="eq:equate1140105">\[\begin{align}
s(x) = \frac{1}{1 + exp(-x)}\ \ \ \ \ \ s&#39;(x) = \frac{\partial\ s(x)}{\partial x} = s(x)(1 - s(x)) \tag{12.110} 
\end{align}\]</span></p>
<p>From a <strong>Neural Network</strong> perspective, we have the equivalent notation in reference to Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:deltarule">12.8</a>:</p>
<p><span class="math display" id="eq:equate1140106">\[\begin{align}
\frac{\partial\ s(x)}{\partial x} = \frac{\partial\ \sigma(\hat{o})}{\partial \hat{o}}  =
\frac{\partial o}{\partial \hat{o}} = \sigma(\hat{o})(1 - \sigma(\hat{o})) = o ( 1 - o) \tag{12.111} 
\end{align}\]</span></p>
<p>where our <strong>activation output</strong> (<strong>activation output</strong>) in the output layer is denoted by (<strong>o</strong>) and our <strong>logits input</strong> (<strong>net input</strong>) as (<span class="math inline">\(\mathbf{\hat{o}}\)</span>).</p>
<p>Additionally, the derivative of a <strong>Cross-Entropy Loss</strong> as our <strong>Loss Function</strong> for <strong>Sigmoid</strong> with respect to an <strong>activation output</strong> (<span class="math inline">\(\mathbf{o}\)</span>) is as follows (no derivations provided):</p>
<p><span class="math display" id="eq:equate1140107">\[\begin{align}
\frac{\partial \mathcal{L}^{(CE)}}{\partial o} = - \frac{t}{o} + \frac{1 - t}{1 - o}
= \frac{o-t}{o( 1 - o)} \tag{12.112} 
\end{align}\]</span></p>
<p>If we recall the <strong>Delta Rule</strong>, the derivative of the <strong>Cross-Entropy Loss</strong> for <strong>Sigmoid function</strong> with respect to a <strong>net input</strong> (<span class="math inline">\(\mathbf{\hat{o}}\)</span>) is as follows:</p>
<p><span class="math display" id="eq:equate1140108">\[\begin{align}
\delta_{o}  = \frac{\partial \mathcal{L}^{(CE)}}{\partial \hat{o}} =  
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o}\right)
\left(\frac{\partial o}{\partial \hat{o}}\right)
= \left[\frac{o-t}{o( 1 - o)}\right] 
o ( 1 - o) = (o - t ) \tag{12.113} 
\end{align}\]</span></p>
<p>Then, the derivative of the <strong>Cross-Entropy Loss</strong> for <strong>Sigmoid function</strong> with respect to a <strong>weight</strong> (<span class="math inline">\(\omega_{(jk)}\)</span>) is as follows:</p>
<p><span class="math display" id="eq:equate1140109">\[\begin{align}
\frac{\partial \mathcal{L}^{(CE)}}{\partial \omega_{(jk)}} = 
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o}\right)
\left(\frac{\partial o}{\partial \hat{o}}\right)
\left(\frac{\partial \hat{o}}{\partial \omega_{(jk)}}\right) =  \delta_{o} \left(\frac{\partial \hat{o}}{\partial \omega_{(jk)}}\right) 
= \delta_{o}(h_j) = (o - t)(h_j)  \tag{12.114} 
\end{align}\]</span></p>
<p><strong>Softmax Loss</strong></p>
<p>Now for <strong>Softmax</strong>, we should be able to get the derivative of the <strong>Softmax</strong> loss function with respect to the raw input.</p>
<p><span class="math display" id="eq:eqnnumber605">\[\begin{align}
s(x_j) = \frac{exp(x_j)}{\sum_{k=1}^Kexp(x_k)}\ \ \ \ \ \ 
s&#39;(x_j) = \frac{\partial\ s(x_j)}{\partial x_j} = 
\begin{cases}
s(x_j)(1 - s(x_j)) &amp; \text{if j = k} \\
-s(x_j) s(x_k) &amp; \text{if j }\ne \text{k} \\
\end{cases} \tag{12.115}
\end{align}\]</span></p>
<p>Here, from a <strong>Neural Network</strong> perspective, we have the equivalent notation in reference to Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:deltarule">12.8</a>:</p>
<p><span class="math display" id="eq:eqnnumber606">\[\begin{align}
\frac{\partial\ s(x_j)}{\partial x_j} = \frac{\partial\ \sigma(\hat{o}_j)}{\partial \hat{o}_j}  =
\frac{\partial o_j}{\partial \hat{o}_j} = 
\begin{cases}
\sigma(\hat{o}_j)(1 - \sigma(\hat{o}_j)) = o_j ( 1 - o_j)   &amp; \text{if j = k} \\
- \sigma(\hat{o}_j) \sigma(\hat{o}_k)  = -(o_j)(o_k)  &amp; \text{if j }\ne \text{k} \\
\end{cases} \tag{12.116}
\end{align}\]</span></p>
<p>Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:softmaxgradient">12.12</a> shows a <strong>Jacobian matrix</strong> as reference when solving for the derivative of the <strong>activation output</strong> - an output from an <strong>activation function</strong> - with respect to the <strong>net input</strong>, e.g. <span class="math inline">\(\frac{\partial o_j}{\partial \hat{o}_j}\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:softmaxgradient"></span>
<img src="softmaxgradient.png" alt="SoftMax Gradient" width="100%" />
<p class="caption">
Figure 12.12: SoftMax Gradient
</p>
</div>
<p>To illustrate, suppose we have the following <strong>net input</strong> to be fed to a <strong>Softmax</strong> activation function (assume one sample and three target classes):</p>
<p><span class="math display">\[
\hat{o}_1 = 3 \ \ \ \ \ \ \ \ \ \ \ \ \
\hat{o}_1 = 4 \ \ \ \ \ \ \ \ \ \ \ \ \ \hat{o}_1 = 5
\]</span></p>
<p>Our <strong>Softmax</strong> yields the following:</p>

<div class="sourceCode" id="cb1800"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1800-1" data-line-number="1">o.hat =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>), <span class="dt">nrow=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1800-2" data-line-number="2">(<span class="dt">o =</span> <span class="kw">softmax</span>(o.hat))</a></code></pre></div>
<pre><code>##         [,1]   [,2]   [,3]
## [1,] 0.09003 0.2447 0.6652</code></pre>

<p>where sum equals one: <span class="math inline">\(\sum_i o_i =\)</span> 0.09 + 0.2447 + 0.6652 = 1.</p>
<p>Now, the <strong>Jacobian matrix</strong> representation of the derivatives of <strong>activation output</strong> with respect to the <strong>net input</strong> is as follows:</p>
<p><span class="math display" id="eq:equate1140110">\[\begin{align}
\sigma(o_j)(\delta_{jk} - \sigma(o_k)) = 
\underbrace{\sigma(o_j)\delta_{jk}}_{\text{identity matrix}} - 
\underbrace{\sigma(o_j) \otimes \sigma(o_k)}_{\text{kronecker product}}
\ \ \ \ \  where\ \ \delta_{jk}\text{ is kronecker delta} \tag{12.117} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb1802"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1802-1" data-line-number="1">(<span class="dt">J =</span> <span class="kw">diag</span>(o) <span class="op">-</span><span class="st"> </span><span class="kw">kronecker</span>(<span class="kw">t</span>(o), o))</a></code></pre></div>
<pre><code>##         [,1]     [,2]     [,3]
## [1,] 0.08193  0.06800  0.03014
## [2,] 0.06800  0.03014 -0.07277
## [3,] 0.03014 -0.07277 -0.35251</code></pre>

<p>To validate, let us tackle the first derivative:</p>
<p><span class="math inline">\(\frac{\partial o_1}{\partial \hat{o}_1} = s(\hat{o}_1)(1 - s(\hat{o}_1)) = o_1 ( 1 - o_1)\)</span> = 0.09 <span class="math inline">\(\times (1-\)</span> 0.09<span class="math inline">\()\)</span> = 0.0819.</p>
<p><span class="math inline">\(\frac{\partial o_2}{\partial \hat{o}_2} = s(\hat{o}_2)(1 - s(\hat{o}_2)) = o_2 ( 1 - o_2)\)</span> = 0.2447 <span class="math inline">\(\times (1-\)</span> 0.2447<span class="math inline">\()\)</span> = 0.0301.</p>
<p><span class="math inline">\(\frac{\partial o_3}{\partial \hat{o}_3} = s(\hat{o}_3)(1 - s(\hat{o}_3)) = o_3 ( 1 - o_3)\)</span> = 0.6652 <span class="math inline">\(\times (1-\)</span> 0.6652<span class="math inline">\()\)</span> = -0.3525.</p>
<p><span class="math inline">\(\frac{\partial o_1}{\partial \hat{o}_2} = -s(\hat{o}_1) s(\hat{o}_2) = - o_1 \times o_2\)</span> = -0.09 <span class="math inline">\(\times\)</span> 0.2447 = 0.068.</p>
<p><span class="math inline">\(\frac{\partial o_1}{\partial \hat{o}_3} = -s(\hat{o}_1) s(\hat{o}_3) = - o_1 \times o_3\)</span> = -0.09 <span class="math inline">\(\times\)</span> 0.6652 = 0.0301.</p>
<p>…</p>
<p><span class="math inline">\(\frac{\partial o_3}{\partial \hat{o}_1} = -s(\hat{o}_3) s(\hat{o}_1) = - o_3 \times o_1\)</span> = -0.6652 <span class="math inline">\(\times\)</span> 0.09 = 0.0301.</p>
<p><span class="math inline">\(\frac{\partial o_3}{\partial \hat{o}_2} = -s(\hat{o}_3) s(\hat{o}_2) = - o_3 \times o_2\)</span> = -0.6652 <span class="math inline">\(\times\)</span> 0.2447 = -0.0728.</p>
<p>Next, the derivative of the <strong>Cross-Entropy Loss</strong> as our <strong>Loss Function</strong> for <strong>Softmax</strong> with respect to an <strong>activation output</strong> (<span class="math inline">\(\mathbf{o_j}\)</span>) is as follows:</p>
<p><span class="math display" id="eq:equate1140111">\[\begin{align}
\frac{\partial \mathcal{L}^{(CE)}}{\partial o_j} = 
\frac{\partial}{\partial o_j}  \left(-\sum_{k=1}^Kt_{k} \log_e(o_{k}) \right) = 
\underbrace{\frac{\partial}{\partial o_j}
\left(- t_j \log_e(o_j)\right)}_{\text{focus on one-hot encoding}} = -\frac{t_j}{o_j} \tag{12.118} 
\end{align}\]</span></p>
<p>Note that <strong>t</strong> is a one-hot vector and only one element is equal to one. Assume therefore that if <span class="math inline">\(\mathbf{t_j= 1}\)</span>, it stands that <span class="math inline">\(-\frac{\partial \mathcal{L}^{(CE)}}{\partial o_j} = -\frac{\partial}{\partial o_j} \left((1)\log_e(o_j)\right)\)</span>, and given that <span class="math inline">\(f&#39;(ln(x)) = \frac{1}{x}\)</span>, therefore <span class="math inline">\(-\frac{\partial}{\partial o_j} \left(\log_e(o_j)\right) = -\frac{1}{o_j} = -\frac{t_j}{o_j}\)</span></p>
<p>For the <strong>Delta Rule</strong>, the derivative of the <strong>Cross-Entropy Loss</strong> function for <strong>Softmax function</strong> with respect to a <strong>net input</strong> (<span class="math inline">\(\mathbf{\hat{o}_j}\)</span>) is as follows:</p>
<p><span class="math display" id="eq:eqnnumber607">\[\begin{align}
\delta_{o_j}  = \frac{\partial \mathcal{L}^{(CE)}}{\partial \hat{o}_j}
= \left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o_j}\right)
\left(\frac{\partial o_j}{\partial \hat{o}_j}\right) = 
\left(- \frac{t_k}{o_j} \right)
\left(
\begin{cases}
o_k ( 1 - o_k) &amp; \text{if j = k}\\
- o_j o_k    &amp; \text{if j }\ne \text{k}
\end{cases}
\right) = (o_j - t_j) \tag{12.119}
\end{align}\]</span></p>
<p>Finally, the derivative of the <strong>Cross-Entropy Loss</strong> function for <strong>softmax function</strong> with respect to a <strong>weight</strong> (<span class="math inline">\(\omega_{(jk)}\)</span>) is as follows:</p>
<p><span class="math display" id="eq:equate1140112">\[\begin{align}
\frac{\partial \mathcal{L}^{(CE)}}{\partial \omega_{(jk)}} =  
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o_j}\right)
\left(\frac{\partial o_j}{\partial \hat{o}_k}\right)
\left(\frac{\partial \hat{o}_k}{\partial \omega_{(jk)}}\right) =
\delta_{o_k} \left(\frac{\partial \hat{o}_k}{\partial \omega_{(jk)}}\right) 
= \delta_{o_k} (h_j) = (o_k - t_k)(h_j)  \tag{12.120} 
\end{align}\]</span></p>
<p>where <strong>J</strong> and <strong>K</strong> are the numbers of classes.</p>
<p>Therefore, if we carefully follow backpropagation using Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:deltarule">12.8</a>, the derivative of the <strong>Softmax</strong> loss with respect to the <strong>weight</strong> (<span class="math inline">\(\alpha_{1,1}\)</span>) is:</p>
<p><span class="math display" id="eq:equate1140118" id="eq:equate1140117" id="eq:equate1140116" id="eq:equate1140115" id="eq:equate1140114" id="eq:equate1140113">\[\begin{align}
\frac{\partial \mathcal{L}^{(CE)}}{\partial \alpha_{1,1}} &amp;=
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o_1}\right) 
\left(\frac{\partial o_1}{\partial h_1}\right)
\left(\frac{\partial h_1}{\partial \alpha_{1,1}}\right)
&amp;+ 
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o_2}\right)
\left(\frac{\partial o_2}{\partial h_1}\right)
\left(\frac{\partial h_1}{\partial \alpha_{1,1}}\right) \tag{12.121} \\
 \tag{12.122} \\
&amp;=
\left(-\mathbf{\frac{ t_1}{o_1}}\right) \left[o_1( 1 - o_1)\right] h_1
&amp;+ 
\left(-\mathbf{\frac{ t_2}{o_2}}\right) \left(-(o_2)(o_1)\right) h_1 \tag{12.123} \\
&amp;=\left(t_2 o_1 - t_1 + t_1 o_1 \right) h_1 \tag{12.124} \\
&amp;=\left((t_2 + t_1) o_1 - t_1 \right) h_1 \tag{12.125} \\
&amp;=(o_1 - t_1) h_1 &amp; \text{where }\sum_i t_i = 1 \tag{12.126} 
\end{align}\]</span></p>
<p>Note that <strong>t</strong> is a one-hot vector. Therefore, in the case above, either we have (<span class="math inline">\(t_1 = 1, t_2 = 0\)</span>) or (<span class="math inline">\(t_1 = 0, t_2 = 1\)</span>).</p>
<p>To illustrate, suppose we have the following <strong>target</strong> (<strong>t</strong>) and <strong>softmax output</strong> (<strong>o</strong>):</p>

<div class="sourceCode" id="cb1804"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1804-1" data-line-number="1">t =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1804-2" data-line-number="2">o =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.49052134</span>, <span class="fl">0.50947866</span>)</a></code></pre></div>

<p>Below is the result of the unsimplified version:</p>

<div class="sourceCode" id="cb1805"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1805-1" data-line-number="1"><span class="op">-</span><span class="st"> </span>t[<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>o[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>o[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>o[<span class="dv">1</span>]) <span class="op">+</span><span class="st"> </span>(<span class="op">-</span>t[<span class="dv">2</span>]<span class="op">/</span>o[<span class="dv">2</span>]) <span class="op">*</span><span class="st"> </span>( <span class="op">-</span>o[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>o[<span class="dv">1</span>])</a></code></pre></div>
<pre><code>## [1] -0.5095</code></pre>

<p>And the simplified version:</p>

<div class="sourceCode" id="cb1807"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1807-1" data-line-number="1">o[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>t[<span class="dv">1</span>]</a></code></pre></div>
<pre><code>## [1] -0.5095</code></pre>

<p>Below, we have our implementation of the <strong>Loss functions</strong>, along with <strong>Helper functions</strong>:</p>

<div class="sourceCode" id="cb1809"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1809-1" data-line-number="1"><span class="co"># Helper Functions</span></a>
<a class="sourceLine" id="cb1809-2" data-line-number="2">ln                       &lt;-<span class="st"> </span><span class="cf">function</span>(x)    { <span class="kw">log</span>(x, <span class="kw">exp</span>(<span class="dv">1</span>))}</a>
<a class="sourceLine" id="cb1809-3" data-line-number="3">relu.loss                &lt;-<span class="st"> </span>leaky.relu.loss &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) { (t<span class="op">-</span>o)<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb1809-4" data-line-number="4">sigmoid.loss             &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) {  eps =<span class="st"> </span><span class="fl">1e-20</span> <span class="co"># Avoid NaN</span></a>
<a class="sourceLine" id="cb1809-5" data-line-number="5">                            <span class="op">-</span><span class="st"> </span>(t <span class="op">*</span><span class="st"> </span><span class="kw">ln</span>(o <span class="op">+</span><span class="st"> </span>eps) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>t) <span class="op">*</span><span class="st"> </span><span class="kw">ln</span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>o <span class="op">+</span><span class="st"> </span>eps))}</a>
<a class="sourceLine" id="cb1809-6" data-line-number="6">softmax.loss             &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) { eps =<span class="st"> </span><span class="fl">1e-20</span>; l =<span class="st"> </span>t <span class="op">*</span><span class="st"> </span><span class="kw">ln</span>(o <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1809-7" data-line-number="7">                            <span class="op">-</span><span class="kw">apply</span>(l, <span class="dv">1</span>, sum)  }</a>
<a class="sourceLine" id="cb1809-8" data-line-number="8">gradient.relu.loss       &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) { o<span class="op">-</span>t } <span class="co"># or -(t-o)</span></a>
<a class="sourceLine" id="cb1809-9" data-line-number="9">gradient.leaky.relu.loss &lt;-<span class="st"> </span>gradient.relu.loss</a>
<a class="sourceLine" id="cb1809-10" data-line-number="10">gradient.sigmoid.loss    &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) { (o<span class="op">-</span>t)<span class="op">/</span>(o <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>o)) }</a>
<a class="sourceLine" id="cb1809-11" data-line-number="11">gradient.softmax.loss    &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) { eps =<span class="st"> </span><span class="fl">1e-20</span>; <span class="op">-</span>t <span class="op">/</span><span class="st"> </span>(o <span class="op">+</span><span class="st"> </span>eps) }</a>
<a class="sourceLine" id="cb1809-12" data-line-number="12">activation               &lt;-<span class="st"> </span><span class="cf">function</span>(x, afunc)    { <span class="kw">afunc</span>(x) }</a>
<a class="sourceLine" id="cb1809-13" data-line-number="13">gradient.activation      &lt;-<span class="st"> </span><span class="cf">function</span>(o, afunc)    { <span class="kw">afunc</span>(o) }     </a>
<a class="sourceLine" id="cb1809-14" data-line-number="14">gradient.loss            &lt;-<span class="st"> </span><span class="cf">function</span>(t, o, afunc) { <span class="kw">afunc</span>(t, o) }</a></code></pre></div>

<p>For generating the <strong>loss function</strong>, we have:</p>

<div class="sourceCode" id="cb1810"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1810-1" data-line-number="1">get.loss &lt;-<span class="st"> </span><span class="cf">function</span>(target, layers, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>) {</a>
<a class="sourceLine" id="cb1810-2" data-line-number="2">    afunc  =<span class="st"> </span><span class="kw">get</span>(<span class="kw">paste0</span>(afunc, <span class="st">&quot;.loss&quot;</span>))</a>
<a class="sourceLine" id="cb1810-3" data-line-number="3">    L      =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb1810-4" data-line-number="4">    output =<span class="st"> </span>layers[[L]]<span class="op">$</span>output</a>
<a class="sourceLine" id="cb1810-5" data-line-number="5">    err    =<span class="st"> </span><span class="kw">afunc</span>(target, output)</a>
<a class="sourceLine" id="cb1810-6" data-line-number="6">    <span class="kw">mean</span>(err)</a>
<a class="sourceLine" id="cb1810-7" data-line-number="7">}</a></code></pre></div>

<p>We now move to the actual implementation of <strong>MLP</strong>.</p>
</div>
<div id="mlp-implementation" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.6</span> MLP Implementation<a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Following an <strong>MLP</strong> algorithm, we assume a <strong>complete Neural Network</strong> with the same setup as diagrammed in Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:deltarule">12.8</a>. Our supposition is that we have at least one hidden layer labeled as <strong>H</strong>.</p>
<p>Now, in the context of data structures, the implementation of <strong>MLP</strong> relies heavily on matrix manipulation, especially with multiple samples. We can see in Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:forwardpass">12.13</a> how the dot product of an input and its corresponding weight results in an output which is taken by the next layer as an input. In turn, the process continues with the following dot product.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:forwardpass"></span>
<img src="forwardpass.png" alt="Forward Pass" width="90%" />
<p class="caption">
Figure 12.13: Forward Pass
</p>
</div>
<p>Below is an <strong>MLP</strong> pseudocode for the <strong>Forward Pass</strong> algorithm. Here, we use <strong>sigmoid</strong> for activation function:</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{x_{(1,1)}, x_{(1,2)}, ..., x_{(n,p)}: x\ \in\ \mathbb{R}^{nxp}\}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rightarrow \text{(p is no. of features, n is no. of samples)}\\
\ \ \ \text{parameters}: \{\omega_{(1,1)}, \omega_{(1,2)}, ..., \omega_{(p,h)}: x\ \in\ \mathbb{R}^{pxh}\}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rightarrow \text{(p is no. of inputs, h is no. of target outputs)}\\
\ \ \ \text{activation function}: f(x) \rightarrow e.g.
\begin{cases} 
sigmoid(x) &amp; \text{if output layer}\\
leaky.relu(x) &amp; \text{if hidden layer}
\end{cases}\\
\mathbf{Algorithm}:\\
\ \ \ net.input = X\\
\ \ \ act.output = \{\} \\
\ \ \ \text{loop}\ L\ in\ 1:\ H\ \ \ \ \ \ \rightarrow \text{(H is no. of layers excluding input)}\\
\ \ \ \ \ \ \ weights = \omega^{(L)} \\
\ \ \ \ \ \ \ act.output^{(L)}  = f(net.input \cdot \omega^{(L)} )\ \ \ \ \ (act.output^{(L) }  \in \mathbb{R}^{nxh})\\
\ \ \ \ \ \ \ net.input = act.output^{(L)}\\
\ \ \ \text{end loop} \\
\mathbf{Output}:\\
\ \ \ act.output 
\end{array}
\]</span></p>
<p>Recall that our <strong>net input</strong> in the matrix includes the biases. Also, our bias constant connects to neurons in different layers with unique weights.</p>
<p>Note that the gradient of the resulting output can also be obtained in the same <strong>forward pass</strong> implementation.</p>
<p>Next, Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:backwardpass">12.14</a> shows the data structure used by <strong>BackPropagation</strong> and <strong>Backward Pass</strong> algorithms.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:backwardpass"></span>
<img src="backwardpass.png" alt="Backward Pass (Back Propagation)" width="90%" />
<p class="caption">
Figure 12.14: Backward Pass (Back Propagation)
</p>
</div>
<p>For all layers, excluding the input layer, a corresponding matrix table is produced for the <strong>deltas</strong>. Next, let us see an <strong>MLP</strong> algorithm for the <strong>BackPropagation</strong> that shows the equations used for the <strong>deltas</strong>:</p>

<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:  \text{(n is no. of samples)}\\
\ \ \ \text{dataset}: \{x_{(1,1)}, x_{(1,2)}, ..., x_{(n,p)}: x\ \in\ \mathbb{R}^{nxp}\}\ \rightarrow \text{(p is no. of features)}\\
\ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ : \{y_{(1,1)}, y_{(1,2)}, ..., y_{(n,o)}: y\ \in\ \mathbb{R}^{nxo}\}\ \rightarrow \text{(o is no. of neurons)}\\
\ \ \ \text{parameters}: \{\omega_{(1,1)}, \omega_{(1,2)}, ..., \omega_{(p,h)}: x\ \in\ \mathbb{R}^{pxh}\}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rightarrow \text{(p is no. of inputs, h is no. of target outputs)}\\
\ \ \ \text{forward-pass output}: act.output\\
\ \ \ \text{activation function}: f(x) \rightarrow e.g. \begin{cases} 
sigmoid(x) &amp; \text{if output layer}\\
leaky.relu(x) &amp; \text{if hidden layer}
\end{cases}\\
\ \ \ \text{gradient function}: g(f(x)) = f(x) (1 - f(x))\ \ \ \ \ \ \ \ \text{(e.g. sigmoid gradient)} \\
\mathbf{Algorithm}:\\
\ \ \ \text{loop}\ L\ in\ H:1 \\
\ \ \ \ \ \ \ \ \ gradient.output = act.output^{(L)}\\
\ \ \ \ \ \ \ \ \ net.input =\ \ \ \begin{cases}
X &amp;  \text{(if input layer)}\\
act.output^{(L-1)} &amp; \text{(otherwise)} 
\end{cases}\\
\ \ \ \ \ \ \ \ \  \frac{\partial \mathcal{L}}{\partial \omega} =\ \ \ \begin{cases}
(t - o) \times act.output^{(L)}  &amp; \text{(if output layer, where L=H)}\\
{\delta}^{(L+1)} \cdot transpose \left({\omega}^{(L+1)}\right) &amp; \text{(if hidden layer)}\\
\end{cases}\\
\ \ \ \ \ \ \ \ \ {\delta}^{(L)} =\ \ \frac{\partial \mathcal{L}}{\partial \omega} \times g(act.output^{(L)})\\
\ \ \ \ \ \ \ \ \ ({\nabla_{\omega}}\mathcal{L})^{(L)} = transpose(net.input) \cdot \delta^{(L)}\\
\ \ \ \text{end loop}\\
\mathbf{Output}: {\nabla_{\omega}} \mathcal{L}
\end{array}
\]</span>
</p>
<p>Note that <strong>Kronecker product</strong>, denoted by the symbol (<span class="math inline">\(\bigotimes\)</span>), also applies to <span class="math inline">\({\Delta _w}^{(L)}\)</span> where every element is multiplied to the matrix:</p>
<p><span class="math display" id="eq:eqnnumber608">\[\begin{align}
\begin{array}{ll}
{\Delta _w}^{(L)}_{\mathbf{h \times o}} &amp;= transpose(net.input) \cdot \delta^{(L)}\\
&amp;= \sum_i^n 
\left\{reshape\left(net.input_{(i,)} \otimes {\delta}^{(L)}_{(i,)} 
\right)\right\} \in \mathbb{R}^{\mathbf{h \times o}}
\end{array} \tag{12.127}
\end{align}\]</span></p>
<p>where <strong>“reshape”</strong> follows the shape of <span class="math inline">\(\omega\)</span>.</p>
<p>For example, using <strong>transpose</strong>, we have:</p>

<div class="sourceCode" id="cb1811"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1811-1" data-line-number="1">net.input =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1811-2" data-line-number="2">delta     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.1</span>,<span class="fl">0.2</span>,<span class="fl">0.3</span>,<span class="fl">0.4</span>,<span class="fl">0.5</span>,<span class="fl">0.6</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1811-3" data-line-number="3"><span class="kw">t</span>(net.input) <span class="op">%*%</span><span class="st"> </span>delta</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  3.5  4.4
## [2,]  4.4  5.6</code></pre>

<p>Using <strong>Kronecker product</strong>, we have:</p>

<div class="sourceCode" id="cb1813"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1813-1" data-line-number="1">reshape &lt;-<span class="st"> </span><span class="cf">function</span>(m, r, c) { <span class="kw">matrix</span>(m, <span class="dt">nrow=</span>r, <span class="dt">ncol=</span>c, <span class="dt">byrow=</span><span class="ot">TRUE</span>)}</a>
<a class="sourceLine" id="cb1813-2" data-line-number="2">s =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>), <span class="dt">nrow=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1813-3" data-line-number="3"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(net.input)) {</a>
<a class="sourceLine" id="cb1813-4" data-line-number="4">  s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span><span class="kw">reshape</span>( <span class="kw">kronecker</span>(net.input[i,], delta[i,]), <span class="dv">2</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1813-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1813-6" data-line-number="6">s</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  3.5  4.4
## [2,]  4.4  5.6</code></pre>

<p>For the <strong>Backward Pass</strong>, we have the following <strong>MLP</strong> pseudocode for the <strong>Gradient Descent</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{parameters}: \{\omega_{(1,1)}, \omega_{(1,2)}, ..., \omega_{(p,h)}: x\ \in\ \mathbb{R}^{pxh}\}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rightarrow \text{(p are inputs, h are  target outputs)}\\
\ \ \ \ \ \ \ \ \ \ \ \ \text{gradient}: \{\Delta \omega_{(1,1)}, \Delta \omega_{(1,2)}, ..., \Delta \omega_{(p,h)}: x\ \in\ \mathbb{R}^{pxh}\}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rightarrow \text{(p are inputs, h are target outputs)}\\
\ \ \ \text{learning rate}: \eta \\
\mathbf{Algorithm}:\\
\ \ \ \text{loop}\ L\ in\ 1:H \\
\ \ \ \ \ \ \ \ \ \omega^{(L)} = \omega^{(L)} - \eta {\nabla_{\omega}}\mathcal{L}^{(L)}\\
\ \ \ \text{end loop} \\
\mathbf{Output}:\\ 
\ \ \ \ \omega\ \ \ \ \ \  \text{(updated)}
\end{array}
\]</span></p>
<p>Now, let us review our example implementation of <strong>forward.pass(.)</strong>, <strong>backward.pass(.)</strong>, and <strong>back.propagation(.)</strong> functions for a <strong>vanilla neural network</strong>. As for the presence of the <strong>batchnorm</strong> functions and <strong>drop.out(.)</strong>, a detailed discussion is up ahead in later section.</p>


<p>For our <strong>Forward Feed</strong>, we use <strong>forward.pass(.)</strong> function implemented like so:</p>

<div class="sourceCode" id="cb1815"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1815-1" data-line-number="1">forward.pass &lt;-<span class="st"> </span><span class="cf">function</span>(X, layers, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>, <span class="dt">mode=</span><span class="st">&quot;train&quot;</span>) {</a>
<a class="sourceLine" id="cb1815-2" data-line-number="2">  afunc                  =<span class="st"> </span><span class="kw">get</span>(afunc)</a>
<a class="sourceLine" id="cb1815-3" data-line-number="3">  H                      =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb1815-4" data-line-number="4">  act.output             =<span class="st"> </span>X</a>
<a class="sourceLine" id="cb1815-5" data-line-number="5">  <span class="cf">for</span> (L <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>H) {</a>
<a class="sourceLine" id="cb1815-6" data-line-number="6">    <span class="co"># add constant (bias)</span></a>
<a class="sourceLine" id="cb1815-7" data-line-number="7">    layer =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb1815-8" data-line-number="8">    net.input            =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(X)), act.output)  </a>
<a class="sourceLine" id="cb1815-9" data-line-number="9">    net.input            =<span class="st"> </span>net.input <span class="op">%*%</span><span class="st"> </span>layer<span class="op">$</span>omega<span class="op">$</span>weight </a>
<a class="sourceLine" id="cb1815-10" data-line-number="10">    <span class="cf">if</span> (L <span class="op">==</span><span class="st"> </span>H) {          <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb1815-11" data-line-number="11">        act.output       =<span class="st"> </span><span class="kw">activation</span>(net.input, afunc)</a>
<a class="sourceLine" id="cb1815-12" data-line-number="12">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1815-13" data-line-number="13">        act.output =<span class="st"> </span>net.input</a>
<a class="sourceLine" id="cb1815-14" data-line-number="14">        <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(layer<span class="op">$</span>drop)) {</a>
<a class="sourceLine" id="cb1815-15" data-line-number="15">            act.output =<span class="st"> </span><span class="kw">drop.out</span>(act.output, <span class="dt">prob=</span>layer<span class="op">$</span>drop)</a>
<a class="sourceLine" id="cb1815-16" data-line-number="16">        }</a>
<a class="sourceLine" id="cb1815-17" data-line-number="17">        <span class="cf">if</span> (layer<span class="op">$</span>batchnorm <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1815-18" data-line-number="18">           <span class="cf">if</span> (mode <span class="op">==</span><span class="st"> &quot;train&quot;</span>) {</a>
<a class="sourceLine" id="cb1815-19" data-line-number="19">             normalized       =<span class="st"> </span><span class="kw">batchnorm.forward</span>(act.output,  layer)</a>
<a class="sourceLine" id="cb1815-20" data-line-number="20">             act.output       =<span class="st"> </span>normalized<span class="op">$</span>act.output</a>
<a class="sourceLine" id="cb1815-21" data-line-number="21">             layer<span class="op">$</span>moments    =<span class="st"> </span>normalized<span class="op">$</span>moments</a>
<a class="sourceLine" id="cb1815-22" data-line-number="22">           } <span class="cf">else</span> { <span class="co"># if test</span></a>
<a class="sourceLine" id="cb1815-23" data-line-number="23">             normalized       =<span class="st"> </span><span class="kw">batchnorm.prediction</span>(act.output,  layer)</a>
<a class="sourceLine" id="cb1815-24" data-line-number="24">             act.output       =<span class="st"> </span>normalized<span class="op">$</span>prediction</a>
<a class="sourceLine" id="cb1815-25" data-line-number="25">           }</a>
<a class="sourceLine" id="cb1815-26" data-line-number="26">        }</a>
<a class="sourceLine" id="cb1815-27" data-line-number="27">        act.output       =<span class="st"> </span><span class="kw">activation</span>(net.input, leaky.relu)</a>
<a class="sourceLine" id="cb1815-28" data-line-number="28">    } </a>
<a class="sourceLine" id="cb1815-29" data-line-number="29">    layer<span class="op">$</span>output =<span class="st"> </span>act.output</a>
<a class="sourceLine" id="cb1815-30" data-line-number="30">    layers[[L]]  =<span class="st"> </span>layer</a>
<a class="sourceLine" id="cb1815-31" data-line-number="31">  }</a>
<a class="sourceLine" id="cb1815-32" data-line-number="32">  <span class="kw">list</span>(<span class="st">&quot;layers&quot;</span> =<span class="st"> </span>layers)</a>
<a class="sourceLine" id="cb1815-33" data-line-number="33">}</a></code></pre></div>

<p>For our <strong>Back Propagation</strong>, we use <strong>back.propagation(.)</strong> function implemented like so:</p>

<div class="sourceCode" id="cb1816"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1816-1" data-line-number="1">back.propagation &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, model, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>) {</a>
<a class="sourceLine" id="cb1816-2" data-line-number="2">    afunc.loss                  =<span class="st"> </span><span class="kw">get</span>(<span class="kw">paste0</span>(<span class="st">&quot;gradient.&quot;</span>, afunc, <span class="st">&quot;.loss&quot;</span>))</a>
<a class="sourceLine" id="cb1816-3" data-line-number="3">    actfunc                     =<span class="st"> </span><span class="kw">get</span>(<span class="kw">paste0</span>(<span class="st">&quot;gradient.&quot;</span>, afunc))</a>
<a class="sourceLine" id="cb1816-4" data-line-number="4">    H                           =<span class="st"> </span><span class="kw">length</span>(model<span class="op">$</span>layers)  </a>
<a class="sourceLine" id="cb1816-5" data-line-number="5">    delta                       =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1816-6" data-line-number="6">    delta.params                =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1816-7" data-line-number="7">    layers                      =<span class="st"> </span>model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb1816-8" data-line-number="8">    <span class="cf">for</span> (L <span class="cf">in</span> H<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1816-9" data-line-number="9">        layer =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb1816-10" data-line-number="10">        delta.params[[L]] =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;omega&quot;</span>=<span class="st"> </span><span class="ot">NULL</span>, <span class="st">&quot;gamma&quot;</span>=<span class="st"> </span><span class="ot">NULL</span>, <span class="st">&quot;beta&quot;</span>=<span class="st"> </span><span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb1816-11" data-line-number="11">        act.output             =<span class="st"> </span>layer<span class="op">$</span>output</a>
<a class="sourceLine" id="cb1816-12" data-line-number="12">        <span class="cf">if</span> (L <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) { net.input =<span class="st"> </span>layers[[L<span class="dv">-1</span>]]<span class="op">$</span>output } <span class="cf">else</span> { net.input=X }</a>
<a class="sourceLine" id="cb1816-13" data-line-number="13">        <span class="co"># include constant for bias</span></a>
<a class="sourceLine" id="cb1816-14" data-line-number="14">        net.input               =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(X)), net.input)</a>
<a class="sourceLine" id="cb1816-15" data-line-number="15">        <span class="cf">if</span> (L <span class="op">==</span><span class="st"> </span>H) { <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb1816-16" data-line-number="16">            <span class="cf">if</span> (afunc <span class="op">==</span><span class="st"> &quot;sigmoid&quot;</span> <span class="op">||</span><span class="st"> </span>afunc <span class="op">==</span><span class="st"> &quot;softmax&quot;</span>) {  </a>
<a class="sourceLine" id="cb1816-17" data-line-number="17">               <span class="co"># the derivation of delta cancels out the activation gradient, </span></a>
<a class="sourceLine" id="cb1816-18" data-line-number="18">               <span class="co"># e.g. simplified version: delta = o - t</span></a>
<a class="sourceLine" id="cb1816-19" data-line-number="19">               delta[[L]]       =<span class="st"> </span><span class="kw">gradient.loss</span>(Y, act.output, </a>
<a class="sourceLine" id="cb1816-20" data-line-number="20">                                                gradient.relu.loss)</a>
<a class="sourceLine" id="cb1816-21" data-line-number="21">            } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1816-22" data-line-number="22">               gradient.loss    =<span class="st"> </span><span class="kw">gradient.loss</span>(Y, act.output, afunc.loss)</a>
<a class="sourceLine" id="cb1816-23" data-line-number="23">               gradient.output  =<span class="st"> </span><span class="kw">gradient.activation</span>(act.output, actfunc)</a>
<a class="sourceLine" id="cb1816-24" data-line-number="24">               delta[[L]]       =<span class="st"> </span>gradient.loss  <span class="op">*</span><span class="st"> </span>gradient.output  </a>
<a class="sourceLine" id="cb1816-25" data-line-number="25">            }</a>
<a class="sourceLine" id="cb1816-26" data-line-number="26">        } <span class="cf">else</span> {    </a>
<a class="sourceLine" id="cb1816-27" data-line-number="27">            net.weights         =<span class="st"> </span>layers[[L<span class="op">+</span><span class="dv">1</span>]]<span class="op">$</span>omega<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb1816-28" data-line-number="28">            gradient.loss       =<span class="st"> </span>delta[[L<span class="op">+</span><span class="dv">1</span>]] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(net.weights) </a>
<a class="sourceLine" id="cb1816-29" data-line-number="29">            gradient.loss       =<span class="st"> </span>gradient.loss[,<span class="op">-</span><span class="dv">1</span>] <span class="co"># exclude bias</span></a>
<a class="sourceLine" id="cb1816-30" data-line-number="30">            gradient.output     =<span class="st"> </span><span class="kw">gradient.activation</span>(act.output, </a>
<a class="sourceLine" id="cb1816-31" data-line-number="31">                                                      gradient.leaky.relu) </a>
<a class="sourceLine" id="cb1816-32" data-line-number="32">            bnorm               =<span class="st"> </span><span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb1816-33" data-line-number="33">            <span class="cf">if</span> (layers[[L]]<span class="op">$</span>batchnorm <span class="op">==</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1816-34" data-line-number="34">                normalized            =<span class="st"> </span><span class="kw">batchnorm.backward</span>( gradient.output, </a>
<a class="sourceLine" id="cb1816-35" data-line-number="35">                                          layer<span class="op">$</span>batch.gamma<span class="op">$</span>weight, </a>
<a class="sourceLine" id="cb1816-36" data-line-number="36">                                          layer<span class="op">$</span>moments)</a>
<a class="sourceLine" id="cb1816-37" data-line-number="37">                gradient.output         =<span class="st"> </span>normalized<span class="op">$</span>gradient.output</a>
<a class="sourceLine" id="cb1816-38" data-line-number="38">                delta.params[[L]]<span class="op">$</span>gamma =<span class="st"> </span>normalized<span class="op">$</span>delta.gamma</a>
<a class="sourceLine" id="cb1816-39" data-line-number="39">                delta.params[[L]]<span class="op">$</span>beta  =<span class="st"> </span>normalized<span class="op">$</span>delta.beta</a>
<a class="sourceLine" id="cb1816-40" data-line-number="40">            } </a>
<a class="sourceLine" id="cb1816-41" data-line-number="41"></a>
<a class="sourceLine" id="cb1816-42" data-line-number="42">            delta[[L]]           =<span class="st"> </span>gradient.loss  <span class="op">*</span><span class="st"> </span>gradient.output </a>
<a class="sourceLine" id="cb1816-43" data-line-number="43">        }</a>
<a class="sourceLine" id="cb1816-44" data-line-number="44">        delta.params[[L]]<span class="op">$</span>omega  =<span class="st"> </span><span class="kw">clipping</span>(<span class="kw">t</span>(net.input) <span class="op">%*%</span><span class="st"> </span>delta[[L]])</a>
<a class="sourceLine" id="cb1816-45" data-line-number="45">    }</a>
<a class="sourceLine" id="cb1816-46" data-line-number="46">    <span class="kw">list</span>( <span class="st">&quot;delta.output&quot;</span>  =<span class="st"> </span>delta, <span class="st">&quot;delta.params&quot;</span> =<span class="st"> </span>delta.params)</a>
<a class="sourceLine" id="cb1816-47" data-line-number="47">}</a></code></pre></div>

<p>For our <strong>updates</strong> of parameters as we propagate our gradients backward, we use <strong>backward.pass(.)</strong> function implemented like so:</p>

<div class="sourceCode" id="cb1817"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1817-1" data-line-number="1">backward.pass &lt;-<span class="st"> </span><span class="cf">function</span>(model, delta.params, eta, t, optimize) {</a>
<a class="sourceLine" id="cb1817-2" data-line-number="2">  eps =<span class="st"> </span><span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb1817-3" data-line-number="3">  layers =<span class="st"> </span>model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb1817-4" data-line-number="4">  H      =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb1817-5" data-line-number="5">  <span class="cf">for</span> (L <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>H) {</a>
<a class="sourceLine" id="cb1817-6" data-line-number="6">    layer =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb1817-7" data-line-number="7">    <span class="cf">if</span> (optimize <span class="op">==</span><span class="st"> &quot;sgd&quot;</span>) {</a>
<a class="sourceLine" id="cb1817-8" data-line-number="8">     layer<span class="op">$</span>omega<span class="op">$</span>weight =<span class="st"> </span>layer<span class="op">$</span>omega<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>delta.params[[L]]<span class="op">$</span>omega</a>
<a class="sourceLine" id="cb1817-9" data-line-number="9">    } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1817-10" data-line-number="10">    <span class="cf">if</span> (optimize <span class="op">==</span><span class="st"> &quot;adam&quot;</span>) {</a>
<a class="sourceLine" id="cb1817-11" data-line-number="11">      layer<span class="op">$</span>omega =<span class="st"> </span><span class="kw">adam</span>(layer<span class="op">$</span>omega, delta.params[[L]]<span class="op">$</span>omega, eta, t)</a>
<a class="sourceLine" id="cb1817-12" data-line-number="12">    }</a>
<a class="sourceLine" id="cb1817-13" data-line-number="13">    <span class="cf">if</span> (layer<span class="op">$</span>batchnorm <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1817-14" data-line-number="14">       <span class="cf">if</span> (optimize <span class="op">==</span><span class="st"> &quot;sgd&quot;</span>) {</a>
<a class="sourceLine" id="cb1817-15" data-line-number="15">         layer<span class="op">$</span>batch.gamma<span class="op">$</span>weight =<span class="st"> </span>layer<span class="op">$</span>batch.gamma<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1817-16" data-line-number="16"><span class="st">                                    </span>eta <span class="op">*</span><span class="st"> </span>delta.params[[L]]<span class="op">$</span>gamma</a>
<a class="sourceLine" id="cb1817-17" data-line-number="17">         layer<span class="op">$</span>batch.beta<span class="op">$</span>weight =<span class="st"> </span>layer<span class="op">$</span>batch.betya<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1817-18" data-line-number="18"><span class="st">                                    </span>eta <span class="op">*</span><span class="st"> </span>delta.params[[L]]<span class="op">$</span>beta</a>
<a class="sourceLine" id="cb1817-19" data-line-number="19">       } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1817-20" data-line-number="20">       <span class="cf">if</span> (optimize <span class="op">==</span><span class="st"> &quot;adam&quot;</span>) {</a>
<a class="sourceLine" id="cb1817-21" data-line-number="21">          layer<span class="op">$</span>batch.gamma =<span class="st"> </span><span class="kw">adam</span>(layer<span class="op">$</span>batch.gamma, </a>
<a class="sourceLine" id="cb1817-22" data-line-number="22">                                   delta.params[[L]]<span class="op">$</span>gamma, eta, t)</a>
<a class="sourceLine" id="cb1817-23" data-line-number="23">          layer<span class="op">$</span>batch.beta  =<span class="st"> </span><span class="kw">adam</span>(layer<span class="op">$</span>batch.beta, </a>
<a class="sourceLine" id="cb1817-24" data-line-number="24">                                   delta.params[[L]]<span class="op">$</span>beta, eta, t)</a>
<a class="sourceLine" id="cb1817-25" data-line-number="25">       }</a>
<a class="sourceLine" id="cb1817-26" data-line-number="26">       layer<span class="op">$</span>moving.mu       =<span class="st"> </span>layer<span class="op">$</span>moments<span class="op">$</span>moving.mu</a>
<a class="sourceLine" id="cb1817-27" data-line-number="27">       layer<span class="op">$</span>moving.variance =<span class="st"> </span>layer<span class="op">$</span>moments<span class="op">$</span>moving.variance</a>
<a class="sourceLine" id="cb1817-28" data-line-number="28">    }</a>
<a class="sourceLine" id="cb1817-29" data-line-number="29">    layers[[L]]<span class="op">$</span>omega           =<span class="st"> </span>layer<span class="op">$</span>omega</a>
<a class="sourceLine" id="cb1817-30" data-line-number="30">    layers[[L]]<span class="op">$</span>batch.gamma     =<span class="st"> </span>layer<span class="op">$</span>batch.gamma</a>
<a class="sourceLine" id="cb1817-31" data-line-number="31">    layers[[L]]<span class="op">$</span>batch.beta      =<span class="st"> </span>layer<span class="op">$</span>batch.beta</a>
<a class="sourceLine" id="cb1817-32" data-line-number="32">    layers[[L]]<span class="op">$</span>moving.mu       =<span class="st"> </span>layer<span class="op">$</span>moving.mu</a>
<a class="sourceLine" id="cb1817-33" data-line-number="33">    layers[[L]]<span class="op">$</span>moving.variance =<span class="st"> </span>layer<span class="op">$</span>moving.variance</a>
<a class="sourceLine" id="cb1817-34" data-line-number="34">  }</a>
<a class="sourceLine" id="cb1817-35" data-line-number="35">  layers</a>
<a class="sourceLine" id="cb1817-36" data-line-number="36">}</a></code></pre></div>

<p>Finally, we come down to the <strong>MLP</strong> implementation itself. Our implementation uses <strong>mini-batch SGD</strong> with <strong>sgd</strong> and <strong>adam</strong> optimization. Our dataset is split into mini batches using <strong>createFolds(.)</strong> function from <strong>caret</strong> library to generate mini batches.</p>

<div class="sourceCode" id="cb1818"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1818-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb1818-2" data-line-number="2">get.batch.dnn &lt;-<span class="st"> </span><span class="cf">function</span>(sample.data,  k, t) {  </a>
<a class="sourceLine" id="cb1818-3" data-line-number="3">   <span class="kw">set.seed</span>(t)</a>
<a class="sourceLine" id="cb1818-4" data-line-number="4">   n =<span class="st"> </span><span class="kw">nrow</span>(sample.data)</a>
<a class="sourceLine" id="cb1818-5" data-line-number="5">   shuffle =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dt">n =</span> n, <span class="dt">size=</span>n, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1818-6" data-line-number="6">   <span class="kw">createFolds</span>(shuffle, <span class="dt">k =</span> k, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1818-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb1818-8" data-line-number="8">my.MLP &lt;-<span class="st"> </span><span class="cf">function</span>(Xset, Yset, layers,  <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>, </a>
<a class="sourceLine" id="cb1818-9" data-line-number="9">                   <span class="dt">console=</span><span class="ot">FALSE</span>, <span class="dt">optimize=</span><span class="st">&quot;sgd&quot;</span>, <span class="dt">minibatch =</span> <span class="dv">30</span>, </a>
<a class="sourceLine" id="cb1818-10" data-line-number="10">                   <span class="dt">eta =</span> <span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">100</span>, <span class="dt">tol=</span><span class="fl">1e-10</span> ) {</a>
<a class="sourceLine" id="cb1818-11" data-line-number="11">  <span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">8</span>)  <span class="co"># 8 digits precision for our example</span></a>
<a class="sourceLine" id="cb1818-12" data-line-number="12">  eta =<span class="st"> </span><span class="kw">c</span>(eta)</a>
<a class="sourceLine" id="cb1818-13" data-line-number="13">  total.loss =<span class="st"> </span>old.delta.params =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1818-14" data-line-number="14">  old.loss =<span class="st"> </span><span class="ot">Inf</span></a>
<a class="sourceLine" id="cb1818-15" data-line-number="15">  k   =<span class="st"> </span><span class="kw">ceiling</span>(<span class="kw">nrow</span>(Xset) <span class="op">/</span><span class="st"> </span>minibatch)</a>
<a class="sourceLine" id="cb1818-16" data-line-number="16">  <span class="cf">if</span> (console<span class="op">==</span><span class="ot">TRUE</span>) { <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Batch Count:&quot;</span>, k)) }</a>
<a class="sourceLine" id="cb1818-17" data-line-number="17">  n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1818-18" data-line-number="18">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) { <span class="co"># one dataset pass per epoch</span></a>
<a class="sourceLine" id="cb1818-19" data-line-number="19">    n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1818-20" data-line-number="20">    batch.loss =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1818-21" data-line-number="21">    <span class="cf">for</span> (batch <span class="cf">in</span> <span class="kw">get.batch.dnn</span>(Xset, k, t)) {</a>
<a class="sourceLine" id="cb1818-22" data-line-number="22">       X        =<span class="st"> </span>Xset[batch,]</a>
<a class="sourceLine" id="cb1818-23" data-line-number="23">       Y        =<span class="st"> </span>Yset[batch,]</a>
<a class="sourceLine" id="cb1818-24" data-line-number="24">       model    =<span class="st"> </span><span class="kw">forward.pass</span>(X, layers, afunc)</a>
<a class="sourceLine" id="cb1818-25" data-line-number="25">       backprop =<span class="st"> </span><span class="kw">back.propagation</span>(X, Y, model,  afunc )</a>
<a class="sourceLine" id="cb1818-26" data-line-number="26">       layers   =<span class="st"> </span><span class="kw">backward.pass</span>(model,  backprop<span class="op">$</span>delta.params, eta, </a>
<a class="sourceLine" id="cb1818-27" data-line-number="27">                                (t <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>k <span class="op">+</span><span class="st"> </span>n,  optimize)</a>
<a class="sourceLine" id="cb1818-28" data-line-number="28">       loss     =<span class="st"> </span><span class="kw">get.loss</span>(Y, layers, afunc)</a>
<a class="sourceLine" id="cb1818-29" data-line-number="29">       batch.loss =<span class="st"> </span><span class="kw">c</span>(batch.loss, loss)</a>
<a class="sourceLine" id="cb1818-30" data-line-number="30">    }</a>
<a class="sourceLine" id="cb1818-31" data-line-number="31">    total.loss =<span class="st"> </span><span class="kw">c</span>(total.loss, <span class="kw">mean</span>(batch.loss))</a>
<a class="sourceLine" id="cb1818-32" data-line-number="32">    <span class="cf">if</span> (<span class="kw">is.na</span>(loss)) { <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;NaN loss encountered at &quot;</span>, t)); <span class="cf">break</span> }</a>
<a class="sourceLine" id="cb1818-33" data-line-number="33">    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.finite</span>(loss)) {</a>
<a class="sourceLine" id="cb1818-34" data-line-number="34">         <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Infinite Loss:&quot;</span>, loss)); <span class="cf">break</span>}</a>
<a class="sourceLine" id="cb1818-35" data-line-number="35">    <span class="cf">if</span> (<span class="kw">abs</span>(old.loss <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(batch.loss)) <span class="op">&lt;=</span><span class="st"> </span>tol) {</a>
<a class="sourceLine" id="cb1818-36" data-line-number="36">         <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Tolerance level reached:&quot;</span>, loss, <span class="st">&quot; at &quot;</span>, t)); <span class="cf">break</span> }</a>
<a class="sourceLine" id="cb1818-37" data-line-number="37">    old.loss         =<span class="st">  </span><span class="kw">mean</span>(batch.loss)</a>
<a class="sourceLine" id="cb1818-38" data-line-number="38">    old.delta.params =<span class="st"> </span>backprop<span class="op">$</span>delta.params</a>
<a class="sourceLine" id="cb1818-39" data-line-number="39">    <span class="cf">if</span> (n <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span> <span class="op">&amp;&amp;</span><span class="st"> </span>console<span class="op">==</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1818-40" data-line-number="40">      <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;epoch:&quot;</span>, t, <span class="st">&quot; loss:&quot;</span>, <span class="kw">mean</span>(batch.loss)))</a>
<a class="sourceLine" id="cb1818-41" data-line-number="41">      <span class="kw">flush.console</span>()</a>
<a class="sourceLine" id="cb1818-42" data-line-number="42">    }</a>
<a class="sourceLine" id="cb1818-43" data-line-number="43">  }</a>
<a class="sourceLine" id="cb1818-44" data-line-number="44">  L =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb1818-45" data-line-number="45">  model =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;layers&quot;</span>       =<span class="st"> </span>model<span class="op">$</span>layers,     <span class="st">&quot;cost&quot;</span>  =<span class="st"> </span>total.loss, </a>
<a class="sourceLine" id="cb1818-46" data-line-number="46">               <span class="st">&quot;delta.params&quot;</span> =<span class="st"> </span>old.delta.params, <span class="st">&quot;afunc&quot;</span> =<span class="st"> </span>afunc,</a>
<a class="sourceLine" id="cb1818-47" data-line-number="47">               <span class="st">&quot;last.iteration&quot;</span>  =<span class="st"> </span>t)</a>
<a class="sourceLine" id="cb1818-48" data-line-number="48">}</a></code></pre></div>

<p>Applying the implementation, we start with a simple dataset. Our dataset assumes having four samples with two features (X) and two targets (Y).</p>

<div class="sourceCode" id="cb1819"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1819-1" data-line-number="1">X =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>(<span class="fl">0.12</span>, <span class="fl">0.18</span>, <span class="fl">0.13</span>, <span class="fl">0.21</span>, <span class="fl">0.15</span>, <span class="fl">0.30</span>, <span class="fl">0.18</span>, <span class="fl">0.40</span>), </a>
<a class="sourceLine" id="cb1819-2" data-line-number="2">            <span class="dt">nrow=</span><span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1819-3" data-line-number="3">Y =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="fl">0.02</span>, <span class="fl">0.98</span>, <span class="fl">0.03</span>, <span class="fl">0.97</span>, <span class="fl">0.07</span>, <span class="fl">0.92</span>), </a>
<a class="sourceLine" id="cb1819-4" data-line-number="4">            <span class="dt">nrow=</span><span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a></code></pre></div>

<p>To be able to feed our data to <strong>MLP</strong>, we also construct a simple parameter structure using a learnable set of coefficients.</p>

<div class="sourceCode" id="cb1820"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1820-1" data-line-number="1">omega =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1820-2" data-line-number="2">omega[[<span class="dv">1</span>]]=<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.40</span>, <span class="fl">0.21</span>, <span class="fl">0.34</span>, <span class="fl">0.19</span>, <span class="fl">0.67</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1820-3" data-line-number="3">omega[[<span class="dv">2</span>]]=<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.18</span>, <span class="fl">0.27</span>, <span class="fl">0.09</span>, <span class="fl">0.06</span>, <span class="fl">0.30</span>, <span class="fl">0.15</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1820-4" data-line-number="4">omega[[<span class="dv">3</span>]]=<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.05</span>, <span class="fl">0.03</span>, <span class="fl">0.35</span>, <span class="fl">0.04</span>, <span class="fl">0.40</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<div class="sourceCode" id="cb1821"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1821-1" data-line-number="1">structure =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1821-2" data-line-number="2">di =<span class="st"> </span><span class="kw">dim</span>(omega[[<span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb1821-3" data-line-number="3"><span class="cf">for</span> (L <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>) { </a>
<a class="sourceLine" id="cb1821-4" data-line-number="4">        params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span>omega[[L]], <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, di), </a>
<a class="sourceLine" id="cb1821-5" data-line-number="5">                      <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, di) )</a>
<a class="sourceLine" id="cb1821-6" data-line-number="6">        structure[[L]] =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;size&quot;</span>  =<span class="st"> </span><span class="dv">2</span>, <span class="st">&quot;omega&quot;</span> =<span class="st"> </span>params, </a>
<a class="sourceLine" id="cb1821-7" data-line-number="7">                              <span class="st">&quot;batchnorm&quot;</span> =<span class="st"> </span><span class="ot">FALSE</span>  )</a>
<a class="sourceLine" id="cb1821-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb1821-9" data-line-number="9"><span class="kw">str</span>(structure)</a></code></pre></div>
<pre><code>## List of 3
##  $ :List of 3
##   ..$ size     : num 2
##   ..$ omega    :List of 3
##   .. ..$ weight: num [1:3, 1:2] 0.05 0.21 0.19 0.4 0.34 0.67
##   .. ..$ rho   : num [1:3, 1:2] 0 0 0 0 0 0
##   .. ..$ nu    : num [1:3, 1:2] 0 0 0 0 0 0
##   ..$ batchnorm: logi FALSE
##  $ :List of 3
##   ..$ size     : num 2
##   ..$ omega    :List of 3
##   .. ..$ weight: num [1:3, 1:2] 0.18 0.09 0.3 0.27 0.06 0.15
##   .. ..$ rho   : num [1:3, 1:2] 0 0 0 0 0 0
##   .. ..$ nu    : num [1:3, 1:2] 0 0 0 0 0 0
##   ..$ batchnorm: logi FALSE
##  $ :List of 3
##   ..$ size     : num 2
##   ..$ omega    :List of 3
##   .. ..$ weight: num [1:3, 1:2] 0.25 0.03 0.04 0.05 0.35 0.4
##   .. ..$ rho   : num [1:3, 1:2] 0 0 0 0 0 0
##   .. ..$ nu    : num [1:3, 1:2] 0 0 0 0 0 0
##   ..$ batchnorm: logi FALSE</code></pre>

<p>Let us now model an <strong>MLP</strong> using the data points and parameters above:</p>

<div class="sourceCode" id="cb1823"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1823-1" data-line-number="1">mlp.model.sigmoid =<span class="st"> </span><span class="kw">my.MLP</span>(X, Y, structure, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>, </a>
<a class="sourceLine" id="cb1823-2" data-line-number="2">                           <span class="dt">eta=</span><span class="fl">0.1</span>, <span class="dt">epoch=</span><span class="dv">100</span>, <span class="dt">console=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] &quot;Batch Count:1&quot;
## [1] &quot;epoch:10 loss:0.192948734912118&quot;
## [1] &quot;epoch:20 loss:0.181075498552071&quot;
## [1] &quot;epoch:30 loss:0.180865970971357&quot;
## [1] &quot;epoch:40 loss:0.180825075208516&quot;
## [1] &quot;epoch:50 loss:0.180799577608614&quot;
## [1] &quot;epoch:60 loss:0.180776485995595&quot;
## [1] &quot;epoch:70 loss:0.180754172643832&quot;
## [1] &quot;epoch:80 loss:0.180732413713138&quot;
## [1] &quot;epoch:90 loss:0.180711164234054&quot;
## [1] &quot;epoch:100 loss:0.180690402798621&quot;</code></pre>

<p>The final optimized parameters are obtained below after 100 <strong>epoch</strong>:</p>

<div class="sourceCode" id="cb1825"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1825-1" data-line-number="1">mlp.model =<span class="st"> </span>mlp.model.sigmoid</a>
<a class="sourceLine" id="cb1825-2" data-line-number="2">mlp.model<span class="op">$</span>layers[[<span class="dv">1</span>]]<span class="op">$</span>omega<span class="op">$</span>weight</a></code></pre></div>
<pre><code>##            [,1]       [,2]
## [1,] 0.18386740 0.87451275
## [2,] 0.22229200 0.38155529
## [3,] 0.20015635 0.69854820</code></pre>
<div class="sourceCode" id="cb1827"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1827-1" data-line-number="1">mlp.model<span class="op">$</span>layers[[<span class="dv">2</span>]]<span class="op">$</span>omega<span class="op">$</span>weight</a></code></pre></div>
<pre><code>##            [,1]       [,2]
## [1,] 0.74517894 0.85117204
## [2,] 0.17553396 0.14730975
## [3,] 0.69936942 0.55839558</code></pre>
<div class="sourceCode" id="cb1829"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1829-1" data-line-number="1">mlp.model<span class="op">$</span>layers[[<span class="dv">3</span>]]<span class="op">$</span>omega<span class="op">$</span>weight</a></code></pre></div>
<pre><code>##             [,1]       [,2]
## [1,] -0.84064501 0.69507475
## [2,] -0.74243477 0.73518925
## [3,] -0.72260868 0.78560822</code></pre>

<p>Compare that to the original values:</p>

<div class="sourceCode" id="cb1831"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1831-1" data-line-number="1">omega</a></code></pre></div>
<pre><code>## [[1]]
##      [,1] [,2]
## [1,] 0.05 0.40
## [2,] 0.21 0.34
## [3,] 0.19 0.67
## 
## [[2]]
##      [,1] [,2]
## [1,] 0.18 0.27
## [2,] 0.09 0.06
## [3,] 0.30 0.15
## 
## [[3]]
##      [,1] [,2]
## [1,] 0.25 0.05
## [2,] 0.03 0.35
## [3,] 0.04 0.40</code></pre>

<p>Note that the <strong>COST</strong> decreases as the parameters are optimized for each iteration.</p>

<div class="sourceCode" id="cb1833"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1833-1" data-line-number="1"><span class="kw">head</span>(mlp.model<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">15</span>)</a></code></pre></div>
<pre><code>##  [1] 0.69242946 0.58055032 0.48232118 0.39731471 0.32746102 0.27462355
##  [7] 0.23823521 0.21510574 0.20115424 0.19294873 0.18815312 0.18533726
## [13] 0.18366618 0.18266038 0.18204485</code></pre>

<p>Let us plot the <strong>COST</strong> (see Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:mlpplot">12.15</a>).</p>

<div class="sourceCode" id="cb1835"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1835-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.sigmoid<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1835-2" data-line-number="2">y =<span class="st"> </span>mlp.model.sigmoid<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1835-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1835-4" data-line-number="4">      <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;MLP Plot&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1835-5" data-line-number="5"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1835-6" data-line-number="6">color =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb1835-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col=</span>color[<span class="dv">1</span>], <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpplot"></span>
<img src="DS_files/figure-html/mlpplot-1.png" alt="MLP Plot" width="70%" />
<p class="caption">
Figure 12.15: MLP Plot
</p>
</div>

<p>In terms of prediction, we implement <strong>prediction(.)</strong> function to use the <strong>forward.pass(.)</strong> function like so:</p>

<div class="sourceCode" id="cb1836"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1836-1" data-line-number="1">my.predict &lt;-<span class="st"> </span><span class="cf">function</span>(X, model) {</a>
<a class="sourceLine" id="cb1836-2" data-line-number="2">  <span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">8</span>)  <span class="co"># 8 digits precision for our example</span></a>
<a class="sourceLine" id="cb1836-3" data-line-number="3">   response =<span class="st"> </span><span class="kw">forward.pass</span>(X, model<span class="op">$</span>layers, <span class="dt">afunc=</span>model<span class="op">$</span>afunc, <span class="dt">mode=</span><span class="st">&quot;test&quot;</span>)</a>
<a class="sourceLine" id="cb1836-4" data-line-number="4">   layers =<span class="st"> </span>response<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb1836-5" data-line-number="5">   L =<span class="st"> </span><span class="kw">length</span>(layers) </a>
<a class="sourceLine" id="cb1836-6" data-line-number="6">   predicted =<span class="st"> </span>layers[[L]]<span class="op">$</span>output</a>
<a class="sourceLine" id="cb1836-7" data-line-number="7">   <span class="kw">list</span>(<span class="st">&quot;prediction&quot;</span> =<span class="st"> </span>predicted)</a>
<a class="sourceLine" id="cb1836-8" data-line-number="8">}</a></code></pre></div>

<p>Now, for prediction, let us use a new set of data. Here, our <strong>MLP</strong> implementation requires our samples to be in matrix form:</p>

<div class="sourceCode" id="cb1837"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1837-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2019</span>)</a>
<a class="sourceLine" id="cb1837-2" data-line-number="2">new.X =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>), <span class="dt">nrow=</span><span class="dv">5</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1837-3" data-line-number="3"><span class="kw">my.predict</span>(new.X, mlp.model.sigmoid)</a></code></pre></div>
<pre><code>## $prediction
##             [,1]       [,2]
## [1,] 0.024982419 0.97394373
## [2,] 0.031984152 0.96638168
## [3,] 0.051672541 0.94489747
## [4,] 0.039240281 0.95849620
## [5,] 0.034823196 0.96330124</code></pre>

<p>Comparing the prediction with the target from the training set (to get an idea), we see a close match, though we may tend towards overfitting the closer we get.</p>

<div class="sourceCode" id="cb1839"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1839-1" data-line-number="1">Y</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,] 0.05 0.95
## [2,] 0.02 0.98
## [3,] 0.03 0.97
## [4,] 0.07 0.92</code></pre>

</div>
<div id="deep-neural-network-dnn" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.7</span> Deep Neural Network (DNN)  <a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, we have shown a simple <strong>neural network</strong>. In this section, let us improve the use of a network structure using our own <strong>deep.neural.layers(.)</strong> function. Note here that we are merely creating a structure of our network using a sequential list of a randomly generated matrix of weights (with default seed equals 2019).</p>

<div class="sourceCode" id="cb1841"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1841-1" data-line-number="1">deep.neural.layers  &lt;-<span class="st"> </span><span class="cf">function</span>(X, ...) {</a>
<a class="sourceLine" id="cb1841-2" data-line-number="2">    <span class="kw">set.seed</span>(<span class="dv">2019</span>)</a>
<a class="sourceLine" id="cb1841-3" data-line-number="3">    K          =<span class="st"> </span><span class="kw">ncol</span>(X)</a>
<a class="sourceLine" id="cb1841-4" data-line-number="4">    layers      =<span class="st"> </span><span class="kw">list</span>(...)</a>
<a class="sourceLine" id="cb1841-5" data-line-number="5">    parameters =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1841-6" data-line-number="6">    structure   =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1841-7" data-line-number="7">    l =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1841-8" data-line-number="8">    <span class="cf">for</span> (layer <span class="cf">in</span> layers) {</a>
<a class="sourceLine" id="cb1841-9" data-line-number="9">        l =<span class="st"> </span>l <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1841-10" data-line-number="10">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>batchnorm))  { layer<span class="op">$</span>batchnorm =<span class="st"> </span><span class="ot">FALSE</span> } </a>
<a class="sourceLine" id="cb1841-11" data-line-number="11">                                  <span class="cf">else</span> { layer<span class="op">$</span>batchnorm =<span class="st"> </span><span class="ot">TRUE</span>  }</a>
<a class="sourceLine" id="cb1841-12" data-line-number="12">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>drop)) { layer<span class="op">$</span>drop =<span class="st"> </span><span class="ot">NULL</span> }</a>
<a class="sourceLine" id="cb1841-13" data-line-number="13">        K =<span class="st"> </span>K <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="co"># include weights for the bias</span></a>
<a class="sourceLine" id="cb1841-14" data-line-number="14">        N =<span class="st"> </span>layer<span class="op">$</span>size</a>
<a class="sourceLine" id="cb1841-15" data-line-number="15">        O =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(K <span class="op">*</span><span class="st"> </span>N, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>), <span class="dt">nrow=</span>K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1841-16" data-line-number="16">        omega =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span>O,  <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">dim</span>(O)), </a>
<a class="sourceLine" id="cb1841-17" data-line-number="17">                     <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">dim</span>(O)) )</a>
<a class="sourceLine" id="cb1841-18" data-line-number="18">        batch.beta  =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N), <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N), </a>
<a class="sourceLine" id="cb1841-19" data-line-number="19">                           <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N) )</a>
<a class="sourceLine" id="cb1841-20" data-line-number="20">        batch.gamma =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, N), <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N), </a>
<a class="sourceLine" id="cb1841-21" data-line-number="21">                           <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N) )</a>
<a class="sourceLine" id="cb1841-22" data-line-number="22">        structure[[l]] =</a>
<a class="sourceLine" id="cb1841-23" data-line-number="23"><span class="st">           </span><span class="kw">list</span>(<span class="st">&quot;size&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>size, <span class="st">&quot;batchnorm&quot;</span> =<span class="st"> </span>layer<span class="op">$</span>batchnorm, </a>
<a class="sourceLine" id="cb1841-24" data-line-number="24">                <span class="st">&quot;drop&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>drop,     <span class="st">&quot;omega&quot;</span> =<span class="st"> </span>omega, </a>
<a class="sourceLine" id="cb1841-25" data-line-number="25">                <span class="st">&quot;batch.gamma&quot;</span> =<span class="st"> </span>batch.gamma,  <span class="st">&quot;batch.beta&quot;</span> =<span class="st"> </span>batch.beta,</a>
<a class="sourceLine" id="cb1841-26" data-line-number="26">                <span class="st">&quot;moving.variance&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, N), <span class="st">&quot;moving.mu&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb1841-27" data-line-number="27">                )</a>
<a class="sourceLine" id="cb1841-28" data-line-number="28">        parameters =<span class="st"> </span>parameters <span class="op">+</span><span class="st"> </span>K <span class="op">*</span><span class="st"> </span>N</a>
<a class="sourceLine" id="cb1841-29" data-line-number="29">        K =<span class="st"> </span>N</a>
<a class="sourceLine" id="cb1841-30" data-line-number="30">    }</a>
<a class="sourceLine" id="cb1841-31" data-line-number="31">    <span class="kw">list</span>(<span class="st">&quot;layers&quot;</span> =<span class="st"> </span>structure, <span class="st">&quot;parameters&quot;</span> =<span class="st"> </span>parameters)</a>
<a class="sourceLine" id="cb1841-32" data-line-number="32">}</a></code></pre></div>

<p>To use the function, consider a simple <strong>classification problem</strong>. Our recent implementation mainly covers the neural network with the <strong>sigmoid</strong> activation function for the output layer. Knowing that a <strong>sigmoid</strong> function squashes the output into a range between 0 and 1 can serve as a probability function for <strong>binomial classification</strong>. However, to generalize, we can use the <strong>softmax</strong> function for a <strong>multinomial classification</strong>.</p>
<p>In this section, let us demonstrate <strong>softmax</strong> for <strong>multinomial classification</strong> using a simple deep neural network. Our application requires us to convert our dataset labels to a set of one-hot encoded vectors. To help us generate a dataset with a one-hot encoded vector for our target, let us create a function called <strong>get.synthetic.samples(.)</strong> that can generate some random batch of the dataset with noise:</p>

<div class="sourceCode" id="cb1842"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1842-1" data-line-number="1">get.synthetic.samples &lt;-<span class="st"> </span><span class="cf">function</span>(N, <span class="dt">seed=</span><span class="dv">2019</span>) {</a>
<a class="sourceLine" id="cb1842-2" data-line-number="2"> <span class="kw">set.seed</span>(seed)</a>
<a class="sourceLine" id="cb1842-3" data-line-number="3">  Xset =<span class="st"> </span>Yset =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1842-4" data-line-number="4">  Xset[[<span class="dv">1</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.10</span>); Yset[[<span class="dv">1</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>) </a>
<a class="sourceLine" id="cb1842-5" data-line-number="5">  Xset[[<span class="dv">2</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.90</span>, <span class="fl">0.90</span>); Yset[[<span class="dv">2</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>) </a>
<a class="sourceLine" id="cb1842-6" data-line-number="6">  Xset[[<span class="dv">3</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.90</span>); Yset[[<span class="dv">3</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>) </a>
<a class="sourceLine" id="cb1842-7" data-line-number="7">  Xset[[<span class="dv">4</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.90</span>, <span class="fl">0.10</span>); Yset[[<span class="dv">4</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1842-8" data-line-number="8">  samples =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dt">n =</span> <span class="dv">4</span>, <span class="dt">size=</span>N, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1842-9" data-line-number="9">  X =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>); <span class="kw">colnames</span>(X) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>)</a>
<a class="sourceLine" id="cb1842-10" data-line-number="10">  Y =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1842-11" data-line-number="11">  <span class="kw">colnames</span>(Y) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Y1&quot;</span>, <span class="st">&quot;Y2&quot;</span>, <span class="st">&quot;Y3&quot;</span>, <span class="st">&quot;Y4&quot;</span>)</a>
<a class="sourceLine" id="cb1842-12" data-line-number="12">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) { </a>
<a class="sourceLine" id="cb1842-13" data-line-number="13">      X[i,] =<span class="st"> </span>Xset[[samples[[i]]]] </a>
<a class="sourceLine" id="cb1842-14" data-line-number="14">      Y[i,]  =<span class="st"> </span>Yset[[samples[[i]]]]</a>
<a class="sourceLine" id="cb1842-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb1842-16" data-line-number="16">  <span class="co"># Add some decent amount of noise</span></a>
<a class="sourceLine" id="cb1842-17" data-line-number="17">  X =<span class="st"> </span>X <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>N, <span class="dt">min=</span><span class="fl">0.001</span>, <span class="dt">max=</span><span class="fl">0.005</span>)</a>
<a class="sourceLine" id="cb1842-18" data-line-number="18">  <span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y)</a>
<a class="sourceLine" id="cb1842-19" data-line-number="19">}</a></code></pre></div>

<p>Barring <strong>interpretability</strong> of data at the moment while emphasizing the <strong>technicality</strong>, we concoct a <strong>synthetic</strong> dataset that can support <strong>Softmax</strong>. The idea is to have the ability to simulate unique and clear input patterns that can be mapped to unique output patterns and see how our model can correctly fit.</p>
<p>Let us split our dataset into train set and test set. Notice the pattern that we explicitly forced into the dataset.</p>

<div class="sourceCode" id="cb1843"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1843-1" data-line-number="1">train =<span class="st"> </span><span class="kw">get.synthetic.samples</span>(<span class="dt">N=</span><span class="dv">200</span>)</a>
<a class="sourceLine" id="cb1843-2" data-line-number="2">test  =<span class="st"> </span><span class="kw">get.synthetic.samples</span>(<span class="dt">N=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb1843-3" data-line-number="3">X =<span class="st"> </span>train<span class="op">$</span>X</a>
<a class="sourceLine" id="cb1843-4" data-line-number="4"><span class="kw">head</span>(<span class="kw">cbind</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y), <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##               X1         X2 Y1 Y2 Y3 Y4
##  [1,] 0.90179617 0.10179617  0  0  0  1
##  [2,] 0.10355535 0.90355535  0  0  1  0
##  [3,] 0.90421814 0.90421814  0  1  0  0
##  [4,] 0.10473834 0.90473834  0  0  1  0
##  [5,] 0.10198845 0.10198845  1  0  0  0
##  [6,] 0.10436652 0.10436652  1  0  0  0
##  [7,] 0.90298293 0.10298293  0  0  0  1
##  [8,] 0.10399414 0.10399414  1  0  0  0
##  [9,] 0.10132054 0.10132054  1  0  0  0
## [10,] 0.10137307 0.90137307  0  0  1  0</code></pre>

<p>Next, let us generate our initial neural network structure. Below, we have three layers. Two of the first hidden layers have two neurons, and the last output layer has four neural outputs.</p>

<div class="sourceCode" id="cb1845"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1845-1" data-line-number="1">dnn    =<span class="st"> </span><span class="kw">deep.neural.layers</span>(X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">4</span>))</a></code></pre></div>

<p>Using our trainset and parameters above, we train our <strong>DNN</strong> model for a <strong>Multi Classification</strong>.</p>

<div class="sourceCode" id="cb1846"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1846-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers,  <span class="dt">afunc=</span><span class="st">&quot;softmax&quot;</span>,  </a>
<a class="sourceLine" id="cb1846-2" data-line-number="2">                       <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>, <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">800</span>)</a></code></pre></div>

<p>Let us compare the prediction and the target using <strong>compare.outcome(.)</strong>:</p>

<div class="sourceCode" id="cb1847"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1847-1" data-line-number="1">compare.outcome &lt;-<span class="st"> </span><span class="cf">function</span>(t, o, <span class="dt">n=</span><span class="dv">10</span>) {</a>
<a class="sourceLine" id="cb1847-2" data-line-number="2">  c =<span class="st"> </span><span class="kw">ncol</span>(t)</a>
<a class="sourceLine" id="cb1847-3" data-line-number="3">  outc =<span class="st"> </span><span class="kw">cbind</span>(t, <span class="kw">round</span>(o,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1847-4" data-line-number="4">  <span class="kw">colnames</span>(outc) =<span class="st"> </span><span class="kw">c</span>(<span class="kw">paste0</span>(<span class="st">&quot;T&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,c)), <span class="kw">paste0</span>(<span class="st">&quot;O&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,c)))</a>
<a class="sourceLine" id="cb1847-5" data-line-number="5">  <span class="kw">print</span>(<span class="kw">head</span>(outc, <span class="dt">n=</span>n))</a>
<a class="sourceLine" id="cb1847-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb1847-7" data-line-number="7">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1847-8" data-line-number="8"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 row</span></a></code></pre></div>
<pre><code>##       T1 T2 T3 T4   O1   O2   O3   O4
##  [1,]  0  0  0  1 0.00 0.00 0.00 1.00
##  [2,]  0  0  1  0 0.00 0.00 1.00 0.00
##  [3,]  0  1  0  0 0.00 1.00 0.00 0.00
##  [4,]  0  0  1  0 0.00 0.00 1.00 0.00
##  [5,]  1  0  0  0 0.97 0.01 0.01 0.01
##  [6,]  1  0  0  0 0.97 0.01 0.01 0.01
##  [7,]  0  0  0  1 0.00 0.00 0.00 1.00
##  [8,]  1  0  0  0 0.97 0.01 0.01 0.01
##  [9,]  1  0  0  0 0.97 0.01 0.01 0.01
## [10,]  0  0  1  0 0.00 0.00 1.00 0.00</code></pre>

<p>We can see that the trained model matches the target by reviewing the result. We can perform prediction and measure performance using <strong>ROC</strong> and other related metrics relevant to <strong>precision</strong> and <strong>recall</strong>.</p>
</div>
<div id="vanishing-and-exploding-gradient" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.8</span> Vanishing and Exploding Gradient  <a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, let us simulate conditions in which gradients tend to have extremely negative or positive values. Here, we edit <strong>get.synthetic.samples(.)</strong> function to generate dataset intended for <strong>DNN</strong> with <strong>RELU</strong> output.</p>

<div class="sourceCode" id="cb1849"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1849-1" data-line-number="1">get.synthetic.samples &lt;-<span class="st"> </span><span class="cf">function</span>(N, <span class="dt">seed=</span><span class="dv">2019</span>) {</a>
<a class="sourceLine" id="cb1849-2" data-line-number="2"> <span class="kw">set.seed</span>(seed)</a>
<a class="sourceLine" id="cb1849-3" data-line-number="3">  Xset =<span class="st"> </span>Yset =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1849-4" data-line-number="4">  Xset[[<span class="dv">1</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.90</span>);  Yset[[<span class="dv">1</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.90</span>, <span class="fl">0.10</span>) </a>
<a class="sourceLine" id="cb1849-5" data-line-number="5">  Xset[[<span class="dv">2</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.20</span>, <span class="fl">0.80</span>);  Yset[[<span class="dv">2</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.80</span>, <span class="fl">0.20</span>) </a>
<a class="sourceLine" id="cb1849-6" data-line-number="6">  Xset[[<span class="dv">3</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.30</span>, <span class="fl">0.70</span>);  Yset[[<span class="dv">3</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.70</span>, <span class="fl">0.30</span>) </a>
<a class="sourceLine" id="cb1849-7" data-line-number="7">  Xset[[<span class="dv">4</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.40</span>, <span class="fl">0.60</span>);  Yset[[<span class="dv">4</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.60</span>, <span class="fl">0.40</span>) </a>
<a class="sourceLine" id="cb1849-8" data-line-number="8">  Xset[[<span class="dv">5</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.50</span>);  Yset[[<span class="dv">5</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.50</span>) </a>
<a class="sourceLine" id="cb1849-9" data-line-number="9">  Xset[[<span class="dv">6</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.60</span>, <span class="fl">0.40</span>);  Yset[[<span class="dv">6</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.40</span>, <span class="fl">0.60</span>) </a>
<a class="sourceLine" id="cb1849-10" data-line-number="10">  Xset[[<span class="dv">7</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.70</span>, <span class="fl">0.30</span>);  Yset[[<span class="dv">7</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.30</span>, <span class="fl">0.70</span>) </a>
<a class="sourceLine" id="cb1849-11" data-line-number="11">  Xset[[<span class="dv">8</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.80</span>, <span class="fl">0.20</span>);  Yset[[<span class="dv">8</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.20</span>, <span class="fl">0.80</span>) </a>
<a class="sourceLine" id="cb1849-12" data-line-number="12">  Xset[[<span class="dv">9</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.90</span>, <span class="fl">0.10</span>);  Yset[[<span class="dv">9</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.90</span>) </a>
<a class="sourceLine" id="cb1849-13" data-line-number="13">  samples =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dt">n =</span> <span class="dv">9</span>, <span class="dt">size=</span>N, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1849-14" data-line-number="14">  X =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>); <span class="kw">colnames</span>(X) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>)</a>
<a class="sourceLine" id="cb1849-15" data-line-number="15">  Y =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>); <span class="kw">colnames</span>(Y) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Y1&quot;</span>, <span class="st">&quot;Y2&quot;</span>)</a>
<a class="sourceLine" id="cb1849-16" data-line-number="16">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) { </a>
<a class="sourceLine" id="cb1849-17" data-line-number="17">      X[i,] =<span class="st"> </span>Xset[[samples[[i]]]] </a>
<a class="sourceLine" id="cb1849-18" data-line-number="18">      Y[i,]  =<span class="st"> </span>Yset[[samples[[i]]]]</a>
<a class="sourceLine" id="cb1849-19" data-line-number="19">  }</a>
<a class="sourceLine" id="cb1849-20" data-line-number="20">  <span class="co"># Add some decent amount of noise</span></a>
<a class="sourceLine" id="cb1849-21" data-line-number="21">  X =<span class="st"> </span>X <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>N, <span class="dt">min=</span><span class="fl">0.01</span>, <span class="dt">max=</span><span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb1849-22" data-line-number="22">  Y =<span class="st"> </span>Y <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>N, <span class="dt">min=</span><span class="fl">0.01</span>, <span class="dt">max=</span><span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb1849-23" data-line-number="23">  <span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y)</a>
<a class="sourceLine" id="cb1849-24" data-line-number="24">}</a>
<a class="sourceLine" id="cb1849-25" data-line-number="25">train =<span class="st"> </span><span class="kw">get.synthetic.samples</span>(<span class="dt">N=</span><span class="dv">50</span>)</a>
<a class="sourceLine" id="cb1849-26" data-line-number="26">test  =<span class="st"> </span><span class="kw">get.synthetic.samples</span>(<span class="dt">N=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb1849-27" data-line-number="27">X =<span class="st"> </span>train<span class="op">$</span>X</a>
<a class="sourceLine" id="cb1849-28" data-line-number="28"><span class="kw">head</span>(<span class="kw">cbind</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y), <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##               X1         X2         Y1         Y2
##  [1,] 0.72217088 0.32217088 0.32915548 0.72915548
##  [2,] 0.71349361 0.31349361 0.34168717 0.74168717
##  [3,] 0.31142471 0.71142471 0.72626247 0.32626247
##  [4,] 0.62385755 0.42385755 0.44342895 0.64342895
##  [5,] 0.13064818 0.93064818 0.92470781 0.12470781
##  [6,] 0.12493056 0.92493056 0.93566241 0.13566241
##  [7,] 0.84983223 0.24983223 0.21899916 0.81899916
##  [8,] 0.14664758 0.94664758 0.93292818 0.13292818
##  [9,] 0.11212189 0.91212189 0.91777823 0.11777823
## [10,] 0.61198812 0.41198812 0.41205500 0.61205500</code></pre>

<p>Here, we use a <strong>DNN</strong> that is wide or shallow - meaning that we have a small number of layers with a large number of neurons. For example, below, we create three hidden layers with 100 neurons for the first two layers and a size of 2 neurons for the third layer.</p>

<div class="sourceCode" id="cb1851"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1851-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1851-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">100</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">100</span>),<span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>) )</a>
<a class="sourceLine" id="cb1851-3" data-line-number="3">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)</a></code></pre></div>

<p>The number of parameters generated for our <strong>DNN</strong> is 10602 parameters. Let us now train our network. Note that we use the whole dataset as our minibatch to force only one iterative pass and be able to capture the error in training.</p>

<div class="sourceCode" id="cb1852"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1852-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">50</span>,</a>
<a class="sourceLine" id="cb1852-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">200</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>
<pre><code>## [1] &quot;Infinite Loss:Inf&quot;</code></pre>

<p>The model results in an early stop and does not complete the iteration. We also notice that <strong>COST</strong> becomes extremely large.</p>

<div class="sourceCode" id="cb1854"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1854-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 4</code></pre>
<div class="sourceCode" id="cb1856"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1856-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>## [1] 6.5924684e+06 7.4100188e+19 1.3648587e+85           Inf</code></pre>

<p>Also, we begin to see signs of <strong>exploding gradient</strong> given the extremely large positive values. See below:</p>

<div class="sourceCode" id="cb1858"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1858-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>
<pre><code>##             [,1]          [,2]          [,3]
##    1.2259206e+72 1.1754450e+72 1.1790274e+72
## X1 6.0578526e+71 5.8084292e+71 5.8261314e+71
## X2 6.9268430e+71 6.6416402e+71 6.6618818e+71</code></pre>

<p>If we evaluate the performance of our prediction, the outcome results in <strong>NaN</strong>. Otherwise, it results in extremely large negative number (e.g. <span class="math inline">\(-\infty\)</span>).</p>

<div class="sourceCode" id="cb1860"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1860-1" data-line-number="1">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1860-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 rows</span></a></code></pre></div>
<pre><code>##               T1         T2              O1              O2
##  [1,] 0.32623900 0.72623900 -4.6659318e+205 -5.9787324e+205
##  [2,] 0.33325993 0.73325993 -4.6520722e+205 -5.9609733e+205
##  [3,] 0.74788943 0.34788943 -4.6806885e+205 -5.9976410e+205
##  [4,] 0.41684077 0.61684077 -4.6142492e+205 -5.9125084e+205
##  [5,] 0.91527475 0.11527475 -4.7837296e+205 -6.1296736e+205
##  [6,] 0.91044131 0.11044131 -4.7762117e+205 -6.1200405e+205
##  [7,] 0.23370034 0.83370034 -4.5557140e+205 -5.8375039e+205
##  [8,] 0.94793236 0.14793236 -4.7844383e+205 -6.1305817e+205
##  [9,] 0.93496225 0.13496225 -4.7431521e+205 -6.0776792e+205
## [10,] 0.42822478 0.62822478 -4.6490108e+205 -5.9570505e+205</code></pre>

<p>Similarly, let us now try to use a <strong>DNN</strong> that is thin or deep - meaning that we have increased the number of layers with a smaller number of neurons.</p>

<div class="sourceCode" id="cb1862"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1862-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1862-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1862-3" data-line-number="3">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1862-4" data-line-number="4">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1862-5" data-line-number="5">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)  </a></code></pre></div>

<p>The number of parameters generated for our <strong>DNN</strong> is 237 parameters. Let us now train our network.</p>

<div class="sourceCode" id="cb1863"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1863-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">50</span>,</a>
<a class="sourceLine" id="cb1863-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">200</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>
<pre><code>## [1] &quot;Infinite Loss:Inf&quot;</code></pre>

<p>The model results in an early stop and does not complete the iteration. We also notice that the <strong>COST</strong> becomes extremely large.</p>

<div class="sourceCode" id="cb1865"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1865-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb1867"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1867-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>## [1] 2.6917433e+06 4.5512916e+70           Inf</code></pre>

<p>Also, we begin to see signs of <strong>exploding gradient</strong> given the extremely large negative values. See below:</p>

<div class="sourceCode" id="cb1869"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1869-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>
<pre><code>##              [,1]           [,2]           [,3]
##    -1.8063676e+67 -2.1878042e+67 -6.3399875e+66
## X1 -8.9154808e+66 -1.0798094e+67 -3.1291548e+66
## X2 -1.0217233e+67 -1.2374726e+67 -3.5860435e+66</code></pre>

<p>If we evaluate the performance of our prediction, the outcome results in <strong>NaN</strong>. Otherwise, it results in an extremely large positive number (e.g. <span class="math inline">\(\infty\)</span>).</p>

<div class="sourceCode" id="cb1871"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1871-1" data-line-number="1">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1871-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 rows</span></a></code></pre></div>
<pre><code>##               T1         T2  O1  O2
##  [1,] 0.32623900 0.72623900 Inf Inf
##  [2,] 0.33325993 0.73325993 Inf Inf
##  [3,] 0.74788943 0.34788943 Inf Inf
##  [4,] 0.41684077 0.61684077 Inf Inf
##  [5,] 0.91527475 0.11527475 Inf Inf
##  [6,] 0.91044131 0.11044131 Inf Inf
##  [7,] 0.23370034 0.83370034 Inf Inf
##  [8,] 0.94793236 0.14793236 Inf Inf
##  [9,] 0.93496225 0.13496225 Inf Inf
## [10,] 0.42822478 0.62822478 Inf Inf</code></pre>

<p>This section shows that a very thin or very deep network can cause an <strong>exploding gradient</strong>. However, for <strong>vanishing gradient</strong>, it helps to use the <strong>sigmoid</strong> activation function to simulate the phenomenon. We know that a sigmoid output is squashed between 0 and 1. Asymptotically, the output may never reach zero or one. It is this asymptotic nature of <strong>sigmoid</strong> that the more layers or more neurons we use, the product of such fractions gets smaller to the point that the effect on learning slows down.</p>
</div>
<div id="dead-relu" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.9</span> Dead Relu <a href="12.3-multi-layer-perceptron-mlp.html#dead-relu" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We know that a <strong>RELU</strong> activation function drops to a flat zero towards the right, eventually manifesting the so-called <strong>Dying RELU</strong> because certain gradients end up with zero values throughout the training. In contrast to the effect of <strong>vanishing gradient</strong> in which learning slows down, a <strong>Dead Relu</strong> can completely stop learning.</p>
<p>One of the many experiments to try in this section is to simulate a <strong>Dying Relu</strong>. To do that, we use our original <strong>forward.pass(.)</strong> function and <strong>back.propagation(.)</strong> function such that we default the activation functions of the hidden layers to <strong>RELU</strong> instead of using <strong>Leaky RELU</strong>.</p>

<div class="sourceCode" id="cb1873"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1873-1" data-line-number="1">forward.pass &lt;-<span class="st"> </span><span class="cf">function</span>(X, layers, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>, <span class="dt">mode=</span><span class="st">&quot;train&quot;</span>) {</a>
<a class="sourceLine" id="cb1873-2" data-line-number="2">  ...</a>
<a class="sourceLine" id="cb1873-3" data-line-number="3">  act.output       =<span class="st"> </span><span class="kw">activation</span>(net.input, relu)</a>
<a class="sourceLine" id="cb1873-4" data-line-number="4">  ...</a>
<a class="sourceLine" id="cb1873-5" data-line-number="5">}</a></code></pre></div>
<div class="sourceCode" id="cb1874"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1874-1" data-line-number="1">back.propagation &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, model, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>) {</a>
<a class="sourceLine" id="cb1874-2" data-line-number="2">  ...</a>
<a class="sourceLine" id="cb1874-3" data-line-number="3">  gradient.output     =<span class="st"> </span><span class="kw">gradient.activation</span>(act.output, gradient.relu)</a>
<a class="sourceLine" id="cb1874-4" data-line-number="4">  ...</a>
<a class="sourceLine" id="cb1874-5" data-line-number="5">}</a></code></pre></div>

<p>If we then use a very thin or very deep network model, we should begin to see some of our <strong>gradients</strong> always set to zero throughout the training.</p>

<div class="sourceCode" id="cb1875"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1875-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>

<p>We leave readers to experiment on this topic.</p>
<p>In the following sections, we discuss some tune-ups to overcome such conditions as <strong>vanishing gradients</strong>, <strong>exploding gradients</strong>, and <strong>overfitting</strong>. </p>
</div>
<div id="gradient-clipping-gc" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.10</span> Gradient Clipping (GC) <a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Clipping</strong> is a technique that <strong>clips</strong> a value within a minimum and maximum range. Similar to <strong>Batch Normalization</strong>, we can use such a technique to avoid <strong>exploding gradients</strong> as our case allows while at the same time achieving some level of performance improvement.</p>
<p>To illustrate, let us review our original <strong>back.propagation(.)</strong> function with emphasis on clipping:</p>

<div class="sourceCode" id="cb1876"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1876-1" data-line-number="1">back.propagation &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, model, params, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>) {</a>
<a class="sourceLine" id="cb1876-2" data-line-number="2">    ...</a>
<a class="sourceLine" id="cb1876-3" data-line-number="3">        <span class="co"># gradient clipping</span></a>
<a class="sourceLine" id="cb1876-4" data-line-number="4">        delta.omega[[L]]        =<span class="st"> </span><span class="kw">clipping</span>( <span class="kw">t</span>(net.input) <span class="op">%*%</span><span class="st"> </span>delta[[L]] )</a>
<a class="sourceLine" id="cb1876-5" data-line-number="5">    ...</a>
<a class="sourceLine" id="cb1876-6" data-line-number="6">}</a></code></pre></div>

<p>A simple implementation of clipping is written as such:</p>
<p>To disable clipping:</p>

<div class="sourceCode" id="cb1877"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1877-1" data-line-number="1">clipping &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">x.min=</span><span class="op">-</span><span class="ot">Inf</span>, <span class="dt">x.max=</span><span class="ot">Inf</span>) { </a>
<a class="sourceLine" id="cb1877-2" data-line-number="2">  <span class="kw">pmax</span>( <span class="kw">pmin</span>(x, x.max),  x.min)</a>
<a class="sourceLine" id="cb1877-3" data-line-number="3">}</a></code></pre></div>

<p>To enable clipping:</p>

<div class="sourceCode" id="cb1878"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1878-1" data-line-number="1">clipping &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">x.min=</span><span class="op">-</span><span class="dv">1</span>, <span class="dt">x.max=</span><span class="dv">1</span>) { </a>
<a class="sourceLine" id="cb1878-2" data-line-number="2">  <span class="kw">pmax</span>( <span class="kw">pmin</span>(x, x.max),  x.min)</a>
<a class="sourceLine" id="cb1878-3" data-line-number="3">}</a></code></pre></div>

<p>Using the following layers as before, let us now see if we are able to avoid the <strong>exploding gradient</strong> condition.</p>

<div class="sourceCode" id="cb1879"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1879-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1879-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1879-3" data-line-number="3">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1879-4" data-line-number="4">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1879-5" data-line-number="5">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)  </a></code></pre></div>

<p>Given that, let us train our model:</p>

<div class="sourceCode" id="cb1880"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1880-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">10</span>,</a>
<a class="sourceLine" id="cb1880-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">1800</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>The model does not result in an early stop and completes the iteration. We also notice that the <strong>COST</strong> becomes stable.</p>

<div class="sourceCode" id="cb1881"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1881-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 1800</code></pre>
<div class="sourceCode" id="cb1883"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1883-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.075484215 0.073736657 0.074051346 0.074300869 0.074657774
##  [6] 0.074209654 0.074944662 0.073766241 0.074005804 0.074472293</code></pre>

<p>Also, we do not see extremely large values. See below:</p>

<div class="sourceCode" id="cb1885"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1885-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>
<pre><code>##             [,1]          [,2]            [,3]
##    -0.0022213189 -0.0063910013 -0.000016248965
## X1 -0.0069608285 -0.0201426031 -0.000046744422
## X2  0.0046131129  0.0133853472  0.000029664716</code></pre>

<p>See a plot of the <strong>COST</strong> for the model in Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:mlpplotdc">12.16</a>.</p>

<div class="sourceCode" id="cb1887"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1887-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.deep<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1887-2" data-line-number="2">y =<span class="st"> </span>mlp.model.deep<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1887-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   </a>
<a class="sourceLine" id="cb1887-4" data-line-number="4">     <span class="dt">main=</span><span class="st">&quot;DNN Plot (Gradient Clipping)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1887-5" data-line-number="5"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1887-6" data-line-number="6">color =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb1887-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col=</span>color[<span class="dv">1</span>], <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpplotdc"></span>
<img src="DS_files/figure-html/mlpplotdc-1.png" alt="DNN Plot (Gradient Clipping)" width="70%" />
<p class="caption">
Figure 12.16: DNN Plot (Gradient Clipping)
</p>
</div>

<p>Reviewing the performance, we see that the predicted result closely matches the target.</p>

<div class="sourceCode" id="cb1888"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1888-1" data-line-number="1">output     =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1888-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) </a></code></pre></div>
<pre><code>##               T1         T2   O1   O2
##  [1,] 0.32623900 0.72623900 0.56 0.50
##  [2,] 0.33325993 0.73325993 0.56 0.50
##  [3,] 0.74788943 0.34788943 0.56 0.50
##  [4,] 0.41684077 0.61684077 0.56 0.50
##  [5,] 0.91527475 0.11527475 0.56 0.50
##  [6,] 0.91044131 0.11044131 0.56 0.50
##  [7,] 0.23370034 0.83370034 0.56 0.50
##  [8,] 0.94793236 0.14793236 0.56 0.50
##  [9,] 0.93496225 0.13496225 0.56 0.49
## [10,] 0.42822478 0.62822478 0.56 0.50</code></pre>

<p>The important note is that we can avoid the <strong>exploding gradient</strong> by clipping our <strong>gradients</strong>. At the same time, we should still be able to obtain higher performance by adjusting the learning rate, epoch limit, and tolerance level. Additionally, notice that our <strong>minimum and maximum thresholds</strong> are hand-selected for our clips. The chosen values are heuristically obtained. However, other literature may provide better ways to derive such thresholds automatically. We leave readers to investigate such techniques.</p>
</div>
<div id="parameter-initialization" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.11</span> Parameter Initialization <a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Deciding on the appropriate parameter initialization is an essential step in <strong>DNN</strong> that cannot be taken lightly without leading to failed convergence or very slow convergence. Here, let us introduce three initialization methods used:</p>
<ul>
<li><strong>Xavier (Glorot) Initialization</strong> - This initialization is introduced by Xavier Glorot and Yoshua Bengio in <span class="citation">(<a href="bibliography.html#ref-ref1188x">2010</a>)</span>. The idea behind the initialization is to generate random values for all weights based on a choice of <strong>uniform</strong> distribution or <strong>normal distribution</strong> using the below formulation (derivation is not included):  </li>
</ul>
<p><span class="math display" id="eq:equate1140119">\[\begin{align}
\text{glorot.init} = \frac{1}{n^{[l]} + n^{[l-1]}},
\ \ \ \ \ \sigma = \sqrt{(2 \times \text{glorot.init})}
\ \ \ \ \ \ limit = \sqrt{(6 \times glorot.init)} \tag{12.128} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140120">\[\begin{align}
\mathbf{W}^{(l)} \sim \mathcal{N}\left(\mu = 0,\sigma =  \sigma\right)
\ \ \ \ \ \ 
\mathbf{W}^{(l)} \sim \mathcal{U}\left(\text{min = -limit, max = +limit}\right) \tag{12.129} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{n^{(l)}}\)</span> is the number of input neurons, <span class="math inline">\(\mathbf{n^{(l-1)}}\)</span> is the number of output neurons, and <strong>l</strong> refers to a layer. The emphasis is on scaling variance to constrain weights from starting with very small or very large values.</p>
<p>Such initialization is used for <strong>Sigmoid</strong> and <strong>Softmax</strong> activations.</p>
<ul>
<li><strong>Kaiming/He Initialization</strong> - the initialization is a variant of <strong>Xavier initialization</strong> introduced by <strong>Kaiming He</strong> et al. <span class="citation">(<a href="bibliography.html#ref-ref1355k">2015</a>)</span>, which considers only the number of input neurons:  </li>
</ul>
<p><span class="math display" id="eq:equate1140121">\[\begin{align}
\text{he.init} = \frac{1}{n^{[l]}  },
\ \ \ \ \ \sigma = \sqrt{(2 \times \text{he.init})}
\ \ \ \ \ \ limit = \sqrt{(6 \times he.init)} \tag{12.130} 
\end{align}\]</span></p>
<p>Such initialization is used for <strong>Relu</strong> activation.</p>
<ul>
<li><strong>LeCun Initialization</strong> - the initialization introduced by <strong>Yann Lecun</strong> has the following variant: </li>
</ul>
<p><span class="math display" id="eq:equate1140122">\[\begin{align}
\text{lecun.init} = \frac{1}{n^{[l]}  },
\ \ \ \ \ \sigma = \sqrt{(3 \times \text{lecun.init})}
\ \ \ \ \ \ limit = \sqrt{(1 \times lecun.init)} \tag{12.131} 
\end{align}\]</span></p>
<p>Such initialization is used for <strong>TanH</strong> activation.</p>
<p>Below is our example implementation of the initialization methods:</p>

<div class="sourceCode" id="cb1890"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1890-1" data-line-number="1">net.initialization &lt;-<span class="st"> </span><span class="cf">function</span>(size, ni, no, <span class="dt">itype =</span> <span class="st">&quot;glorot&quot;</span>, </a>
<a class="sourceLine" id="cb1890-2" data-line-number="2">                               <span class="dt">dist=</span><span class="st">&quot;normal&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-3" data-line-number="3">   <span class="cf">if</span> (itype <span class="op">==</span><span class="st"> &quot;glorot&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-4" data-line-number="4">     glorot.init =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(ni <span class="op">+</span><span class="st"> </span>no); sd =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>glorot.init)</a>
<a class="sourceLine" id="cb1890-5" data-line-number="5">     lim =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">6</span> <span class="op">*</span><span class="st"> </span>glorot.init)</a>
<a class="sourceLine" id="cb1890-6" data-line-number="6">   } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1890-7" data-line-number="7">   <span class="cf">if</span> (itype <span class="op">==</span><span class="st"> &quot;he&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-8" data-line-number="8">     he.init =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(ni); sd =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>he.init)</a>
<a class="sourceLine" id="cb1890-9" data-line-number="9">     lim =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">6</span> <span class="op">*</span><span class="st"> </span>he.init)</a>
<a class="sourceLine" id="cb1890-10" data-line-number="10">   } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1890-11" data-line-number="11">   <span class="cf">if</span> (itype <span class="op">==</span><span class="st"> &quot;lecun&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-12" data-line-number="12">     lecun.init =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(ni); sd =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">3</span> <span class="op">*</span><span class="st"> </span>lecun.init)</a>
<a class="sourceLine" id="cb1890-13" data-line-number="13">     lim =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">1</span> <span class="op">*</span><span class="st"> </span>lecun.init)</a>
<a class="sourceLine" id="cb1890-14" data-line-number="14">   }</a>
<a class="sourceLine" id="cb1890-15" data-line-number="15">   <span class="cf">if</span> (dist <span class="op">==</span><span class="st"> &quot;normal&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-16" data-line-number="16">     init =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd =</span> sd)</a>
<a class="sourceLine" id="cb1890-17" data-line-number="17">   } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1890-18" data-line-number="18">   <span class="cf">if</span> (dist <span class="op">==</span><span class="st"> &quot;uniform&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-19" data-line-number="19">     init =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>size, <span class="dt">min=</span><span class="op">-</span>lim, <span class="dt">max=</span>lim)</a>
<a class="sourceLine" id="cb1890-20" data-line-number="20">   }</a>
<a class="sourceLine" id="cb1890-21" data-line-number="21">   init</a>
<a class="sourceLine" id="cb1890-22" data-line-number="22">}</a></code></pre></div>

<p>Let us modify our <strong>deep.neural.layers(.)</strong> function to use the <strong>net.initialization(.)</strong> function. To do that, we change from:</p>

<div class="sourceCode" id="cb1891"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1891-1" data-line-number="1">deep.neural.layers &lt;-<span class="st"> </span><span class="cf">function</span>(X, ...) {</a>
<a class="sourceLine" id="cb1891-2" data-line-number="2">  ...</a>
<a class="sourceLine" id="cb1891-3" data-line-number="3">  O =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(K <span class="op">*</span><span class="st"> </span>N, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>), <span class="dt">nrow=</span>K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1891-4" data-line-number="4">  ...</a>
<a class="sourceLine" id="cb1891-5" data-line-number="5">}</a></code></pre></div>

<p>to:</p>

<div class="sourceCode" id="cb1892"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1892-1" data-line-number="1">deep.neural.layers &lt;-<span class="st"> </span><span class="cf">function</span>(X, ...) {</a>
<a class="sourceLine" id="cb1892-2" data-line-number="2">   ...</a>
<a class="sourceLine" id="cb1892-3" data-line-number="3">   weights =<span class="st"> </span><span class="kw">net.initialization</span>( K <span class="op">*</span><span class="st"> </span>N, K, N, <span class="dt">itype=</span><span class="st">&quot;glorot&quot;</span>, <span class="dt">dist=</span><span class="st">&quot;normal&quot;</span> )</a>
<a class="sourceLine" id="cb1892-4" data-line-number="4">   O =<span class="st"> </span><span class="kw">matrix</span>(weights, <span class="dt">nrow=</span>K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1892-5" data-line-number="5">   ...</a>
<a class="sourceLine" id="cb1892-6" data-line-number="6">}</a></code></pre></div>



<p>To see the effect of the <strong>Glorot</strong> initialization scheme, let us follow the same line of thought using a wide <strong>DNN</strong> to demonstrate the benefit, then followed by using a thin <strong>DNN</strong> after:</p>

<div class="sourceCode" id="cb1893"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1893-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1893-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">100</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">100</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1893-3" data-line-number="3">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)   </a></code></pre></div>

<p>The number of parameters generated for our <strong>DNN</strong> is 10602 parameters. Let us now train our network.</p>

<div class="sourceCode" id="cb1894"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1894-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">50</span>,</a>
<a class="sourceLine" id="cb1894-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">500</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>This time, the model does not result in an early stop though it reaches our tolerance level. Furthermore, the <strong>COST</strong> is tamed.</p>

<div class="sourceCode" id="cb1895"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1895-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 500</code></pre>
<div class="sourceCode" id="cb1897"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1897-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.00017286656 0.00017277647 0.00017268774 0.00017260033
##  [5] 0.00017251487 0.00017243049 0.00017234708 0.00017226500
##  [9] 0.00017218433 0.00017210568</code></pre>

<p>In addition, we find no sign of vanishing gradient. See below:</p>

<div class="sourceCode" id="cb1899"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1899-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>
<pre><code>##              [,1]           [,2]          [,3]
##     1.2549305e-06 0.001745049023 -0.0023383996
## X1 -1.6533580e-05 0.001748552505 -0.0045640682
## X2  1.0058240e-05 0.000019985832  0.0017300846</code></pre>

<p>If we evaluate the performance of our prediction, the outcome results in values that marginally match the target.</p>

<div class="sourceCode" id="cb1901"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1901-1" data-line-number="1">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1901-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 rows</span></a></code></pre></div>
<pre><code>##               T1         T2   O1   O2
##  [1,] 0.32623900 0.72623900 0.33 0.74
##  [2,] 0.33325993 0.73325993 0.33 0.74
##  [3,] 0.74788943 0.34788943 0.73 0.31
##  [4,] 0.41684077 0.61684077 0.42 0.64
##  [5,] 0.91527475 0.11527475 0.93 0.13
##  [6,] 0.91044131 0.11044131 0.93 0.13
##  [7,] 0.23370034 0.83370034 0.23 0.83
##  [8,] 0.94793236 0.14793236 0.93 0.13
##  [9,] 0.93496225 0.13496225 0.93 0.13
## [10,] 0.42822478 0.62822478 0.43 0.64</code></pre>

<p>On the other hand, let us also try to use a <strong>DNN</strong> that is thin.</p>

<div class="sourceCode" id="cb1903"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1903-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1903-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1903-3" data-line-number="3">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1903-4" data-line-number="4">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1903-5" data-line-number="5">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)  </a></code></pre></div>

<p>The number of parameters generated for our <strong>DNN</strong> is 237 parameters. Let us now train our network.</p>

<div class="sourceCode" id="cb1904"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1904-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">50</span>,</a>
<a class="sourceLine" id="cb1904-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">500</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>Similarly, the model does not result in an early stop and the iteration also completes.</p>

<div class="sourceCode" id="cb1905"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1905-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 500</code></pre>
<div class="sourceCode" id="cb1907"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1907-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.074299949 0.074299945 0.074299942 0.074299939 0.074299936
##  [6] 0.074299933 0.074299929 0.074299926 0.074299923 0.074299920</code></pre>

<p>However, while it does show that <strong>COST</strong> is stable, we start to see the <strong>gradients</strong> getting much smaller.</p>

<div class="sourceCode" id="cb1909"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1909-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>
<pre><code>##              [,1]           [,2]           [,3]
##     2.6874065e-09 -2.5275332e-07 -3.9684473e-07
## X1  2.5275477e-05 -2.3778705e-03 -3.7350164e-03
## X2 -2.5381638e-05  2.3878567e-03  3.7506992e-03</code></pre>

<p>If we evaluate the performance of our prediction, the outcome results in values that are way off compared to the target.</p>

<div class="sourceCode" id="cb1911"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1911-1" data-line-number="1">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1911-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 rows</span></a></code></pre></div>
<pre><code>##               T1         T2   O1  O2
##  [1,] 0.32623900 0.72623900 0.56 0.5
##  [2,] 0.33325993 0.73325993 0.56 0.5
##  [3,] 0.74788943 0.34788943 0.56 0.5
##  [4,] 0.41684077 0.61684077 0.56 0.5
##  [5,] 0.91527475 0.11527475 0.56 0.5
##  [6,] 0.91044131 0.11044131 0.56 0.5
##  [7,] 0.23370034 0.83370034 0.56 0.5
##  [8,] 0.94793236 0.14793236 0.56 0.5
##  [9,] 0.93496225 0.13496225 0.56 0.5
## [10,] 0.42822478 0.62822478 0.56 0.5</code></pre>

<p>This is the effect of having a <strong>DNN</strong> that is too deep.</p>
<p>Apart from merely using initialization to mitigate vanishing and exploding gradient, it should be apparent that a <strong>DNN</strong> that is too wide or too thin may not give any advantage. In architecting a <strong>DNN</strong>, it sometimes helps to design a decent and simple one.</p>
<p>To illustrate, suppose we adjust the number of layers used in our example above and come up with a decent number of layers, e.g., only two hidden layers instead of the 100 hidden layers we used prior, each corresponding to a select number of neurons: (5,5,2) - the first two layers are the <strong>hidden</strong> layers while the last layer represents the <strong>output</strong> layer.</p>

<div class="sourceCode" id="cb1913"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1913-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1913-2" data-line-number="2">dnn =<span class="st"> </span><span class="kw">deep.neural.layers</span>(X, <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>), </a>
<a class="sourceLine" id="cb1913-3" data-line-number="3">                            <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">2</span>))  </a></code></pre></div>

<p>Let us continue to use the same configurations as before to train our network.</p>

<div class="sourceCode" id="cb1914"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1914-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">10</span>,</a>
<a class="sourceLine" id="cb1914-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">500</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>Here, the training completes with <strong>no early stop</strong>.</p>

<div class="sourceCode" id="cb1915"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1915-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 500</code></pre>
<div class="sourceCode" id="cb1917"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1917-1" data-line-number="1"><span class="kw">head</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>) <span class="co"># Reviewing first few MSEs</span></a></code></pre></div>
<pre><code>##  [1] 0.35822684 0.35964301 0.35890920 0.35693836 0.35793780 0.35811155
##  [7] 0.36048859 0.35749239 0.35851598 0.35929211</code></pre>
<div class="sourceCode" id="cb1919"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1919-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>) <span class="co"># Reviewing last few MSEs</span></a></code></pre></div>
<pre><code>##  [1] 0.00020791055 0.00020416317 0.00020717125 0.00020579111
##  [5] 0.00020944271 0.00020917991 0.00019817092 0.00020185509
##  [9] 0.00020208305 0.00019340582</code></pre>

<p>Reviewing our <strong>gradients</strong>, the values are not extreme. See below:</p>

<div class="sourceCode" id="cb1921"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1921-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega</a></code></pre></div>
<pre><code>##               [,1]          [,2]          [,3]           [,4]
##     0.000055319940 -0.0061976654 -0.0072942866 -0.00254141681
## X1 -0.000090601529 -0.0025536053 -0.0059808568 -0.00251385332
## X2  0.000158613103 -0.0053286353 -0.0033576009 -0.00074866972
##            [,5]
##    0.0079363948
## X1 0.0066501195
## X2 0.0035133448</code></pre>

<p>Let us plot the error (see Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:mlpplotd">12.17</a>).</p>

<div class="sourceCode" id="cb1923"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1923-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.deep<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1923-2" data-line-number="2">y =<span class="st"> </span>mlp.model.deep<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1923-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1923-4" data-line-number="4">      <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;DNN Plot&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1923-5" data-line-number="5"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1923-6" data-line-number="6">color =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb1923-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col=</span>color[<span class="dv">1</span>], <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpplotd"></span>
<img src="DS_files/figure-html/mlpplotd-1.png" alt="DNN Plot" width="70%" />
<p class="caption">
Figure 12.17: DNN Plot
</p>
</div>

<p>Let us now use our trained model to test using our test set:</p>

<div class="sourceCode" id="cb1924"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1924-1" data-line-number="1">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1924-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 rows</span></a></code></pre></div>
<pre><code>##               T1         T2   O1   O2
##  [1,] 0.32623900 0.72623900 0.34 0.73
##  [2,] 0.33325993 0.73325993 0.34 0.73
##  [3,] 0.74788943 0.34788943 0.72 0.33
##  [4,] 0.41684077 0.61684077 0.43 0.64
##  [5,] 0.91527475 0.11527475 0.92 0.12
##  [6,] 0.91044131 0.11044131 0.92 0.12
##  [7,] 0.23370034 0.83370034 0.23 0.85
##  [8,] 0.94793236 0.14793236 0.92 0.12
##  [9,] 0.93496225 0.13496225 0.92 0.12
## [10,] 0.42822478 0.62822478 0.44 0.63</code></pre>

<p>We see that the set of predicted outcomes is marginally equivalent to the set of targets.</p>
</div>
<div id="regularization-by-dropouts" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.12</span> Regularization by Dropouts <a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Neural Networks</strong> are not immune from <strong>overfitting</strong>. As we recall in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>), we introduce <strong>L1-loss</strong> and <strong>L2-loss</strong> to reduce overfitting by amplifying or diminishing the effect of the <strong>loss function</strong> on the coefficients (weights). Here, in <strong>Neural Network</strong>, we introduce <strong>Dropout</strong> to <strong>regularize</strong> <strong>neural networks</strong> (Nitish Srivastava, Geoffrey Hinton, et al., <span class="citation">(<a href="bibliography.html#ref-ref1196n">2014</a>)</span>).</p>
<p>The idea is to disconnect incoming and outgoing connections to and from a neuron, effectively dropping out the neuron from the network. For example, in Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:dropout">12.18</a>, there are three dropout neurons in <strong>H1 layer</strong>, one dropout neuron in <strong>H2 layer</strong>, two dropout neurons in <strong>H3 layer</strong>, and so on.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dropout"></span>
<img src="dropout.png" alt="Dropout" width="90%" />
<p class="caption">
Figure 12.18: Dropout
</p>
</div>
<p>To use <strong>dropouts</strong>, we apply the modification to our forward pass equation:</p>
<p><span class="math display" id="eq:equate1140123">\[\begin{align}
H = activation( X \cdotp \omega + b) \circ \mathbf{r}\ \ \ \ \ \leftarrow H = activation( X \cdotp  \omega + b) \tag{12.132} 
\end{align}\]</span></p>
<p>where <strong>r</strong> is a Bernoulli distribution, e.g. <span class="math inline">\(r \sim Bernoulli(p)\)</span>, with <strong>p</strong> probability set preferably greater than or equal to 0.50. A higher probability indicates a lesser dropout. The random dropout is also scaled by <span class="math inline">\(1/p\)</span> to compensate for dropping some neurons. For example, below is our implementation of the <strong>drop.out(.)</strong> function motivated by a python code segment from CS231n PPT by Fei Fei et. al.</p>

<div class="sourceCode" id="cb1926"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1926-1" data-line-number="1">drop.out &lt;-<span class="st"> </span><span class="cf">function</span>(data, prob) {  </a>
<a class="sourceLine" id="cb1926-2" data-line-number="2">    v =<span class="st"> </span>data</a>
<a class="sourceLine" id="cb1926-3" data-line-number="3">    di  =<span class="st"> </span><span class="kw">length</span>(v)</a>
<a class="sourceLine" id="cb1926-4" data-line-number="4">    is.dim =<span class="st"> </span><span class="kw">is.matrix</span>(data) <span class="op">||</span><span class="st"> </span><span class="kw">is.array</span>(data)</a>
<a class="sourceLine" id="cb1926-5" data-line-number="5">    <span class="cf">if</span> (is.dim) {</a>
<a class="sourceLine" id="cb1926-6" data-line-number="6">        di =<span class="st"> </span><span class="kw">dim</span>(data); v =<span class="st"> </span><span class="kw">c</span>(data)</a>
<a class="sourceLine" id="cb1926-7" data-line-number="7">    }</a>
<a class="sourceLine" id="cb1926-8" data-line-number="8">    len =<span class="st"> </span><span class="kw">prod</span>(di)</a>
<a class="sourceLine" id="cb1926-9" data-line-number="9">    p =<span class="st"> </span>(<span class="kw">runif</span>(<span class="dt">n =</span> len, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>) <span class="op">&lt;</span><span class="st"> </span>prob) <span class="op">/</span><span class="st"> </span>prob  </a>
<a class="sourceLine" id="cb1926-10" data-line-number="10">    <span class="kw">array</span>(v <span class="op">*</span><span class="st"> </span>p, di)</a>
<a class="sourceLine" id="cb1926-11" data-line-number="11">}</a></code></pre></div>

<p>To illustrate further, we modify our original implementation of <strong>forward.pass(.)</strong> and <strong>my.MLP(.)</strong> to accommodate our <strong>dropout</strong> functionality. All other code and functions in our <strong>MLP implementation</strong> are unchanged. See below:</p>

<div class="sourceCode" id="cb1927"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1927-1" data-line-number="1">forward.pass &lt;-<span class="cf">function</span>(X, params, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>, <span class="dt">drop=</span><span class="ot">NULL</span>, </a>
<a class="sourceLine" id="cb1927-2" data-line-number="2">                        <span class="dt">batchnorm=</span><span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb1927-3" data-line-number="3">  ...</a>
<a class="sourceLine" id="cb1927-4" data-line-number="4">  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(drop)) {</a>
<a class="sourceLine" id="cb1927-5" data-line-number="5">     <span class="cf">if</span> (<span class="kw">length</span>(drop) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) { prob =<span class="st"> </span>drop[L]} <span class="cf">else</span> { prob =<span class="st"> </span>drop }</a>
<a class="sourceLine" id="cb1927-6" data-line-number="6">  } <span class="cf">else</span> ...</a>
<a class="sourceLine" id="cb1927-7" data-line-number="7">  ...</a>
<a class="sourceLine" id="cb1927-8" data-line-number="8">}</a></code></pre></div>

<p>Let us use some arbitrary number of layers like so without dropouts, then train our model.</p>

<div class="sourceCode" id="cb1928"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1928-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1928-2" data-line-number="2">dnn =<span class="st"> </span><span class="kw">deep.neural.layers</span>(X, <span class="kw">list</span>(<span class="st">&quot;size&quot;</span>=<span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span>=<span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span>=<span class="dv">2</span>))  </a></code></pre></div>

<p>Here, let us train a model without dropout and another model with a 50% dropout.</p>

<div class="sourceCode" id="cb1929"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1929-1" data-line-number="1">mlp.model.nodrop =<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">10</span>, </a>
<a class="sourceLine" id="cb1929-2" data-line-number="2">                   <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>,<span class="dt">eta=</span><span class="fl">0.001</span>,  <span class="dt">epoch=</span><span class="dv">200</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>Now, let us the same number of layers but with 50% dropout, then train our model.</p>

<div class="sourceCode" id="cb1930"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1930-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1930-2" data-line-number="2">dnn =<span class="st"> </span><span class="kw">deep.neural.layers</span>(X, <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>, <span class="st">&quot;drop&quot;</span>=<span class="fl">0.50</span>), </a>
<a class="sourceLine" id="cb1930-3" data-line-number="3">                            <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>, <span class="st">&quot;drop&quot;</span>=<span class="fl">0.50</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">2</span>))</a></code></pre></div>
<div class="sourceCode" id="cb1931"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1931-1" data-line-number="1">mlp.model.withdrop =<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, </a>
<a class="sourceLine" id="cb1931-2" data-line-number="2">                            <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">200</span>,  <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>Using the same trainset as before, we can see that our model with dropout can reach our <strong>epoch</strong> limit with <strong>no</strong> early stop. Also, notice how the <strong>COST</strong> oscillates.</p>

<div class="sourceCode" id="cb1932"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1932-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;Epoch (no Dropouts)&quot;</span> =<span class="st"> </span>mlp.model.nodrop<span class="op">$</span>last.iteration,</a>
<a class="sourceLine" id="cb1932-2" data-line-number="2">  <span class="st">&quot;Epoch (with Dropouts)&quot;</span> =<span class="st"> </span>mlp.model.withdrop<span class="op">$</span>last.iteration )</a></code></pre></div>
<pre><code>##   Epoch (no Dropouts) Epoch (with Dropouts) 
##                   200                   200</code></pre>
<div class="sourceCode" id="cb1934"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1934-1" data-line-number="1"><span class="kw">head</span>(mlp.model.nodrop<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.35804328 0.34882459 0.32788790 0.30374957 0.28486765 0.26892971
##  [7] 0.25912319 0.24184753 0.23954416 0.23575337</code></pre>
<div class="sourceCode" id="cb1936"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1936-1" data-line-number="1"><span class="kw">head</span>(mlp.model.withdrop<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.35844878 0.35822198 0.35845522 0.35818881 0.35801963 0.35856555
##  [7] 0.35818907 0.35812269 0.35810622 0.35872444</code></pre>

<p>Let us plot the results in which we can see a change in the landscape for <strong>COST</strong> and a noticeable oscillation for the model with <strong>dropout</strong>. See Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:mlpplot11">12.19</a>.</p>

<div class="sourceCode" id="cb1938"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1938-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.nodrop<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1938-2" data-line-number="2"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1938-3" data-line-number="3">y =<span class="st"> </span>mlp.model.nodrop<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1938-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1938-5" data-line-number="5">     <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;MLP (No Dropouts)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1938-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1938-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1938-8" data-line-number="8">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.withdrop<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1938-9" data-line-number="9">y =<span class="st"> </span>mlp.model.withdrop<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1938-10" data-line-number="10"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1938-11" data-line-number="11">    <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;MLP (With Dropouts)&quot;</span>, <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1938-12" data-line-number="12"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1938-13" data-line-number="13"><span class="kw">lines</span>(x, y, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpplot11"></span>
<img src="DS_files/figure-html/mlpplot11-1.png" alt="MLP (Dropouts)" width="70%" />
<p class="caption">
Figure 12.19: MLP (Dropouts)
</p>
</div>

<p>Note that <strong>dropouts</strong> may best suit <strong>fully connected neural networks</strong> in which neurons in hidden layers can be turned on and off randomly. This behavior somehow <strong>simulates</strong> the nature of <strong>missing data</strong> or <strong>perturbation</strong>; thus, it generalizes the model, effectively resisting an overfit.</p>
<p>Note that other <strong>Neural networks</strong> may not be that accommodating towards <strong>dropouts</strong> especially for those that are not <strong>fully connected</strong>. While this may be true on a case-by-case basis, we leave readers to investigate <strong>CNN</strong> and <strong>RNN</strong> (which we cover later) as these <strong>Neural networks</strong> support datasets with <strong>Spatial Relationships</strong> or <strong>Time-Series</strong> properties.</p>
</div>
<div id="batch-normalization" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.13</span> Batch Normalization <a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Internal Covariance shift</strong> is a phenomenon noted by Serge Loffe and Christian Szegedy in their <span class="citation">(<a href="bibliography.html#ref-ref986s">2019</a>)</span> paper. The intuition behind the shift emphasizes the data distribution of the activation output being fed to the next layer. Inconsistent distribution in scale (<strong>variance</strong>) and shift (<strong>mean</strong>) may manifest across <strong>mini-batches</strong> that are processed. <strong>Batch Normalization</strong> intends to reduce the <strong>shifts</strong>. In other words, it attempts to compose consistency in the data distribution across <strong>mini-batches</strong>. On the other hand, a paper published by Shibani Santunkar, Dimitris Tsipras, et al. in <span class="citation">(<a href="bibliography.html#ref-ref1002s">2018</a>)</span> explains that <strong>Batch Normalization</strong> works because it lays out a smoother landscape in the direction of the gradients traveling from layer to layer instead of a more coarse or rugged terrain.</p>
<p>To illustrate, using Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:forwardpass">12.13</a>, let us take the <strong>mean</strong> and <strong>variance</strong> of an <strong>activation output</strong> as starting point.</p>
<p><span class="math display" id="eq:equate1140124">\[\begin{align}
\vec{\mu}_B - \frac{1}{m}\sum_{i=1}^m h_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\vec{\sigma^2}_B = \frac{1}{m}\sum_{i=1}^m (h_i - \vec{\mu}_B)^2 \tag{12.133} 
\end{align}\]</span></p>
<p>where: <span class="math inline">\(\vec{\mu}_B \in \mathbb{R}^p\ and\ \vec{\sigma^2} \in \mathbb{R}^p\)</span>.</p>
<p>Recall that <strong>activation output</strong> is an output from an <strong>activation function</strong>. Here, for now, let us represent the <strong>activation output</strong> as <strong>h</strong> and <strong>m</strong> as the number of samples in a <strong>mini-batch</strong>. Also, <strong>p</strong> is represented as number of <strong>activation neurons</strong>. Now, the normalized version is expressed as:</p>
<p><span class="math display" id="eq:equate1140125">\[\begin{align}
h^{(norm)}_i = \frac{h_i - \vec{\mu}_B}{\sqrt{\vec{\sigma^2}_B + \epsilon}}  \tag{12.134} 
\end{align}\]</span></p>
<p>We then use the normalized version and scale it down using two learnable hyperparameters: gamma (<span class="math inline">\(\gamma\)</span>) and beta (<span class="math inline">\(\beta\)</span>).</p>
<p><span class="math display" id="eq:equate1140126">\[\begin{align}
\hat{h}_i = \gamma * h^{(norm)}_i   + \beta = BN_{\gamma,\beta}(h) \tag{12.135} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(h^{(norm)} \in \mathbb{R}^{nxp}\)</span> and <span class="math inline">\(i = 1 ... m\)</span>.</p>
<p>In R, the use of <strong>var(.)</strong> gives sample variance using <strong>Bessel’s correction</strong>, <span class="math inline">\(\frac{1}{(n-1)}\)</span>. Here, we use our <strong>batch variance</strong> (a.l.a population variance):</p>

<div class="sourceCode" id="cb1939"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1939-1" data-line-number="1"><span class="co"># not using the Bessel&#39;s correction ( for sample variance).</span></a>
<a class="sourceLine" id="cb1939-2" data-line-number="2">batch.variance &lt;-<span class="st"> </span><span class="cf">function</span>(h) { <span class="kw">mean</span>( (h <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(h))<span class="op">^</span><span class="dv">2</span>  ) }</a></code></pre></div>

<p>Let us obtain the <strong>mean</strong> (<span class="math inline">\(\vec{\mu}_B\)</span>) and <strong>variance</strong> (<span class="math inline">\(\vec{\sigma^2}_B\)</span>) given a simple dataset like so:</p>

<div class="sourceCode" id="cb1940"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1940-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">digits=</span><span class="dv">8</span>)</a>
<a class="sourceLine" id="cb1940-2" data-line-number="2">(<span class="dt">Input =</span> <span class="kw">matrix</span>( <span class="kw">c</span>(<span class="fl">0.12</span>, <span class="fl">0.18</span>, <span class="fl">0.13</span>, <span class="fl">0.21</span>, <span class="fl">0.15</span>, <span class="fl">0.30</span>, <span class="fl">0.18</span>, <span class="fl">0.40</span>), </a>
<a class="sourceLine" id="cb1940-3" data-line-number="3">                 <span class="dt">nrow=</span><span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,] 0.12 0.18
## [2,] 0.13 0.21
## [3,] 0.15 0.30
## [4,] 0.18 0.40</code></pre>
<div class="sourceCode" id="cb1942"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1942-1" data-line-number="1">(<span class="dt">mu =</span> <span class="kw">apply</span>(Input, <span class="dv">2</span>, mean))</a></code></pre></div>
<pre><code>## [1] 0.1450 0.2725</code></pre>
<div class="sourceCode" id="cb1944"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1944-1" data-line-number="1">(<span class="dt">variance  =</span> <span class="kw">apply</span>(Input, <span class="dv">2</span>, batch.variance )) </a></code></pre></div>
<pre><code>## [1] 0.00052500 0.00736875</code></pre>

<p>Given all that, below is our example implementation of a <strong>batchnorm</strong> forward function:</p>

<div class="sourceCode" id="cb1946"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1946-1" data-line-number="1">batchnorm.forward &lt;-<span class="st"> </span><span class="cf">function</span>(H, layer, <span class="dt">eps=</span><span class="fl">1e-8</span>, <span class="dt">momentum =</span> <span class="fl">0.90</span>,</a>
<a class="sourceLine" id="cb1946-2" data-line-number="2">                              <span class="dt">rmax=</span><span class="dv">1</span>, <span class="dt">dmax=</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1946-3" data-line-number="3">    gamma             =<span class="st"> </span>layer<span class="op">$</span>batch.gamma<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb1946-4" data-line-number="4">    beta              =<span class="st"> </span>layer<span class="op">$</span>batch.beta<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb1946-5" data-line-number="5">    moving.mu         =<span class="st"> </span>layer<span class="op">$</span>moving.mu</a>
<a class="sourceLine" id="cb1946-6" data-line-number="6">    moving.variance   =<span class="st"> </span>layer<span class="op">$</span>moving.variance</a>
<a class="sourceLine" id="cb1946-7" data-line-number="7">    <span class="co">## For Training</span></a>
<a class="sourceLine" id="cb1946-8" data-line-number="8">    <span class="co">#</span></a>
<a class="sourceLine" id="cb1946-9" data-line-number="9">    mu                =<span class="st"> </span><span class="kw">apply</span>(H, <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1946-10" data-line-number="10">    H.mu              =<span class="st"> </span><span class="kw">sweep</span>(H, <span class="dv">2</span>, mu, <span class="st">&quot;-&quot;</span>)</a>
<a class="sourceLine" id="cb1946-11" data-line-number="11">    var               =<span class="st"> </span><span class="kw">apply</span>( H.mu<span class="op">^</span><span class="dv">2</span> , <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1946-12" data-line-number="12">    istd              =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(var <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb1946-13" data-line-number="13">    H.norm            =<span class="st"> </span><span class="kw">sweep</span>( H.mu, <span class="dv">2</span>, istd, <span class="st">&quot;*&quot;</span> ) </a>
<a class="sourceLine" id="cb1946-14" data-line-number="14">    <span class="co">## For Inference</span></a>
<a class="sourceLine" id="cb1946-15" data-line-number="15">    moving.mu         =<span class="st"> </span>momentum <span class="op">*</span><span class="st"> </span>moving.mu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>momentum) <span class="op">*</span><span class="st"> </span>mu</a>
<a class="sourceLine" id="cb1946-16" data-line-number="16">    moving.variance   =<span class="st"> </span>momentum <span class="op">*</span><span class="st"> </span>moving.variance <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>momentum) <span class="op">*</span><span class="st"> </span>var</a>
<a class="sourceLine" id="cb1946-17" data-line-number="17">    <span class="co">## Now generate the act.output</span></a>
<a class="sourceLine" id="cb1946-18" data-line-number="18">    H.hat             =<span class="st"> </span>H.norm <span class="op">*</span><span class="st"> </span>gamma <span class="op">+</span><span class="st"> </span>beta</a>
<a class="sourceLine" id="cb1946-19" data-line-number="19">    moments           =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;H.norm&quot;</span>          =<span class="st"> </span>H.norm,</a>
<a class="sourceLine" id="cb1946-20" data-line-number="20">                             <span class="st">&quot;H.mu&quot;</span>            =<span class="st"> </span>H.mu,</a>
<a class="sourceLine" id="cb1946-21" data-line-number="21">                             <span class="st">&quot;istd&quot;</span>            =<span class="st"> </span>istd,</a>
<a class="sourceLine" id="cb1946-22" data-line-number="22">                             <span class="st">&quot;moving.mu&quot;</span>       =<span class="st"> </span>moving.mu,</a>
<a class="sourceLine" id="cb1946-23" data-line-number="23">                             <span class="st">&quot;moving.variance&quot;</span> =<span class="st"> </span>moving.variance)</a>
<a class="sourceLine" id="cb1946-24" data-line-number="24">    <span class="kw">list</span>(<span class="st">&quot;act.output&quot;</span> =<span class="st"> </span>H.hat, <span class="st">&quot;moments&quot;</span> =<span class="st"> </span>moments)</a>
<a class="sourceLine" id="cb1946-25" data-line-number="25">}</a></code></pre></div>
<div class="sourceCode" id="cb1947"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1947-1" data-line-number="1">batchnorm.prediction &lt;-<span class="st"> </span><span class="cf">function</span>(H, layer, <span class="dt">eps=</span><span class="fl">1e-8</span>) {</a>
<a class="sourceLine" id="cb1947-2" data-line-number="2">    gamma             =<span class="st"> </span>layer<span class="op">$</span>batch.gamma<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb1947-3" data-line-number="3">    beta              =<span class="st"> </span>layer<span class="op">$</span>batch.beta<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb1947-4" data-line-number="4">    moving.mu         =<span class="st"> </span>layer<span class="op">$</span>moving.mu</a>
<a class="sourceLine" id="cb1947-5" data-line-number="5">    moving.variance   =<span class="st"> </span>layer<span class="op">$</span>moving.variance</a>
<a class="sourceLine" id="cb1947-6" data-line-number="6">    istd              =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(moving.variance   <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb1947-7" data-line-number="7">    H.mu              =<span class="st"> </span><span class="kw">sweep</span>(H, <span class="dv">2</span>, moving.mu, <span class="st">&quot;-&quot;</span>)</a>
<a class="sourceLine" id="cb1947-8" data-line-number="8">    H.norm            =<span class="st"> </span><span class="kw">sweep</span>( H.mu, <span class="dv">2</span>, istd, <span class="st">&quot;*&quot;</span> )</a>
<a class="sourceLine" id="cb1947-9" data-line-number="9">    H.hat             =<span class="st"> </span>(H.norm <span class="op">*</span><span class="st"> </span>gamma <span class="op">+</span><span class="st"> </span>beta)</a>
<a class="sourceLine" id="cb1947-10" data-line-number="10">    <span class="kw">list</span>(<span class="st">&quot;prediction&quot;</span> =<span class="st"> </span>H.hat)</a>
<a class="sourceLine" id="cb1947-11" data-line-number="11">}</a></code></pre></div>

<p>Notice the inclusion of three parameters, namely <strong>running average</strong>, <strong>running variance</strong>, and <strong>momentum</strong>. We need these parameters for our <strong>inference</strong> which we cover in our later discussion.</p>
<p>From here, it helps to review <strong>forward.pass(.)</strong> function in <strong>MLP Implementation</strong> section once more in order to see how we use the <strong>batchnorm.forward(.)</strong> function.</p>
<p>Note that two learnable parameters are passed to the <strong>forward.pass(.)</strong> and <strong>batchnorm.forward(.)</strong> functions, namely one with the symbol <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) and the other one with symbol <strong>beta</strong> (<span class="math inline">\(\beta\)</span>). Both correspondingly represent variance (for scaling) and mean (for shifting) such that their convergence ultimately produces the same original (un-normalized) <strong>activation output</strong> - in other words, a gamma of 1 and beta of 0 readily dissolve the effect to the original distribution. Note that each activation (each neuron) in each layer requires a pair of these scalar parameters.</p>
<p>The following list of derivatives is formulated to obtain the deltas we need for our update rules, especially for the <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) and <strong>beta</strong> (<span class="math inline">\(\beta\)</span>) parameters (note that we are excluding derivations of the derivatives):</p>
<p><strong>First</strong>, we obtain the derivative of the <strong>total loss</strong> with respect to the normalized <strong>output</strong> - or <strong>input</strong> to a layer:</p>
<p><span class="math display" id="eq:equate1140127">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}} = 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}}\right) 
\left(\frac{\partial \hat{h}}{\partial h^{(norm)}}\right) = \frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}} \cdotp \gamma   \tag{12.136} 
\end{align}\]</span></p>
<p><strong>Second</strong>, we obtain the derivative of the <strong>total loss</strong> with respect to <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) and <strong>beta</strong> (<span class="math inline">\(\beta\)</span>) respectively:</p>
<p><span class="math display" id="eq:equate1140128">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \gamma} =  
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}_i}\right)
\left(\frac{\partial \hat{h}_i}{\partial \gamma}\right) =  
\sum_{i=1}^m\frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}_i} \times h^{(norm)}_i  \tag{12.137} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140129">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \beta} = 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}_i}\right)
\left(\frac{\partial \hat{h}_i}{\partial \beta}\right) =
\sum_{i=1}^m \frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}_i}   
\ \ \ \  where\ \left(\frac{\partial \hat{h}_i}{\partial \beta}\right) = 1 \tag{12.138} 
\end{align}\]</span></p>
<p><strong>Third</strong>, we obtain the derivative of the <strong>total loss</strong> with respect to <strong>variance</strong> (<span class="math inline">\(\vec{\sigma^2}_B\)</span>):</p>
<p><span class="math display" id="eq:eqnnumber633">\[\begin{align}
\begin{array}{ll} 
\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\sigma}^2_B}  
&amp;= \left(\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}}\right) 
\left(\frac{\partial h^{(norm)}}{\partial \vec{\sigma}^2_B}\right)\\
&amp; = 
 \sum_{i=1}^m \left[\frac{\partial \mathcal{L}_{(total)}}{\partial  h^{(norm)}_i}  \times
 \left(h_i - \vec{\mu}_B\right) \right]\times \frac{-1}{2} \left(\frac{1}{\sqrt{\vec{\sigma^2}_B + \epsilon}}\right)^3 \\
\end{array} \tag{12.139}
\end{align}\]</span></p>
<p><strong>Fourth</strong>, we obtain the derivative of the <strong>total loss</strong> with respect to <strong>mean</strong> (<span class="math inline">\(\vec{\mu}_B\)</span>):</p>

<p><span class="math display" id="eq:equate1140131" id="eq:equate1140130">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\mu}_B}  
&amp;= 
\left[
\sum_{i=1}^m\left(\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}_i}\right) 
\left(\frac{\partial h^{(norm)}_i}{\partial \vec{\mu}_B}\right)\right] + 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial  \vec{\sigma^2}_B}\right) 
\left(\frac{\partial  \vec{\sigma^2}_B}{\partial \vec{\mu}_B}\right)  \tag{12.140} \\
&amp;= \left[\sum_{i=1}^m\left(\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}_i}\right)  \left(\frac{-1}{\sqrt{\vec{\sigma^2}_B + \epsilon}}\right)\right] + 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\sigma^2}_B}\right)
 \left( \frac{\sum_{i=1}^m -2 (h_i - \vec{\mu}_B)}{m}\right) \tag{12.141} 
\end{align}\]</span>
</p>
<p><strong>Fifth</strong>, we obtain the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(\mathbf{h_i}\)</span>:</p>
<p><span class="math display" id="eq:equate1140133" id="eq:equate1140132">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h_i} 
&amp;= 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}_i}\right)
\left(\frac{\partial h^{(norm)}_i}{\partial h_i}\right) + 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\sigma^2}_B}\right)
\left(\frac{\partial \vec{\sigma^2}_B}{\partial h_i}\right) + \nonumber \\
&amp;\ \ \ \left(\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\mu}_B}\right)
\left(\frac{\partial \vec{\mu}_B}{\partial h_i}\right)  \tag{12.142} \\
&amp;= 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}_i}\right)
\left(\frac{1}{\sqrt{\vec{\sigma^2}_B + \epsilon}}\right) + 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\sigma^2}_B}\right)
\left(\frac{2(h_i - \vec{\mu}_B)}{m}\right) + \nonumber \\ 
&amp;\ \ \ \left(\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\mu}_B}\right)
\left(\frac{1}{m}\right) \tag{12.143} 
\end{align}\]</span></p>
<p><strong>Finally</strong>, given all that, we now implement a backward pass for our learnable parameters with the following example implementation of <strong>batchnorm.backward(.)</strong>:</p>

<div class="sourceCode" id="cb1948"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1948-1" data-line-number="1">batchnorm.backward &lt;-<span class="st"> </span><span class="cf">function</span>(Dout, gamma, moments) {</a>
<a class="sourceLine" id="cb1948-2" data-line-number="2">   H.norm         =<span class="st"> </span>moments<span class="op">$</span>H.norm</a>
<a class="sourceLine" id="cb1948-3" data-line-number="3">   H.mu           =<span class="st"> </span>moments<span class="op">$</span>H.mu </a>
<a class="sourceLine" id="cb1948-4" data-line-number="4">   istd           =<span class="st"> </span>moments<span class="op">$</span>istd </a>
<a class="sourceLine" id="cb1948-5" data-line-number="5">   m              =<span class="st"> </span><span class="kw">apply</span>(H.norm, <span class="dv">2</span>, length)</a>
<a class="sourceLine" id="cb1948-6" data-line-number="6">   delta.gamma    =<span class="st"> </span><span class="kw">apply</span>(Dout <span class="op">*</span><span class="st"> </span>H.norm, <span class="dv">2</span>, sum)</a>
<a class="sourceLine" id="cb1948-7" data-line-number="7">   delta.beta     =<span class="st"> </span><span class="kw">apply</span>(Dout, <span class="dv">2</span>, sum)</a>
<a class="sourceLine" id="cb1948-8" data-line-number="8">   delta.H.norm   =<span class="st"> </span>Dout <span class="op">*</span><span class="st"> </span>gamma</a>
<a class="sourceLine" id="cb1948-9" data-line-number="9">   delta.std      =<span class="st"> </span><span class="kw">apply</span>(delta.H.norm <span class="op">*</span><span class="st"> </span>H.mu, <span class="dv">2</span>, sum) </a>
<a class="sourceLine" id="cb1948-10" data-line-number="10">   delta.var      =<span class="st"> </span>delta.std <span class="op">*</span><span class="st"> </span><span class="fl">-0.5</span> <span class="op">*</span><span class="st"> </span>istd<span class="op">^</span><span class="dv">3</span> </a>
<a class="sourceLine" id="cb1948-11" data-line-number="11">   delta.Hmu1     =<span class="st"> </span><span class="kw">apply</span>( delta.H.norm <span class="op">*</span><span class="st"> </span><span class="op">-</span>istd, <span class="dv">2</span>, sum)</a>
<a class="sourceLine" id="cb1948-12" data-line-number="12">   delta.Hmu2     =<span class="st"> </span>delta.var <span class="op">*</span><span class="st"> </span><span class="kw">apply</span>( <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>H.mu,<span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1948-13" data-line-number="13">   delta.mu       =<span class="st"> </span>delta.Hmu1 <span class="op">+</span><span class="st"> </span>delta.Hmu2</a>
<a class="sourceLine" id="cb1948-14" data-line-number="14">   delta.out      =<span class="st"> </span><span class="kw">sweep</span>(delta.H.norm <span class="op">*</span><span class="st"> </span>istd <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1948-15" data-line-number="15"><span class="st">                    </span>delta.var <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>H.mu) <span class="op">/</span><span class="st"> </span>m, <span class="dv">2</span>, (delta.mu <span class="op">/</span><span class="st"> </span>m), <span class="st">&#39;+&#39;</span>)</a>
<a class="sourceLine" id="cb1948-16" data-line-number="16">   <span class="kw">list</span>(<span class="st">&quot;gradient.output&quot;</span> =<span class="st"> </span>delta.out, </a>
<a class="sourceLine" id="cb1948-17" data-line-number="17">        <span class="st">&quot;delta.gamma&quot;</span> =<span class="st"> </span>delta.gamma, <span class="st">&quot;delta.beta&quot;</span> =<span class="st"> </span>delta.beta)</a>
<a class="sourceLine" id="cb1948-18" data-line-number="18">}</a></code></pre></div>

<p>From here, it also helps to review <strong>back.propagation(.)</strong> function in the same <strong>MLP Implementation</strong> section in order to see the use of <strong>batchnorm.backward(.)</strong> function. Additionally, we may have to review <strong>backward.pass(.)</strong> function which enforces the update rules for hyperparameters, along with the use of <strong>Adam</strong> optimization:</p>

<div class="sourceCode" id="cb1949"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1949-1" data-line-number="1">optimize.adam =<span class="st"> </span>adam &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, t) {</a>
<a class="sourceLine" id="cb1949-2" data-line-number="2">    beta1 =<span class="st"> </span><span class="fl">0.90</span>; beta2 =<span class="st"> </span><span class="fl">0.999</span>; eps=<span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb1949-3" data-line-number="3">    param<span class="op">$</span>rho    =<span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>rho <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1) <span class="op">*</span><span class="st"> </span>gradient</a>
<a class="sourceLine" id="cb1949-4" data-line-number="4">    param<span class="op">$</span>nu     =<span class="st"> </span>beta2 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta2) <span class="op">*</span><span class="st"> </span>gradient<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1949-5" data-line-number="5">    rho.hat      =<span class="st"> </span>param<span class="op">$</span>rho <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1949-6" data-line-number="6">    nu.hat       =<span class="st"> </span>param<span class="op">$</span>nu <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta2<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1949-7" data-line-number="7">    phi          =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(nu.hat) <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb1949-8" data-line-number="8">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>phi <span class="op">*</span><span class="st"> </span>rho.hat</a>
<a class="sourceLine" id="cb1949-9" data-line-number="9">    param</a>
<a class="sourceLine" id="cb1949-10" data-line-number="10">}</a></code></pre></div>

<p>Lastly, the <strong>deep.neural.layers(.)</strong> function incorporates initialization of the two new hyperparameters, namely <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) and <strong>beta</strong> (<span class="math inline">\(\beta\)</span>) for optimization.</p>
<p>To illustrate, let us continue to use the following layers.</p>

<div class="sourceCode" id="cb1950"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1950-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1950-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1950-3" data-line-number="3">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)</a></code></pre></div>

<p>That gives us about 57 parameters (including biases).</p>
<p>Now, let us try modeling DNN without batch normalization.</p>

<div class="sourceCode" id="cb1951"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1951-1" data-line-number="1">mlp.model.noBN =<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">10</span>, </a>
<a class="sourceLine" id="cb1951-2" data-line-number="2">                 <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>, <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">100</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a>
<a class="sourceLine" id="cb1951-3" data-line-number="3">mlp.model.noBN<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb1953"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1953-1" data-line-number="1"><span class="kw">head</span>(mlp.model.noBN<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.35804328 0.34882459 0.32788790 0.30374957 0.28486765 0.26892971
##  [7] 0.25912319 0.24184753 0.23954416 0.23575337</code></pre>
<div class="sourceCode" id="cb1955"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1955-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.noBN<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.00023908686 0.00022170241 0.00021220855 0.00020488970
##  [5] 0.00019650187 0.00018757701 0.00018158010 0.00017569945
##  [9] 0.00017344208 0.00016758827</code></pre>
<div class="sourceCode" id="cb1957"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1957-1" data-line-number="1">mlp.model.noBN<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega</a></code></pre></div>
<pre><code>##              [,1]         [,2]         [,3]          [,4]
##    -0.00034719214  0.008882285  0.002632272  0.0027318712
## X1 -0.00018229795 -0.004817867 -0.011059380 -0.0029795080
## X2 -0.00017847808  0.012816578  0.012179239  0.0052451620
##             [,5]
##    -0.0043827349
## X1  0.0102768353
## X2 -0.0131979135</code></pre>

<p>Next, we model <strong>DNN</strong> with batch normalization. For now, let us keep the same learning rate - eta symbol (<span class="math inline">\(\eta\)</span>).</p>

<div class="sourceCode" id="cb1959"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1959-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1959-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>, <span class="st">&quot;batchnorm&quot;</span> =<span class="st"> </span><span class="ot">TRUE</span>), </a>
<a class="sourceLine" id="cb1959-3" data-line-number="3">                       <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1959-4" data-line-number="4">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)</a></code></pre></div>
<div class="sourceCode" id="cb1960"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1960-1" data-line-number="1">mlp.model.BN =<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">10</span>, </a>
<a class="sourceLine" id="cb1960-2" data-line-number="2">               <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>, <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">100</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a>
<a class="sourceLine" id="cb1960-3" data-line-number="3">mlp.model.BN<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb1962"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1962-1" data-line-number="1"><span class="kw">head</span>(mlp.model.BN<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.35807389 0.35782997 0.34387280 0.32203210 0.30398898 0.28888063
##  [7] 0.27927648 0.26171434 0.25589441 0.24918166</code></pre>
<div class="sourceCode" id="cb1964"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1964-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.BN<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.051191994 0.050035899 0.050687903 0.052175072 0.050586036
##  [6] 0.050977164 0.050522264 0.049580550 0.051174961 0.050063166</code></pre>
<div class="sourceCode" id="cb1966"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1966-1" data-line-number="1">mlp.model.BN<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega</a></code></pre></div>
<pre><code>##           [,1] [,2] [,3]        [,4] [,5]
##    0.026829669   -1   -1 -0.47626824    1
## X1 0.011910041   -1   -1 -0.18591176    1
## X2 0.016637621   -1   -1 -0.32358615    1</code></pre>

<p>We can observe that the model with <strong>Batch Normalization</strong> seems to have a higher <strong>COST</strong> at every <strong>epoch</strong> compared to one without normalization. However, if we see the plot, the landscape of the <strong>cost</strong> function indeed follows a more comfortable trajectory. See Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:mlpplot2">12.20</a>.</p>

<div class="sourceCode" id="cb1968"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1968-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.noBN<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1968-2" data-line-number="2"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1968-3" data-line-number="3">y =<span class="st"> </span>mlp.model.noBN<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1968-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1968-5" data-line-number="5">      <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;MLP (No BatchNorm)&quot;</span>, <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1968-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1968-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1968-8" data-line-number="8">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.BN<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1968-9" data-line-number="9">y =<span class="st"> </span>mlp.model.BN<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1968-10" data-line-number="10"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1968-11" data-line-number="11">    <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;MLP (With BatchNorm)&quot;</span>, <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1968-12" data-line-number="12"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1968-13" data-line-number="13"><span class="kw">lines</span>(x, y, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpplot2"></span>
<img src="DS_files/figure-html/mlpplot2-1.png" alt="MLP (Batch Normalization)" width="70%" />
<p class="caption">
Figure 12.20: MLP (Batch Normalization)
</p>
</div>

<p>Now, in terms of <strong>inference</strong>, we use <strong>moving average</strong> and <strong>moving variance</strong> to keep track of the <strong>population average</strong> of all mini-batches throughout the training time - granting the choice of normalization is <strong>batch normalization</strong> versus <strong>layer normalization</strong>. Such parameters are then used at test time. See our <strong>batchnorm.forward(.)</strong> function and <strong>batchnorm.prediction(.)</strong> function. Also, we use <strong>momentum</strong> denoted by <strong>alpha</strong> (<span class="math inline">\(\alpha\)</span>) which serves as a <strong>decaying rate</strong> for the moments (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>) during training. The default we use in our implementation is 0.90.</p>
<p><span class="math display" id="eq:equate1140135" id="eq:equate1140134">\[\begin{align}
\mu_{(moving)} &amp;= \alpha \times \mu_{(moving)} + (1 - \alpha) \times \mu_B \tag{12.144} \\
\sigma^2_{(moving)} &amp;= \alpha \times \sigma^2_{(moving)} + (1 - \alpha) \times \sigma^2_{B} \tag{12.145} 
\end{align}\]</span></p>
<p>To illustrate, we compare the result of a model without <strong>BN</strong> (recall that our dataset is concocted to have a <strong>RELU</strong> pattern for output):</p>

<div class="sourceCode" id="cb1969"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1969-1" data-line-number="1">output.noBN   =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.noBN)</a>
<a class="sourceLine" id="cb1969-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output.noBN<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>)  </a></code></pre></div>
<pre><code>##               T1         T2   O1   O2
##  [1,] 0.32623900 0.72623900 0.34 0.73
##  [2,] 0.33325993 0.73325993 0.34 0.73
##  [3,] 0.74788943 0.34788943 0.72 0.33
##  [4,] 0.41684077 0.61684077 0.43 0.64
##  [5,] 0.91527475 0.11527475 0.92 0.13
##  [6,] 0.91044131 0.11044131 0.92 0.13
##  [7,] 0.23370034 0.83370034 0.23 0.84
##  [8,] 0.94793236 0.14793236 0.92 0.13
##  [9,] 0.93496225 0.13496225 0.92 0.13
## [10,] 0.42822478 0.62822478 0.43 0.63</code></pre>

<p>and the result of a model with <strong>BN</strong></p>

<div class="sourceCode" id="cb1971"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1971-1" data-line-number="1">output.BN     =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.BN)</a>
<a class="sourceLine" id="cb1971-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output.BN<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) </a></code></pre></div>
<pre><code>##               T1         T2   O1   O2
##  [1,] 0.32623900 0.72623900 0.53 0.53
##  [2,] 0.33325993 0.73325993 0.53 0.54
##  [3,] 0.74788943 0.34788943 0.59 0.47
##  [4,] 0.41684077 0.61684077 0.54 0.53
##  [5,] 0.91527475 0.11527475 0.63 0.42
##  [6,] 0.91044131 0.11044131 0.63 0.42
##  [7,] 0.23370034 0.83370034 0.50 0.57
##  [8,] 0.94793236 0.14793236 0.63 0.42
##  [9,] 0.93496225 0.13496225 0.62 0.43
## [10,] 0.42822478 0.62822478 0.54 0.52</code></pre>

<p>Note that while the prediction for the normalization seems a bit off in our particular case, we leave readers to try to use a learning rate of 0.01 for improvement. Additionally, the learning speed for batch normalization is more visible in the next section when we deal with <strong>CNN</strong> against the <strong>cifar-10</strong> dataset.</p>
<p>Also, the <strong>batch normalization</strong> step is positioned before the activation function in our implementation. In a section up ahead, dealing with <strong>CNN</strong>, we shall try to position the <strong>batch normalization</strong> after the activation function at the convolution layers. We leave readers to investigate the effectiveness of <strong>batch normalization</strong> based on whether it should be before or after the activation function.</p>
</div>
<div id="optimization" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.14</span> Optimization <a href="12.3-multi-layer-perceptron-mlp.html#optimization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our recent discussions put emphasis on <strong>vanishing gradients</strong> and <strong>exploding gradients</strong>. Certain techniques are introduced, such as <strong>Parameter initialization</strong>, <strong>Gradient Clipping</strong>, and designing a <strong>Decent Architecture</strong> in terms of the number of layers and neurons to use.</p>
<p>In this section, our emphasis is on <strong>overfitting</strong>, <strong>underfitting</strong>, and avoiding <strong>local minima</strong>. We begin to show the importance of adjusting a combination of some performance knobs such as <strong>floating-point precision</strong>, <strong>learning rate</strong>, <strong>epoch limit</strong>, and <strong>tolerance level</strong>. If we use a very small <strong>learning rate</strong>, we may end up with an <strong>underfit</strong> model. If we use a very large <strong>learning rate</strong>, we may end up with an <strong>overfit</strong> model, <strong>overshooting</strong> the <strong>target</strong>. If our <strong>epoch limit</strong> is too small (e.g., inducing an <strong>early stop</strong>), we may <strong>underfit</strong>, if it is too large and we do not hit a <strong>tolerance threshold</strong>, we may <strong>overfit</strong>. Heuristically, we also learn that <strong>Batch Normalization</strong> and <strong>Gradient Clipping</strong> can increase performance. This affects the effectiveness of the hyperparameters. Thus they require adjusting along with the use of the mentioned techniques. For <strong>Batch Normalization</strong>, it helps to tune the <strong>learning rate</strong> to balance down the learning to a more optimal speed. So far, what we have done heuristically is to find the best value for the learning rate. However, this trial and error approach may not be as effective if we are not careful.</p>
<p>Also, in this section, let us delve deeper into the <strong>update rule</strong> of the vanilla <strong>Gradient Descent</strong> algorithm, which is implemented in our <strong>backward.pass(.)</strong> function.</p>
<p><span class="math display" id="eq:equate1140136">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \eta\times g^{(t)} \ \ \ \ \ \ where\ \ \ \ \ g^{(t)} = \nabla \omega \mathcal{L} \tag{12.146} 
\end{align}\]</span></p>
<p>Note that the <strong>omega</strong> symbol (<span class="math inline">\(\omega\)</span>) represents coefficients or weights (also called parameters) that dictate the degrees of freedom (df) with which our model is fitted. Such parameters are to be tuned <strong>optimally</strong> to fit the model <strong>generally</strong> - we call this training the model (or, in other words, machine learning). One of the important components or elements of all these is the <strong>learning rate</strong> denoted by the <strong>eta</strong> symbol (<span class="math inline">\(\eta\)</span>). Specifically, we aim to find the most <strong>optimal</strong> <strong>learning rate</strong>. With that, there are enhancements to optimizing the <strong>Gradient Descent</strong> algorithm.  </p>
<p>Here, we list about eight optimizers that are regarded as state-of-art optimizers, whether currently or at one point in time in their rights. Note that, in principle, the aim of the different optimizers is to avoid local minima, avoid overfitting, avoid underfitting, and improve performance. Therefore, without covering the derivations, let us briefly explore and review the equations and implementation of each optimizer.</p>
<p><strong>First</strong>, let us introduce <strong>Momentum</strong> which is represented by the <strong>nu</strong> (<span class="math inline">\(\nu\)</span>) symbol added to <strong>Gradient Descent</strong>, in particular, to the <strong>Stochastic Gradient Descent (SGD)</strong>. This is also known as <strong>SGD with Momentum</strong>. The enhancement is as follows:   </p>
<p><span class="math display" id="eq:equate1140137">\[\begin{align}
\nu^{(t+1)} = \gamma \times \nu^{(t)} + \eta \times  g^{(t)}  \tag{12.147} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140138">\[\begin{align}
\omega^{(t+1)} =\omega^{(t)} - \nu^{(t+1)} \tag{12.148} 
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1973"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1973-1" data-line-number="1">v =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1973-2" data-line-number="2"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1973-3" data-line-number="3">    v =<span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>v <span class="op">+</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span><span class="kw">gradient</span>( ... ) </a>
<a class="sourceLine" id="cb1973-4" data-line-number="4">    w =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>v</a>
<a class="sourceLine" id="cb1973-5" data-line-number="5">}</a></code></pre></div>

<p>Here, we use <strong>momentum</strong> to complement our <strong>learning rate</strong> as a way to apply <strong>moving average</strong> to the gradient. Additionally, it also comes with a <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) hyperparameter, a decaying sum that controls the <strong>momentum</strong>. A starting value to try for <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) is 0.99.</p>
<p><strong>Second</strong>, let us introduce <strong>Adaptive Gradient (AdaGrad)</strong> optimizer formulated by John Duchi, Elad Hazan, and Yoram Singer in <span class="citation">(<a href="bibliography.html#ref-ref1013d">2011</a>)</span>. The enhancement is expressed as follows: </p>
<p><span class="math display" id="eq:equate1140139">\[\begin{align}
\nu^{(t+1)} = \nu^{(t)} + (g^{(t)})^2  \tag{12.149} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140140">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi \times g^{(t)}
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\sqrt{v^{(t+1)} + \epsilon}} \tag{12.150} 
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1974"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1974-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1974-2" data-line-number="2">    g =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1974-3" data-line-number="3">    v  =<span class="st"> </span>v <span class="op">+</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1974-4" data-line-number="4">    pi =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1974-5" data-line-number="5">    w  =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1974-6" data-line-number="6">}</a></code></pre></div>

<p>Similarly here, a <strong>momentum</strong> hyperparameter is enforced as a <strong>decaying rate</strong> denoted by the <strong>nu</strong> (<span class="math inline">\(\nu\)</span>) symbol to control the <strong>Learning Rate</strong> (<span class="math inline">\(\eta\)</span>). It should be apparent that if the <strong>denominator</strong> is smaller, it makes the <strong>Learning Rate</strong> larger (thus more aggressively fast). Therefore, there is a risk of <strong>overshooting</strong>. On the other hand, the further into the iteration we get, the smaller and slower the learning rate becomes.</p>
<p>Note that the <strong>epsilon</strong> (<span class="math inline">\(\epsilon\)</span>) symbol takes an extremely small number that exists only to prevent the division from zero, e.g., <span class="math inline">\(\epsilon =\)</span> 1e-10.</p>
<p><strong>Third</strong>, let us introduce the <strong>Root Mean Square Propagation (RMSProp)</strong> optimizer lectured by Tijmen Tieleman and Geoffrey Hinton in <span class="citation">(<a href="bibliography.html#ref-ref1022t">2012</a>)</span>. The enhancement is expressed as follows: </p>
<p><span class="math display" id="eq:equate1140141">\[\begin{align}
\nu^{(t+1)} = \beta \times \nu^{(t)} + (1 - \beta) (g^{(t)})^2   \tag{12.151} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140142">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi\ \times g^{(t)}
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\sqrt{v^{(t+1)} + \epsilon}}   \tag{12.152} 
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1975"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1975-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1975-2" data-line-number="2">    g  =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1975-3" data-line-number="3">    v  =<span class="st"> </span>B <span class="op">*</span><span class="st"> </span>v <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B) <span class="op">*</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1975-4" data-line-number="4">    pi =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1975-5" data-line-number="5">    w  =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1975-6" data-line-number="6">}</a></code></pre></div>

<p><strong>RMSProp</strong> is a variant of <strong>AdaGrad</strong>. The presence of the <strong>Beta</strong> (<span class="math inline">\(\beta\)</span>) hyperparameter is to control the decay growth of the <strong>denominator</strong> in the case the <strong>Decay Rate</strong> grows exponentially.</p>
<p><strong>Fourth</strong>, let us introduce the <strong>Adaptive Delta (AdaDelta)</strong> optimizer formulated by Matthew Zeiler in <span class="citation">(<a href="bibliography.html#ref-ref1011m">2012</a>)</span>. This optimizer calculates <strong>Root Means Square (RMS)</strong>. The enhancement is expressed as follows: </p>
<p><span class="math display" id="eq:equate1140143">\[\begin{align}
s^{(t+1)}  = \rho  \times s^{(t)}   + (1 - \rho)(g^{(t+1)})^2
\ \ \ \ \ where\ s = \mathbb{E}[g^{(t)})^2] \tag{12.153} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140144">\[\begin{align}
\pi = \frac{\text{RMS}[\Delta \omega^{(t)}]^{(t)}}{\text{RMS}[g^{(t)}]^{(t+1)}}  g^{(t)} 
\ \ \ \ \ \ \ \ \ where\ \ \ \ \text{RMS}[g^{(t)}]^{(t+1)} = \sqrt{s^{(t+1)} + \epsilon} \tag{12.154} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140145">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi   \tag{12.155} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140146">\[\begin{align}
\Delta \omega^{(t+1)} = \rho \times \Delta \omega^{(t)} + ( 1 - \rho) \times  \pi^2 \tag{12.156} 
\end{align}\]</span></p>
<p>where the <strong>rho</strong> (<span class="math inline">\(\rho\)</span>) symbol represents the <strong>Decay Rate</strong>. A good starting point to try for the empirical analysis is a value of 0.90.</p>
<p>The corresponding implementation is as such:</p>

<div class="sourceCode" id="cb1976"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1976-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1976-2" data-line-number="2">    g      =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1976-3" data-line-number="3">    s      =<span class="st"> </span>rho <span class="op">*</span><span class="st"> </span>s <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rho) <span class="op">*</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1976-4" data-line-number="4">    pi     =<span class="st"> </span><span class="kw">sqrt</span>(delta <span class="op">+</span><span class="st"> </span>eps) <span class="op">*</span><span class="st"> </span>( g <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(s <span class="op">+</span><span class="st"> </span>eps) )</a>
<a class="sourceLine" id="cb1976-5" data-line-number="5">    w      =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi  </a>
<a class="sourceLine" id="cb1976-6" data-line-number="6">    delta  =<span class="st"> </span>rho <span class="op">*</span><span class="st"> </span>delta <span class="op">+</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rho) <span class="op">*</span><span class="st"> </span>pi<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1976-7" data-line-number="7">}</a></code></pre></div>

<p>Note that <strong>s</strong> carries the average of the first moment of the gradient, and <span class="math inline">\(\Delta \omega\)</span> carries the average of the second moment.</p>
<p><strong>Fifth</strong>, let us introduce <strong>Adaptive Moment Estimation (Adam)</strong> optimizer formulated by Diederik P. Kingma and Jimmy Lei Ba in <span class="citation">(<a href="bibliography.html#ref-ref1003d">2015</a>)</span>. The enhancement is expressed as follows: </p>
<p><span class="math display" id="eq:equate1140147">\[\begin{align}
\tau ^{(t+1)} = \beta_1 \times \tau^{(t)} + (1 - \beta_1) g^{(t)} \tag{12.157} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140148">\[\begin{align}
\nu^{(t+1)} = \beta_2 \times \nu^{(t)}+ (1 - \beta_2) (g^{(t)})^2  \tag{12.158} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140149">\[\begin{align}
\hat{\tau} = \frac{\tau^{(t+1)}}{1 - {(\beta_1)}^t}
\ \ \ \ \ \ \ \ \ \ 
\hat{\nu} = \frac{\nu^{(t+1)}}{1 - {(\beta_2)}^t} \tag{12.159} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140150">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi\ \times \hat{\tau}
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\sqrt{\hat{v} + \epsilon}} \tag{12.160} 
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1977"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1977-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1977-2" data-line-number="2">    g      =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1977-3" data-line-number="3">    r      =<span class="st"> </span>B1 <span class="op">*</span><span class="st"> </span>r <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1) <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1977-4" data-line-number="4">    v      =<span class="st"> </span>B2 <span class="op">*</span><span class="st"> </span>v <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B2) <span class="op">*</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1977-5" data-line-number="5">    r.hat  =<span class="st"> </span>r <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1<span class="op">^</span>t)  </a>
<a class="sourceLine" id="cb1977-6" data-line-number="6">    v.hat  =<span class="st"> </span>v <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B2<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1977-7" data-line-number="7">    pi     =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v.hat <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1977-8" data-line-number="8">    w      =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>r.hat</a>
<a class="sourceLine" id="cb1977-9" data-line-number="9">}</a></code></pre></div>

<p><strong>Adam</strong> is one of the popular optimizers used in <strong>DNN</strong>. Apart from using the same concept of moving averages as that of <strong>SGD with Momentum</strong>, here, we use two <strong>moments</strong> in the form of the <strong>rho</strong> (<span class="math inline">\(\rho\)</span>) and <strong>nu</strong> (<span class="math inline">\(\nu\)</span>) symbols. The former is a moving average, and the latter is an exponential moving average for the respective gradients, along with their respective <strong>bias corrections</strong> denoted by <strong>rho-hat</strong> (<span class="math inline">\(\hat{\rho}\)</span>) and <strong>nu-hat</strong> (<span class="math inline">\(\hat{\nu}\)</span>) symbols. Additionally, the moving averages are controlled by two other extra hyperparameters as <strong>Decay Rate</strong> for the moments in the form of the <strong>beta</strong> (<span class="math inline">\(\beta\)</span>) symbols. Empirically and in typical practice, the (<span class="math inline">\(\beta\)</span>) values default to 0.90 and 0.999, respectively, with a learning rate (<span class="math inline">\(\eta\)</span>) of 0.001.</p>
<p><strong>Sixth</strong>, let us introduce a variant of <strong>Adam</strong> called <strong>AMSGrad</strong> optimizer formulated by Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar in <span class="citation">(<a href="bibliography.html#ref-ref1014s">2018</a>)</span>. The enhancement is expressed as follows: </p>
<p><span class="math display" id="eq:equate1140151">\[\begin{align}
\tau ^{(t+1)} = \beta_1 \times \tau^{(t)} + (1 - \beta_1) g^{(t)} \tag{12.161} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140152">\[\begin{align}
\nu^{(t+1)} = \beta_2 \times \nu^{(t)}+ (1 - \beta_2) (g^{(t)})^2  \tag{12.162} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140153">\[\begin{align}
\hat{\nu}^{(t+1)} = max \left(\nu^{(t+1)} , \hat{\nu}^{(t)} \right) \tag{12.163} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140154">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi\ \times \tau^{(t+1)}
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\sqrt{\hat{v}^{(t+1)} + \epsilon}} \tag{12.164} 
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1978"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1978-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1978-2" data-line-number="2">    g     =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1978-3" data-line-number="3">    r     =<span class="st"> </span>B1 <span class="op">*</span><span class="st"> </span>r <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1) <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1978-4" data-line-number="4">    v     =<span class="st"> </span>B2 <span class="op">*</span><span class="st"> </span>v <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B2) <span class="op">*</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1978-5" data-line-number="5">    v.hat =<span class="st"> </span><span class="kw">max</span>(v, v.hat)</a>
<a class="sourceLine" id="cb1978-6" data-line-number="6">    pi    =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v.hat <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1978-7" data-line-number="7">    w     =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb1978-8" data-line-number="8">}</a></code></pre></div>

<p>Like <strong>Adam</strong>, two <strong>moments</strong> are also used by <strong>AMSGrad</strong> for <strong>moving averages</strong>; however, unlike <strong>Adam</strong>, the <strong>bias corrections</strong> are eliminated. In their place, the maximum between current and past gradients is considered for an update, replacing the original formulation of <strong>nu-hat</strong> (<span class="math inline">\(\hat{\nu}\)</span>).</p>
<p><strong>Seventh</strong>, let us introduce a variant of <strong>Adam</strong> called <strong>Adaptive Max Pooling (AdaMax)</strong> optimizer, also formulated by Diederik P. Kingma and Jimmy Lei Ba in the same work in <span class="citation">(<a href="bibliography.html#ref-ref1003d">2015</a>)</span>. The enhancement is expressed as follows: </p>
<p><span class="math display" id="eq:equate1140155">\[\begin{align}
\tau ^{(t+1)} = \beta_1 \times \tau^{(t)} + (1 - \beta_1) g^{(t)}  \tag{12.165} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140156">\[\begin{align}
\nu^{(t+1)} =\max\left(\beta_2 \times \nu^{(t)}, |g^{(t)}|\right)   \tag{12.166} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140157">\[\begin{align}
\hat{\tau}^{(t+1)} = \frac{\tau^{(t+1)}}{1 - {(\beta_1)}^t} \tag{12.167} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140158">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi\ \times \hat{\tau}^{(t+1)}
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\nu^{(t+1)} + \epsilon } \tag{12.168} 
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1979"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1979-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1979-2" data-line-number="2">    g     =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1979-3" data-line-number="3">    r     =<span class="st"> </span>B1 <span class="op">*</span><span class="st"> </span>r <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1) <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1979-4" data-line-number="4">    v     =<span class="st"> </span><span class="kw">max</span>(B<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>v, <span class="kw">abs</span>(g) )</a>
<a class="sourceLine" id="cb1979-5" data-line-number="5">    r.hat =<span class="st"> </span>r <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1979-6" data-line-number="6">    pi    =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1979-7" data-line-number="7">    w     =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>r.hat</a>
<a class="sourceLine" id="cb1979-8" data-line-number="8">}</a></code></pre></div>

<p>While <strong>Adam</strong> optimization uses <strong>L2-norm</strong> (euclidean), the <strong>AdaMax</strong> optimization uses <strong>max-norm</strong>, also called <strong>infinity norm</strong>. This is apparent in the equation used. Empirically, and in common practice, the (<span class="math inline">\(\beta\)</span>) values also default to 0.90 and 0.999 respectively with a learning rate (<span class="math inline">\(\eta\)</span>) of 0.001.</p>
<p><strong>Eight</strong>, let us introduce <strong>Nesterov-Accelerated Adaptive Moment (Nadam)</strong> optimizer formulated by Timothy Dozat in <span class="citation">(<a href="bibliography.html#ref-ref1057t">2016</a>)</span>. The enhancement is expressed as follows:  </p>
<p><span class="math display" id="eq:equate1140159">\[\begin{align}
\tau ^{(t+1)} = \beta_1 \times \tau^{(t)} + (1 - \beta_1) g^{(t)} \tag{12.169} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140160">\[\begin{align}
\nu^{(t+1)} = \beta_2 \times \nu^{(t)}+ (1 - \beta_2) (g^{(t)})^2   \tag{12.170} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140161">\[\begin{align}
\hat{\tau}^{(t+1)} = \frac{\tau^{(t+1)}}{1 - {(\beta_1)}^t} + 
\mathbf{\frac{(1 - \beta_1) \times (g)^{(t)}}{1 - {(\beta_1)}^t}}
\ \ \ \ \ \ \ \ \ \ 
\hat{\nu}^{(t+1)} = \frac{\nu^{(t+1)}}{1 - {(\beta_2)}^t}  \tag{12.171} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140162">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi\ \times \hat{\tau}^{(t+1)} 
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\sqrt{\hat{v}^{(t+1)} + \epsilon}} \tag{12.172} 
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1980"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1980-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1980-2" data-line-number="2">    g      =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1980-3" data-line-number="3">    r      =<span class="st"> </span>B1 <span class="op">*</span><span class="st"> </span>r <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1) <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1980-4" data-line-number="4">    v      =<span class="st"> </span>B2 <span class="op">*</span><span class="st"> </span>v <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B2) <span class="op">*</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1980-5" data-line-number="5">    r.hat  =<span class="st"> </span>r <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1<span class="op">^</span>t) <span class="op">+</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1) <span class="op">*</span><span class="st"> </span>g <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1980-6" data-line-number="6">    v.hat  =<span class="st"> </span>v <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B2<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1980-7" data-line-number="7">    pi     =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v.hat <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb1980-8" data-line-number="8">    w      =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>r.hat</a>
<a class="sourceLine" id="cb1980-9" data-line-number="9">}</a></code></pre></div>

<p><strong>Nadam</strong> optimizer expresses identical equations as <strong>Adam</strong> with one slight change to the <strong>rho-hat</strong> (<span class="math inline">\(\hat{\rho}\)</span>) - the addition of <strong>Nesterov Momentum</strong>. As stated in the title of Dozat’s paper <span class="citation">(<a href="bibliography.html#ref-ref1057t">2016</a>)</span>, the idea is to incorporate <strong>Nesterov Momentum</strong> into <strong>Adam</strong>.</p>
<p><strong>Finally</strong>, apart from the eight optimizers we discussed, other strategies can influence <strong>SGD</strong> learning speed. Let us present two of them.</p>
<p><strong>First</strong>, one strategy that allows a non-static learning rate is what we call <strong>Step Decay Schedule</strong> <span class="citation">(Rong Ge et al. <a href="bibliography.html#ref-ref1070r">2019</a>)</span>. Instead of just being static, the idea here is to reduce progressively (or decay) the <strong>learning rate</strong> (e.g., by half) at a given interval up to the <strong>epoch</strong> limit. An example formula for the <strong>learning rate</strong> is: </p>
<p><span class="math display" id="eq:equate1140163">\[\begin{align}
\eta^{(t+1)} = \eta^{(initial)} \times DF^{floor\left(\frac{t}{step size}\right)} \tag{12.173} 
\end{align}\]</span></p>
<p>where <strong>t</strong> is the epoch and <span class="math inline">\(\mathbf{\eta}\)</span> is the learning rate.</p>
<p>We cover this more on <strong>CNN</strong>.</p>
<p><strong>Second</strong>, another strategy that allows a non-static learning rate is what we call <strong>Cyclical Learning Rate</strong> <span class="citation">(Leslie N. Smith <a href="bibliography.html#ref-ref1082l">2017</a>)</span>. Instead of a <strong>step-wise decay</strong>, the idea is to vary the value of the <strong>learning rate</strong> within a <strong>reasonable</strong> minimum and maximum boundary. We define a <strong>cycle</strong> as an interval based on certain choices, e.g., at fix interval or every batch update. We then oscillate the learning rate by allowing it to increase up to the maximum boundary monotonically, and then at the next cycle, we monotonically decrease up to a minimum boundary. </p>
<p><strong>Lastly</strong>, we now come down to a more general perspective of <strong>learning rate</strong>. The onus is upon us to find the most optimal learning rate during training. We know that it takes trial and error to find a value for our learning rate if we use a static value throughout the training. Alternatively, we can resort to a more adaptive mechanism by performing <strong>simulated annealing</strong> (or in the case of <strong>DNN</strong>, we call it <strong>learning rate annealing</strong>) - that is to say, not only do we control the speed of the decay but also at which stage or cycle in training to vary the decay.</p>
<p>We leave readers to investigate other proposals for optimizing the <strong>learning rate</strong>.</p>
<p>In terms of implementation, we also leave readers to modify our <strong>backward.pass(.)</strong> function and experiment on the different optimizers recently discussed.</p>
</div>
<div id="interpretability" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.15</span> Interpretability<a href="12.3-multi-layer-perceptron-mlp.html#interpretability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The assumption, particularly in our early examples in <strong>DNN</strong>, is that our two output neurons in the output layer are mutually exclusive (e.g., linearly independent). Particularly for the output layer, the first output neuron produces a sigmoid value ranging between 0 and 1, irrespective of values from the other output neuron. It is essential to note here that if we contrive our samples so that we have a carefully crafted input and target output as shown in Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:interpretability">12.21</a> using a modified version of our <strong>get.synthetic.samples(.)</strong> function, we hope to see predictable output patterns as a way to validate our <strong>DNN</strong>. Indeed, Figure <a href="12.3-multi-layer-perceptron-mlp.html#fig:interpretability">12.21</a> shows that if we adjust the <strong>epoch</strong> count or the new <strong>X</strong> values, the predicted output expectedly follows the intended pattern.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:interpretability"></span>
<img src="interpretability.png" alt="Interpretability of Predicted Output" width="90%" />
<p class="caption">
Figure 12.21: Interpretability of Predicted Output
</p>
</div>
<p>If we can simulate a synthetic dataset (both input and output) in a very controlled fashion with precise and unique patterns, then we should be able to also yield precise and unique patterns for inference, no matter if we also induce a synthetic perturbation and a set of outliers. However, that is because we understand our data - we crafted it. Likewise, our implementation of <strong>DNN</strong> using <strong>my.MLP(.)</strong> makes it possible for us to tailor the complexity of our network (e.g., number of layers, number of neurons). We can make it in such a way as to avoid the perception of a <strong>black-box</strong> model. However, the context and its interpretation are essential. Somehow, our <strong>synthetic</strong> data has to simulate a real-world process that produces our samples naturally in the first place, like a toss of a coin. As easy as it may sound, real-world data, in actuality, is a lot more diluted with natural patterns and noise that we may not be able to simulate. We can contrive <strong>synthetic</strong> samples by any holistic and stochastic means, but it requires domain expertise to qualify a comprehensive sample and representative of a domain’s entire population sample.</p>
<p>Nevertheless, the point to take here is that as long as there is indeed a pattern, no matter how inconspicuous it may be, upon which we can infer, then <strong>DNN</strong> can train. Our <strong>mini.batch(.)</strong> function can provide such pattern that may simulate a <strong>gaussian</strong> distribution. This is useful for simple empirical purposes to allow our <strong>DNN</strong> to yield some expected pattern for training and testing, thereby having the ability to quantify the correctness and mistakes.</p>
<p>However, we may also have to account for one intended to prevent <strong>DNN</strong> from the ability to train rather than make mistakes even. One, in particular, that may offset the ability for <strong>DNN</strong> to train is the so-called <strong>Adversarial Sample</strong>, which has no pattern whatsoever. Another challenge that a <strong>Neural Network</strong> finds difficult to train is the so-called <strong>Bongard Problem</strong> which roots in puzzle-based samples.</p>
<p>Lastly, we also have to account for the data types. For example, data with spatial or sequential relationships calls for a different kind of <strong>Neural Network</strong> which we cover in the next section.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="12.2-adaptive-linear-neuron-adaline.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="12.4-convolutional-neural-network-cnn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
