<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.8 Approximating Polynomial Functions by Smoothing | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="3.8 Approximating Polynomial Functions by Smoothing | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.8 Approximating Polynomial Functions by Smoothing | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3.7-polynomialinterpolation.html"/>
<link rel="next" href="3.9-polynomial-optimization.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i>Caveat</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="polynomialsmoothing" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.8</span> Approximating Polynomial Functions by Smoothing<a href="3.8-polynomialsmoothing.html#polynomialsmoothing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have partially covered polynomial smoothing under the <strong>P-spline regression</strong> section. In this section, we explain polynomial smoothing methods. The main focus of our discussion is around LOWESS and LOESS; however, it helps to have a prior understanding of concepts that complement them. </p>
<div id="bin-smoothing" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.8.1</span> Bin Smoothing <a href="3.8-polynomialsmoothing.html#bin-smoothing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to <strong>piecewise techniques</strong> used in <strong>B-spline interpolation</strong>, the idea of <strong>bin smoothing</strong> is to discretize data into <strong>buckets</strong> called <strong>bins</strong>. There are different methods to perform <strong>Bin Smoothing</strong>.</p>
<p><strong>First</strong>, let us start with the following polynomial data:</p>

<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb118-2" data-line-number="2">sample_size =<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb118-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">30</span> </a>
<a class="sourceLine" id="cb118-4" data-line-number="4">sample.poly =<span class="st"> </span><span class="kw">poly</span>(<span class="dv">1</span><span class="op">:</span>sample_size, <span class="dt">degree=</span><span class="dv">3</span>, <span class="dt">simple=</span><span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb118-5" data-line-number="5">data =<span class="st"> </span>sample.poly[,<span class="dv">3</span>] <span class="op">+</span><span class="st"> </span>e <span class="co"># add Gauss. residual using 3rd degree poly</span></a>
<a class="sourceLine" id="cb118-6" data-line-number="6"><span class="kw">names</span>(data ) =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, sample_size)</a>
<a class="sourceLine" id="cb118-7" data-line-number="7"><span class="kw">summary</span>(data)</a></code></pre></div>
<pre><code>##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -0.2326270 -0.0663426  0.0068862 -0.0001126  0.0624455  0.2035865</code></pre>

<p><strong>Second</strong>, choose a window size.</p>
<p>The following formulas are available to determine the window size:</p>
<p><span class="math display">\[\begin{align*}
W_{size}{}&amp;= 1 + 3.322 log N\ \ &amp; \text{Sturge&#39;s Rule} \\
W_{size} &amp;= 2(IQR)n^{-1/3}\ \ &amp; \text{Freedman-Diaconis&#39; Rule} \\
W_{size} &amp;= 3.49\sigma n^{-1/3}\ \ &amp; \text{Scott&#39;s Rule} \\
W_{size} &amp;= (\text{sample size})^{1/3} \times 2\ \ &amp; \text{Rice&#39;s Rule} 
\end{align*}\]</span>
 
 </p>
<p>Here, we use <strong>Riceâs rule</strong>.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1">(<span class="dt">window_size =</span> sample_size<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)<span class="op">*</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 11.69607</code></pre>
<p><strong>Third</strong>, choose a partitioning method.</p>
<p><strong>Partitioning methods</strong>:</p>
<ul>
<li>By equal <strong>width (bandwidth)</strong> - note here that <strong>width</strong> is an interval and also can be called <strong>window size</strong> or <strong>span</strong>. This works best for ordered (sorted data); thus, it may not be ideal for the smoothing we need. Nonetheless, here is an example naive implementation of binning by width.</li>
</ul>

<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1">sample.data =<span class="kw">c</span> (<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">12</span>,<span class="dv">18</span>,<span class="dv">20</span>,<span class="dv">23</span>,<span class="dv">50</span>,<span class="dv">61</span>,<span class="dv">70</span>)</a>
<a class="sourceLine" id="cb122-2" data-line-number="2">bin_count =<span class="st"> </span><span class="kw">floor</span>(<span class="kw">length</span>(sample.data)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)<span class="op">*</span><span class="dv">2</span>) <span class="co"># Rice&#39;s rule</span></a>
<a class="sourceLine" id="cb122-3" data-line-number="3">smallest =<span class="st"> </span><span class="kw">min</span>(sample.data); largest =<span class="st"> </span><span class="kw">max</span>(sample.data)</a>
<a class="sourceLine" id="cb122-4" data-line-number="4">interval =<span class="st">  </span><span class="kw">ceiling</span>((largest <span class="op">-</span><span class="st"> </span>smallest) <span class="op">/</span><span class="st"> </span>bin_count)</a>
<a class="sourceLine" id="cb122-5" data-line-number="5">ordered.data =<span class="st"> </span><span class="kw">sort</span>(sample.data, <span class="dt">decreasing=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb122-6" data-line-number="6">start =<span class="st"> </span>smallest</a>
<a class="sourceLine" id="cb122-7" data-line-number="7">bins =<span class="st"> </span>ordered.data</a>
<a class="sourceLine" id="cb122-8" data-line-number="8">intervals =<span class="st"> </span>ordered.data</a>
<a class="sourceLine" id="cb122-9" data-line-number="9"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>bin_count) {</a>
<a class="sourceLine" id="cb122-10" data-line-number="10">   end =<span class="st"> </span>start <span class="op">+</span><span class="st"> </span>interval</a>
<a class="sourceLine" id="cb122-11" data-line-number="11">   idx =<span class="st"> </span><span class="kw">which</span>(ordered.data <span class="op">&gt;=</span><span class="st"> </span>start  <span class="op">&amp;</span><span class="st"> </span>ordered.data <span class="op">&lt;=</span><span class="st"> </span>end)</a>
<a class="sourceLine" id="cb122-12" data-line-number="12">   intervals[idx] =<span class="st">   </span><span class="kw">paste0</span>(start,<span class="st">&quot;-&quot;</span>,end)</a>
<a class="sourceLine" id="cb122-13" data-line-number="13">   bins[idx] =<span class="st"> </span>i</a>
<a class="sourceLine" id="cb122-14" data-line-number="14">   start =<span class="st"> </span>end <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb122-15" data-line-number="15">}</a>
<a class="sourceLine" id="cb122-16" data-line-number="16">binned.data =<span class="st"> </span><span class="kw">rbind</span>(intervals , ordered.data)</a>
<a class="sourceLine" id="cb122-17" data-line-number="17">binned.data =<span class="st"> </span><span class="kw">rbind</span>(binned.data , bins)</a>
<a class="sourceLine" id="cb122-18" data-line-number="18">binned.data[,<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>]</a></code></pre></div>
<pre><code>##              [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]    [,8]   
## intervals    &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;22-39&quot; &quot;40-57&quot;
## ordered.data &quot;4&quot;    &quot;8&quot;    &quot;9&quot;    &quot;12&quot;   &quot;18&quot;   &quot;20&quot;   &quot;23&quot;    &quot;50&quot;   
## bins         &quot;1&quot;    &quot;1&quot;    &quot;1&quot;    &quot;1&quot;    &quot;1&quot;    &quot;1&quot;    &quot;2&quot;     &quot;3&quot;</code></pre>

<ul>
<li>By equal <strong>depth (frequency)</strong> - note that each window contains an equal number of data points. The idea is to slice the dataset into an equal number of data points. Let us use this partition for our illustration.</li>
</ul>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1">bin_count =<span class="st"> </span><span class="kw">ceiling</span>(<span class="kw">length</span>(data) <span class="op">/</span><span class="st"> </span>window_size)</a>
<a class="sourceLine" id="cb124-2" data-line-number="2">bins =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>bin_count, <span class="kw">rep</span>(window_size, bin_count))[<span class="dv">1</span><span class="op">:</span>sample_size]</a>
<a class="sourceLine" id="cb124-3" data-line-number="3">binned.data =<span class="st"> </span><span class="kw">rbind</span>(data, bins)</a>
<a class="sourceLine" id="cb124-4" data-line-number="4">binned.data[,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</a></code></pre></div>
<pre><code>##               1          2          3          4         5
## data -0.1689877 -0.1605539 -0.1965348 -0.1872164 -0.232627
## bins  1.0000000  1.0000000  1.0000000  1.0000000  1.000000</code></pre>
<p><strong>Fourth</strong>, choose a smoothing method.</p>
<p>Before that, let us add three rows to accommodate the three smoothing methods.</p>

<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" data-line-number="1">mean =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,sample_size)</a>
<a class="sourceLine" id="cb126-2" data-line-number="2">median =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,sample_size)</a>
<a class="sourceLine" id="cb126-3" data-line-number="3">boundary =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,sample_size)</a>
<a class="sourceLine" id="cb126-4" data-line-number="4">smooth.data =<span class="st"> </span><span class="kw">rbind</span>(binned.data, mean )</a>
<a class="sourceLine" id="cb126-5" data-line-number="5">smooth.data =<span class="st"> </span><span class="kw">rbind</span>(smooth.data, median )</a>
<a class="sourceLine" id="cb126-6" data-line-number="6">smooth.data =<span class="st"> </span><span class="kw">rbind</span>(smooth.data, boundary )</a>
<a class="sourceLine" id="cb126-7" data-line-number="7">smooth.data[,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</a></code></pre></div>
<pre><code>##                   1          2          3          4         5
## data     -0.1689877 -0.1605539 -0.1965348 -0.1872164 -0.232627
## bins      1.0000000  1.0000000  1.0000000  1.0000000  1.000000
## mean      0.0000000  0.0000000  0.0000000  0.0000000  0.000000
## median    0.0000000  0.0000000  0.0000000  0.0000000  0.000000
## boundary  0.0000000  0.0000000  0.0000000  0.0000000  0.000000</code></pre>

<p><strong>Smoothing methods</strong>:</p>
<ul>
<li>By median, mean - values in a bin are replaced with the mean (or median) of the bin.</li>
</ul>

<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" data-line-number="1">bin.means  =<span class="st"> </span><span class="kw">aggregate</span>(binned.data[<span class="dv">1</span>,], <span class="dt">by=</span><span class="kw">list</span>(binned.data[<span class="dv">2</span>,]), mean)</a>
<a class="sourceLine" id="cb128-2" data-line-number="2">bin.median =<span class="st"> </span><span class="kw">aggregate</span>(binned.data[<span class="dv">1</span>,], <span class="dt">by=</span><span class="kw">list</span>(binned.data[<span class="dv">2</span>,]), </a>
<a class="sourceLine" id="cb128-3" data-line-number="3">                       median)</a></code></pre></div>

<ul>
<li>By boundary - observations in a bin are replaced with the min or max value of the bin, depending on which boundary is closest.</li>
</ul>

<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb129-1" data-line-number="1">boundary &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb129-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb129-3" data-line-number="3">  a =<span class="st"> </span><span class="kw">min</span>(x);  b =<span class="st"> </span><span class="kw">max</span>(x)</a>
<a class="sourceLine" id="cb129-4" data-line-number="4">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb129-5" data-line-number="5">    dist_a =<span class="st"> </span><span class="kw">abs</span>(a <span class="op">-</span><span class="st"> </span>x[i])</a>
<a class="sourceLine" id="cb129-6" data-line-number="6">    dist_b =<span class="st"> </span><span class="kw">abs</span>(b <span class="op">-</span><span class="st"> </span>x[i])</a>
<a class="sourceLine" id="cb129-7" data-line-number="7">    <span class="cf">if</span> (dist_a <span class="op">&lt;</span><span class="st"> </span>dist_b ) { x[i] =<span class="st"> </span>a } <span class="cf">else</span> {x[i] =<span class="st"> </span>b}</a>
<a class="sourceLine" id="cb129-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb129-9" data-line-number="9">  x</a>
<a class="sourceLine" id="cb129-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb129-11" data-line-number="11">bin.boundary =<span class="st"> </span><span class="kw">aggregate</span>(binned.data[<span class="dv">1</span>,], <span class="dt">by=</span><span class="kw">list</span>(binned.data[<span class="dv">2</span>,]), </a>
<a class="sourceLine" id="cb129-12" data-line-number="12">                         boundary)</a></code></pre></div>

<p><strong>Fifth</strong>, populate bins with new values.</p>

<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" data-line-number="1"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>bin_count) {</a>
<a class="sourceLine" id="cb130-2" data-line-number="2">  idx =<span class="st"> </span><span class="kw">which</span>(smooth.data[<span class="dv">2</span>,] <span class="op">==</span><span class="st"> </span>i)</a>
<a class="sourceLine" id="cb130-3" data-line-number="3">  smooth.data[<span class="dv">3</span>, idx] =<span class="st"> </span>bin.means[i,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb130-4" data-line-number="4">  smooth.data[<span class="dv">4</span>, idx] =<span class="st"> </span>bin.median[i,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb130-5" data-line-number="5">  smooth.data[<span class="dv">5</span>, idx] =<span class="st"> </span>bin.boundary[i,<span class="dv">2</span>][[<span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb130-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb130-7" data-line-number="7">smooth.data[,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</a></code></pre></div>
<pre><code>##                   1          2          3          4          5
## data     -0.1689877 -0.1605539 -0.1965348 -0.1872164 -0.2326270
## bins      1.0000000  1.0000000  1.0000000  1.0000000  1.0000000
## mean     -0.1366121 -0.1366121 -0.1366121 -0.1366121 -0.1366121
## median   -0.1182712 -0.1182712 -0.1182712 -0.1182712 -0.1182712
## boundary -0.2326270 -0.2326270 -0.2326270 -0.2326270 -0.2326270</code></pre>

<p><strong>Sixth</strong>, create a separate curve line for the groups (use the average).</p>

<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" data-line-number="1">group.means =<span class="st"> </span><span class="kw">length</span>(bin.means[,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb132-2" data-line-number="2">curve.fit =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb132-3" data-line-number="3">location =<span class="st"> </span><span class="dv">0</span>; step =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb132-4" data-line-number="4"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(group.means)) {</a>
<a class="sourceLine" id="cb132-5" data-line-number="5">   idx =<span class="st"> </span><span class="kw">which</span>(smooth.data[<span class="dv">2</span>,] <span class="op">==</span><span class="st"> </span>i)</a>
<a class="sourceLine" id="cb132-6" data-line-number="6">   bin  =<span class="st"> </span>smooth.data[<span class="dv">3</span>, idx]</a>
<a class="sourceLine" id="cb132-7" data-line-number="7">   center =<span class="st"> </span><span class="kw">round</span>( <span class="kw">length</span>(bin) <span class="op">/</span><span class="st"> </span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb132-8" data-line-number="8">   location =<span class="st"> </span>step <span class="op">+</span><span class="st"> </span>center</a>
<a class="sourceLine" id="cb132-9" data-line-number="9">   step =<span class="st"> </span>step <span class="op">+</span><span class="st"> </span><span class="kw">length</span>(bin)</a>
<a class="sourceLine" id="cb132-10" data-line-number="10">   curve.fit =<span class="st"> </span><span class="kw">cbind</span>(curve.fit, location)</a>
<a class="sourceLine" id="cb132-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb132-12" data-line-number="12">smooth.fit =<span class="st"> </span><span class="kw">as.numeric</span>(curve.fit)</a></code></pre></div>

<p><strong>Finally</strong>, let us plot. See <a href="3.8-polynomialsmoothing.html#fig:binsmoothing">3.31</a>.</p>

<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">1</span>,sample_size), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="fl">0.3</span>,<span class="fl">0.2</span>), </a>
<a class="sourceLine" id="cb133-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>,</a>
<a class="sourceLine" id="cb133-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Bin Smoothing&quot;</span>)</a>
<a class="sourceLine" id="cb133-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb133-5" data-line-number="5"><span class="kw">points</span>(data, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>)</a>
<a class="sourceLine" id="cb133-6" data-line-number="6"><span class="co">#lines(smooth.data[4,], col=&quot;magenta&quot;, pch=16) # median</span></a>
<a class="sourceLine" id="cb133-7" data-line-number="7"><span class="kw">lines</span>(smooth.data[<span class="dv">5</span>,], <span class="dt">col=</span><span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>) <span class="co"># boundary</span></a>
<a class="sourceLine" id="cb133-8" data-line-number="8"><span class="kw">lines</span>(smooth.data[<span class="dv">3</span>,], <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">lwd=</span><span class="dv">2</span>) <span class="co"># mean</span></a>
<a class="sourceLine" id="cb133-9" data-line-number="9"><span class="kw">points</span>(smooth.fit, bin.means[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb133-10" data-line-number="10"><span class="kw">lines</span>(smooth.fit, bin.means[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb133-11" data-line-number="11"><span class="kw">legend</span>(<span class="dv">40</span>, <span class="fl">-0.1</span>, </a>
<a class="sourceLine" id="cb133-12" data-line-number="12">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;boundary&quot;</span>, <span class="st">&quot;mean&quot;</span>,  <span class="st">&quot;smooth.fit&quot;</span>),</a>
<a class="sourceLine" id="cb133-13" data-line-number="13">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;dodgerblue&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>),</a>
<a class="sourceLine" id="cb133-14" data-line-number="14">           , <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:binsmoothing"></span>
<img src="embed0020.png" alt="Bin Smoothing" width="70%" />
<p class="caption">
Figure 3.31: Bin Smoothing
</p>
</div>

<p>As an extension to our previous discussion around <strong>polynomial regression</strong> and our recent discussion around <strong>bin smoothing</strong>, here, we introduce two <strong>smoothing techniques</strong>: <strong>LOWESS</strong> and <strong>LOESS</strong>, which use <strong>local weighted regression</strong> techniques <span class="citation">(Cleveland, W. S. et al. <a href="bibliography.html#ref-ref373c">1988</a>)</span>.</p>
<p><strong>LOWESS</strong> and <strong>LOESS</strong> are polynomial smoothers that use locally weighted (linear) functions to fit a <strong>smooth</strong> model to each bin. <strong>LOWESS</strong> stands for <strong>locally weighted scatterplot smoothing</strong> and <strong>LOESS</strong> stands for <strong>locally estimated scatterplot smoothing</strong>.  </p>
<p><strong>LOWESS</strong> uses the following two weighing functions:</p>
<ul>
<li>A <strong>tricube weight function</strong> for width (distance amongst neighboring points) <span class="citation">(Cleveland, W. S. et al. <a href="bibliography.html#ref-ref373c">1988</a>)</span> (See also <strong>Kernel Smoothing</strong> for the tricube kernel):</li>
</ul>
<p><span class="math display" id="eq:eqnnumber704">\[\begin{align}
W_{(width)}(w) = \begin{cases}
\left[1 - |w|^3 \right]^3 &amp; |w| &lt; 1\\
0 &amp; |w| \ge 1
\end{cases}\ \ \ \ \ \ \ \ \
where\ \  
w = w_k(x_i) = \frac{x_i - x_k}{d_k},\ \ \ \tag{3.212}
\end{align}\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{d_k}\)</span> is kth nearest neighbor (kth smallest distance) derived from a list of <strong>x</strong> distances arranged in ascending order, e.g. <span class="math inline">\(d_i = \{ |x_i - x_1|, |x_i - x_2|,|x_i - x_3|,...,|x_i - x_n| \}_{sort}\)</span>.</p>
<p><span class="math display" id="eq:equate1050197">\[\begin{align}
d_k = knn(d_i^{(sort)}, kth),\ \ \ \leftarrow\ \ \ \ \ kth = f\times n \tag{3.213} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>n</strong> is the number of data points</li>
<li><strong>f</strong> is a proportion of <strong>n</strong>.</li>
</ul>
<p>Note that by setting the weight to zero when <span class="math inline">\(|w| \ge 1\)</span>, it eliminates the influence of distant neighbors.</p>
<ul>
<li>An optional <strong>bisquare weight function</strong> for depth (to control the influence of outliers) - this is also called <strong>robust weighing</strong>:</li>
</ul>
<p><span class="math display" id="eq:eqnnumber705">\[\begin{align}
W_{(depth)}(w) = \begin{cases}
\left[1 - |w|^2 \right]^2 &amp; |w| &lt; 1\\
0 &amp; |w| \ge 1
\end{cases} \tag{3.214}
\end{align}\]</span></p>
<p><span class="math display">\[
where\ \ 
w = w_k(x_i) = \frac{e_k}{6 \times median(|e_1|, ...,|e_n|)}
\]</span></p>
<p>The <strong>local weight</strong> becomes:</p>
<p><span class="math display" id="eq:equate1050198">\[\begin{align}
W^{local}_k = W_{(width)}(w_k) W_{(depth)}(w_k)  \tag{3.215} 
\end{align}\]</span></p>
<p>Note that by setting the weight to zero when <span class="math inline">\(|w| \ge 1\)</span>, it eliminates the influence of outliers.</p>
<p>Now, recall our discussion around <strong>OLS</strong> in the <strong>polynomial regression</strong> section.</p>
<p><span class="math display" id="eq:equate1050201" id="eq:equate1050200" id="eq:equate1050199">\[\begin{align}
RSS(\beta) &amp;= \sum_{k=1}^n | y_k - \hat{y}_k |^2  = \sum_{k=1}^n |\epsilon_k|^2  \tag{3.216} \\
\text{1st degree or multivariate} &amp;= \sum_{k=1}^n  \left| y_k - \left(\beta_0 + \sum_{j=1}^m \beta_j x_{j,k} \right) \right| ^2  \tag{3.217} \\
\text{higher degree} &amp;= \sum_{k=1}^n \left| y_k - \left(\beta_0 + \sum_{j=1}^m \beta_j x_k^j \right) \right| ^2 \tag{3.218} 
\end{align}\]</span></p>
<p>where <strong>RSS</strong> is residual sum square.  </p>
<p>Here, we inject the <strong>local weight</strong> into <strong>RSS</strong>. The equation becomes:</p>
<p><span class="math display" id="eq:equate1050204" id="eq:equate1050203" id="eq:equate1050202">\[\begin{align}
WSS(\beta, w_k)  &amp;= \sum_{i=1}^n w_k | y_k - \hat{y}_k |^2  = \sum_{k=1}^n w_k |\epsilon_k|^2  \tag{3.219} \\
\text{1st degree or multivariate} &amp;= \sum_{k=1}^n w_k \left| y_k - \left(\beta_0 + \sum_{j=1}^m \beta_j x_{j,k} \right) \right| ^2  \tag{3.220} \\
\text{higher degree} &amp;= \sum_{k=1}^n w_k \left| y_k - \left(\beta_0 + \sum_{j=1}^m \beta_j x_k^j \right) \right| ^2 \tag{3.221} 
\end{align}\]</span></p>
<p>where <strong>WSS</strong> is weighted residual sum square and <span class="math inline">\(\mathbf{w_k}\)</span> is our local weight, <span class="math inline">\(W^{(local)}_k\)</span>.  </p>
<p>By performing <strong>weighted least squares</strong>, we generate a vector of <strong>coefficients</strong> that are weighted - we call them <strong>beta hats</strong>, <span class="math inline">\(\hat{\beta}\)</span>:</p>
<p>For first degree equations, we derive the coefficients (beta hats) using the following formulation:</p>
<p><span class="math display" id="eq:equate1050205">\[\begin{align}
\hat{\beta}_1 
 =  \frac{n\sum_{k=1}^n{(x_ky_k)} - \sum_{k=1}^n{x_k}\sum_{k=1}^n{y_k}}{n\sum_{k=1}^n{(x_k^2)} - (\sum_{k=1}^n{x_k})^2} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} \tag{3.222} 
\end{align}\]</span></p>
<p>We then modify the <span class="math inline">\(\beta\)</span> formulae to inject the weighted function:</p>
<p><span class="math display" id="eq:equate1050206">\[\begin{align}
\hat{\beta}_1 =  \frac{\sum_{k=1}^n (w_k  x_k y_k) - \bar{x} \cdot \bar{y}\sum_{k=1}^n w_k}
{\sum_{k=1} (w_k x_k^2) - \bar{x}^2 \cdot \sum_{k=1}^n w_k}
 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} \tag{3.223} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display" id="eq:equate1050207">\[\begin{align}
\bar{x} = \frac{\sum_{k=1}^n(w_k x_k)}{\sum_{k=1}^n w_k}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\bar{y} = \frac{\sum_{k=1}^n(w_k y_k)}{\sum_{k=1}^n w_k} \tag{3.224} 
\end{align}\]</span></p>
<p>For multivariate equations and non-linear (e.g., quadratic) equations, recall the use of a Vandermonde matrix - for our matrix equation - under the <strong>polynomial regression</strong> section.</p>
<p><span class="math display" id="eq:equate1050208">\[\begin{align}
\hat{\beta} \approx (A^T \cdot A)^{-1} \cdot A^T \cdot y\ \ \ \ \leftarrow\ \ \ \ \ \ y = \hat{\beta}^TA \tag{3.225} 
\end{align}\]</span></p>
<p>We modify the equation to include the weighted function:</p>
<p><span class="math display" id="eq:equate1050209">\[\begin{align}
\hat{\beta} \approx (A^T \cdot W \cdot A)^{-1} \cdot A^T \cdot  W  \cdot y \tag{3.226} 
\end{align}\]</span></p>
<p>Finally, we can perform local weighted linear regression to our data points.</p>
<p>For <strong>LOWESS</strong>, we use a simple linear equation for regression: <span class="math inline">\(y_k = \beta_0 + \beta_1 x_k\)</span>.</p>
<p><span class="math display" id="eq:equate1050210">\[\begin{align}
WSS(\beta, W_k^{(local)}) = \sum_{k=1}^n W_k^{local}(y_k - (\beta_0 + \beta_1 x_k)) \tag{3.227} 
\end{align}\]</span></p>
<p>For <strong>LOESS</strong>, we also can use a quadratic (parabolic) equation for regression: <span class="math inline">\(y_k = \beta_0 + \beta_1 x_k + \beta_2 x_k^2\)</span>.</p>
<p><span class="math display" id="eq:equate1050211">\[\begin{align}
WSS(\beta, W_k^{(local)}) = \sum_{k=1}^n W_k^{local}(y_k - (\beta_0 + \beta_1 x_k + \beta_2 x_k^2)) \tag{3.228} 
\end{align}\]</span></p>
<p>Note that <strong>LOESS</strong> is a generalized polynomial version of <strong>LOWESS</strong>. For that reason, we use the normal matrix equation.</p>
<p>Let us illustrate:</p>
<p><strong>First</strong>, let us create a distance table using the built-in R function called <strong>dist()</strong>:</p>

<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb134-2" data-line-number="2">sample_size =<span class="st"> </span>n =<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb134-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">30</span> </a>
<a class="sourceLine" id="cb134-4" data-line-number="4">sample.poly =<span class="st"> </span><span class="kw">poly</span>(<span class="dv">1</span><span class="op">:</span>sample_size, <span class="dt">degree=</span><span class="dv">3</span>, <span class="dt">simple=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb134-5" data-line-number="5">y =<span class="st"> </span>sample.poly[,<span class="dv">3</span>] <span class="op">+</span><span class="st"> </span>e <span class="co"># add Gausian residual using 3rd degree poly</span></a>
<a class="sourceLine" id="cb134-6" data-line-number="6">x =<span class="st"> </span><span class="kw">sort</span>( <span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">80</span>), <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb134-7" data-line-number="7">ymin =<span class="st"> </span><span class="kw">min</span>(y); ymax =<span class="st"> </span><span class="kw">max</span>(y); xmax =<span class="st"> </span><span class="kw">max</span>(x)</a>
<a class="sourceLine" id="cb134-8" data-line-number="8">D =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">abs</span>( <span class="kw">dist</span>(x, <span class="dt">upper=</span><span class="ot">TRUE</span>)))</a>
<a class="sourceLine" id="cb134-9" data-line-number="9"><span class="kw">colnames</span>(D) =<span class="st"> </span><span class="kw">paste0</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)), <span class="st">&quot;k&quot;</span>)</a>
<a class="sourceLine" id="cb134-10" data-line-number="10"><span class="kw">rownames</span>(D) =<span class="st"> </span><span class="kw">paste0</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)), <span class="st">&quot;i&quot;</span>)</a>
<a class="sourceLine" id="cb134-11" data-line-number="11">D[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]  <span class="co"># limit display to 10</span></a></code></pre></div>
<pre><code>##     1k 2k 3k 4k 5k 6k 7k 8k 9k 10k
## 1i   0  1  2  4  5  9 11 13 16  17
## 2i   1  0  1  3  4  8 10 12 15  16
## 3i   2  1  0  2  3  7  9 11 14  15
## 4i   4  3  2  0  1  5  7  9 12  13
## 5i   5  4  3  1  0  4  6  8 11  12
## 6i   9  8  7  5  4  0  2  4  7   8
## 7i  11 10  9  7  6  2  0  2  5   6
## 8i  13 12 11  9  8  4  2  0  3   4
## 9i  16 15 14 12 11  7  5  3  0   1
## 10i 17 16 15 13 12  8  6  4  1   0</code></pre>

<p><strong>Second</strong>, now use a fraction number for our range of nearest neighbors:</p>

<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" data-line-number="1">f =<span class="st"> </span><span class="fl">0.40</span></a>
<a class="sourceLine" id="cb136-2" data-line-number="2">n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb136-3" data-line-number="3"><span class="co"># kth nearest neighbor (for our kth smallest distance)</span></a>
<a class="sourceLine" id="cb136-4" data-line-number="4">(<span class="dt">kth =</span> <span class="kw">round</span>( f <span class="op">*</span><span class="st"> </span>n )) </a></code></pre></div>
<pre><code>## [1] 20</code></pre>

<p><strong>Third</strong>, let us compute for <strong>kth</strong> nearest distance:</p>

<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" data-line-number="1">knn &lt;-<span class="st"> </span><span class="cf">function</span>(D, kth) {</a>
<a class="sourceLine" id="cb138-2" data-line-number="2">  dk  =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb138-3" data-line-number="3">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb138-4" data-line-number="4">    sorted_dist =<span class="st"> </span><span class="kw">sort</span>(D[i,], <span class="dt">decreasing=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb138-5" data-line-number="5">    <span class="co">#  kth nearest neighbor (kth smallest distance)</span></a>
<a class="sourceLine" id="cb138-6" data-line-number="6">    dk [i] =<span class="st"> </span>sorted_dist[kth] </a>
<a class="sourceLine" id="cb138-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb138-8" data-line-number="8">  <span class="kw">names</span>(dk) =<span class="st"> </span><span class="kw">paste0</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)), <span class="st">&quot;i&quot;</span>)</a>
<a class="sourceLine" id="cb138-9" data-line-number="9">  dk</a>
<a class="sourceLine" id="cb138-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb138-11" data-line-number="11">(<span class="dt">dk =</span> <span class="kw">knn</span>(D,kth))[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]  <span class="co"># limit display to 10</span></a></code></pre></div>
<pre><code>##  1i  2i  3i  4i  5i  6i  7i  8i  9i 10i 
##  31  30  29  27  26  22  20  18  16  16</code></pre>

<p><strong>Fourth</strong>, compute for the weight (for width):</p>

<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb140-1" data-line-number="1">weight_width &lt;-<span class="st"> </span><span class="cf">function</span>(D, dk) {</a>
<a class="sourceLine" id="cb140-2" data-line-number="2">  w =<span class="st">  </span><span class="kw">abs</span>(D)  <span class="op">/</span><span class="st"> </span>dk</a>
<a class="sourceLine" id="cb140-3" data-line-number="3">  <span class="co"># by assigning 1, this is equivalent to 0 for ( 1 - W^3)^3 if W &gt;=1</span></a>
<a class="sourceLine" id="cb140-4" data-line-number="4">  w[w <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span> </a>
<a class="sourceLine" id="cb140-5" data-line-number="5">  <span class="co"># relaxed version of tricube</span></a>
<a class="sourceLine" id="cb140-6" data-line-number="6">  <span class="co"># without the 70/81 scale</span></a>
<a class="sourceLine" id="cb140-7" data-line-number="7">  (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>w<span class="op">^</span><span class="dv">3</span>)<span class="op">^</span><span class="dv">3</span> <span class="co"># tricube smoother</span></a>
<a class="sourceLine" id="cb140-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb140-9" data-line-number="9"><span class="co"># limit display to 10x10</span></a>
<a class="sourceLine" id="cb140-10" data-line-number="10">(<span class="dt">W =</span> <span class="kw">round</span>( <span class="kw">weight_width</span>(D, dk), <span class="dv">3</span>))[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]  </a></code></pre></div>
<pre><code>##        1k    2k    3k    4k    5k    6k    7k    8k    9k   10k
## 1i  1.000 1.000 0.999 0.994 0.987 0.928 0.872 0.795 0.642 0.582
## 2i  1.000 1.000 1.000 0.997 0.993 0.944 0.893 0.820 0.670 0.610
## 3i  0.999 1.000 1.000 0.999 0.997 0.958 0.913 0.845 0.699 0.640
## 4i  0.990 0.996 0.999 1.000 1.000 0.981 0.949 0.893 0.759 0.701
## 5i  0.979 0.989 0.995 1.000 1.000 0.989 0.964 0.915 0.790 0.733
## 6i  0.808 0.863 0.906 0.965 0.982 1.000 0.998 0.982 0.906 0.863
## 7i  0.579 0.670 0.751 0.877 0.921 0.997 1.000 0.997 0.954 0.921
## 8i  0.242 0.348 0.460 0.670 0.759 0.967 0.996 1.000 0.986 0.967
## 9i  0.000 0.005 0.036 0.193 0.308 0.769 0.911 0.980 1.000 0.999
## 10i 0.000 0.000 0.005 0.100 0.193 0.670 0.850 0.954 0.999 1.000</code></pre>

<p><strong>Fifth</strong>, perform local regression using the computed weights for width (close neighbors). Generate our fitted <strong>y</strong>:</p>

<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb142-1" data-line-number="1">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x), n, <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb142-2" data-line-number="2">B =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb142-3" data-line-number="3">y.hat =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb142-4" data-line-number="4">residual =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb142-5" data-line-number="5"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb142-6" data-line-number="6">  Wk =<span class="st"> </span><span class="kw">as.numeric</span>( W[,k] )  </a>
<a class="sourceLine" id="cb142-7" data-line-number="7">  Wk =<span class="st"> </span><span class="kw">diag</span>(Wk)</a>
<a class="sourceLine" id="cb142-8" data-line-number="8">  <span class="co"># least square</span></a>
<a class="sourceLine" id="cb142-9" data-line-number="9">  B[[k]] =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>Wk <span class="op">%*%</span><span class="st"> </span>A) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>Wk <span class="op">%*%</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb142-10" data-line-number="10">  beta =<span class="st"> </span>B[[k]]</a>
<a class="sourceLine" id="cb142-11" data-line-number="11">  y.fit =<span class="st"> </span>beta[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x[k] <span class="co"># linear fit</span></a>
<a class="sourceLine" id="cb142-12" data-line-number="12">  y.hat =<span class="st"> </span><span class="kw">c</span>(y.hat, y.fit)</a>
<a class="sourceLine" id="cb142-13" data-line-number="13">  residual =<span class="st"> </span><span class="kw">c</span>(residual, <span class="kw">abs</span>(y[k] <span class="op">-</span><span class="st"> </span>y.fit))</a>
<a class="sourceLine" id="cb142-14" data-line-number="14">}</a></code></pre></div>

<p><strong>Sixth</strong>, let us work on the <strong>robust weight</strong> for outliers by computing for the residuals, <strong>e</strong>:</p>

<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb143-1" data-line-number="1">weight_depth &lt;-<span class="st"> </span><span class="cf">function</span>(residual) {</a>
<a class="sourceLine" id="cb143-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(residual)</a>
<a class="sourceLine" id="cb143-3" data-line-number="3">  s =<span class="st"> </span><span class="kw">median</span>(residual)</a>
<a class="sourceLine" id="cb143-4" data-line-number="4">  w =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb143-5" data-line-number="5">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb143-6" data-line-number="6">    e_k =<span class="st"> </span>residual[k]</a>
<a class="sourceLine" id="cb143-7" data-line-number="7">    w =<span class="st"> </span><span class="kw">c</span>(w, e_k <span class="op">/</span><span class="st"> </span>(<span class="dv">6</span><span class="op">*</span>s) )</a>
<a class="sourceLine" id="cb143-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb143-9" data-line-number="9">  <span class="co"># by assigning 1, this is equivalent to 0 for ( 1 - W^2)^2 if W &gt;=1</span></a>
<a class="sourceLine" id="cb143-10" data-line-number="10">  w[ w <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb143-11" data-line-number="11">  (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>w<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span> <span class="co"># bisquare smoother</span></a>
<a class="sourceLine" id="cb143-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb143-13" data-line-number="13">w.depth =<span class="st"> </span><span class="kw">weight_depth</span>(residual)</a>
<a class="sourceLine" id="cb143-14" data-line-number="14"><span class="kw">round</span>(w.depth,<span class="dv">4</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>] <span class="co"># limit display to 8</span></a></code></pre></div>
<pre><code>## [1] 0.9516 0.9835 0.9932 0.9891 0.9911 0.8109 0.7614 0.9652</code></pre>

<p><strong>Finally</strong>, perform smoothing using both width and depth weights:</p>

<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" data-line-number="1">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x), n, <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb145-2" data-line-number="2">B=<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb145-3" data-line-number="3">y.hat2 =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb145-4" data-line-number="4"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb145-5" data-line-number="5">  w.width =<span class="st"> </span><span class="kw">as.numeric</span>( W[,k] )  </a>
<a class="sourceLine" id="cb145-6" data-line-number="6">  w =<span class="st"> </span><span class="kw">diag</span>(w.width <span class="op">*</span><span class="st"> </span>w.depth[k])</a>
<a class="sourceLine" id="cb145-7" data-line-number="7">  <span class="cf">if</span> ( <span class="kw">length</span>( <span class="kw">which</span>(w <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>) ) <span class="op">==</span><span class="st"> </span><span class="dv">0</span> ) { w =<span class="st"> </span><span class="kw">diag</span>(w.width) }</a>
<a class="sourceLine" id="cb145-8" data-line-number="8">  B[[k]] =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>w <span class="op">%*%</span><span class="st"> </span>A) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>w <span class="op">%*%</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb145-9" data-line-number="9">  beta =<span class="st"> </span>B[[k]]</a>
<a class="sourceLine" id="cb145-10" data-line-number="10">  y.fit =<span class="st"> </span>beta[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x[k]</a>
<a class="sourceLine" id="cb145-11" data-line-number="11">  y.hat2 =<span class="st"> </span><span class="kw">c</span>(y.hat2, y.fit)</a>
<a class="sourceLine" id="cb145-12" data-line-number="12">}</a></code></pre></div>

<p>Here is a naive implementation of <strong>scatterplot smoothing</strong> for <strong>LOESS</strong> and <strong>LOWESS</strong> in R (emphasizing <strong>LOWESS</strong>). Also, note that the <strong>robustness</strong> of fit can be processed iteratively against the fit itself, which we skip implementing here:</p>

<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb146-1" data-line-number="1">scatterplot_smoothing &lt;-<span class="st"> </span><span class="cf">function</span>(x,y, <span class="dt">f=</span><span class="fl">0.40</span>, <span class="dt">span=</span><span class="fl">0.75</span>, </a>
<a class="sourceLine" id="cb146-2" data-line-number="2">                                  <span class="dt">smooth=</span><span class="st">&quot;lowess&quot;</span>, <span class="dt">degree=</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb146-3" data-line-number="3">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb146-4" data-line-number="4">    weight_width &lt;-<span class="st"> </span><span class="cf">function</span>(d, h) {</a>
<a class="sourceLine" id="cb146-5" data-line-number="5">      w =<span class="st">   </span>d <span class="op">/</span><span class="st"> </span>h</a>
<a class="sourceLine" id="cb146-6" data-line-number="6">      w[w <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span> </a>
<a class="sourceLine" id="cb146-7" data-line-number="7">      <span class="co"># relaxed version of tricube</span></a>
<a class="sourceLine" id="cb146-8" data-line-number="8">      <span class="co"># without the 70/81 scale</span></a>
<a class="sourceLine" id="cb146-9" data-line-number="9">      (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>w<span class="op">^</span><span class="dv">3</span>)<span class="op">^</span><span class="dv">3</span> <span class="co"># tricube smoother</span></a>
<a class="sourceLine" id="cb146-10" data-line-number="10">    }</a>
<a class="sourceLine" id="cb146-11" data-line-number="11">    weight_depth &lt;-<span class="st"> </span><span class="cf">function</span>(e) {</a>
<a class="sourceLine" id="cb146-12" data-line-number="12">      s =<span class="st"> </span><span class="kw">median</span>(e)</a>
<a class="sourceLine" id="cb146-13" data-line-number="13">      w =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb146-14" data-line-number="14">      <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb146-15" data-line-number="15">        e_k =<span class="st"> </span>e[k]</a>
<a class="sourceLine" id="cb146-16" data-line-number="16">        w =<span class="st"> </span><span class="kw">c</span>(w, e_k <span class="op">/</span><span class="st"> </span>(<span class="dv">6</span><span class="op">*</span>s) )</a>
<a class="sourceLine" id="cb146-17" data-line-number="17">      }</a>
<a class="sourceLine" id="cb146-18" data-line-number="18">      w[ w <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb146-19" data-line-number="19">      (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>w<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span> <span class="co"># bisquare smoother</span></a>
<a class="sourceLine" id="cb146-20" data-line-number="20">    }</a>
<a class="sourceLine" id="cb146-21" data-line-number="21">    knn &lt;-<span class="st"> </span><span class="cf">function</span>(d_i, kth) {</a>
<a class="sourceLine" id="cb146-22" data-line-number="22">        <span class="kw">sort</span>(d_i, <span class="dt">decreasing=</span><span class="ot">FALSE</span>)[kth] </a>
<a class="sourceLine" id="cb146-23" data-line-number="23">    }   </a>
<a class="sourceLine" id="cb146-24" data-line-number="24">    coeffs &lt;-<span class="st"> </span><span class="cf">function</span>(A, y, <span class="dt">W =</span> <span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb146-25" data-line-number="25">        n =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb146-26" data-line-number="26">        <span class="cf">if</span> (<span class="kw">length</span>(W) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { W =<span class="st"> </span><span class="kw">diag</span>(W,n) } <span class="cf">else</span> { W =<span class="st"> </span><span class="kw">diag</span>(W) }</a>
<a class="sourceLine" id="cb146-27" data-line-number="27">        <span class="kw">solve</span>(<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>W <span class="op">%*%</span><span class="st"> </span>A) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>W <span class="op">%*%</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb146-28" data-line-number="28">    }</a>
<a class="sourceLine" id="cb146-29" data-line-number="29">    fit.lowess &lt;-<span class="st"> </span><span class="cf">function</span>(B, x, <span class="dt">degree=</span><span class="dv">1</span>) {  B[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>B[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x<span class="op">^</span>degree }</a>
<a class="sourceLine" id="cb146-30" data-line-number="30">    fit.loess &lt;-<span class="st"> </span><span class="cf">function</span>(B, x, <span class="dt">degree=</span><span class="dv">2</span>)  {  </a>
<a class="sourceLine" id="cb146-31" data-line-number="31">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb146-32" data-line-number="32">            B[<span class="dv">1</span>] </a>
<a class="sourceLine" id="cb146-33" data-line-number="33">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb146-34" data-line-number="34">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb146-35" data-line-number="35">            B[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>B[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x </a>
<a class="sourceLine" id="cb146-36" data-line-number="36">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb146-37" data-line-number="37">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) {</a>
<a class="sourceLine" id="cb146-38" data-line-number="38">            B[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>B[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>B[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> </a>
<a class="sourceLine" id="cb146-39" data-line-number="39">        }</a>
<a class="sourceLine" id="cb146-40" data-line-number="40">    } </a>
<a class="sourceLine" id="cb146-41" data-line-number="41">    fit &lt;-<span class="st"> </span><span class="cf">function</span>(A, x, y, kth, fit.smooth, <span class="dt">degree =</span> <span class="dv">2</span>) {</a>
<a class="sourceLine" id="cb146-42" data-line-number="42">        <span class="co"># Generate distance of data points</span></a>
<a class="sourceLine" id="cb146-43" data-line-number="43">        D =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">abs</span>( <span class="kw">dist</span>(x, <span class="dt">upper=</span><span class="ot">TRUE</span>)))</a>
<a class="sourceLine" id="cb146-44" data-line-number="44">        y.hat =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb146-45" data-line-number="45">        y.res =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb146-46" data-line-number="46">        y.width =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb146-47" data-line-number="47">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb146-48" data-line-number="48">            <span class="co"># kth nearest neighbor (kth smallest distance)</span></a>
<a class="sourceLine" id="cb146-49" data-line-number="49">            dk =<span class="st"> </span><span class="kw">knn</span>(D[k,], kth)</a>
<a class="sourceLine" id="cb146-50" data-line-number="50">            w =<span class="st"> </span><span class="kw">weight_width</span>(D[,k], dk)</a>
<a class="sourceLine" id="cb146-51" data-line-number="51">            B =<span class="st"> </span><span class="kw">coeffs</span>(A, y, w)</a>
<a class="sourceLine" id="cb146-52" data-line-number="52">            y.fit =<span class="st"> </span><span class="kw">fit.smooth</span>(B, x[k], degree)  </a>
<a class="sourceLine" id="cb146-53" data-line-number="53">            y.hat =<span class="st"> </span><span class="kw">c</span>(y.hat, y.fit)</a>
<a class="sourceLine" id="cb146-54" data-line-number="54">            y.res =<span class="st"> </span><span class="kw">c</span>(y.res, <span class="kw">abs</span>(y[k] <span class="op">-</span><span class="st"> </span>y.hat))</a>
<a class="sourceLine" id="cb146-55" data-line-number="55">            y.width[[k]] =<span class="st"> </span>w</a>
<a class="sourceLine" id="cb146-56" data-line-number="56">        }</a>
<a class="sourceLine" id="cb146-57" data-line-number="57">        y.hat =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb146-58" data-line-number="58">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb146-59" data-line-number="59">            w.width =<span class="st"> </span>y.width[[k]]</a>
<a class="sourceLine" id="cb146-60" data-line-number="60">            w.depth =<span class="st"> </span><span class="kw">weight_depth</span>(y.res)</a>
<a class="sourceLine" id="cb146-61" data-line-number="61">            w =<span class="st"> </span>w.width <span class="op">*</span><span class="st"> </span>w.depth</a>
<a class="sourceLine" id="cb146-62" data-line-number="62">            <span class="cf">if</span> (<span class="kw">length</span>(<span class="kw">which</span>(w <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>)) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) { w =<span class="st"> </span>w.width }</a>
<a class="sourceLine" id="cb146-63" data-line-number="63">            B =<span class="st"> </span><span class="kw">coeffs</span>(A, y, w)</a>
<a class="sourceLine" id="cb146-64" data-line-number="64">            y.fit =<span class="st"> </span><span class="kw">fit.smooth</span>(B, x[k], degree)  </a>
<a class="sourceLine" id="cb146-65" data-line-number="65">            y.hat =<span class="st"> </span><span class="kw">c</span>(y.hat, y.fit)</a>
<a class="sourceLine" id="cb146-66" data-line-number="66">        }</a>
<a class="sourceLine" id="cb146-67" data-line-number="67">        <span class="kw">list</span>(<span class="st">&quot;x&quot;</span>=x, <span class="st">&quot;y&quot;</span>=y.hat)</a>
<a class="sourceLine" id="cb146-68" data-line-number="68">    } </a>
<a class="sourceLine" id="cb146-69" data-line-number="69">    lowess &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, f) {</a>
<a class="sourceLine" id="cb146-70" data-line-number="70">        A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x), n, <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb146-71" data-line-number="71">        <span class="co"># kth nearest neighbor</span></a>
<a class="sourceLine" id="cb146-72" data-line-number="72">        kth =<span class="st"> </span><span class="kw">round</span>(f <span class="op">*</span><span class="st"> </span>n)</a>
<a class="sourceLine" id="cb146-73" data-line-number="73">        <span class="kw">fit</span>(A, x, y, kth, fit.lowess, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb146-74" data-line-number="74">    }</a>
<a class="sourceLine" id="cb146-75" data-line-number="75">    loess &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, span, degree) {</a>
<a class="sourceLine" id="cb146-76" data-line-number="76">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb146-77" data-line-number="77">            A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n)), n, <span class="dv">1</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb146-78" data-line-number="78">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb146-79" data-line-number="79">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb146-80" data-line-number="80">             A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x), n, <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb146-81" data-line-number="81">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb146-82" data-line-number="82">        <span class="cf">if</span> (degree <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) {</a>
<a class="sourceLine" id="cb146-83" data-line-number="83">             A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x, x<span class="op">^</span><span class="dv">2</span>), n, <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb146-84" data-line-number="84">        }</a>
<a class="sourceLine" id="cb146-85" data-line-number="85">        <span class="co"># kth nearest neighbor</span></a>
<a class="sourceLine" id="cb146-86" data-line-number="86">        kth =<span class="st"> </span><span class="kw">round</span>(span <span class="op">*</span><span class="st"> </span>n)</a>
<a class="sourceLine" id="cb146-87" data-line-number="87">        <span class="kw">fit</span>(A, x, y, kth, fit.loess, degree)</a>
<a class="sourceLine" id="cb146-88" data-line-number="88">    }</a>
<a class="sourceLine" id="cb146-89" data-line-number="89">    <span class="cf">if</span> (smooth <span class="op">==</span><span class="st"> &quot;lowess&quot;</span>) {</a>
<a class="sourceLine" id="cb146-90" data-line-number="90">        <span class="kw">lowess</span>(x, y, f)</a>
<a class="sourceLine" id="cb146-91" data-line-number="91">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb146-92" data-line-number="92">    <span class="cf">if</span> (smooth <span class="op">==</span><span class="st"> &quot;loess&quot;</span>) {</a>
<a class="sourceLine" id="cb146-93" data-line-number="93">        <span class="kw">loess</span>(x, y, span, degree)</a>
<a class="sourceLine" id="cb146-94" data-line-number="94">    }</a>
<a class="sourceLine" id="cb146-95" data-line-number="95">}</a></code></pre></div>

<p>Let us plot the outcome of <strong>scatterplot smoothing</strong> function:</p>

<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb147-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb147-2" data-line-number="2">sample_size =<span class="st"> </span>n =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb147-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">30</span> </a>
<a class="sourceLine" id="cb147-4" data-line-number="4">sample.poly =<span class="st"> </span><span class="kw">poly</span>(<span class="dv">1</span><span class="op">:</span>n, <span class="dt">degree=</span><span class="dv">3</span>, <span class="dt">simple=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb147-5" data-line-number="5">y =<span class="st"> </span>sample.poly[,<span class="dv">3</span>] <span class="op">+</span><span class="st"> </span>e <span class="co"># add Gausian residual using 3rd degree poly</span></a>
<a class="sourceLine" id="cb147-6" data-line-number="6">x =<span class="st"> </span><span class="kw">sort</span>( <span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">size=</span>n, <span class="dt">replace=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb147-7" data-line-number="7">ymin =<span class="st"> </span><span class="kw">min</span>(y); ymax =<span class="st"> </span><span class="kw">max</span>(y); xmin =<span class="st"> </span><span class="kw">min</span>(x); xmax =<span class="st"> </span><span class="kw">max</span>(x)</a>
<a class="sourceLine" id="cb147-8" data-line-number="8">D =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">abs</span>( <span class="kw">dist</span>(x, <span class="dt">upper=</span><span class="ot">TRUE</span>)))</a>
<a class="sourceLine" id="cb147-9" data-line-number="9">our.lowess.fit =<span class="st"> </span><span class="kw">scatterplot_smoothing</span>(x, y, <span class="dt">f=</span><span class="fl">0.40</span>, <span class="dt">smooth=</span><span class="st">&quot;lowess&quot;</span>)</a>
<a class="sourceLine" id="cb147-10" data-line-number="10">our.loess.fit =<span class="st"> </span><span class="kw">scatterplot_smoothing</span>(x, y, <span class="dt">span=</span><span class="fl">0.40</span>, <span class="dt">smooth=</span><span class="st">&quot;loess&quot;</span>, </a>
<a class="sourceLine" id="cb147-11" data-line-number="11">                                      <span class="dt">degree=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb147-12" data-line-number="12">lowess.fit =<span class="st"> </span><span class="kw">lowess</span>(x, y, <span class="dt">f=</span><span class="fl">0.40</span> )</a>
<a class="sourceLine" id="cb147-13" data-line-number="13">loess.model =<span class="st"> </span><span class="kw">loess</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">span=</span><span class="fl">0.75</span>, <span class="dt">degree=</span><span class="dv">2</span>, <span class="dt">family=</span><span class="kw">c</span>(<span class="st">&quot;gaussian&quot;</span>))</a>
<a class="sourceLine" id="cb147-14" data-line-number="14">loess.fit =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(loess.model)</a>
<a class="sourceLine" id="cb147-15" data-line-number="15"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim =</span> <span class="kw">range</span>(xmin, xmax), <span class="dt">ylim=</span><span class="kw">range</span>(ymin,ymax ),</a>
<a class="sourceLine" id="cb147-16" data-line-number="16">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>,</a>
<a class="sourceLine" id="cb147-17" data-line-number="17">     <span class="dt">main=</span><span class="st">&quot;Scatterplot Smoothing (Local Linear Regression)&quot;</span>)</a>
<a class="sourceLine" id="cb147-18" data-line-number="18"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb147-19" data-line-number="19"><span class="kw">points</span>(x, y, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb147-20" data-line-number="20"><span class="kw">lines</span>(our.lowess.fit, <span class="dt">col=</span><span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb147-21" data-line-number="21"><span class="kw">lines</span>(our.loess.fit, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb147-22" data-line-number="22"><span class="kw">lines</span>(lowess.fit, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb147-23" data-line-number="23"><span class="kw">lines</span>(loess.fit, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lowess"></span>
<img src="embed0021.png" alt="Scatterplot Smoothing" width="80%" />
<p class="caption">
Figure 3.32: Scatterplot Smoothing
</p>
</div>

</div>
<div id="kernel-smoothing" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.8.2</span> Kernel Smoothing <a href="3.8-polynomialsmoothing.html#kernel-smoothing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Kernel Smoothing</strong>, also considered as <strong>Kernel Regression</strong>, is another <strong>smoothing or regression technique</strong> similar to <strong>LOESS</strong> and <strong>LOWESS</strong> smoothers.</p>
<p>There are a few <strong>kernel functions</strong> that can be used for <strong>Kernel regression (estimation)</strong> (See Table <a href="3.8-polynomialsmoothing.html#tab:kernfunction">3.1</a>) <span class="citation">(JoÅe E. ChacÃ³n J E. et al <a href="bibliography.html#ref-ref389j">2018</a>; Cheruiyot L. R. et al <a href="bibliography.html#ref-ref381l">2020</a>)</span>:        </p>

<table>
<caption><span id="tab:kernfunction">Table 3.1: </span>Kernel Functions</caption>
<colgroup>
<col width="9%" />
<col width="2%" />
<col width="45%" />
<col width="16%" />
<col width="17%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Kernel</th>
<th align="left">â</th>
<th align="left">Formula: <span class="math inline">\(I(x) = 1_{( \mid x \mid \le 1)}\)</span></th>
<th align="left"><span class="math inline">\(\mu_2(K)\)</span></th>
<th align="left">R(K)</th>
<th align="left">Efficiency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Epanechnikov</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{3}{4}(1 - x^2)\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{5}\)</span></td>
<td align="left"><span class="math inline">\(\frac{3}{5}\)</span></td>
<td align="left">1.0000</td>
</tr>
<tr class="even">
<td align="left">Cosine</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{\pi}{4}cos\left(\frac{\pi}{2}x\right)\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(1 - \frac{8}{\pi^2}\)</span></td>
<td align="left"><span class="math inline">\(\frac{\pi^2}{16}\)</span></td>
<td align="left">0.9995</td>
</tr>
<tr class="odd">
<td align="left">Tricube</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{70}{81}(1 - \mid x \mid^3)^3\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{35}{243}\)</span></td>
<td align="left"><span class="math inline">\(\frac{175}{247}\)</span></td>
<td align="left">0.9979</td>
</tr>
<tr class="even">
<td align="left">Quartic</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{15}{16}(1 - x^2)^2\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{7}\)</span></td>
<td align="left"><span class="math inline">\(\frac{5}{7}\)</span></td>
<td align="left">0.9939</td>
</tr>
<tr class="odd">
<td align="left">Triweight</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{35}{32}(1 - x^2)^3\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{9}\)</span></td>
<td align="left"><span class="math inline">\(\frac{350}{429}\)</span></td>
<td align="left">0.9867</td>
</tr>
<tr class="even">
<td align="left">Triangular</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = (1 - \mid x \mid)\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{6}\)</span></td>
<td align="left"><span class="math inline">\(\frac{2}{3}\)</span></td>
<td align="left">0.9859</td>
</tr>
<tr class="odd">
<td align="left">Gaussian</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{2\sqrt{\pi}}\)</span></td>
<td align="left">0.9512</td>
</tr>
<tr class="even">
<td align="left">Uniform</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = \frac{1}{2}\cdot I(x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{3}\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{2}\)</span></td>
<td align="left">0.9295</td>
</tr>
<tr class="odd">
<td align="left">Logistic</td>
<td align="left"></td>
<td align="left"><span class="math inline">\(K(x) = (e^x +_ 2 + e^{-x})^{-1}\)</span></td>
<td align="left"><span class="math inline">\(\frac{\pi^2}{3}\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{6}\)</span></td>
<td align="left">0.8876</td>
</tr>
</tbody>
</table>

<p>As shown in the table, the performance of the <strong>kernel functions</strong> in terms of efficiency is calculated based on the following equation and comparably measured against <strong>Epanechnikov Kernel</strong>, which effectively gets 100% efficiency:</p>
<p><span class="math display" id="eq:equate1050212">\[\begin{align}
Efficiency = \sqrt{\mu_2(K)}\cdot R(K) \tag{3.229} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1050213">\[\begin{align}
where \ \ \ \
\mu_2(K) = \int x^2K(x)dx, \ \ \ \ \ R(K) = \int K^2(x) dx \tag{3.230} 
\end{align}\]</span></p>
<p>For example, the efficiency of <strong>Epanechnikov Kernel</strong> is computed like so (let A = <span class="math inline">\(\mu_2(K)\)</span> and B = <span class="math inline">\(R(K)\)</span>):</p>
<p><span class="math display">\[
A = \mu_2(K) = \frac{1}{5},\ \ \ \ \ \ 
B = R(K) = \frac{3}{5}\ \ \rightarrow\ \ \ \ \ \ \ \ 
Efficiency = \sqrt{\frac{1}{5}} \cdot \frac{3}{5} = 0.2683282.
\]</span></p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb148-1" data-line-number="1">epanechnikov_kernel &lt;-<span class="cf">function</span>(x) <span class="dv">3</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>x<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb148-2" data-line-number="2">f1 &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">epanechnikov_kernel</span>(x)</a>
<a class="sourceLine" id="cb148-3" data-line-number="3">f2 &lt;-<span class="st"> </span><span class="cf">function</span>(x) (<span class="kw">epanechnikov_kernel</span>(x))<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb148-4" data-line-number="4">A =<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> f1, <span class="dt">lower =</span> <span class="dv">-1</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value</a>
<a class="sourceLine" id="cb148-5" data-line-number="5">B =<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> f2, <span class="dt">lower =</span> <span class="dv">-1</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value</a>
<a class="sourceLine" id="cb148-6" data-line-number="6">E_epanechnikov =<span class="st"> </span><span class="kw">sqrt</span>(A) <span class="op">*</span><span class="st"> </span>B</a>
<a class="sourceLine" id="cb148-7" data-line-number="7"><span class="kw">c</span>(<span class="st">&quot;A&quot;</span> =<span class="st"> </span>A, <span class="st">&quot;B&quot;</span> =<span class="st"> </span>B, <span class="st">&quot;Efficiency&quot;</span> =<span class="st"> </span>E_epanechnikov)</a></code></pre></div>
<pre><code>##          A          B Efficiency 
##  0.2000000  0.6000000  0.2683282</code></pre>
<p>The efficiency of <strong>Gaussian Kernel</strong> is computed like so:</p>
<p><span class="math display">\[
A = \mu_2(K) = 1,\ \ \ \ \ \ 
B = R(K) = \frac{1}{2\sqrt{\pi}}\ \ \rightarrow\ \ \ \ \ \
Efficiency = \sqrt{1} \cdot \frac{1}{2\sqrt{\pi}} = 0.2820948.
\]</span></p>

<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb150-1" data-line-number="1">gaussian_kernel &lt;-<span class="cf">function</span>(x)   <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi)) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>( <span class="dv">-1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span>)  </a>
<a class="sourceLine" id="cb150-2" data-line-number="2">f1 &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">gaussian_kernel</span>(x)</a>
<a class="sourceLine" id="cb150-3" data-line-number="3">f2 &lt;-<span class="st"> </span><span class="cf">function</span>(x) (<span class="kw">gaussian_kernel</span>(x))<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb150-4" data-line-number="4">A =<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> f1, <span class="dt">lower =</span> <span class="op">-</span><span class="ot">Inf</span>, <span class="dt">upper =</span> <span class="ot">Inf</span>)<span class="op">$</span>value</a>
<a class="sourceLine" id="cb150-5" data-line-number="5">B =<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> f2, <span class="dt">lower =</span> <span class="op">-</span><span class="ot">Inf</span>, <span class="dt">upper =</span> <span class="ot">Inf</span>)<span class="op">$</span>value</a>
<a class="sourceLine" id="cb150-6" data-line-number="6">E_gaussian =<span class="st"> </span><span class="kw">sqrt</span>(A) <span class="op">*</span><span class="st"> </span>B</a>
<a class="sourceLine" id="cb150-7" data-line-number="7"><span class="kw">c</span>(<span class="st">&quot;A&quot;</span> =<span class="st"> </span>A, <span class="st">&quot;B&quot;</span> =<span class="st"> </span>B, <span class="st">&quot;Efficiency&quot;</span> =<span class="st"> </span>E_gaussian)</a></code></pre></div>
<pre><code>##          A          B Efficiency 
##  1.0000000  0.2820948  0.2820948</code></pre>

<p>We then compare each efficiency against the calculated efficiency of <strong>Epanechnikov efficiency</strong>:</p>
<p><span class="math display">\[\begin{align*}
E_{epanechnikov} {}&amp;= 0.2683282 / 0.2683282 = 1.0000 \\
E_{gaussian} &amp;= 0.2683282 / 0.2820948 = 0.9511987 = 0.9512
\end{align*}\]</span></p>

<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb152-1" data-line-number="1">E.epanechnikov =<span class="st"> </span>E_epanechnikov <span class="op">/</span><span class="st"> </span>E_epanechnikov</a>
<a class="sourceLine" id="cb152-2" data-line-number="2">E.gaussian =<span class="st">  </span>E_epanechnikov <span class="op">/</span><span class="st"> </span>E_gaussian</a>
<a class="sourceLine" id="cb152-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;Efficiency (epanechnikov)&quot;</span> =<span class="st"> </span><span class="kw">round</span>(E.epanechnikov, <span class="dv">4</span>), </a>
<a class="sourceLine" id="cb152-4" data-line-number="4">   <span class="st">&quot;Efficiency (gaussian)&quot;</span> =<span class="st"> </span><span class="kw">round</span>(E.gaussian, <span class="dv">4</span>))</a></code></pre></div>
<pre><code>## Efficiency (epanechnikov)     Efficiency (gaussian) 
##                    1.0000                    0.9512</code></pre>

<p>Any function can be used as a kernel as long as the following properties are satisfied <span class="citation">(Zucchini W. <a href="bibliography.html#ref-ref116z">2003</a>)</span>:</p>
<p><span class="math display">\[
\int K(x)dx = 1,\ \ \ \ \ \ \ \ \ \
\int xK(x)dx = 0,\ \ \ \ \ \ \ \ \ \
\mu_2(K) := \int x^2K(x)dx &lt; \infty
\]</span></p>
<p>We can see the different functions plotted in Figure <a href="3.8-polynomialsmoothing.html#fig:kfunctions">3.33</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kfunctions"></span>
<img src="embed0022.png" alt="Kernel Functions" width="90%" />
<p class="caption">
Figure 3.33: Kernel Functions
</p>
</div>

<p>Now, to illustrate how <strong>Kernel Smoothing works</strong>, we start with a <strong>simple linear equation</strong>:</p>
<p><span class="math display" id="eq:equate1050214">\[\begin{align}
y_i = \beta_0 + \beta_1 x_i + \epsilon \tag{3.231} 
\end{align}\]</span></p>
<p>We focus on a <strong>linear equation</strong> with the goal of estimating the following:</p>
<p><span class="math display" id="eq:equate1050215">\[\begin{align}
\hat{m}(x_i) \approx \beta_0 + \beta_1 x_i \tag{3.232} 
\end{align}\]</span></p>
<p>so that we end up with the following linear equation:</p>
<p><span class="math display" id="eq:equate1050216">\[\begin{align}
\hat{y}_i = \hat{m}(x_i) + \epsilon \tag{3.233} 
\end{align}\]</span></p>
<p>The function <span class="math inline">\(\hat{m}(x)\)</span> is derived like so:</p>
<p><span class="math display" id="eq:equate1050219" id="eq:equate1050218" id="eq:equate1050217">\[\begin{align}
\mathbb{E}(Y) \rightarrow \mathbb{E}(Y|X = x) {}&amp;= \int y \cdot f(y|x) dy  \tag{3.234} \\
&amp;= \int y \cdot \frac{f(x,y)}{f(x)} dy \tag{3.235} \\
&amp;= \hat{m}(x). \tag{3.236} 
\end{align}\]</span></p>
<p>From there, we obtain an equation for <span class="math inline">\(\hat{m}(x)\)</span>. For (<strong>NWKE, PCKE, GMKE</strong>), we have:</p>
<p><span class="math display" id="eq:equate1050220">\[\begin{align}
\hat{m}(x) = \sum_{i=1}^n \omega_i(x)y_i \tag{3.237} 
\end{align}\]</span></p>
<p>For <strong>KDE</strong>, we have:</p>
<p><span class="math display" id="eq:equate1050221">\[\begin{align}
\hat{m}(x) = \sum_{i=1}^n \omega_i(x) \tag{3.238} 
\end{align}\]</span></p>
<p>We then perform <strong>non-parametric</strong> estimation using a <strong>Kernel Estimator</strong>, <span class="math inline">\(\omega_i(x)\)</span>. There are choices for <span class="math inline">\(\omega_i(x)\)</span>:</p>
<ul>
<li><strong>Nadaraya-Watson Kernel Estimator (NWKE)</strong>:  </li>
</ul>
<p><span class="math display" id="eq:equate1050223" id="eq:equate1050222">\[\begin{align}
\omega_i(x) {}&amp;= \frac{K \left(\frac{x - x_i}{h}\right)} 
{\sum_{j=1}^n K \left(\frac{x - x_j}{h}\right)}\ \ \ \ \ \ \ \ or \ \ \ \ \ \ \   \tag{3.239} \\
\omega_i(x) &amp;= \frac{K \left(\frac{\|x - x_i\|_p}{h}\right)}
{\sum_{j=1}^n K \left(\frac{\|x - x_j\|_p}{h}\right)}\ \ \ \text{for multi-dimension} \tag{3.240} 
\end{align}\]</span></p>
<ul>
<li><strong>Priestley-Chao Kernel Estimator (PCKE)</strong>:  </li>
</ul>
<p><span class="math display" id="eq:equate1050225" id="eq:equate1050224">\[\begin{align}
\omega_i(x) {}&amp;= \frac{\psi}{h} K\left(\frac{x - x_{(i+1)}}{h}\right)
\ \ where\  \psi = \frac{(b - a)}{n}\ \ \leftarrow\ \ \ \text{equally spaced} \tag{3.241} \\
\omega_i(x) {}&amp;= \frac{1}{h}(x_{(i+1)} - x_i) K\left(\frac{x - x_{(i+1)}}{h}\right)
\ \ \leftarrow\ \ \ \text{not equally spaced} \tag{3.242} 
\end{align}\]</span></p>
<ul>
<li><strong>Gasser-Muller Kernel Estimator (GMKE)</strong>:  </li>
</ul>
<p><span class="math display" id="eq:equate1050226">\[\begin{align}
\omega_i(x) = \frac{1}{h}\left|\int_{s_{i-1}}^{s_i} K \left(\frac{ x - t}{h}\right)dt \right|\ \ \ \ \ \leftarrow \ 
where\ \ s_i = \frac{x_i + x_{(i+1)}}{2} \tag{3.243} 
\end{align}\]</span></p>
<ul>
<li><strong>Parzen-Rosenblatt window - Kernel Density estimator (KDE)</strong>:   </li>
</ul>
<p><span class="math display" id="eq:equate1050228" id="eq:equate1050227">\[\begin{align}
\omega_i(x) {}&amp;= \frac{1}{n}\sum_{j=1}^n \frac{1}{h}K\left(\frac{x - x_i}{h}\right)
\ \ \ \ \ \ \ \ or \ \ \ \ \ \ \  \tag{3.244} \\
\omega_i(x) &amp;= \frac{1}{n}\sum_{j=1}^n \frac{1}{h^d}K\left(\frac{\|x - x_i\|_p}{h}\right)
\ \ \ \text{for multi-dimension} \tag{3.245} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>d</strong> - d-dimensions</li>
<li><strong>p</strong> - p-norm, e.g.Â 2 for euclidean</li>
</ul>
<p>The <strong>kernel estimators</strong> rely on:</p>
<ul>
<li><strong>K</strong> - a choice of kernel as above (e.g.Â Gaussian, Box, Epanechnikov).</li>
<li><strong>h</strong> - the bandwidth.</li>
</ul>
<p>Let us now illustrate <strong>kernel smoothing</strong> by choosing one combination to implement in R. For our <strong>kernel function</strong>, let us choose <strong>Gaussian Kernel</strong>. For our <strong>kernel estimator</strong>, let us choose <strong>Nadaraya-Watson (NWKE)</strong>.</p>
<p>Here is a naive implementation of our <strong>kernel regression (smoothing)</strong> in R code using the chosen combination:</p>

<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb154-1" data-line-number="1">K &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">kernel =</span> <span class="st">&quot;normal&quot;</span>) {</a>
<a class="sourceLine" id="cb154-2" data-line-number="2">  I &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb154-3" data-line-number="3">    one =<span class="st"> </span><span class="kw">which</span>(x<span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>); zero =<span class="st"> </span><span class="kw">which</span>(x<span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb154-4" data-line-number="4">    x[one] =<span class="st"> </span><span class="dv">1</span>; x[zero] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb154-5" data-line-number="5">    x</a>
<a class="sourceLine" id="cb154-6" data-line-number="6">  }</a>
<a class="sourceLine" id="cb154-7" data-line-number="7">  <span class="cf">switch</span> (kernel,</a>
<a class="sourceLine" id="cb154-8" data-line-number="8">          <span class="st">&quot;epanechnikov&quot;</span> =<span class="st"> </span><span class="dv">3</span><span class="op">/</span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x),</a>
<a class="sourceLine" id="cb154-9" data-line-number="9">          <span class="st">&quot;normal&quot;</span>       =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x <span class="op">^</span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb154-10" data-line-number="10">          <span class="st">&quot;tricube&quot;</span>      =<span class="st"> </span><span class="dv">70</span><span class="op">/</span><span class="dv">81</span> <span class="op">*</span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">abs</span>(x)<span class="op">^</span><span class="dv">3</span>)<span class="op">^</span><span class="dv">3</span>   <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x),</a>
<a class="sourceLine" id="cb154-11" data-line-number="11">          <span class="st">&quot;rectangular&quot;</span>  =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x),</a>
<a class="sourceLine" id="cb154-12" data-line-number="12">          <span class="st">&quot;triangular&quot;</span>   =<span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">abs</span>(x))  <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x),</a>
<a class="sourceLine" id="cb154-13" data-line-number="13">          <span class="st">&quot;quartic&quot;</span>      =<span class="st"> </span><span class="dv">15</span><span class="op">/</span><span class="dv">16</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span>  <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x)</a>
<a class="sourceLine" id="cb154-14" data-line-number="14">          )</a>
<a class="sourceLine" id="cb154-15" data-line-number="15">} </a>
<a class="sourceLine" id="cb154-16" data-line-number="16">estimator &lt;-<span class="st"> </span><span class="cf">function</span>(X, x, y, h, <span class="dt">etype =</span> <span class="st">&quot;nwke&quot;</span>, <span class="dt">kernel =</span> <span class="st">&quot;normal&quot;</span>) {</a>
<a class="sourceLine" id="cb154-17" data-line-number="17">  nwke &lt;-<span class="st"> </span><span class="cf">function</span>(X, x, y, h, kernel) {</a>
<a class="sourceLine" id="cb154-18" data-line-number="18">      h =<span class="st"> </span><span class="fl">0.25</span> <span class="op">*</span><span class="st"> </span>h <span class="co"># see ksmooth for adjustment of bandwidth</span></a>
<a class="sourceLine" id="cb154-19" data-line-number="19">      n =<span class="st"> </span><span class="kw">length</span>(x); m =<span class="st"> </span><span class="kw">length</span>(X)</a>
<a class="sourceLine" id="cb154-20" data-line-number="20">      mx =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb154-21" data-line-number="21">      <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb154-22" data-line-number="22">          w =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb154-23" data-line-number="23">          <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb154-24" data-line-number="24">              numer =<span class="st"> </span><span class="kw">K</span> ( ( X[k] <span class="op">-</span><span class="st"> </span>x[i] ) <span class="op">/</span><span class="st"> </span>h , kernel )</a>
<a class="sourceLine" id="cb154-25" data-line-number="25">              denom =<span class="st"> </span><span class="kw">sum</span> ( <span class="kw">K</span> ( ( X[k] <span class="op">-</span><span class="st"> </span>x ) <span class="op">/</span><span class="st"> </span>h , kernel ) )</a>
<a class="sourceLine" id="cb154-26" data-line-number="26">              w =<span class="st"> </span><span class="kw">c</span>(w, numer <span class="op">/</span><span class="st"> </span>denom   )</a>
<a class="sourceLine" id="cb154-27" data-line-number="27">          }</a>
<a class="sourceLine" id="cb154-28" data-line-number="28">          mx =<span class="st"> </span><span class="kw">c</span>(mx, <span class="kw">sum</span>( w <span class="op">*</span><span class="st"> </span>y ) )</a>
<a class="sourceLine" id="cb154-29" data-line-number="29">      }</a>
<a class="sourceLine" id="cb154-30" data-line-number="30">      mx  </a>
<a class="sourceLine" id="cb154-31" data-line-number="31">  }</a>
<a class="sourceLine" id="cb154-32" data-line-number="32">  <span class="cf">if</span> (etype <span class="op">==</span><span class="st"> &quot;nwke&quot;</span>) {</a>
<a class="sourceLine" id="cb154-33" data-line-number="33">    <span class="kw">nwke</span>(X, x, y, h, kernel)</a>
<a class="sourceLine" id="cb154-34" data-line-number="34">  } </a>
<a class="sourceLine" id="cb154-35" data-line-number="35">}</a>
<a class="sourceLine" id="cb154-36" data-line-number="36">kernel_smoothing &lt;-<span class="st"> </span><span class="cf">function</span>( x, y, <span class="dt">kernel=</span><span class="st">&quot;normal&quot;</span>, <span class="dt">h =</span> <span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb154-37" data-line-number="37">  m &lt;-<span class="st"> </span>estimator</a>
<a class="sourceLine" id="cb154-38" data-line-number="38">  xmin =<span class="st"> </span><span class="kw">min</span>(x); xmax =<span class="st"> </span><span class="kw">max</span>(x)</a>
<a class="sourceLine" id="cb154-39" data-line-number="39">  X =<span class="st"> </span><span class="kw">seq</span>(xmin, xmax, <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb154-40" data-line-number="40">  Y =<span class="st"> </span><span class="kw">m</span>(X, x, y,  h, <span class="dt">etype=</span><span class="st">&quot;nwke&quot;</span>, kernel) </a>
<a class="sourceLine" id="cb154-41" data-line-number="41">  <span class="kw">list</span>(<span class="st">&quot;x&quot;</span>=X, <span class="st">&quot;y&quot;</span>=Y)</a>
<a class="sourceLine" id="cb154-42" data-line-number="42">}</a></code></pre></div>

<p>Let us plot and introduce a built-in R function called <strong>ksmooth()</strong> (see Figure <a href="3.8-polynomialsmoothing.html#fig:ksmooth">3.34</a>).</p>

<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb155-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb155-2" data-line-number="2">sample_size =<span class="st"> </span>n =<span class="st"> </span><span class="dv">100</span> </a>
<a class="sourceLine" id="cb155-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">30</span> </a>
<a class="sourceLine" id="cb155-4" data-line-number="4">sample.poly =<span class="st"> </span><span class="kw">poly</span>(<span class="dv">1</span><span class="op">:</span>n, <span class="dt">degree=</span><span class="dv">4</span>, <span class="dt">simple=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb155-5" data-line-number="5">y =<span class="st"> </span>sample.poly[,<span class="dv">4</span>] <span class="op">+</span><span class="st"> </span>e <span class="co"># add Gaussian residual using 3rd degree poly</span></a>
<a class="sourceLine" id="cb155-6" data-line-number="6">x =<span class="st"> </span><span class="kw">sort</span>( <span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">size=</span>n, <span class="dt">replace=</span><span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb155-7" data-line-number="7">ymin =<span class="st"> </span><span class="kw">min</span>(y); ymax =<span class="st"> </span><span class="kw">max</span>(y); xmin =<span class="st"> </span><span class="kw">min</span>(x); xmax =<span class="st"> </span><span class="kw">max</span>(x)</a>
<a class="sourceLine" id="cb155-8" data-line-number="8"></a>
<a class="sourceLine" id="cb155-9" data-line-number="9"><span class="co"># Run our own NW implementation</span></a>
<a class="sourceLine" id="cb155-10" data-line-number="10">nwke.fit =<span class="st"> </span><span class="kw">kernel_smoothing</span>(x, y,  <span class="dt">kernel=</span><span class="st">&quot;normal&quot;</span>, <span class="dt">h =</span> <span class="dv">7</span> )</a>
<a class="sourceLine" id="cb155-11" data-line-number="11"><span class="co"># Use ksmooth</span></a>
<a class="sourceLine" id="cb155-12" data-line-number="12">ksmooth.fit =<span class="st"> </span><span class="kw">ksmooth</span>(x, y, <span class="dt">kernel =</span> <span class="kw">c</span>( <span class="st">&quot;normal&quot;</span>), <span class="dt">bandwidth =</span> <span class="dv">5</span> )</a>
<a class="sourceLine" id="cb155-13" data-line-number="13"></a>
<a class="sourceLine" id="cb155-14" data-line-number="14"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim =</span> <span class="kw">range</span>(xmin, xmax), <span class="dt">ylim=</span><span class="kw">range</span>(ymin,ymax ),</a>
<a class="sourceLine" id="cb155-15" data-line-number="15">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis&quot;</span>,</a>
<a class="sourceLine" id="cb155-16" data-line-number="16">     <span class="dt">main=</span><span class="st">&quot;Kernel Smoothing&quot;</span>)</a>
<a class="sourceLine" id="cb155-17" data-line-number="17"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb155-18" data-line-number="18"><span class="kw">points</span>(x, y, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb155-19" data-line-number="19"><span class="kw">lines</span>(ksmooth.fit, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb155-20" data-line-number="20"><span class="kw">lines</span>(nwke.fit, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb155-21" data-line-number="21"><span class="kw">legend</span>(<span class="dv">3</span>, ymax , </a>
<a class="sourceLine" id="cb155-22" data-line-number="22">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;our nwke(h=7)&quot;</span>, </a>
<a class="sourceLine" id="cb155-23" data-line-number="23">              <span class="st">&quot;built-in ksmooth(bandwidth=5)&quot;</span>),</a>
<a class="sourceLine" id="cb155-24" data-line-number="24">    <span class="dt">col=</span><span class="kw">c</span>( <span class="st">&quot;brown&quot;</span>, <span class="st">&quot;navyblue&quot;</span>), <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ksmooth"></span>
<img src="embed0023.png" alt="Kernel Smoothing" width="80%" />
<p class="caption">
Figure 3.34: Kernel Smoothing
</p>
</div>

<p>Note that our curve overlaps right on top of the curve fit produced by the <strong>ksmooth()</strong> function. It is an exact match; however, we can accomplish this only because we arbitrarily choose a bandwidth (<span class="math inline">\(h=7\)</span>), which is practically not optimal by hand.</p>
<p>Choosing an optimal bandwidth and the correct Kernel is a good exercise and study in <strong>Kernel regression or Kernel Smoothing</strong>.</p>
<p>For <strong>bandwidth selection</strong>, we have a few choices:</p>
<p><strong>First</strong>, we can use <strong>cross-validation</strong> techniques.</p>
<ul>
<li>LOOCV (Leave One Out Cross-Validation) </li>
<li>K-Fold CV (K-Fold Cross-Validation) </li>
</ul>
<p>The idea is to have an initial list of random bandwidths as data points for cross-validation. We then use <strong>mean squared error (MSE)</strong> to evaluate the cross-validation result. The result with the least <strong>MSE</strong> serves as the optimal bandwidth.  </p>
<p>The equation for <strong>MSE</strong> is shown below:</p>
<p><span class="math display" id="eq:equate1050231" id="eq:equate1050230" id="eq:equate1050229">\[\begin{align}
MSE {}&amp;= \mathbb{E}\left[(f(x) - \hat{f}(x)^2\right]  \tag{3.246} \\
&amp;= Bias(\hat{f}(x))^2 +  Var(\hat{f}(x))  \tag{3.247} \\
&amp;= \frac{1}{n}\sum_{i=1}^n\left(y_i - \hat{y}_i\right)^2 \tag{3.248} 
\end{align}\]</span></p>
<p>Note that <strong>MSE</strong> is measured based on its two components and how the trade-off plays along: <strong>Bias</strong> and <strong>Variance</strong>. We can find more discussion around adjusting the components in Chapter <strong>6</strong> (<strong>Statistical Computation</strong>) under the <strong>Significance of Regression</strong> Section and Chapter <strong>9</strong> (<strong>Computational Learning I</strong>) under the <strong>Regularization</strong> Section.</p>
<p>For <strong>least MSE</strong> equation, we have:</p>
<p><span class="math display" id="eq:equate1050232">\[\begin{align}
CV(bandwidth) = \underset{h}{\mathrm{argmin}}\ MSE(h) \tag{3.249} 
\end{align}\]</span></p>
<p>We cover more of <strong>cross-validation</strong> topic in Chapter <strong>6</strong> (<strong>Statistical Computation</strong>) under <strong>Model Selection</strong> Section and Chapter <strong>9</strong> (<strong>Computational Learning I</strong>).</p>
<p>There are also other measurements adapted from <strong>MSE</strong>:</p>
<ul>
<li><strong>MISE</strong> - mean integrated squared error (Jones 1990). We simply integrate <strong>MSE</strong> like so:</li>
</ul>
<p><span class="math display" id="eq:equate1050235" id="eq:equate1050234" id="eq:equate1050233">\[\begin{align}
MISE(h) {}&amp;= \int MSE(h) dx  \tag{3.250} \\
&amp;= \int Bias(\hat{f}(x))^2 dx + \int Var(\hat{f}(x)) dx  \tag{3.251} \\
&amp;= \int(f(x) - \hat{f}(x))^2  dx + + \int Var(\hat{f}(x)) dx \tag{3.252} 
\end{align}\]</span></p>
<ul>
<li><strong>AMISE</strong> - asymptotic limit of mean integrated squared error (Scott 1992 and Wand and Jones 1995):</li>
</ul>
<p><span class="math display" id="eq:equate1050236">\[\begin{align}
AMISE(h) = \frac{1}{Nh} R(K) + \frac{1}{4}h^4 u_2(K)^2 R(f&#39;&#39;) \tag{3.253} 
\end{align}\]</span></p>
<p>A simplification (or derivation) of the above <strong>AMISE</strong> equation is as follows:</p>
<p><span class="math display" id="eq:equate1050237">\[\begin{align}
h_{(AMISE)} = 
\left(\frac{R(K)^{\frac{1}{5}}}{\mu_2(K)^\frac{2}{5}R(f&#39;&#39;)^{\frac{1}{5}}}\right) n^{-\frac{1}{5}}
= \left(\frac{R(K)}{\mu_2(K)^2R(f&#39;&#39;)}\right)^{\frac{1}{5}} n^{-\frac{1}{5}} \tag{3.254} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display" id="eq:equate1050238">\[\begin{align}
R(K) = \int K(x)^2 dx,\ \ \ \ \ \ \ \ \
\mu_2 (K) = \int x^2K(x) dx,\ \ \ \ \ \ \ \ \ \
R(f&#39;&#39;) = \int f&#39;&#39;(y)^2 dy \tag{3.255} 
\end{align}\]</span></p>
<p><strong>Second</strong>, for <strong>optimal bandwidth</strong> calculation, we can use the <strong>Silverman Rule of Thumb</strong>, also called <strong>Normal Rule of Thumb</strong>. For example, when using <strong>Gaussian kernel</strong>, we use the below equation (Silverman 1986, Scott 1992, Jones et al.Â 1996, Venables-Ripley 2002):</p>
<p><span class="math display" id="eq:equate1050239">\[\begin{align}
h_{(optimal)} = \left(\frac{4 \hat{\sigma} ^5}{3n}\right)^{\frac{1}{5}} = 1.06 \hat{\sigma} n^{-\frac{1}{5}} \tag{3.256} 
\end{align}\]</span>
A variant version was proposed (Silverman and Scott 1992) to avoid oversmoothing:</p>
<p><span class="math display" id="eq:equate1050240">\[\begin{align}
h_{(optimal)} = 0.9 A n^{-\frac{1}{5}}\ \ \ \ \ \leftarrow\ \ \ \ \ \ 
A =  min\left(\hat{\sigma}, \frac{IQR(x)}{1.349}\right)  \tag{3.257} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\hat{\sigma}\)</span> is the sample standard deviation</li>
<li>IQR is <strong>inter-quartile range</strong></li>
</ul>
<p>The implementation of <strong>KDE</strong> and sample bandwidth choices are discussed in Chapter <strong>5</strong> (<strong>Probability and Distribution</strong>) under the <strong>Non-parametric distribution</strong> Section, in which we cover <strong>KDE</strong> in detail as part of estimating probability density.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3.7-polynomialinterpolation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3.9-polynomial-optimization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
