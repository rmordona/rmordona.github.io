<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Computational Deep Learning I | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Computational Deep Learning I | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Computational Deep Learning I | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="machinelearning3.html"/>
<link rel="next" href="deeplearning2.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#acknowledgment-and-motivations"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#caveat"><i class="fa fa-check"></i><b>0.2</b> Caveat</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i><b>0.3</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.4" data-path="mathematical-notation.html"><a href="mathematical-notation.html#notation"><i class="fa fa-check"></i><b>0.4</b> Notation</a></li>
<li class="chapter" data-level="0.5" data-path="mathematical-notation.html"><a href="mathematical-notation.html#number-system"><i class="fa fa-check"></i><b>0.5</b> Number System</a></li>
<li class="chapter" data-level="0.6" data-path="mathematical-notation.html"><a href="mathematical-notation.html#implementation"><i class="fa fa-check"></i><b>0.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numericalmethods.html"><a href="numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="numericalmethods.html"><a href="numericalmethods.html#closed-form-equation"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="numericalmethods.html"><a href="numericalmethods.html#analytical-and-numerical-solutions"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="numericalmethods.html"><a href="numericalmethods.html#significant-figures"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="numericalmethods.html"><a href="numericalmethods.html#accuracy"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="numericalmethods.html"><a href="numericalmethods.html#precision"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="numericalmethods.html"><a href="numericalmethods.html#stability-and-sensitivity"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="numericalmethods.html"><a href="numericalmethods.html#stiffness-and-implicitness"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="numericalmethods.html"><a href="numericalmethods.html#conditioning-and-posedness"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="linearalgebra.html"><a href="linearalgebra.html#system-of-linear-equations"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="linearalgebra.html"><a href="linearalgebra.html#scalar-vector-and-matrix-tensor"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition-and-multiplication"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="linearalgebra.html"><a href="linearalgebra.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="linearalgebra.html"><a href="linearalgebra.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linearalgebra.html"><a href="linearalgebra.html#magnitude-direction-unit-vectors"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-combination-and-independence"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="linearalgebra.html"><a href="linearalgebra.html#space-span-and-basis"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="linearalgebra.html"><a href="linearalgebra.html#determinants"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="linearalgebra.html"><a href="linearalgebra.html#minors-cofactors-and-adjugate-forms"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="linearalgebra.html"><a href="linearalgebra.html#inverse-form-and-row-echelon-form"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-transformations"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="linearalgebra.html"><a href="linearalgebra.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="linearalgebra.html"><a href="linearalgebra.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="linearalgebra.html"><a href="linearalgebra.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="linearalgebra.html"><a href="linearalgebra.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="linearalgebra.html"><a href="linearalgebra.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="linearalgebra.html"><a href="linearalgebra.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="linearalgebra.html"><a href="linearalgebra.html#rank-and-nullity"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="linearalgebra.html"><a href="linearalgebra.html#singularity-and-triviality"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="linearalgebra.html"><a href="linearalgebra.html#orthogonality-and-orthonormality"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="linearalgebra.html"><a href="linearalgebra.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-reconstruction-using-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="linearalgebra.html"><a href="linearalgebra.html#diagonalizability-of-a-matrix"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="linearalgebra.html"><a href="linearalgebra.html#trace-of-a-square-matrix"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="linearalgebra.html"><a href="linearalgebra.html#algebraic-and-geometric-multiplicity"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="linearalgebra.html"><a href="linearalgebra.html#types-of-matrices"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-factorization"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="linearalgebra.html"><a href="linearalgebra.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="linearalgebra.html"><a href="linearalgebra.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="linearalgebra.html"><a href="linearalgebra.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="linearalgebra.html"><a href="linearalgebra.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="linearalgebra.html"><a href="linearalgebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="linearalgebra.html"><a href="linearalgebra.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="linearalgebra.html"><a href="linearalgebra.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="linearalgebra.html"><a href="linearalgebra.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="linearalgebra.html"><a href="linearalgebra.html#software-libraries"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="linearalgebra.html"><a href="linearalgebra.html#summary"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#iteration-and-convergence"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-root-and-fixed-point-by-iteration"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-solutions-to-systems-of-eqs-by-iteration-ax-b"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialregression"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-polynomial-functions-by-series-expansion"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialinterpolation"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialsmoothing"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-optimization"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#summary-1"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numericalcalculus.html"><a href="numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#introductory-calculus"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-integration"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-differentiation"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-ordinary-differential-equations"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="numericalcalculus.html"><a href="numericalcalculus.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="numericalcalculus.html"><a href="numericalcalculus.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="numericalcalculus.html"><a href="numericalcalculus.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="numericalcalculus.html"><a href="numericalcalculus.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="numericalcalculus.html"><a href="numericalcalculus.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="numericalcalculus.html"><a href="numericalcalculus.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-functional-differential-equations"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-partial-differential-equations"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-fourier-series-and-transform"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#summary-2"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="numericalprobability.html"><a href="numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="numericalprobability.html"><a href="numericalprobability.html#approximation-based-on-random-chances"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="numericalprobability.html"><a href="numericalprobability.html#distribution"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="numericalprobability.html"><a href="numericalprobability.html#mass-and-density"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="numericalprobability.html"><a href="numericalprobability.html#probability"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-density-function-pdf"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-mass-function-pmf"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="numericalprobability.html"><a href="numericalprobability.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="numericalprobability.html"><a href="numericalprobability.html#special-functions"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="numericalprobability.html"><a href="numericalprobability.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="numericalprobability.html"><a href="numericalprobability.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="numericalprobability.html"><a href="numericalprobability.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="numericalprobability.html"><a href="numericalprobability.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="numericalprobability.html"><a href="numericalprobability.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="numericalprobability.html"><a href="numericalprobability.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="numericalprobability.html"><a href="numericalprobability.html#distributiontypes"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="numericalprobability.html"><a href="numericalprobability.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="numericalprobability.html"><a href="numericalprobability.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="numericalprobability.html"><a href="numericalprobability.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="numericalprobability.html"><a href="numericalprobability.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="numericalprobability.html"><a href="numericalprobability.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="numericalprobability.html"><a href="numericalprobability.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="numericalprobability.html"><a href="numericalprobability.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="numericalprobability.html"><a href="numericalprobability.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="numericalprobability.html"><a href="numericalprobability.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="numericalprobability.html"><a href="numericalprobability.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="numericalprobability.html"><a href="numericalprobability.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="numericalprobability.html"><a href="numericalprobability.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="numericalprobability.html"><a href="numericalprobability.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="numericalprobability.html"><a href="numericalprobability.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="numericalprobability.html"><a href="numericalprobability.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="numericalprobability.html"><a href="numericalprobability.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="numericalprobability.html"><a href="numericalprobability.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="numericalprobability.html"><a href="numericalprobability.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="numericalprobability.html"><a href="numericalprobability.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="numericalprobability.html"><a href="numericalprobability.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="numericalprobability.html"><a href="numericalprobability.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="numericalprobability.html"><a href="numericalprobability.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="numericalprobability.html"><a href="numericalprobability.html#summary-3"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="statistics.html"><a href="statistics.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="statistics.html"><a href="statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="statistics.html"><a href="statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="statistics.html"><a href="statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="statistics.html"><a href="statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="statistics.html"><a href="statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="statistics.html"><a href="statistics.html#inferential-statistics"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="statistics.html"><a href="statistics.html#the-significance-of-difference"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="statistics.html"><a href="statistics.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="statistics.html"><a href="statistics.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="statistics.html"><a href="statistics.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="statistics.html"><a href="statistics.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="statistics.html"><a href="statistics.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="statistics.html"><a href="statistics.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="statistics.html"><a href="statistics.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="statistics.html"><a href="statistics.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="statistics.html"><a href="statistics.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="statistics.html"><a href="statistics.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="statistics.html"><a href="statistics.html#post-hoc-analysis"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="statistics.html"><a href="statistics.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="statistics.html"><a href="statistics.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="statistics.html"><a href="statistics.html#multiple-comparison-tests"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="statistics.html"><a href="statistics.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="statistics.html"><a href="statistics.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="statistics.html"><a href="statistics.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="statistics.html"><a href="statistics.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="statistics.html"><a href="statistics.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="statistics.html"><a href="statistics.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="statistics.html"><a href="statistics.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="statistics.html"><a href="statistics.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="statistics.html"><a href="statistics.html#statistical-modeling"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="statistics.html"><a href="statistics.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="statistics.html"><a href="statistics.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="statistics.html"><a href="statistics.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="statistics.html"><a href="statistics.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="statistics.html"><a href="statistics.html#regression-analysis"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="statistics.html"><a href="statistics.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="statistics.html"><a href="statistics.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="statistics.html"><a href="statistics.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="statistics.html"><a href="statistics.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="statistics.html"><a href="statistics.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="statistics.html"><a href="statistics.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="statistics.html"><a href="statistics.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="statistics.html"><a href="statistics.html#the-significance-of-regression"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="statistics.html"><a href="statistics.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="statistics.html"><a href="statistics.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="statistics.html"><a href="statistics.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="statistics.html"><a href="statistics.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="statistics.html"><a href="statistics.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="statistics.html"><a href="statistics.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="statistics.html"><a href="statistics.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="statistics.html"><a href="statistics.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="statistics.html"><a href="statistics.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="statistics.html"><a href="statistics.html#inference-for-regression"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="statistics.html"><a href="statistics.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="statistics.html"><a href="statistics.html#summary-4"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian.html"><a href="bayesian.html#probability-1"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian.html"><a href="bayesian.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian.html"><a href="bayesian.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian.html"><a href="bayesian.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian.html"><a href="bayesian.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="bayesian.html"><a href="bayesian.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian.html"><a href="bayesian.html#probability-rules"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian.html"><a href="bayesian.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian.html"><a href="bayesian.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian.html"><a href="bayesian.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="bayesian.html"><a href="bayesian.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="bayesian.html"><a href="bayesian.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="bayesian.html"><a href="bayesian.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="bayesian.html"><a href="bayesian.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="bayesian.html"><a href="bayesian.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="bayesian.html"><a href="bayesian.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian.html"><a href="bayesian.html#bayes-theorem"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian.html"><a href="bayesian.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian.html"><a href="bayesian.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="bayesian.html"><a href="bayesian.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian.html"><a href="bayesian.html#conjugacy"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian.html"><a href="bayesian.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian.html"><a href="bayesian.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian.html"><a href="bayesian.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="bayesian.html"><a href="bayesian.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="bayesian.html"><a href="bayesian.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="bayesian.html"><a href="bayesian.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="bayesian.html"><a href="bayesian.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="bayesian.html"><a href="bayesian.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="bayesian.html"><a href="bayesian.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="bayesian.html"><a href="bayesian.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="bayesian.html"><a href="bayesian.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="bayesian.html"><a href="bayesian.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian.html"><a href="bayesian.html#information-theory"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian.html"><a href="bayesian.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian.html"><a href="bayesian.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian.html"><a href="bayesian.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian.html"><a href="bayesian.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian.html"><a href="bayesian.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian.html"><a href="bayesian.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian.html"><a href="bayesian.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bayesian.html"><a href="bayesian.html#bayesianinference"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="bayesian.html"><a href="bayesian.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="bayesian.html"><a href="bayesian.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="bayesian.html"><a href="bayesian.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="bayesian.html"><a href="bayesian.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian2.html"><a href="bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-models"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="bayesian2.html"><a href="bayesian2.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="bayesian2.html"><a href="bayesian2.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="bayesian2.html"><a href="bayesian2.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="bayesian2.html"><a href="bayesian2.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="bayesian2.html"><a href="bayesian2.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="bayesian2.html"><a href="bayesian2.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="bayesian2.html"><a href="bayesian2.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="bayesian2.html"><a href="bayesian2.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="bayesian2.html"><a href="bayesian2.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="bayesian2.html"><a href="bayesian2.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="bayesian2.html"><a href="bayesian2.html#simulation-and-sampling"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="bayesian2.html"><a href="bayesian2.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="bayesian2.html"><a href="bayesian2.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="bayesian2.html"><a href="bayesian2.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="bayesian2.html"><a href="bayesian2.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="bayesian2.html"><a href="bayesian2.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="bayesian2.html"><a href="bayesian2.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-analysis"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian2.html"><a href="bayesian2.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian2.html"><a href="bayesian2.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian2.html"><a href="bayesian2.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="bayesian2.html"><a href="bayesian2.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian2.html"><a href="bayesian2.html#summary-5"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="machinelearning1.html"><a href="machinelearning1.html#observation-and-measurement"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="machinelearning1.html"><a href="machinelearning1.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="machinelearning1.html"><a href="machinelearning1.html#input-data"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="machinelearning1.html"><a href="machinelearning1.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="machinelearning1.html"><a href="machinelearning1.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="machinelearning1.html"><a href="machinelearning1.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="machinelearning1.html"><a href="machinelearning1.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="machinelearning1.html"><a href="machinelearning1.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="machinelearning1.html"><a href="machinelearning1.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="machinelearning1.html"><a href="machinelearning1.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="machinelearning1.html"><a href="machinelearning1.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machinelearning1.html"><a href="machinelearning1.html#primitive-methods"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="machinelearning1.html"><a href="machinelearning1.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="machinelearning1.html"><a href="machinelearning1.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="machinelearning1.html"><a href="machinelearning1.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="machinelearning1.html"><a href="machinelearning1.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="machinelearning1.html"><a href="machinelearning1.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="machinelearning1.html"><a href="machinelearning1.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="machinelearning1.html"><a href="machinelearning1.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="machinelearning1.html"><a href="machinelearning1.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="machinelearning1.html"><a href="machinelearning1.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="machinelearning1.html"><a href="machinelearning1.html#distance-metrics"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="machinelearning1.html"><a href="machinelearning1.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="machinelearning1.html"><a href="machinelearning1.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="machinelearning1.html"><a href="machinelearning1.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="machinelearning1.html"><a href="machinelearning1.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="machinelearning1.html"><a href="machinelearning1.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="machinelearning1.html"><a href="machinelearning1.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="machinelearning1.html"><a href="machinelearning1.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="machinelearning1.html"><a href="machinelearning1.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="machinelearning1.html"><a href="machinelearning1.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="machinelearning1.html"><a href="machinelearning1.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="machinelearning1.html"><a href="machinelearning1.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="machinelearning1.html"><a href="machinelearning1.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="machinelearning1.html"><a href="machinelearning1.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="machinelearning1.html"><a href="machinelearning1.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="machinelearning1.html"><a href="machinelearning1.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="machinelearning1.html"><a href="machinelearning1.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="machinelearning1.html"><a href="machinelearning1.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="machinelearning1.html"><a href="machinelearning1.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="machinelearning1.html"><a href="machinelearning1.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="machinelearning1.html"><a href="machinelearning1.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="machinelearning1.html"><a href="machinelearning1.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="machinelearning1.html"><a href="machinelearning1.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="machinelearning1.html"><a href="machinelearning1.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="machinelearning1.html"><a href="machinelearning1.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="machinelearning1.html"><a href="machinelearning1.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="machinelearning1.html"><a href="machinelearning1.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureengineering"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="machinelearning1.html"><a href="machinelearning1.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="machinelearning1.html"><a href="machinelearning1.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="machinelearning1.html"><a href="machinelearning1.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="machinelearning1.html"><a href="machinelearning1.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="machinelearning1.html"><a href="machinelearning1.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="machinelearning1.html"><a href="machinelearning1.html#general-modeling"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="machinelearning1.html"><a href="machinelearning1.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="machinelearning1.html"><a href="machinelearning1.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="machinelearning1.html"><a href="machinelearning1.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="machinelearning1.html"><a href="machinelearning1.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="machinelearning1.html"><a href="machinelearning1.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="machinelearning1.html"><a href="machinelearning1.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="machinelearning1.html"><a href="machinelearning1.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="machinelearning1.html"><a href="machinelearning1.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="machinelearning1.html"><a href="machinelearning1.html#supervised-vs.unsupervised-learning"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="machinelearning1.html"><a href="machinelearning1.html#summary-6"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="machinelearning2.html"><a href="machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="machinelearning2.html"><a href="machinelearning2.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="machinelearning2.html"><a href="machinelearning2.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="machinelearning2.html"><a href="machinelearning2.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="machinelearning2.html"><a href="machinelearning2.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="machinelearning2.html"><a href="machinelearning2.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="machinelearning2.html"><a href="machinelearning2.html#binary-classification-supervised"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="machinelearning2.html"><a href="machinelearning2.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="machinelearning2.html"><a href="machinelearning2.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="machinelearning2.html"><a href="machinelearning2.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="machinelearning2.html"><a href="machinelearning2.html#multi-class-classification-supervised"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="machinelearning2.html"><a href="machinelearning2.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="machinelearning2.html"><a href="machinelearning2.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="machinelearning2.html"><a href="machinelearning2.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="machinelearning2.html"><a href="machinelearning2.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="machinelearning2.html"><a href="machinelearning2.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="machinelearning3.html"><a href="machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="machinelearning3.html"><a href="machinelearning3.html#clustering-unsupervised"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="machinelearning3.html"><a href="machinelearning3.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="machinelearning3.html"><a href="machinelearning3.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="machinelearning3.html"><a href="machinelearning3.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="machinelearning3.html"><a href="machinelearning3.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="machinelearning3.html"><a href="machinelearning3.html#meta-learning"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="machinelearning3.html"><a href="machinelearning3.html#natural-language-processing-nlp"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="machinelearning3.html"><a href="machinelearning3.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="machinelearning3.html"><a href="machinelearning3.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="machinelearning3.html"><a href="machinelearning3.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="machinelearning3.html"><a href="machinelearning3.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="machinelearning3.html"><a href="machinelearning3.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="machinelearning3.html"><a href="machinelearning3.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="machinelearning3.html"><a href="machinelearning3.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="machinelearning3.html"><a href="machinelearning3.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-forecasting"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="machinelearning3.html"><a href="machinelearning3.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="machinelearning3.html"><a href="machinelearning3.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="machinelearning3.html"><a href="machinelearning3.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="machinelearning3.html"><a href="machinelearning3.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="machinelearning3.html"><a href="machinelearning3.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="machinelearning3.html"><a href="machinelearning3.html#recommender-systems"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deeplearning1.html"><a href="deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="deeplearning1.html"><a href="deeplearning1.html#simple-perceptron"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="deeplearning1.html"><a href="deeplearning1.html#adaptive-linear-neuron-adaline"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="deeplearning1.html"><a href="deeplearning1.html#multi-layer-perceptron-mlp"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="deeplearning1.html"><a href="deeplearning1.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="deeplearning1.html"><a href="deeplearning1.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="deeplearning1.html"><a href="deeplearning1.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="deeplearning1.html"><a href="deeplearning1.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="deeplearning1.html"><a href="deeplearning1.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="deeplearning1.html"><a href="deeplearning1.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="deeplearning1.html"><a href="deeplearning1.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="deeplearning1.html"><a href="deeplearning1.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="deeplearning1.html"><a href="deeplearning1.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="deeplearning1.html"><a href="deeplearning1.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="deeplearning1.html"><a href="deeplearning1.html#convolutional-neural-network-cnn"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="deeplearning1.html"><a href="deeplearning1.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="deeplearning1.html"><a href="deeplearning1.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="deeplearning1.html"><a href="deeplearning1.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="deeplearning1.html"><a href="deeplearning1.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="deeplearning1.html"><a href="deeplearning1.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="deeplearning1.html"><a href="deeplearning1.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="deeplearning1.html"><a href="deeplearning1.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="deeplearning1.html"><a href="deeplearning1.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="deeplearning1.html"><a href="deeplearning1.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="deeplearning1.html"><a href="deeplearning1.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="deeplearning1.html"><a href="deeplearning1.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="deeplearning2.html"><a href="deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="deeplearning2.html"><a href="deeplearning2.html#residual-network-resnet"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="deeplearning2.html"><a href="deeplearning2.html#recurrent-neural-network-rnn"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="deeplearning2.html"><a href="deeplearning2.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="deeplearning2.html"><a href="deeplearning2.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="deeplearning2.html"><a href="deeplearning2.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-rnn"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-bidirectional-rnn"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-neural-network-tnn"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="deeplearning2.html"><a href="deeplearning2.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="deeplearning2.html"><a href="deeplearning2.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="deeplearning2.html"><a href="deeplearning2.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="deeplearning2.html"><a href="deeplearning2.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="deeplearning2.html"><a href="deeplearning2.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="deeplearning2.html"><a href="deeplearning2.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="deeplearning2.html"><a href="deeplearning2.html#applications-using-tnn-and-rnn"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="deeplearning2.html"><a href="deeplearning2.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="deeplearning2.html"><a href="deeplearning2.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="deeplearning2.html"><a href="deeplearning2.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="deeplearning2.html"><a href="deeplearning2.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="deeplearning2.html"><a href="deeplearning2.html#generative-adversarial-network-gan"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-reinforcement-network-dqn"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="deeplearning2.html"><a href="deeplearning2.html#summary-8"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="distributedcomputation.html"><a href="distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#integration-and-interoperability"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#ml-pipelines"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-standards"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="distributedcomputation.html"><a href="distributedcomputation.html#general-summary"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>15</b> Appendix</a><ul>
<li class="chapter" data-level="15.1" data-path="appendix.html"><a href="appendix.html#appendix-a"><i class="fa fa-check"></i><b>15.1</b> Appendix A</a><ul>
<li class="chapter" data-level="15.1.1" data-path="appendix.html"><a href="appendix.html#trigonometry"><i class="fa fa-check"></i><b>15.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="15.1.2" data-path="appendix.html"><a href="appendix.html#logarithms"><i class="fa fa-check"></i><b>15.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="15.1.3" data-path="appendix.html"><a href="appendix.html#category-theory"><i class="fa fa-check"></i><b>15.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="appendix.html"><a href="appendix.html#appendix-b"><i class="fa fa-check"></i><b>15.2</b> Appendix B</a><ul>
<li class="chapter" data-level="15.2.1" data-path="appendix.html"><a href="appendix.html#on-random-chances"><i class="fa fa-check"></i><b>15.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="15.2.2" data-path="appendix.html"><a href="appendix.html#on-replacements"><i class="fa fa-check"></i><b>15.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="15.2.3" data-path="appendix.html"><a href="appendix.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>15.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="15.2.4" data-path="appendix.html"><a href="appendix.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>15.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="15.2.5" data-path="appendix.html"><a href="appendix.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>15.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="15.2.6" data-path="appendix.html"><a href="appendix.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>15.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="15.2.7" data-path="appendix.html"><a href="appendix.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>15.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="appendix.html"><a href="appendix.html#appendix-c"><i class="fa fa-check"></i><b>15.3</b> Appendix C</a></li>
<li class="chapter" data-level="15.4" data-path="appendix.html"><a href="appendix.html#appendix-d"><i class="fa fa-check"></i><b>15.4</b> Appendix D</a><ul>
<li class="chapter" data-level="15.4.1" data-path="appendix.html"><a href="appendix.html#lubridate-library"><i class="fa fa-check"></i><b>15.4.1</b> Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="deeplearning1" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 12</span> Computational Deep Learning I<a href="deeplearning1.html#deeplearning1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
            autoNumber: "AMS",
            formatNumber: function (n) {return '12.'+n}
      } 
  }
});
</script>
<p>We begin this chapter with an extension of <strong>Computational Learning</strong>, focusing on <strong>Computational Deep Learning</strong>. The term <strong>Computational</strong> is added in this literature to emphasize the computational nature of machine learning. We also now emphasize the term <strong>Deep</strong> in a new context. While there may be many definitions and descriptions of <strong>Deep Learning</strong>, the <strong>computation</strong> involves a <strong>Layered Network</strong> of <strong>nodes</strong>. On the other hand, it is unavoidable to encounter the common bigram <strong>Neural Network</strong> in reference to our biological network structure of neurons. Figure <a href="deeplearning1.html#fig:neuron">12.1</a> shows a picture of a single neuron in the brain.  </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:neuron"></span>
<img src="neuron.png" alt="Neuron" width="80%" />
<p class="caption">
Figure 12.1: Neuron
</p>
</div>
<p>In the figure, a single neuron takes an input as a neural stimulus that stimulates - whether to excite or inhibit. When a neuron is excited around a certain threshold in units of millivolts, it fires an <strong>action potential</strong> at the axon hillock. This <strong>action potential</strong>, also called <strong>nerve impulse</strong>, travels through the neuron to the axon terminals at the <strong>synapse</strong> - a junction where communication happens between neurons. At the <strong>synapse</strong>, <strong>neurotransmitters</strong> are emitted in the form of chemical molecules or electrical ions. The molecules enter receptors of another neuron, which takes them as input.</p>
<p><strong>Deep Learning</strong> follows a similar concept; thus, the term is associated with <strong>Neural network</strong> in which a message gets transmitted from neuron to neuron. The most basic premise from which deep learning thrives is the <strong>activation function</strong> - a function that takes the activation output of a <strong>hidden layer</strong> and translates it into a bounded output. See Figure <a href="deeplearning1.html#fig:simpleneuron">12.2</a>.  </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simpleneuron"></span>
<img src="simpleneuron.png" alt="Simple Neuron" width="60%" />
<p class="caption">
Figure 12.2: Simple Neuron
</p>
</div>
<p>The term <strong>activation</strong> is akin to the idea that neurons measure a metric called <strong>millivolts</strong>. There is a certain threshold to meet such that if the conditions are right, the neuron can trigger an <strong>action potential</strong>, eventually transmitting an output message to the next neuron.</p>
<p>In the next section, let us discuss <strong>Neural Network</strong> starting with the most basic structure of a simple <strong>Perceptron</strong>.</p>
<div id="simple-perceptron" class="section level2 hasAnchor">
<h2><span class="header-section-number">12.1</span> Simple Perceptron  <a href="deeplearning1.html#simple-perceptron" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <strong>Perceptron</strong> is a simple single-layered <strong>Neural Network</strong>. An intuition behind <strong>Perceptron</strong> starts with a typical schema shown in Figure <a href="deeplearning1.html#fig:perceptron">12.3</a> based on <strong>Frank Rosenblatt</strong>âs machinery that he built in 1958 called <strong>Perceptron</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:perceptron"></span>
<img src="perceptron.png" alt="Perceptron" width="80%" />
<p class="caption">
Figure 12.3: Perceptron
</p>
</div>
<p>The schema can be expressed mathematically as a linear equation like so (where we use <span class="math inline">\(\omega_i\)</span> coefficients as <strong>weights</strong> in the equation).</p>
<p><span class="math display">\[\begin{align}
z = f(X) =  \sum_{j=1}^p \omega_j x_j + \omega_0 = \omega_0 + \omega_1 x_1 + \omega_2 x_2\ + ...\ + \omega_p x_p + \epsilon
\end{align}\]</span></p>
<p>The intercept is denoted by the symbol <span class="math inline">\(\omega_0\)</span>, which represents the <strong>bias</strong>. Disregarding the white noise (<span class="math inline">\(\epsilon\)</span>), we then have the approximation below:</p>
<p><span class="math display">\[\begin{align}
\hat{z} = \hat{f}(X) =  \sum_{j=1}^p \omega_j x_j + \omega_0 = \omega_0 + \omega_1 x_1 + \omega_2 x_2\ + ...\ + \omega_p x_p
\end{align}\]</span></p>
<p>From there, <strong>Perceptron</strong> feeds the result of the approximation, <span class="math inline">\(\hat{z}\)</span>, into a <strong>binary step</strong> <strong>activation function</strong> which enforces the <strong>decision boundary</strong> or <strong>threshold</strong>. The schema uses a <strong>unit step function</strong> as our <strong>activation function</strong> if our target is binomial in the range (0,1); otherwise, we use a <strong>sign function</strong> if the target is in the range (-1, 1).</p>
<p><span class="math display">\[\begin{align}
\hat{y} = 
\underbrace{
\begin{cases}
1 &amp; \hat{z} &gt; 0\\
0 &amp; otherwise
\end{cases}
}_{\text{heaviside step or unit step function}}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\hat{y} = \underbrace{ sign(\hat{z}) }_{\text{sign function}} \label{eqn:eqnnumber600}
\end{align}\]</span></p>
<p>Our goal is to be able to predict <span class="math inline">\(\hat{y}\)</span> as close to the target <span class="math inline">\(y\)</span>, which can be achieved if we can optimize the <span class="math inline">\(\omega\)</span> coefficients. To meet this goal, we train the <strong>Perceptron</strong> based on the few steps below:</p>
<p><strong>First</strong>, we initialize the <span class="math inline">\(\omega\)</span> coefficients with arbitrary values. For our purpose, we choose to start with zeroes:</p>
<p><span class="math display">\[
\omega_j = 0,\ \ \ \ \ \ \ for\ j\ in\ 0,..,p
\]</span></p>
<p><strong>Second</strong>, we then compute for <span class="math inline">\(\hat{y}\)</span>:</p>
<p><span class="math display">\[\begin{align}
\hat{y} = step(\hat{z})  = step\left(\sum_{j=1}^p \omega_j x_j + \omega_0 \right)
\end{align}\]</span></p>
<p><strong>Third</strong>, we then determine the close estimate of the coefficients by optimization (minimization) using the <strong>Perceptron learning rule</strong> expressed below (where <span class="math inline">\(\eta\)</span> is the learning rate):</p>
<p><span class="math display">\[\begin{align}
\omega_j = \omega_j + \eta \nabla \omega_j \mathcal{L}
\end{align}\]</span>
<span class="math display">\[\begin{align}
where \ \ \ \ \nabla \omega_j \mathcal{L}= 
\begin{cases}
(y_i - \hat{y}_i) (1) &amp; if\ j = 0,\ then\ x_{i,0} = 1 \ \ \ \ \text{(bias)} \\
(y_i - \hat{y}_i) (x_{i,j}) &amp; otherwise 
\end{cases} \label{eqn:eqnnumber601}
\end{align}\]</span></p>
<p><strong>Fourth</strong>, we repeat the second and third steps until we reach a given maximum iteration.</p>
<p>Below is our example implementation of a simple <strong>Perceptron</strong>:</p>

<div class="sourceCode" id="cb1783"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1783-1" data-line-number="1">my.perceptron &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, <span class="dt">epoch=</span><span class="dv">50</span>, <span class="dt">eta =</span> <span class="fl">0.01</span>) {</a>
<a class="sourceLine" id="cb1783-2" data-line-number="2">  A  &lt;-<span class="st"> </span><span class="cf">function</span>(h) { <span class="kw">ifelse</span>(h <span class="op">&gt;=</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">-1</span>)  }</a>
<a class="sourceLine" id="cb1783-3" data-line-number="3">  H  &lt;-<span class="st"> </span><span class="cf">function</span>(x, omega) { x <span class="op">%*%</span><span class="st"> </span>omega } <span class="co"># assume x0 = 1</span></a>
<a class="sourceLine" id="cb1783-4" data-line-number="4">  n =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1783-5" data-line-number="5">  error =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, epoch)</a>
<a class="sourceLine" id="cb1783-6" data-line-number="6">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1783-7" data-line-number="7">     <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1783-8" data-line-number="8">         y.hat =<span class="st"> </span><span class="kw">A</span>(<span class="kw">H</span>(x[i,], omega))</a>
<a class="sourceLine" id="cb1783-9" data-line-number="9">         delta =<span class="st"> </span><span class="kw">c</span>((y[i] <span class="op">-</span><span class="st"> </span>y.hat) )</a>
<a class="sourceLine" id="cb1783-10" data-line-number="10">         omega =<span class="st"> </span>omega <span class="op">+</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>delta <span class="op">*</span><span class="st"> </span>x[i,]</a>
<a class="sourceLine" id="cb1783-11" data-line-number="11">         <span class="cf">if</span> (delta <span class="op">!=</span><span class="st"> </span><span class="fl">0.0</span>) {</a>
<a class="sourceLine" id="cb1783-12" data-line-number="12">           error[t] =<span class="st"> </span>error[t] <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1783-13" data-line-number="13">         }</a>
<a class="sourceLine" id="cb1783-14" data-line-number="14">     }</a>
<a class="sourceLine" id="cb1783-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb1783-16" data-line-number="16">  <span class="kw">list</span>(<span class="st">&quot;coefficients&quot;</span> =<span class="st"> </span>omega, <span class="st">&quot;error&quot;</span> =<span class="st"> </span>error)</a>
<a class="sourceLine" id="cb1783-17" data-line-number="17">}</a></code></pre></div>

<p>We set <span class="math inline">\(x_0 = 1\)</span> and include in <span class="math inline">\(X\)</span> so that we can just conveniently use a simple equation:</p>
<p><span class="math display">\[\begin{align}
\hat{z} = X \cdot \omega\ \ \ \ \leftarrow \hat{z} = \mathbf{(\omega_0)} 1 + \mathbf{(\omega_1)} x_1 +\ ... \mathbf{(\omega_p)} x_p
\end{align}\]</span></p>
<p>Before illustrating the use of our <strong>Perceptron</strong> implementation, let us first recall <strong>SVM</strong>. In Chapter <strong>10</strong> (<strong>Computational Learning II</strong>), we use our <strong>SVM</strong> implementation of <strong>PEGASOS</strong> to perform a <strong>supervised</strong> binary classification. We start using the following dataset below:</p>

<div class="sourceCode" id="cb1784"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1784-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1784-2" data-line-number="2">N         =<span class="st"> </span><span class="dv">20</span>; v =<span class="st"> </span><span class="dv">1</span> <span class="co"># variance</span></a>
<a class="sourceLine" id="cb1784-3" data-line-number="3">x1.blue   =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v);  x2.blue =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); y1 =<span class="st"> </span><span class="kw">rep</span>( <span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1784-4" data-line-number="4">x1.red    =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v);  x2.red  =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); y2 =<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1784-5" data-line-number="5">x1        =<span class="st"> </span><span class="kw">c</span>(x1.blue, x1.red); x2      =<span class="st"> </span><span class="kw">c</span>(x2.blue, x2.red)</a>
<a class="sourceLine" id="cb1784-6" data-line-number="6">x         =<span class="st"> </span><span class="kw">cbind</span>(x1, x2)   </a>
<a class="sourceLine" id="cb1784-7" data-line-number="7">y         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(y1, y2))</a>
<a class="sourceLine" id="cb1784-8" data-line-number="8">data      =<span class="st"> </span><span class="kw">cbind</span>(x, y)</a></code></pre></div>

<p>We then model our dataset using <strong>PEGASOS SVM</strong> like so:</p>

<div class="sourceCode" id="cb1785"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1785-1" data-line-number="1">svm.pegasos.model =<span class="st"> </span><span class="kw">my.pegasos.svm</span>(data) </a>
<a class="sourceLine" id="cb1785-2" data-line-number="2">hplanes           =<span class="st"> </span><span class="kw">svm.hyperplanes</span>(svm.pegasos.model)</a></code></pre></div>

<p>The <strong>decision boundary</strong> along with the hyperplanes are shown in Figure <a href="deeplearning1.html#fig:pegsvm">12.4</a>.</p>

<div class="sourceCode" id="cb1786"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1786-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x1), <span class="dt">ylim=</span><span class="kw">range</span>(x2),</a>
<a class="sourceLine" id="cb1786-2" data-line-number="2">     <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">main=</span><span class="st">&quot;PEGASOS SVM&quot;</span>)</a>
<a class="sourceLine" id="cb1786-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1786-4" data-line-number="4"><span class="kw">points</span>(x, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span><span class="dv">-1</span>, <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>))</a>
<a class="sourceLine" id="cb1786-5" data-line-number="5"><span class="kw">abline</span>(<span class="dt">a=</span>hplanes<span class="op">$</span>H0.int, <span class="dt">b=</span>hplanes<span class="op">$</span>slope, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1786-6" data-line-number="6"><span class="kw">abline</span>(<span class="dt">a=</span>hplanes<span class="op">$</span>Hp.int, <span class="dt">b=</span>hplanes<span class="op">$</span>slope, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a>
<a class="sourceLine" id="cb1786-7" data-line-number="7"><span class="kw">abline</span>(<span class="dt">a=</span>hplanes<span class="op">$</span>Hn.int, <span class="dt">b=</span>hplanes<span class="op">$</span>slope, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pegsvm"></span>
<img src="DS_files/figure-html/pegsvm-1.png" alt="PEGASOS SVM" width="70%" />
<p class="caption">
Figure 12.4: PEGASOS SVM
</p>
</div>

<p>Now, let us take the same dataset to include the constant <strong>1</strong> for the intercept. At the same time, we initializeour coefficients to an arbitrary value, e.g., using zeroes for now.</p>

<div class="sourceCode" id="cb1787"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1787-1" data-line-number="1">n =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1787-2" data-line-number="2">x0 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, n)      <span class="co"># adding constant 1 for the intercept</span></a>
<a class="sourceLine" id="cb1787-3" data-line-number="3">x.p  =<span class="st"> </span><span class="kw">cbind</span>(x0, x)</a>
<a class="sourceLine" id="cb1787-4" data-line-number="4">omega =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)  <span class="co"># initialize our coefficients</span></a>
<a class="sourceLine" id="cb1787-5" data-line-number="5"><span class="kw">head</span>(x.p, <span class="dt">n =</span> <span class="dv">10</span>)   <span class="co"># display first 10 observations</span></a></code></pre></div>
<pre><code>##       x0      x1      x2
##  [1,]  1  0.3216  2.8066
##  [2,]  1 -2.6603  1.4240
##  [3,]  1 -1.6949  3.0092
##  [4,]  1 -3.1667  1.7801
##  [5,]  1 -3.2775  0.4221
##  [6,]  1 -2.7310  2.4981
##  [7,]  1 -1.5136 -0.3908
##  [8,]  1 -2.7902  2.2235
##  [9,]  1 -2.6862  1.8131
## [10,]  1 -1.7199  3.6344</code></pre>

<p>Next, we then generate our model using our implementation of <strong>Perceptron</strong> (this gives an approximation of our <span class="math inline">\(\omega\)</span> coefficients along with the error count per iteration):</p>

<div class="sourceCode" id="cb1789"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1789-1" data-line-number="1">(<span class="dt">model =</span> <span class="kw">my.perceptron</span>(x.p, y, <span class="dt">epoch=</span><span class="dv">3</span>))</a></code></pre></div>
<pre><code>## $coefficients
##       x0       x1       x2 
##  0.00000 -0.04380  0.03507 
## 
## $error
## [1] 1 1 0</code></pre>

<p>Let us use the optimized <strong>coefficients</strong> to visualize the decision boundary. We concoct the points (<span class="math inline">\(\mathbf{x_1, x_2}\)</span>) that run through the decision boundary like so:</p>

<div class="sourceCode" id="cb1791"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1791-1" data-line-number="1">p.x1 =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dt">length.out=</span><span class="dv">40</span>)</a>
<a class="sourceLine" id="cb1791-2" data-line-number="2">cof  =<span class="st"> </span>model<span class="op">$</span>coefficients</a>
<a class="sourceLine" id="cb1791-3" data-line-number="3">b    =<span class="st"> </span>cof[<span class="dv">1</span>]; w1 =<span class="st"> </span>cof[<span class="dv">2</span>]; w2 =<span class="st"> </span>cof[<span class="dv">3</span>] </a>
<a class="sourceLine" id="cb1791-4" data-line-number="4">p.x2 =<span class="st"> </span>( <span class="op">-</span>w1 <span class="op">/</span><span class="st"> </span>w2 ) <span class="op">*</span><span class="st"> </span>p.x1 <span class="op">+</span><span class="st"> </span>b</a></code></pre></div>

<p>Finally, we plot the decision boundary based on <strong>Perceptron</strong> (see Figure <a href="deeplearning1.html#fig:perceptmodel">12.5</a>).</p>

<div class="sourceCode" id="cb1792"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1792-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x1), <span class="dt">ylim=</span><span class="kw">range</span>(x2),</a>
<a class="sourceLine" id="cb1792-2" data-line-number="2">     <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Perceptron&quot;</span>)</a>
<a class="sourceLine" id="cb1792-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1792-4" data-line-number="4"><span class="kw">points</span>(x, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span><span class="dv">-1</span>, <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>))</a>
<a class="sourceLine" id="cb1792-5" data-line-number="5"><span class="kw">lines</span>(p.x1, p.x2, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:perceptmodel"></span>
<img src="DS_files/figure-html/perceptmodel-1.png" alt="Perceptron Model" width="70%" />
<p class="caption">
Figure 12.5: Perceptron Model
</p>
</div>

<p>Notice that the <strong>decision boundary</strong> closely matches the same boundary line produced by implementing <strong>PEGASOS SVM</strong>.</p>
</div>
<div id="adaptive-linear-neuron-adaline" class="section level2 hasAnchor">
<h2><span class="header-section-number">12.2</span> Adaptive Linear Neuron (ADALINE)  <a href="deeplearning1.html#adaptive-linear-neuron-adaline" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bernard Widrow and Ted Hoff, in 1960, formulated a variant of <strong>Perceptron</strong> called <strong>Adaptive Linear Neuron (ADALINE)</strong>. The schema is shown in Figure <a href="deeplearning1.html#fig:adaline">12.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:adaline"></span>
<img src="adaline.png" alt="Adaptive Linear Regression" width="80%" />
<p class="caption">
Figure 12.6: Adaptive Linear Regression
</p>
</div>
<p>The <strong>ADALINE</strong> schema is almost similar to the <strong>Perceptron</strong> schema. However, the algorithm is slightly modified:</p>
<p><strong>First</strong>, we initialize the <span class="math inline">\(\omega\)</span> coefficients with arbitrary values. For that purpose, we choose to start with zeroes:</p>
<p><span class="math display">\[
\omega_j = 0,\ \ \ \ \ \ \ for\ j\ in\ 0,..,p
\]</span></p>
<p><strong>Second</strong>, we then compute for <span class="math inline">\(\hat{y}\)</span>. Notice that we omit the <strong>step function</strong>. Instead, our <strong>activation function</strong> is linear and thus is still identical to the <strong>hypothesis function</strong>; thus, our activation function becomes an <strong>identity function</strong>.</p>
<p><span class="math display">\[\begin{align}
\hat{y} = \hat{z}  = \sum_{j=1}^p \omega_j x_j + \omega_0 
\end{align}\]</span></p>
<p><strong>Third</strong>, we then determine the close estimate of the coefficients by optimization (minimization) using the <strong>ADALINE learning rule</strong> expressed below (where <span class="math inline">\(\eta\)</span> is the learning rate):</p>
<p><span class="math display">\[\begin{align}
\omega_j = \omega_j + \eta \nabla \omega_j \mathcal{L}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
where \ \ \ \ \nabla  \omega_j \mathcal{L}= 
\begin{cases}
(y_i - \hat{y}_i) (1) &amp; if\ j = 0,\ then\ x_{i,0} = 1 \ \ \ \ \text{(bias)} \\
(y_i - \hat{y}_i) (x_{i,j}) &amp; otherwise 
\end{cases}  \label{eqn:eqnnumber602}
\end{align}\]</span></p>
<p><strong>Fourth</strong>, we repeat the second and third steps until we reach a given maximum iteration. For each iteration, we calculate the error. In <strong>ADALINE</strong>, we can use any of the <strong>residual</strong> measures such as <strong>MSE</strong>, <strong>RMSE</strong>, or <strong>MAE</strong> for our <strong>loss function</strong> â of which the latter two are based on the <strong>sum of squared errors (SSE)</strong>, also called <strong>residual sum of squares (RSS)</strong>. For example:</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}\left\{
MSE =  \frac{1}{N} \sum(y - \hat{y})^2\ \ \ \ \
RMSE = \sqrt{\frac{1}{N} \sum (y - \hat{y})^2}\ \ \ \ 
MAE =  \frac{1}{N} \sum|y - \hat{y}|
\right\}
\end{align}\]</span></p>
<p>Below is our example implementation of <strong>ADALINE</strong>:</p>

<div class="sourceCode" id="cb1793"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1793-1" data-line-number="1">my.adaline &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, <span class="dt">epoch=</span><span class="dv">50</span>, <span class="dt">eta =</span> <span class="fl">0.01</span>, <span class="dt">tol=</span><span class="fl">1e-3</span>) {</a>
<a class="sourceLine" id="cb1793-2" data-line-number="2">  A  &lt;-<span class="st"> </span><span class="cf">function</span>(h) { <span class="kw">ifelse</span>(h <span class="op">&gt;=</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">-1</span>)  }</a>
<a class="sourceLine" id="cb1793-3" data-line-number="3">  H  &lt;-<span class="st"> </span><span class="cf">function</span>(x, omega) { x <span class="op">%*%</span><span class="st"> </span>omega } <span class="co"># assume x0 = 1</span></a>
<a class="sourceLine" id="cb1793-4" data-line-number="4">  n =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1793-5" data-line-number="5">  y.hat =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1793-6" data-line-number="6">  mse =<span class="st"> </span><span class="ot">NULL</span> </a>
<a class="sourceLine" id="cb1793-7" data-line-number="7">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1793-8" data-line-number="8">     sse =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1793-9" data-line-number="9">     <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1793-10" data-line-number="10">         y.hat[i] =<span class="st">   </span><span class="kw">H</span>(x[i,], omega) </a>
<a class="sourceLine" id="cb1793-11" data-line-number="11">         delta    =<span class="st"> </span>(<span class="kw">c</span>(y[i] <span class="op">-</span><span class="st"> </span>y.hat[i])) </a>
<a class="sourceLine" id="cb1793-12" data-line-number="12">         omega    =<span class="st"> </span>omega <span class="op">+</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>delta <span class="op">*</span><span class="st"> </span>x[i,]</a>
<a class="sourceLine" id="cb1793-13" data-line-number="13">         sse      =<span class="st"> </span>sse <span class="op">+</span><span class="st"> </span>delta<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1793-14" data-line-number="14">     }</a>
<a class="sourceLine" id="cb1793-15" data-line-number="15">     mse[t] =<span class="st"> </span>sse <span class="op">/</span><span class="st"> </span>n</a>
<a class="sourceLine" id="cb1793-16" data-line-number="16">     <span class="cf">if</span> (t <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">&amp;&amp;</span><span class="st"> </span>( mse[t <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>mse[t] <span class="op">&lt;=</span><span class="st"> </span>tol)) <span class="cf">break</span></a>
<a class="sourceLine" id="cb1793-17" data-line-number="17">  }</a>
<a class="sourceLine" id="cb1793-18" data-line-number="18">  <span class="kw">list</span>(<span class="st">&quot;coefficients&quot;</span> =<span class="st"> </span>omega, <span class="st">&quot;error&quot;</span> =<span class="st"> </span>mse)</a>
<a class="sourceLine" id="cb1793-19" data-line-number="19">}</a></code></pre></div>
<div class="sourceCode" id="cb1794"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1794-1" data-line-number="1">omega =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)  <span class="co"># initialize our coefficients</span></a>
<a class="sourceLine" id="cb1794-2" data-line-number="2">(<span class="dt">model =</span> <span class="kw">my.adaline</span>(x.p, y, <span class="dt">epoch=</span><span class="dv">50</span>))</a></code></pre></div>
<pre><code>## $coefficients
##       x0       x1       x2 
##  0.02996 -0.29061  0.19347 
## 
## $error
## [1] 0.2534 0.1191 0.1182</code></pre>

<p>Let us use the optimized <strong>coefficients</strong> to visualize the decision boundary. We concoct the points (<span class="math inline">\(\mathbf{x_1, x_2}\)</span>) that run through the decision boundary like so:</p>

<div class="sourceCode" id="cb1796"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1796-1" data-line-number="1">p.x1 =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dt">length.out=</span><span class="dv">40</span>)</a>
<a class="sourceLine" id="cb1796-2" data-line-number="2">cof  =<span class="st"> </span>model<span class="op">$</span>coefficients</a>
<a class="sourceLine" id="cb1796-3" data-line-number="3">b    =<span class="st"> </span>cof[<span class="dv">1</span>]; w1 =<span class="st"> </span>cof[<span class="dv">2</span>]; w2 =<span class="st"> </span>cof[<span class="dv">3</span>] </a>
<a class="sourceLine" id="cb1796-4" data-line-number="4">p.x2 =<span class="st"> </span>( <span class="op">-</span>w1 <span class="op">/</span><span class="st"> </span>w2 ) <span class="op">*</span><span class="st"> </span>p.x1 <span class="op">+</span><span class="st"> </span>b</a></code></pre></div>

<p>Finally, we plot the decision boundary based on <strong>ADALINE</strong> (see Figure <a href="deeplearning1.html#fig:adalinemodel">12.7</a>).</p>

<div class="sourceCode" id="cb1797"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1797-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x1), <span class="dt">ylim=</span><span class="kw">range</span>(x2),</a>
<a class="sourceLine" id="cb1797-2" data-line-number="2">     <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">main=</span><span class="st">&quot;ADALINE&quot;</span>)</a>
<a class="sourceLine" id="cb1797-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1797-4" data-line-number="4"><span class="kw">points</span>(x, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span><span class="dv">-1</span>, <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>))</a>
<a class="sourceLine" id="cb1797-5" data-line-number="5"><span class="kw">lines</span>(p.x1, p.x2, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:adalinemodel"></span>
<img src="DS_files/figure-html/adalinemodel-1.png" alt="ADALINE Model" width="70%" />
<p class="caption">
Figure 12.7: ADALINE Model
</p>
</div>

<p>In the figure, the <strong>decision boundary</strong> closely matches the same boundary line produced by our simple <strong>PERCEPTRON</strong> implementation.</p>
</div>
<div id="multi-layer-perceptron-mlp" class="section level2 hasAnchor">
<h2><span class="header-section-number">12.3</span> Multi Layer Perceptron (MLP)  <a href="deeplearning1.html#multi-layer-perceptron-mlp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now delve deep into <strong>MLP</strong>, one of the many essential topics in <strong>Deep Learning</strong>. The fundamental inner workings of <strong>MLP</strong> serve as a necessary precursor following other advanced <strong>neural networks</strong> such as <strong>Convolutional Neural Network</strong> and <strong>Recurrent Neural Network</strong>.</p>
<p>From a simple <strong>Perceptron</strong>, let us add a bit of complexity using Figure <a href="deeplearning1.html#fig:deltarule">12.8</a> to demonstrate a <strong>two-layer</strong> <strong>Neural Network</strong> in which each layer is a composite of <strong>Simple Perceptrons</strong>. Note that whereas <strong>Perceptrons</strong> use <strong>step functions</strong>, let us use <strong>sigmoid functions</strong>. Hereafter, if we use the acronym <strong>MLP</strong>, we refer to a vanilla <strong>Multi-Layer Neural Network</strong>.</p>
<p>There are two hidden layers in the network, each consisting of two perceptrons, followed by an output layer.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:deltarule"></span>
<img src="deltarule.png" alt="Multi-Layer Perceptron" width="80%" />
<p class="caption">
Figure 12.8: Multi-Layer Perceptron
</p>
</div>
<p>The schema demonstrates a <strong>Multi-Layered Perceptron</strong> that has the property of a <strong>complete Neural Network</strong>. That means that every feature input connects to every neuron in each layer. Moreover, every neuron in each layer connects to all other neurons of the next layer. Therefore, the number of parameters (or <span class="math inline">\(\omega\)</span> coefficients) of a complete <strong>Neural Network</strong> is calculated. For example, suppose we have four hundred input features, three <strong>hidden</strong> layers of five neurons each, and two outputs in the output layer ( granting we consider full bias - each input or hidden layer has a bias), then we are looking at the following computation:</p>
<p><span class="math display">\[
\begin{array}{ll}
\text{number of }&amp;\text{parameters (}\omega\ \text{coefficients)} \\
&amp;= \text{no. of features} \times \text{no. of neuron in HL1}  \\
&amp;+ \text{ no. of neuron in HL1} \times \text{no. of neuron in HL2}\\
&amp;+ \text{ no. of neuron in HL2} \times \text{no. of neuron in HL3}\\
&amp;+ \text{ no. of neuron in HL3} \times \text{no. of output}\\
&amp;= 400 \times 5 + 5 \times 5 + 5 \times 5 + 5 \times 2\\
&amp;= 2060\\
\\
\text{number of }&amp;\text{parameters (bias)} \\
&amp;= \text{no. of features} \times \text{no. of neuron in HL1}  \\
&amp;+ \text{ no. of neuron in HL1} \times \text{no. of neuron in HL2}\\
&amp;+ \text{ no. of neuron in HL2} \times \text{no. of neuron in HL3}\\
&amp;+ \text{ no. of neuron in HL3} \times \text{no. of output}\\
&amp;= 1\times 5 + 1 \times 5 + 1 \times 5 +1 \times 2\\
&amp;= 17
\end{array}
\]</span></p>
<p>The number of parameters becomes too large as we accommodate more input features, more neurons, and more hidden layers. As we recall in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>), we have gone through a review of <strong>Exploratory Data Analysis (EDA)</strong> that allows us to reduce the dimensionality of our input. Unfortunately, as we begin to go deeper into neural networks and work on images, feature extraction may not be that simple. Nonetheless, we cover <strong>Convolutional Neural Network (CNN)</strong> up ahead, which performs feature extraction. There are ways to reduce parameters in <strong>CNN</strong>, as we shall see later.</p>
<p>Using Figure <a href="deeplearning1.html#fig:deltarule">12.8</a>, let us demonstrate three major aspects of <strong>modeling</strong> an <strong>MLP</strong>.</p>
<ul>
<li><p><strong>The Forward Pass (or Forward Feed)</strong> - each input data passes through each layer to the output layer.</p></li>
<li><p><strong>The BackPropagation</strong> - each connection that input data passes through carries an initial arbitrary <strong>weight</strong> that needs to be optimized. The choice of optimization method is a consideration to make. Here, we use the common <strong>Gradient Descent</strong>. The result produced by the output layer may not necessarily match the target immediately, and so it takes a sufficient iteration to meet a close match. This mismatch error needs to be propagated back through the network. We can use <strong>sum squared error</strong> for our <strong>loss function</strong> to calculate the error that we need to propagate. For this, we require the <strong>loss function</strong> to be differentiable with respect to each weight.</p></li>
<li><p><strong>The Backward Pass (or Backward Feed)</strong> - for <strong>Optimization</strong> to happen, we need to re-calculate the weights of all connections. This is where we implement our update rule.</p></li>
</ul>
<div id="forward-feed" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.1</span> Forward Feed <a href="deeplearning1.html#forward-feed" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the <strong>Forward Feed</strong> (also called <strong>Forward Pass</strong>) portion, our first hidden layer performs a dot product of the input <span class="math inline">\(x_{(i:p)}\)</span> and the weights <span class="math inline">\(\omega_{(i:p,1:2)}\)</span>, and then it feeds the result to a <strong>sigmoid function</strong>. Our choice of <strong>activation function</strong> is scaled between 0 and 1. Here, <strong>p</strong> represents the dimensionality of our model, e.g., the number of features.</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\hat{z}_1 = \sum_{i=0}^{p=2} \omega_{(i,1)} x_{i}\ \ \ \ \ z_1 = \text{sigmoid}\left( \hat{z}_1 \right)\\
\hat{z}_2 = \sum_{i=0}^{p=2} \omega_{(i,2)} x_{i}\ \ \ \ \ z_2 = \text{sigmoid}\left( \hat{z}_2 \right)\\
\end{array}
\ \ \ 
where\ \ x_0 = 1\ \ \text{(constant used for bias)}   
\end{align*}\]</span></p>
<p>Note that for the sake of notation, let us consider <span class="math inline">\(\hat{z}_1\)</span> and <span class="math inline">\(\hat{z}_2\)</span> to be the <strong>net input</strong> and <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> to be the <strong>activation output</strong>.</p>
<p>Also, note that for a given constant equal to 1, we have chosen to have a separate bias weight for each connection to a perceptron; otherwise, if we have a fixed weight for bias, we also can use the below formula:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\hat{z}_1 = \sum_{i=1}^{p=2} \omega_{(i,1)} x_{i} + \omega_0\ \ \ \ \ z_1 = \text{sigmoid}\left( \hat{z}_1 \right)\\
\hat{z}_2 = \sum_{i=1}^{p=2} \omega_{(i,2)} x_{i} + \omega_0\ \ \ \ \ z_2 = \text{sigmoid}\left( \hat{z}_2 \right)\\
\end{array}
\ \ \ 
where\ \ x_0 = 1\ \ \text{(constant for bias)}
\end{align*}\]</span></p>
<p>In this first hidden layer, we have six parameters to optimize.</p>
<p>Then, our second hidden layer performs a dot product of the output from the first hidden layer and the weights <span class="math inline">\(\alpha_{(i:p,1:2)}\)</span>, and then it feeds the result to a <strong>sigmoid function</strong>.</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\hat{h}_1 = \sum_{i=0}^{p=2} \alpha_{(i,1)} z_{i}\ \ \ \ \ h_1 = \text{sigmoid}\left( \hat{h}_1 \right)\\
\hat{h}_2 = \sum_{i=0}^{p=2} \alpha_{(i,2)} z_{i}\ \ \ \ \ h_2 = \text{sigmoid}\left( \hat{h}_2 \right)\\
\end{array}
\ \ 
where\ \ z_0 = 1\ \ \text{(constant used for bias)}
\end{align*}\]</span></p>
<p>Let us also consider <span class="math inline">\(\hat{h}_1\)</span> and <span class="math inline">\(\hat{h}_2\)</span> to be the <strong>net input</strong> and <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span> to be the <strong>activation output</strong>.</p>
<p>In this second hidden layer, we have six parameters to optimize.</p>
<p>Finally, our output layer performs a dot product of the output from the second hidden layer and the weights <span class="math inline">\(\varphi_{(i:p,1:2)}\)</span>, and then it feeds the result to a <strong>sigmoid function</strong>.</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\hat{o}_1 = \sum_{i=0}^{p=2} \varphi_{(i,1)} h_{i}\ \ \ \ \ o_1 = \text{sigmoid}\left( \hat{o}_1 \right)\\
\hat{o}_2 = \sum_{i=0}^{p=2} \varphi_{(i,2)} h_{i}\ \ \ \ \ o_2 = \text{sigmoid}\left( \hat{o}_2 \right)\\
\end{array} 
\ \  
where\ \ h_0 = 1\ \ \text{(constant used for bias)}
\end{align*}\]</span></p>
<p>In this output layer, we have six parameters to optimize.</p>
</div>
<div id="backward-feed" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.2</span> Backward Feed <a href="deeplearning1.html#backward-feed" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the <strong>Backward Feed</strong> (also called <strong>Backward Pass</strong>) portion, we formulate our <strong>update function</strong> for our weight parameters like so (where <span class="math inline">\(\eta\)</span> is the learning rate between 0 and 1):</p>
<p><span class="math display">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)}  - \eta \nabla \omega  \mathcal{L} 
\end{align}\]</span></p>
<p>That is true for all weights.</p>
<p>We then iterate multiple times until our <strong>loss function</strong> is minimized. For that to happen, we need to optimize a total of 18 parameters corresponding to the 18 neural connections:</p>
<p><span class="math display">\[
params = \underbrace{( 2 \times 2 + 2 \times 2 + 2 \times 2 )}_{\text{feature inputs}} + \underbrace{( 1 \times 2 + 1 \times 2 + 1 \times 2 )}_{\text{bias}} = 18 
\]</span></p>
<p>Before we provide an example of optimizing the parameters, let us first discuss <strong>BackPropagation</strong> in the next section.</p>
</div>
<div id="backpropagation" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.3</span> BackPropagation <a href="deeplearning1.html#backpropagation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To illustrate the concept of <strong>Backpropagation</strong>, we have to recall <strong>Partial Differentiation</strong> covered in Chapter <strong>4</strong> (<strong>Numerical Calculus</strong>) and the concept of <strong>Chain rule</strong> and <strong>Delta rule</strong> in the context of <strong>Backpropagation</strong>.</p>
<p><strong>Chain rule</strong> </p>
<p>Let us use Figure <a href="deeplearning1.html#fig:backprop">12.9</a> to discuss the <strong>Chain rule</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:backprop"></span>
<img src="backprop.png" alt="BackPropagation" width="90%" />
<p class="caption">
Figure 12.9: BackPropagation
</p>
</div>
<p>We start with the idea that each neuron in the <strong>Neural Network</strong> contributes an <strong>effect</strong> to another <strong>Neuron</strong> from one layer to the next.</p>
<p>The diagram shows how neuron <strong>a</strong> in <strong>layer H1</strong>, for example, contributes indirectly to neuron <strong>o</strong> in the <strong>Output Layer</strong>. Mathematically, the contribution or effect of <strong>a</strong> to <strong>o</strong> can be expressed as <span class="math inline">\(\frac{\partial o}{\partial a}\)</span>.</p>
<p>To determine the effect of <strong>a</strong> to <strong>o</strong>, we first need to know the effect of <strong>a</strong> to <strong>d</strong>, the effect of <strong>d</strong> to <strong>g</strong>, and the effect of <strong>g</strong> to <strong>o</strong>.</p>
<p><span class="math display">\[
a\rightarrow d\rightarrow g\rightarrow o
\]</span></p>
<p>Mathematically, it is written as:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial o}{\partial a} = 
\left(\frac{\partial o}{\partial g} \right)
\left(\frac{\partial g}{\partial d} \right)
\left(\frac{\partial d}{\partial a} \right)
\end{align}\]</span></p>
<p>However, notice in Figure <a href="deeplearning1.html#fig:backprop">12.9</a> that there is <strong>no</strong> direct connection from <strong>a</strong> to <strong>d</strong> nor from <strong>d</strong> to <strong>g</strong>. To get to <strong>d</strong> from <strong>a</strong>, we can either go to <strong>b</strong> first or <strong>c</strong> first. Similarly, to get to <strong>g</strong> from <strong>d</strong>, we can either go to <strong>e</strong> or <strong>f</strong> first.</p>
<p><span class="math display">\[\begin{align}
\left(\frac{\partial d}{\partial a} \right) = 
\left(\frac{\partial d}{\partial b} \right)
\left(\frac{\partial b}{\partial a} \right)
\ \ \ \ \ or
\ \ \ \ \ 
\left(\frac{\partial d}{\partial a} \right) = 
\left(\frac{\partial d}{\partial c} \right)
\left(\frac{\partial c}{\partial a} \right)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\left(\frac{\partial g}{\partial d} \right) = 
\left(\frac{\partial g}{\partial e} \right)
\left(\frac{\partial e}{\partial d} \right)
\ \ \ \ \ or
\ \ \ \ \ 
\left(\frac{\partial g}{\partial d} \right) = 
\left(\frac{\partial g}{\partial f} \right)
\left(\frac{\partial f}{\partial d} \right)
\end{align}\]</span></p>
<p>Because we are interested in the effect of <strong>a</strong> to <strong>d</strong> and <strong>d</strong> to <strong>g</strong>, we can combine the effects of both paths like so:</p>
<p><span class="math display">\[\begin{align}
\left(\frac{\partial d}{\partial a} \right) = 
\left(\frac{\partial d}{\partial b} \right)
\left(\frac{\partial b}{\partial a} \right)
+
\left(\frac{\partial d}{\partial c} \right)
\left(\frac{\partial c}{\partial a} \right) 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\left(\frac{\partial g}{\partial d} \right) = 
\left(\frac{\partial g}{\partial e} \right)
\left(\frac{\partial e}{\partial d} \right)
+
\left(\frac{\partial g}{\partial f} \right)
\left(\frac{\partial f}{\partial d} \right) 
\end{align}\]</span></p>
<p>Therefore, to get from <strong>a</strong> to <strong>o</strong>, we have the following complete <strong>Chain rule</strong> equation:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial o}{\partial a} = 
\left(\frac{\partial o}{\partial g} \right)
\left(\frac{\partial g}{\partial d} \right)
\left(\frac{\partial d}{\partial a} \right) = 
\left(\frac{\partial o}{\partial g} \right)
\left(
\left[
\frac{\partial g}{\partial e}
\frac{\partial e}{\partial d}
\right]
 +  
\left[
\frac{\partial g}{\partial f}
\frac{\partial f}{\partial d}
\right]
 \right) 
\left(
\left[
\frac{\partial d}{\partial b}
\frac{\partial b}{\partial a}
\right]
 +  
\left[
\frac{\partial d}{\partial c}
\frac{\partial c}{\partial a}
\right]
 \right) 
\end{align}\]</span></p>
<p><strong>Delta rule</strong> </p>
<p>Our ultimate goal in <strong>Backpropagation</strong> is to optimize the <strong>weights</strong> or <strong>parameters</strong> of a <strong>Neural Network</strong>. In Figure <a href="deeplearning1.html#fig:backprop">12.9</a>, the <strong>Neural Network</strong> in the diagram shows that there are 229 parameters to optimize. For optimization, we need to know first the effect of each parameter on the overall performance of the <strong>MLP model</strong>. This effect can be measured in terms of comparing the output of the model against the actual target using the following <strong>error</strong> formula denoted by the <strong>epsilon</strong> symbol (<span class="math inline">\(\epsilon\)</span>):</p>
<p><span class="math display">\[\begin{align}
\epsilon = (\text{o - t}) = -(\text{t - o})\ \ \ \ \ where\ \ \ \mathbf{t} = target\ \ and\ \  \mathbf{o} = output
\end{align}\]</span></p>
<p>From there, we can formulate our <strong>total loss function</strong>, which is required to be differentiable. Note that the <span class="math inline">\(\frac{1}{2}\)</span> is added for mathematical convenience (e.g., ease of differentiation).</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}_{(total)} = \frac{1}{2} \sum_{i=1}^m \left(t_i - o_i\right)^2 = 
\underbrace{\frac{1}{2}\left(t_1 - o_1\right)^2}_{\mathcal{L}_{o_1}} + 
\underbrace{\frac{1}{2}\left(t_2 - o_2\right)^2 }_{\mathcal{L}_{o_2}} +\ ...+\ 
\underbrace{\frac{1}{2}\left(t_m - o_m\right)^2 }_{\mathcal{L}_{o_m}}
\end{align}\]</span></p>
<p>Ideally, we want the <strong>total loss function</strong> to produce zero error. That can only be achieved if we can optimize the effect of each <strong>weight</strong> to the <strong>loss function</strong>. To calculate the effect of a <strong>weight</strong> (<span class="math inline">\(\omega\)</span>) to the <strong>loss function</strong>, we use the following notation: <span class="math inline">\(\frac{\partial \mathcal{L}_{(total)}}{\partial \omega}\)</span>.</p>
<p>For example, to determine the effect of a <strong>weight</strong>, e.g., <span class="math inline">\(\omega_{10}\)</span> between <strong>layer H5</strong> and the <strong>Output layer</strong> to the <strong>loss function</strong>, we can form the following equation:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{10}} = 
 \sum_{o=i}
\frac{\partial \mathcal{L}_{o}}{\partial \omega_{10}}  
\end{align}\]</span></p>
<p>To determine the effect to the <strong>loss function</strong> of a <strong>weight</strong>, e.g. <span class="math inline">\(\omega_{8}\)</span> between <strong>layer H4</strong> and the <strong>Output layer</strong>, we can form the following equation:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{8}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_5}
\frac{\partial H_5}{\partial \hat{H}_5}
\frac{\partial \hat{H}_5}{\partial \omega_{8}}
\end{align}\]</span></p>
<p>Now, to determine the effect to the <strong>loss function</strong> with respect to a <strong>weight</strong> between the <strong>Input layer</strong> and the <strong>Output layer</strong>, we can form the following equation:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{1}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_5}
\frac{\partial H_5}{\partial \hat{H}_5}
\frac{\partial \hat{H}_5}{\partial H_4}
\frac{\partial H_4}{\partial \hat{H}_4}
\frac{\partial \hat{H}_4}{\partial H_3}
\frac{\partial H_3}{\partial \hat{H}_3}
\frac{\partial \hat{H}_3}{\partial H_2}
\frac{\partial H_2}{\partial \hat{H}_2}
\frac{\partial \hat{H}_2}{\partial H_1}
\frac{\partial H_1}{\partial \hat{H}_1}
\frac{\partial \hat{H}_{1}}{\partial \omega_{1}}
\end{align}\]</span></p>
<p>As we can imagine, the equation gets longer as the number of layers increases. However, algorithmically, there is a better way to express the equation. It is where the <strong>Delta rule</strong> comes into the picture. In the equation, we notice three contributions.</p>
<p><strong>First</strong>, let us momentarily use Figure <a href="deeplearning1.html#fig:deltarule">12.8</a> to review the contribution of a <strong>net input</strong> to an <strong>activation output</strong>. Both <strong>net input</strong> and <strong>activation output</strong> are part of a neuron. </p>
<p>Each of the <strong>neuron</strong> in <strong>hidden layers</strong> comes with two variables, namely the <strong>net input</strong> and the <strong>activation output</strong>. In fact, the <strong>activation output</strong> is called the <strong>activation output</strong> obtained from the <strong>activation function</strong>. In our case, we are using the <strong>sigmoid function</strong>. </p>
<p>We can get the contribution of <span class="math inline">\(\mathbf{\hat{h}_2}\)</span> to <span class="math inline">\(\mathbf{h_2}\)</span> by differentiating the <strong>sigmoid function</strong>.</p>
<p><span class="math display">\[\begin{align}
a(z) = sigmoid(f(x)) = \frac{1}{1 + exp(-z)}\ \ \ \rightarrow a&#39;(z) = a(z) ( 1 - a(z))
\ \  where\ z = f(x)
\end{align}\]</span></p>
<p>Because <span class="math inline">\(\mathbf{h_2}\)</span> is the <strong>activation output</strong> of a <strong>sigmoid function</strong>, we can therefore obtain the derivative of the function with respect to the <strong>net input</strong>, namely <span class="math inline">\(\mathbf{\hat{h}_2}\)</span>:</p>
<p><span class="math display">\[\begin{align}
f(x) = \hat{h}_2 = 1 \times \alpha_{0,2} +  z_1 \times \alpha_{1,2}  +  z_2 \times \alpha_{2,2}
\ \ \ \ \ \ \ \ \ \ \ 
\frac{\partial \mathbf{h_2}}{\partial \mathbf{\hat{h}_2}} = \mathbf{h_2} ( 1 - \mathbf{h_2} )
\end{align}\]</span></p>
<p>Note that the full derivation of the derivative is excluded.</p>
<p>Note also that other <strong>activation function</strong> of choice has different derivatives.</p>
<p><strong>Second</strong>, we also have the contribution of individual <strong>weights</strong> to the <strong>net input</strong>, e.g. <span class="math inline">\(\frac{\partial \hat{h_2}}{\partial \alpha_{2,2}}\)</span>.</p>
<p>Given <span class="math inline">\(f(x) = \hat{h}_2\)</span>, we can obtain the derivative of the <strong>net input</strong> with respect to a given <strong>weight</strong>. Similarly, we also can obtain the derivative of the <strong>weight</strong> given a <strong>net input</strong>):</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \hat{h_2}}{\partial \alpha_{2,2}} =  z_2 \times {\alpha_{2,2}}^{(1-1)} = \mathbf{z_2}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\frac{\partial \hat{h_2}}{\partial x_2} = {\mathbf{z_2}}^{(1-1)} \times {\mathbf{\alpha_{2,2}}} = \mathbf{\alpha_{2,2}}
\end{align}\]</span></p>
<p><strong>Third</strong>, we have the contribution of the <strong>activation output</strong> to the <strong>total loss function</strong>, e.g. <span class="math inline">\(\frac{\partial \mathcal{L}_{(total)}}{\partial o}\)</span>.</p>
<p>We can then differentiate the <strong>total loss function</strong> with respect to a <strong>given output</strong> so that given the equation below:</p>
<p><span class="math display">\[\begin{align}
\mathcal{L_{(total)}}  = 
2 \frac{1}{2} \left(t_1 - o_1\right)^{2-1}  +  
2 \frac{1}{2} \left(t_2 - o_2\right)^{2-1} +\ ...\ +\ 
2 \frac{1}{2} \left(t_m - o_m\right)^{2-1}
\end{align}\]</span></p>
<p>we obtain the following derivatives:</p>
<p><span class="math display">\[\begin{align}
\frac{ \partial \mathcal{L_{(total)}} } {\partial o_1}= -\underbrace{ \left(t_1 - o_1\right) }_{\partial \mathcal{L_1}} 
\ \ \ where\ \ \ \ \partial \mathcal{L}_1 = \epsilon_1 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{ \partial \mathcal{L_{(total)}} } {\partial o_2}  = -\underbrace{ \left(t_2 - o_2\right) }_{\partial \mathcal{L_2}} 
\ \ \ where\ \ \ \ \partial \mathcal{L}_2 = \epsilon_2
\end{align}\]</span></p>
<p><strong>Fourth</strong>, the contribution of an <strong>activation output</strong> to the <strong>total loss function</strong>, e.g. <span class="math inline">\(\left(\frac{\partial L_{(total)}}{\partial h_2} \right)\)</span>, can be decomposed into the following:</p>
<p><span class="math display">\[\begin{align}
\left(\frac{\partial L_{(total)}}{\partial h_2} \right) =
\left(\frac{\partial L_1}{\partial h_2} +  \frac{\partial L_2}{\partial h_2}\right) 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial L_1}{\partial h_2} = 
\left(\frac{\partial L_1}{\partial o_1}\right)
\left(\frac{\partial o_1}{\partial \hat{o}_1}\right)
\left(\frac{\partial \hat{o}_1}{\partial h_2}\right) = 
\underbrace{-(t_1 - o_1) \times o_1 ( 1 - o_1)}_{\delta_1\leftarrow \text{delta rule}} \times \varphi_{2,1}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial L_2}{\partial h_2} = 
\left(\frac{\partial L_2}{\partial o_2}\right)
\left(\frac{\partial o_2}{\partial \hat{o}_2}\right)
\left(\frac{\partial \hat{o}_2}{\partial h_2}\right) =
\underbrace{-(t_2 - o_2) \times o_2 ( 1 - o_2)}_{\delta_2\leftarrow \text{delta rule}} \times \varphi_{2,2}
\end{align}\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[\begin{align}
\left(\frac{\partial L_{(total)}}{\partial h_2} \right) &amp;= 
\underbrace{-(t_1 - o_1) \times o_1 ( 1 - o_1)}_{\delta_1\leftarrow \text{delta rule}} \times \varphi_{2,1} + \underbrace{-(t_2 - o_2) \times o_2 ( 1 - o_2)}_{\delta_2\leftarrow \text{delta rule}} \times \varphi_{2,2}\\
&amp;= \sum_{y=1}^{Y=2}\delta_y \varphi_{2,y}
\end{align}\]</span></p>
<p><strong>Lastly</strong>, if we then combine all three contributions (or derivatives), we end up with the following equation for the example in Figure <a href="deeplearning1.html#fig:deltarule">12.8</a>.</p>
<p><span class="math display">\[\begin{align}
\frac{\partial L_{(total)}}{\partial \alpha_{2,2}} =  
\left(\frac{\partial L_{(total)}}{\partial h_2} \right)
\left(\frac{\partial h_2}{\partial \hat{h}_2} \right)
\left(\frac{\partial \hat{h}_2}{\partial \alpha_{2,2}} \right) =
\underbrace{\sum_{y=1}^{Y=2}\delta_y \varphi_{2,o} \times o_2 (1 - o_2)}_{\delta_h \leftarrow \text{delta rule}} \times z_2 = \delta_h z_2
\end{align}\]</span></p>
<p>Now, back to Figure <a href="deeplearning1.html#fig:backprop">12.9</a> as reference, having the <strong>Delta rules</strong> in mind, let us figure out how to solve for <span class="math inline">\(\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{i,q}}\)</span> where <strong>i</strong> is the ith feature (including constant) in the <strong>Input Layer</strong> and <strong>q</strong> is the qth neuron in the next layer.</p>
<p><strong>First</strong>, we deal with the effect of a <strong>neuron</strong> to the <strong>total loss function</strong>. Consider <strong>L=5</strong> such that <span class="math inline">\(H_5 = H_{L}\)</span>. We then have:</p>
<p><span class="math display">\[\begin{align}
\left\{
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L,k)}} =  
\sum_{y=1}
\frac{\partial \mathcal{L}_y}{\partial H_{(L,k)}}  = 
\sum_{y=1} \delta_{y}\times \omega_{(L, k)}
\right\}_{k=1}^K
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_y}{\partial H_{(L,k)}}= 
\underbrace{ \frac{\partial \mathcal{L}_y}{\partial O_y} 
\frac{\partial O_y}{\partial \hat{O}_y} }_{\delta_y}
\frac{\partial \hat{O}_y}{\partial H_{(L,k)}} = 
\underbrace{-(t_y - o_y)\times o_y(1 - o_y)}_{\delta_{y}} \times \omega_{(L, k)}
\end{align}\]</span></p>
<p>Here, <strong>L</strong> is the <strong>H5 layer</strong> index with <strong>K</strong> neurons. The indices are used such that <span class="math inline">\(\mathbf{H_{(5, 3)}}\)</span> refers to the third neuron in the <strong>H5 layer</strong> and <span class="math inline">\(\mathbf{\omega_{(5, 2)}}\)</span> refers to the second weight from <strong>H5 layer</strong>.</p>
<p><strong>Second</strong>, we deal with the effect of the <strong>activation output</strong> with respect to the <strong>net input</strong>.</p>
<p><span class="math display">\[\begin{align}
\left\{
\frac{\partial H_{(L,k)}}{\partial \hat{H}_{(L,k)}} = H_{(L,k)} \left(1 - H_{(L,k)}\right)
\right\}_{k=1}^K
\end{align}\]</span></p>
<p><strong>Third</strong>, we deal with the effect of <strong>neuronâs output</strong> to another <strong>neuronâs activation output</strong>:</p>
<p><span class="math display">\[\begin{align}
\left\{
\frac{\partial \hat{H}_{(L,k)}}{\partial H_{(L-1,m)}} = 
\frac{\partial \left(\sum_{j=1} \omega_{(L-1, j)} H_{(L-1,j)}\right)}{\partial H_{(L-1,m)}} = \omega_{(L-1, m)}
\right\}_{m=1}^{M}
\end{align}\]</span></p>
<p>Assume there are <strong>M</strong> neurons in <strong>H4 layer</strong> denoted by <span class="math inline">\(H_{(L-1, m)}\)</span>.</p>
<p><strong>Fourth</strong>, if we combine the first three factors to affect the <strong>total loss</strong> with respect to the weight (in <strong>H4 layer</strong>), we get the following:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{(L-1,m)}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L,k)}}
\frac{\partial H_{(L,k)}}{\partial \hat{H}_{(L,k)}}
\frac{\partial \hat{H}_{(L,k)}}{\partial \omega_{(L-1,m)}} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{(L-1,m)}} = 
\underbrace{ \left[  \sum_{k=1}
\left(\sum_{y=1} \delta_{y}\times \omega_{(L, k)} \right) \times H_{(L,k)} (1 - H_{(L,k)})
\right] }_{\delta_{(L,k)}}  \times H_{(L-1,m)}
\end{align}\]</span></p>
<p><strong>Fifth</strong>, similarly, if we combine the first three factors for the effect to the <strong>total loss</strong> with respect to the <strong>activation output</strong>, we get the following equation instead:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-1,m)}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L,k)}}
\frac{\partial H_{(L,k)}}{\partial \hat{H}_{(L,k)}}
\frac{\partial \hat{H}_{(L,k)}}{\partial \omega_{(L-1,m)}}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-1,m)}}  = 
\underbrace{ \left[  \sum_{k=1}
\left(\sum_{y=1} \delta_{y}\times \omega_{(L, k)} \right) \times H_{(L,k)} (1 - H_{(L,k)})
\right] }_{\delta_{(L,k)}}  \times \omega_{(L-1,m)}
\end{align}\]</span></p>
<p><strong>Sixth</strong>, moving one more layer back (e.g. <strong>H3 layer</strong>) and using the equation below to expand further:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-1,m)}} = \delta_{(L,k)} \times \omega_{(L-1,m)}
\end{align}\]</span></p>
<p>we get:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{(L-2,n)}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L,k)}}
\frac{\partial H_{(L,k)}}{\partial \hat{H}_{(L,k)}}
\frac{\partial \hat{H}_{(L,k)}}{\partial H_{(L-1,m)}}
\frac{\partial H_{(L-1,m)}}{\partial \hat{H}_{(L-1,m)}}
\frac{\partial \hat{H}_{(L-1,m)}}{\partial \omega_{(L-2,n)}}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-2,n)}} = 
\underbrace{ \left[  \sum_{m=1}
\left(\sum_{k=1} \delta_{_{(L,k)}}\times \omega_{(L-1, m)} \right) \times H_{(L-1,m)} (1 - H_{(L-1,m)})
\right] }_{\delta_{(L-1,m)}}  \times \omega_{(L-2,n)}
\end{align}\]</span></p>
<p>Assume there are <strong>N</strong> neurons in <strong>H3 layer</strong> denoted by <span class="math inline">\(H_{(L-2, n)}\)</span>.</p>
<p><strong>Seventh</strong>, moving even further back, for another layer (e.g. <strong>H2 layer</strong>) in which:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{(L-3,o)}} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L,k)}}
\frac{\partial H_{(L,k)}}{\partial \hat{H}_{(L,k)}}
\frac{\partial \hat{H}_{(L,k)}}{\partial H_{(L-1,m)}}
\frac{\partial H_{(L-1,m)}}{\partial \hat{H}_{(L-1,m)}}
\frac{\partial \hat{H}_{(L-1,m)}}{\partial H_{(L-2,n)}}
\frac{\partial H_{(L-2,n)}}{\partial \hat{H}_{(L-2,n)}}
\frac{\partial \hat{H}_{(L-2,n)}}{\partial H_{(L-3,o)}}
\end{align}\]</span></p>
<p>we get:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-3,o)}} = 
\underbrace{ \left[  \sum_{n=1}
\left(\sum_{m=1} \delta_{_{(L-1,m)}}\times \omega_{(L-2, n)} \right) \times H_{(L-2,n)} (1 - H_{(L-2,n)})
\right] }_{\delta_{(L-2,n)}}  \times \omega_{(L-3,o)}
\end{align}\]</span></p>
<p>Assume there are <strong>O</strong> neurons in <strong>H2 layer</strong> denoted by <span class="math inline">\(H_{(L-3, o)}\)</span>.</p>
<p><strong>Eight</strong>, and for the <strong>H1 layer</strong>, we get:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial H_{(L-4,p)}} = 
\underbrace{ \left[  \sum_{p=1}
\left(\sum_{n=1} \delta_{_{(L-2,n)}}\times \omega_{(L-3, o)} \right) \times H_{(L-3,o)} (1 - H_{(L-3,o)})
\right] }_{\delta_{(L-3,o)}}  \times \omega_{(L-4,p)}
\end{align}\]</span></p>
<p>Assume there are <strong>P</strong> neurons in <strong>H1 layer</strong> denoted by <span class="math inline">\(H_{(L-4, p)}\)</span>.</p>
<p>Equivalently, we get:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{(L-4,p)}} = 
\underbrace{ \left[  \sum_{p=1}
\left(\sum_{n=1} \delta_{_{(L-2,n)}}\times \omega_{(L-3, o)} \right) \times H_{(L-3,o)} (1 - H_{(L-3,o)})
\right] }_{\delta_{(L-3,o)}}  \times H_{(L-4,p)}
\end{align}\]</span></p>
<p>Simplifying the notation, we get:</p>
<p><span class="math display">\[\begin{align}
\nabla \omega_{(L-4, p)} = \delta_{(L-3, o)} \times H_{(L-4,p)}
\end{align}\]</span></p>
<p><strong>Finally</strong>, for the <strong>Input Layer</strong>, having gone through all the example equations above, we get the final effect of the <strong>Input Layer</strong> to the <strong>Output Layer</strong> like so:</p>
<p><span class="math display">\[\begin{align}
\nabla \omega_{(I, q)} = \delta_{(L-4, p)} \times H_{(I,q)}
\end{align}\]</span></p>
<p>Assume there are <strong>Q</strong> input features in the <strong>Input layer</strong> denoted by <span class="math inline">\(H_{(I, q)}\)</span>.</p>
<p>So far, our equations work only on a single sample. In the next section, we continue to discuss <strong>MLP</strong> further by example. Then, another section follows with the implementation, focusing on multiple samples.</p>
</div>
<div id="mlp-example" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.4</span> MLP Example<a href="deeplearning1.html#mlp-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we continue to use Figure <a href="deeplearning1.html#fig:deltarule">12.8</a>. Suppose we have the following data points, initial weights, and biases (see Figure <a href="deeplearning1.html#fig:mlpdatapoint">12.10</a>):</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpdatapoint"></span>
<img src="mlpdatapoint.png" alt="Multiple Perceptron (Data Points)" width="80%" />
<p class="caption">
Figure 12.10: Multiple Perceptron (Data Points)
</p>
</div>
<p>Note that our <strong>inputs</strong> in the table include the biases. Also, our bias constant connects to neurons in different layers with unique weights.</p>
<p>Our goal is to update the 18 (weight) parameters. Here, we show the steps in solving only three parameters as an example.</p>
<p><strong>First</strong>, for the <strong>Forward Pass</strong>, we calculate the <strong>net input and output</strong> of each layer. Starting with the <strong>Z layer</strong>, we have the <strong>net input</strong> as:</p>
<p><span class="math display">\[\begin{align*}
\hat{z} = \left[\begin{array}{rrr} 1 &amp; 0.12 &amp; 0.18 \end{array}\right]_{X} \cdot
   \left[\begin{array}{rr} 0.05 &amp; 0.40 \\ 0.21 &amp; 0.34 \\ 0.19 &amp; 0.67 \end{array}\right]_{\omega}
   = \left[\begin{array}{rr} 0.1094 &amp; 0.5614 \end{array}\right] 
\end{align*}\]</span></p>
<p>and <strong>activation output</strong> as:</p>
<p><span class="math display">\[\begin{align*}
z = sigmoid( \left[\begin{array}{rr} 0.1094 &amp; 0.5614 \end{array}\right]) = 
\left[\begin{array}{rr} 0.52732275 &amp; 0.63677641 \end{array}\right] 
\end{align*}\]</span></p>
<p>Note, hereafter, that we use a precision of 8 digits in our examples.</p>
<p>For <strong>H layer</strong>, we have the <strong>net input</strong> as:</p>
<p><span class="math display">\[\begin{align*}
\hat{h} &amp;= \left[\begin{array}{rrr} 1 &amp; 0.52732275 &amp;  0.63677641 \end{array}\right]_{Z} \cdot
   \left[\begin{array}{rr} 0.18 &amp; 0.27 \\ 0.09 &amp; 0.06 \\ 0.30 &amp; 0.15 \end{array}\right]_{\alpha}\\
   &amp;= \left[\begin{array}{rr} 0.41849197 &amp; 0.39715583 \end{array}\right] \nonumber
\end{align*}\]</span></p>
<p>and <strong>activation output</strong> as:</p>
<p><span class="math display">\[\begin{align*}
h = sigmoid( \left[\begin{array}{rr} 0.41849197 &amp; 0.39715583 \end{array}\right]) = 
\left[\begin{array}{rr} 0.60312234 &amp; 0.59800413 \end{array}\right]
\end{align*}\]</span></p>
<p>For <strong>Output layer</strong>, we have the <strong>net input</strong> as:</p>
<p><span class="math display">\[\begin{align*}
\hat{o} &amp;= \left[\begin{array}{rrr} 1 &amp; 0.60312234 &amp;  0.59800413 \end{array}\right]_{H} \cdot
   \left[\begin{array}{rr} 0.25 &amp; 0.05 \\ 0.03 &amp; 0.35 \\ 0.04 &amp; 0.40 \end{array}\right]_{\varphi}\\
   &amp;= \left[\begin{array}{rr} 0.29201384 &amp; 0.50029447 \end{array}\right]  \nonumber
\end{align*}\]</span></p>
<p>and <strong>activation output</strong> as:</p>
<p><span class="math display">\[\begin{align*}
o = sigmoid( \left[\begin{array}{rr} 0.29201384 &amp; 0.50029447  \end{array}\right]) = 
\left[\begin{array}{rr} 0.57248908 &amp; 0.62252853 \end{array}\right]
\end{align*}\]</span></p>
<p><strong>Second</strong>, for the <strong>Backpropagation</strong>, we calculate our <strong>loss function</strong>:</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}_{(total)} &amp;= \frac{1}{2}\sum_{i=1}^m( t_i - o_i)^2 \\
&amp;= \frac{1}{2}( t_1 - o_1)^2 +  \frac{1}{2}( t_2 - o_2)^2\\
&amp; = \frac{1}{2} (0.05 - 0.57248908)^2 + \frac{1}{2} (0.95 - 0.62252853)^2 \nonumber \\
&amp; = 0.1901162 \nonumber
\end{align}\]</span></p>
<p>Now, we calculate the gradients starting with the <strong>loss function</strong> with respect to the <strong>activation output</strong> of the <strong>O layer</strong>.</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial o_1} &amp;= \frac{\partial L_1}{\partial o_1}  = -(t_1 - o_1) = -(0.05 - 0.57248908) = 0.52248908\\ 
\frac{\partial \mathcal{L}_{(total)}}{\partial o_2} &amp;= \frac{\partial L_2}{\partial o_2}  = -(t_2 - o_2) = -(0.95 - 0.62252853) = -0.32747147
\end{align}\]</span></p>
<p><strong>Third</strong>, we calculate the gradients of our <strong>activation output</strong>, <span class="math inline">\(\mathbf{o_1}\)</span>, (from our activation function) with respect to the <strong>net input</strong>, <span class="math inline">\(\mathbf{\hat{o}_1}\)</span>, of the <strong>O layer</strong> :</p>
<p><span class="math display">\[\begin{align}
\frac{\partial o_1}{\partial \hat{o}_1} &amp;= o_1 \left( 1 - o_1\right) =  0.57248908 ( 1 - 0.57248908) = 0.24474533\\
\frac{\partial o_2}{\partial \hat{o}_2} &amp;= o_2 \left( 1 - o_2\right) = 0.62252853 ( 1 - 0.62252853) = 0.23498676
\end{align}\]</span></p>
<p><strong>Fourth</strong>, regarding the weights and biases for the <strong>O Layer</strong>, we first calculate the gradient of the <strong>total loss function</strong> with respect to the weights. Starting with (<span class="math inline">\(\varphi\)</span>) weights, in particular <span class="math inline">\(\varphi_{1,1}\)</span>, we perform the following derivatives:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \varphi_{1,1}} &amp;= 
\left(\frac{\partial L_{(total)}}{\partial o_1} \right) 
\left(\frac{\partial o_1}{\partial \hat{o}_1} \right)
\left(\frac{\partial \hat{o}_1}{\partial \varphi_{1,1}} \right) \\
&amp;=
\underbrace{\left(\frac{\partial L_1}{\partial  o_1} \right)}_{\text{1st factor}}\ \ 
\underbrace{\left(\frac{\partial o_1}{\partial \hat{o}_1} \right)}_{\text{2nd factor}}\ \ 
\underbrace{\left(\frac{\partial \hat{o}_1}{\partial \varphi_{1,1}} \right)}_{\text{3rd factor}}
\end{align}\]</span></p>
<p>The first two factors, namely <span class="math inline">\(\left(\frac{\partial L_1}{\partial o_1} \right)\)</span> and <span class="math inline">\(\left(\frac{\partial o_1}{\partial \hat{o}_1} \right)\)</span> are already calculated for us above. Both equates to the <span class="math inline">\(\delta_1\)</span> from our <strong>delta.rule</strong>.</p>
<p><span class="math display">\[\begin{align}
\delta_1 = \left(\frac{\partial L_{(total)}}{\partial o_1} \right) 
\left(\frac{\partial o_1}{\partial \hat{o}_1} \right) = (0.52248908)(0.24474533) = 0.12787676
\end{align}\]</span></p>
<p>Equivalently, for <span class="math inline">\(\delta_2\)</span>, we have:</p>
<p><span class="math display">\[\begin{align}
\delta_2 = \left(\frac{\partial L_{(total)}}{\partial o_2} \right) 
\left(\frac{\partial o_2}{\partial \hat{o}_2} \right) = (-0.32747147)(0.23498676) =  -0.07695146
\end{align}\]</span></p>
<p>We now have to calculate the third factor, namely <span class="math inline">\(\left(\frac{\partial \hat{o}_1}{\partial \varphi_{1,1}} \right)\)</span>:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \hat{o}_1} {\partial \varphi_{1,1}} = \frac{\partial \left(\varphi_{0,1} \times h_0 +  \varphi_{1,1} \times h_1 + \varphi_{2,1} \times h_2 \right) } {\partial \varphi_{1,1}}
= {\varphi_{1,1}}^{(1-1)} \times h_1 = h_1 =  0.60312234  
\end{align}\]</span></p>
<p>Therefore, the gradient of the <strong>total loss function</strong> with respect to <span class="math inline">\(\varphi_{1,1}\)</span> is:
<span class="math display">\[\begin{align}
\nabla \varphi_{1,1} \mathcal{L} = 
\frac{\partial \mathcal{L}_{(total)}}{\partial \varphi_{1,1}} = \delta_1 h_1
= (0.12787676)(0.60312234)
 = 0.077125331
\end{align}\]</span></p>
<p>Calculation of gradient for other <strong>phi</strong> <span class="math inline">\((\varphi)\)</span> weights follow the same process.</p>
<p><strong>Fifth</strong>, in terms of the weights and biases for the <strong>H Layer</strong>, the gradient of the <strong>total loss function</strong> with respect to <span class="math inline">\(\alpha_{1,1}\)</span> is written as:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \alpha_{1,1}} = 
\underbrace{ \left(\frac{\partial L_{(total)}}{\partial h_1} \right) 
\left(\frac{\partial h_1}{\partial \hat{h}_1} \right)}_{\delta_{h_1}}
\left(\frac{\partial \hat{h}_1}{\partial \alpha_{1,1}} \right) 
\end{align}\]</span></p>
<p>We can expand the first factor into the following:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \alpha_{1,1}} =
\underbrace{\left(\frac{\partial \mathcal{L}_1}{\partial h_1} +  \frac{\partial \mathcal{L}_2}{\partial h_1}\right) }_{\text{1st factor}}
\underbrace{\left(\frac{\partial h_1}{\partial \hat{h}_1} \right)}_{\text{2nd factor}}
\underbrace{\left(\frac{\partial \hat{h}_1}{\partial \alpha_{1,1}} \right)}_{\text{3rd factor}} 
\end{align}\]</span></p>
<p>Decomposing the 1st factor into two terms, we have:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \alpha_{1,1}} = 
\underbrace{
\left(\overbrace{
      \underbrace{\frac{\partial \mathcal{L}_{1}}{\partial o_1}
       \frac{\partial o_1}{\partial \hat{o}_1}}_{\delta_{1}}
       \frac{\partial \hat{o}_1}{\partial h_1}}^{\partial \mathcal{L}_1 / \partial h_1} + 
       \overbrace{
       \underbrace{
       \frac{\partial \mathcal{L}_{2}}{\partial o_2}
       \frac{\partial o_2}{\partial \hat{o}_2} }_{\delta_{2}}
       \frac{\partial \hat{o}_2}{\partial h_1}}^{\partial \mathcal{L}_2 / \partial h_1}
       \right)
\frac{\partial h_1}{\partial \hat{h}_1}}_{\delta_{h_1}}
\frac{\partial \hat{h}_1}{\partial \alpha_{1,1}}
\end{align}\]</span></p>
<p>Now, let us calculate the two terms:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_1}{\partial h_1} = 
\underbrace{\left(\frac{\partial \mathcal{L}_1}{\partial o_1 }\right)
\left(\frac{\partial o_1}{\partial \hat{o}_1 }\right)}_{\delta_1}
\left(\frac{\partial  \hat{o}_1 }{\partial h_1}\right)
\ \ \ \ \ \ \ \ \ \
\frac{\partial \mathcal{L}_2}{\partial h_1} = 
\underbrace{
\left(\frac{\partial \mathcal{L}_2}{\partial o_2 }\right)
\left(\frac{\partial o_2}{\partial \hat{o}_2 }\right)}_{\delta_2}
\left(\frac{\partial  \hat{o}_2 }{\partial h_1}\right)
\end{align}\]</span></p>
<p>The first two factors of the 1st term, namely <span class="math inline">\(\left(\frac{\partial \mathcal{L}_1}{\partial o_1 }\right)\)</span> and <span class="math inline">\(\left(\frac{\partial o_1 }{\partial \hat{o}_1}\right)\)</span> are already solved, denoted by <span class="math inline">\(\delta_1\)</span>. Equivalently, the first two factors of the 2nd term, namely <span class="math inline">\(\left(\frac{\partial \mathcal{L}_2}{\partial o_2 }\right)\)</span> and <span class="math inline">\(\left(\frac{\partial o_2 }{\partial \hat{o}_2}\right)\)</span> are also solved, denoted by <span class="math inline">\(\delta_2\)</span>.</p>
<p>The derivatives of the two <strong>net inputs</strong> with respect to <span class="math inline">\(h_1\)</span> are:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial  \hat{o}_1 }{\partial h_1} = 
\frac{\partial \left(\varphi_{0,1} \times h_0 +  \varphi_{1,1} \times h_1 + \varphi_{2,1} \times h_2 \right)}{\partial h_1} = \varphi_{1,1} \times {h_1}^{(1-1)} = \varphi_{1,1} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial  \hat{o}_2 }{\partial h_1} = 
\frac{\partial \left(\varphi_{0,2} \times h_0 +  \varphi_{1,2} \times h_1 + \varphi_{2,2} \times h_2 \right)}{\partial h_1} = \varphi_{1,2} \times {h_1}^{(1-1)} = \varphi_{1,2} 
\end{align}\]</span></p>
<p>That gives us the following:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_1}{\partial h_1} = \delta_1 (\varphi_{1,1}) = (0.12787676)(0.03) = 0.0038363028 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
 \frac{\partial \mathcal{L}_2}{\partial h_1} = \delta_2 \left( \varphi_{1,2}\right) = (-0.07695146)(0.35) =  -0.026933011
\end{align}\]</span></p>
<p>Therefore, the first factor is:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h_1}  = 
\left(\frac{\partial \mathcal{L}_1}{\partial h_1} +  \frac{\partial \mathcal{L}_2}{\partial h_1}\right) = 0.0038363028 + -0.026933011 = -0.023096708
\end{align}\]</span></p>
<p>Now, let us solve for the second factor (<strong>activation output</strong> from our <strong>sigmoid</strong> function).</p>
<p><span class="math display">\[\begin{align}
\frac{\partial h_1}{\partial \hat{h}_1} = h_1( 1 - h_1) = 0.60312234 ( 1 - 0.60312234) = 
0.23936578
\end{align}\]</span></p>
<p>At this point, we do not need <span class="math inline">\(\left(\frac{\partial h_2}{\partial \hat{h}_2}\right)\)</span>.</p>
<p>And for the third factor, we have:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \hat{h}_1}{\partial \alpha_{1,1}} = \frac{\partial \left(\alpha_{0,1} \times z_0 +  \alpha_{1,1} \times z_1 + \alpha_{2,1} \times z_2 \right) } {\partial \alpha_{1,1}}
= {\varphi_{1,1}}^{(1-1)} \times z_1 = z_1 =  0.52732275 
\end{align}\]</span></p>
<p>As for the <strong>delta</strong>, e.g. <span class="math inline">\(\delta_{h_1}\)</span>, we have:</p>
<p><span class="math display">\[\begin{align}
\delta_{h_1} =  \left(\frac{\partial L_{(total)}}{\partial h_1} \right) 
\left(\frac{\partial h_1}{\partial \hat{h}_1} \right) = (-0.023096708)(0.23936578) = -0.0055285615
\end{align}\]</span></p>
<p>Therefore, the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(\alpha_{1,1}\)</span> is:</p>
<p><span class="math display">\[\begin{align}
\nabla \alpha_{1,1} \mathcal{L} =  
\frac{\partial \mathcal{L}_{(total)}}{\partial \alpha_{1,1}}  = \delta_{h_1} z_1
= (-0.0055285615)(0.52732275) = -0.0029153363 
\end{align}\]</span></p>
<p>Calculation of gradient for other <strong>alpha</strong> <span class="math inline">\((\alpha)\)</span> weights follow the same process.</p>
<p><strong>Sixth</strong>, in terms of the weights and biases for the <strong>Z Layer</strong>, the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(\omega_{1,1}\)</span> is written as:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{1,1}} = 
\underbrace{ \left(\frac{\partial L_{(total)}}{\partial z_1} \right) 
\left(\frac{\partial z_1}{\partial \hat{z}_1} \right)}_{\delta_{z_1}}
\left(\frac{\partial \hat{z}_1}{\partial \omega_{1,1}} \right) 
\end{align}\]</span></p>
<p>We can expand the first factor into the following:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{1,1}} =
\underbrace{\left(\frac{\partial \mathcal{L}_1}{\partial z_1} +  \frac{\partial \mathcal{L}_2}{\partial z_1}\right) }_{\text{1st factor}}
\underbrace{\left(\frac{\partial z_1}{\partial \hat{z}_1} \right)}_{\text{2nd factor}}
\underbrace{\left(\frac{\partial \hat{z}_1}{\partial \omega_{1,1}} \right)}_{\text{3rd factor}} 
\end{align}\]</span></p>
<p>Decomposing the 1st factor into two terms, we have:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{1,1}} = 
\underbrace{
\left(\overbrace{
      \underbrace{\frac{\partial \mathcal{L}_{(total)}}{\partial h_1}
       \frac{\partial h_1}{\partial \hat{h}_1}}_{\delta_{h_1}}
       \frac{\partial \hat{h}_1}{\partial z_1}}^{\partial \mathcal{L}_1 / \partial z_1} + 
       \overbrace{
       \underbrace{
       \frac{\partial \mathcal{L}_{(total)}}{\partial h_2}
       \frac{\partial h_2}{\partial \hat{h}_2} }_{\delta_{h_2}}
       \frac{\partial \hat{h}_2}{\partial z_1}}^{\partial \mathcal{L}_2 / \partial z_1}
       \right)
\frac{\partial z_1}{\partial \hat{z}_1}}_{\delta_{z_1}}
\frac{\partial \hat{z}_1}{\partial \omega_{1,1}}
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h_1} = 
\left(\frac{\partial \mathcal{L}_{1}}{\partial h_1} + 
       \frac{\partial \mathcal{L}_{2}}{\partial h_1}\right)
\ \ \ \ \ \ \ \ 
\frac{\partial \mathcal{L}_{(total)}}{\partial h_2} = 
\left(\frac{\partial \mathcal{L}_{1}}{\partial h_2} + 
       \frac{\partial \mathcal{L}_{2}}{\partial h_2}\right)
\end{align}\]</span></p>
<p>Note that we have previously solved for the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(h_1\)</span>:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h_1} = -0.02309672
\end{align}\]</span></p>
<p>Here, we also have to solve for the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(h_2\)</span>:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{1}}{\partial h_2} = \delta_1
 \left(\frac{\partial \hat{o}_1}{\partial h_2}\right) = \delta_1 \varphi_{2,1} 
= (0.12787676)(0.04) = 0.0051150704 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{2}}{\partial h_2} = \delta_2
\left(\frac{\partial \hat{o}_2}{\partial h_2}\right) = \delta_2 \varphi_{2,2}
=  (-0.07695146)(0.40) = -0.030780584 
\end{align}\]</span></p>
<p>where the derivatives of the two <strong>net inputs</strong> with respect to <span class="math inline">\(h_2\)</span> are:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial  \hat{o}_1 }{\partial h_2} = 
\frac{\partial \left(\varphi_{0,1} \times h_0 +  \varphi_{1,1} \times h_1 + \varphi_{2,1} \times h_2 \right)}{\partial h_2} = \varphi_{2,1} \times {h_2}^{(1-1)} = \varphi_{2,1}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial  \hat{o}_2 }{\partial h_2} = 
\frac{\partial \left(\varphi_{0,2} \times h_0 +  \varphi_{1,2} \times h_1 + \varphi_{2,2} \times h_2 \right)}{\partial h_2} = \varphi_{2,2} \times {h_2}^{(1-1)} = \varphi_{2,2}
\end{align}\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h_2} = 
\left(\frac{\partial \mathcal{L}_{1}}{\partial h_2}  +
\frac{\partial \mathcal{L}_{2}}{\partial h_2} \right) = 0.0051150704 + -0.030780584 = -0.025665514
\end{align}\]</span></p>
<p>Now, as for the <strong>H</strong> <strong>deltas</strong>, we already have solved for <span class="math inline">\(\delta_{h_1}\)</span> in the previous step. We should solve for the <span class="math inline">\(\delta_{h_2}\)</span> next. For that, we first need to solve for the derivatives of the <strong>activation outputs</strong> in <strong>H layer</strong> with respect to their corresponding <strong>net inputs</strong>. Note that <span class="math inline">\(\frac{\partial h_1}{\partial \hat{h}_1}\)</span> is already solved for us. We have to solve for <span class="math inline">\(\frac{\partial h_2}{\partial \hat{h}_2}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\frac{\partial h_2}{\partial \hat{h}_2} = h_2(1 - h_2) = 0.59800413 ( 1 - 0.59800413) = 0.24039519 
\end{align}\]</span></p>
<p>Therefore, for <span class="math inline">\(\delta_{h_2}\)</span>, we get:</p>
<p><span class="math display">\[\begin{align}
\delta_{h_2} = \left(\frac{\partial \mathcal{L}_{(total)}}{\partial h_2}\right)
\left(\frac{\partial h_2}{\partial \hat{h}_2}\right) = (-0.025665514)(0.24039519) = -0.0061698661
\end{align}\]</span></p>
<p>Next, we still have to solve for the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(\mathbf{z_1}\)</span>:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial z_1} = \left(\frac{\partial \mathcal{L}_{1}}{\partial z_1} + 
       \frac{\partial \mathcal{L}_{2}}{\partial z_1}\right) 
\end{align}\]</span></p>
<p>The equation relies on solving for the derivative of <strong>net inputs</strong> ( <span class="math inline">\(\hat{h}_1\)</span> and <span class="math inline">\(\hat{h}_2\)</span>) with respect to the <strong>activation output</strong> (<span class="math inline">\(z_1\)</span>):</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \hat{h}_1}{\partial z_1} = 
\frac{\partial (\alpha_{0,1} \times z_0 + \alpha_{1,1} \times z_1 + \alpha_{2,1} \times z_2)} {\partial z_1} = \alpha_{1,1} = 0.09 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial \hat{h}_2}{\partial z_1} = 
\frac{\partial (\alpha_{0,2} \times z_0 + \alpha_{1,2} \times z_1 + \alpha_{2,2} \times z_2)} {\partial z_1} = \alpha_{1,2} = 0.06  
\end{align}\]</span></p>
<p>Finally, we can solve for the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(z_1\)</span>:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial z_1} 
&amp;= (\delta_{h_1} \alpha_{1,1} + \delta_{h_2} \alpha_{1,2}) \\
&amp;= (-0.0055285615)(0.09) + (-0.0061698661)(0.06) \nonumber \\
&amp;= -0.00049757053 + -0.00037019197 \nonumber \\
&amp;= -0.0008677625 \nonumber
\end{align}\]</span></p>
<p>Next, we solve for the derivative of the <strong>activation output</strong> (<span class="math inline">\(z_1\)</span>) with respect to its <strong>net input</strong> (<span class="math inline">\(\hat{z}_1\)</span>) using <strong>sigmoid gradient</strong>:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial z_1}{\partial \hat{z}_1} = z_1(1 - z_1)  = 0.52732275(1 - 0.52732275) = 0.24925348
\end{align}\]</span></p>
<p>That should suffice to solve for <span class="math inline">\(\delta_{z_1}\)</span> <strong>delta</strong>:</p>
<p><span class="math display">\[\begin{align}
\delta_{z_1} &amp;= \left[\left(\delta_{h_1} \alpha_{1,1} + \delta_{h_2} \alpha_{1,2}\right) \right] \left(\frac{\partial z_1}{\partial \hat{z}_1}\right) =  (-0.0008677625) (0.24925348) \\
 &amp;=-0.00021629282 \nonumber
\end{align}\]</span></p>
<p>Lastly, we solve for the derivative of the <strong>net input</strong> (<span class="math inline">\(\hat{z}_1\)</span>) with respect to the <strong>weight</strong> (<span class="math inline">\(\omega_{1,1}\)</span>):</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \hat{z}_1}{\partial \omega_{1,1}} = \frac{\partial (\omega_{0,1} \times x_0 + \omega_{1,1} \times x_1 + \omega_{2,1} \times x_2)} {\partial \omega_{1,1}} =  x_1 = 0.12
\end{align}\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[\begin{align}
\nabla \omega_{1,1} \mathcal{L} =  
\frac{\partial \mathcal{L}_{(total)}}{\partial \omega_{1,1}} = \delta_{z_1} x_1 = 
(-0.00021629282)(0.12) = \text{-2.5955138e-5}
\end{align}\]</span></p>
<p>The gradient calculation for other <strong>omega</strong> <span class="math inline">\((\omega)\)</span> weights follows the same process.</p>
<p><strong>Seventh</strong>, for <strong>Backward Pass</strong>, we now can perform update to our parameters. Here, we update the parameters we covered in our previous steps, namely <span class="math inline">\(\varphi_{1,1}\)</span>, <span class="math inline">\(\alpha_{1,1}\)</span>, and <span class="math inline">\(\omega_{1,1}\)</span>:</p>
<p><span class="math display">\[\begin{align}
{\varphi_{1,1}}^{(t+1)} = {\varphi_{1,1}}^{(t)} - \eta \nabla \omega_{1,1} \mathcal{L}= 0.03 - 0.01 \times \text{0.077125331} = 0.029228747
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
{\alpha_{1,1}}^{(t+1)} = {\alpha_{1,1}}^{(t)} - \eta \nabla \omega_{1,1} \mathcal{L}= 0.09 - 0.01 \times \text{-0.0029153363} = 0.090029153
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
{\omega_{1,1}}^{(t+1)} = {\omega_{1,1}}^{(t)} - \eta \nabla \omega_{1,1} \mathcal{L}= 0.21 - 0.01 \times \text{-2.5955138e-5} = 0.21000026
\end{align}\]</span></p>
<p>The same <strong>update rule</strong> applies to all other parameters.</p>
<p>Note here that <strong>eta</strong> <span class="math inline">\((\eta)\)</span> symbol represents the learning rate set at 0.01 and <strong>t</strong> serves as an iteration index.</p>
<p><strong>Lastly</strong>, we iterate through the process until the <strong>cost</strong> minimizes into a tolerable threshold.</p>
<p>We emphasize that our entire <strong>MLP example</strong> uses <strong>sigmoid function</strong> and <strong>least square loss</strong>. However, while it may help showcase the operational and technical aspects of <strong>MLP</strong>, the choice of the <strong>activation function</strong> and <strong>loss function</strong> in our example may not necessarily render the expected result and interpretability. Later, we showcase <strong>Cross-Entropy loss</strong> as one of the alternative loss functions for <strong>Sigmoid</strong>. With that in mind, let us introduce alternative <strong>Activation Functions</strong> commonly used in <strong>Neural Networks</strong>.</p>
</div>
<div id="activation-function" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.5</span> Activation Function <a href="deeplearning1.html#activation-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>From the perspective of an <strong>activation function</strong>, most of our discussions center around <strong>sigmoid function</strong>. In this section, we outline other <strong>activation functions</strong>.</p>
<p>We use an activation function mainly for solving non-linear problems; thus, we may often read phrases such as adding nonlinearity to our network, which implies adding activation functions. Choosing an activation function depends on many factors, especially for our output layer. For example, if we are looking to solve a problem based on <strong>Linear or Logistic Regression</strong>, then perhaps <strong>RELU</strong> and its variations may apply. If it is based on <strong>Binomial Classification</strong>, then perhaps <strong>Sigmoid</strong> and its improved counterparts may apply. Otherwise, perhaps <strong>Softmax</strong> may apply, generalizing for <strong>Multinomial Classification</strong>.</p>
<p>The list below enumerates just a few of the common activation functions, among many others. We also introduce <strong>Swish</strong> <span class="citation">(Prajit Ramachandran et al. <a href="bibliography.html#ref-ref962p">2017</a>)</span> and <strong>Mish</strong> <span class="citation">(Diganta Misra <a href="bibliography.html#ref-ref972d">2019</a>)</span>.   </p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Sigmoid} &amp;= \frac{1}{1 + \text{exp}(-x)} \\ {} \\
\mathbf{RELU} &amp;= \text{max}(0, x) \\ {} \\
\mathbf{\text{Softmax}} &amp;= \frac{exp(x_i - max(x))}{\sum_j exp(x_j - max(x))} \\ {} \\
\mathbf{Mish} &amp;=  x \times \text{TanH}(\text{softplus}(x ))
\end{array}
\left|
\begin{array}{ll}
 \mathbf{TanH} &amp;= \frac{\text{exp}(x) - \text{exp}(-x)}{\text{exp}(x) + \text{exp}(-x)} \\ {} \\
 \mathbf{Leaky\ RELU} &amp;= \begin{array}{l}\text{max}(\alpha x, x),\\ \alpha=0.01 - 0.3\end{array}\\ {} \\
 \mathbf{Swish}  &amp;= x \times \text{sigmoid}(x)\\ {} \\
 \mathbf{Softplus} &amp;= \log_e( 1 + \text{exp}(x)) 
\end{array}
\right.
\]</span></p>
<p>Other variants of <strong>Swish</strong> are:</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{\text{Switch (RELU)}} &amp;= x \times \text{RELU}(x) 
\end{array}
\left| 
\begin{array}{ll}
\mathbf{\text{Switch (TanH)}} &amp;= x \times \text{TanH}(x)
\end{array}
\right.
\]</span></p>
<p>Note that when handling large numbers, a <strong>trick</strong> used for <strong>softmax</strong> - and perhaps for other formulas which rely on <strong>exp(.)</strong> for that matter - is to subtract the maximum x value from each x value to achieve some level of numerical stability.</p>
<p>Here is the implementation of each <strong>activation function</strong>:</p>

<div class="sourceCode" id="cb1798"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1798-1" data-line-number="1">linear          &lt;-<span class="st"> </span><span class="cf">function</span>(x) { x }</a>
<a class="sourceLine" id="cb1798-2" data-line-number="2">binary.step     &lt;-<span class="st"> </span><span class="cf">function</span>(x) { idx =<span class="st"> </span><span class="kw">which</span>(x <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> ); x[idx] =<span class="st"> </span><span class="dv">0</span>;  </a>
<a class="sourceLine" id="cb1798-3" data-line-number="3">                                  x[<span class="op">-</span>idx] =<span class="st"> </span><span class="dv">1</span>; x }</a>
<a class="sourceLine" id="cb1798-4" data-line-number="4">sigmoid         &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) }</a>
<a class="sourceLine" id="cb1798-5" data-line-number="5">tan.h           &lt;-<span class="st"> </span><span class="cf">function</span>(x) { (<span class="kw">exp</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) <span class="op">/</span><span class="st"> </span>( <span class="kw">exp</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) }</a>
<a class="sourceLine" id="cb1798-6" data-line-number="6">relu            &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">pmax</span>(x, <span class="dv">0</span>)  }</a>
<a class="sourceLine" id="cb1798-7" data-line-number="7">leaky.relu      &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">a=</span><span class="fl">0.01</span>) { <span class="kw">pmax</span>( x, a<span class="op">*</span>x) }</a>
<a class="sourceLine" id="cb1798-8" data-line-number="8">softplus        &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">log</span> ( <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x), <span class="kw">exp</span>(<span class="dv">1</span>)) }</a>
<a class="sourceLine" id="cb1798-9" data-line-number="9">swish.sigmoid   &lt;-<span class="st"> </span><span class="cf">function</span>(x) { x <span class="op">*</span><span class="st"> </span><span class="kw">sigmoid</span>(x) }</a>
<a class="sourceLine" id="cb1798-10" data-line-number="10">swish.relu      &lt;-<span class="st"> </span><span class="cf">function</span>(x) { x <span class="op">*</span><span class="st"> </span><span class="kw">relu</span>(x) }</a>
<a class="sourceLine" id="cb1798-11" data-line-number="11">swish.tanh      &lt;-<span class="st"> </span><span class="cf">function</span>(x) { x <span class="op">*</span><span class="st"> </span><span class="kw">tan.h</span>(x) }</a>
<a class="sourceLine" id="cb1798-12" data-line-number="12">mish.tan.h      &lt;-<span class="st"> </span><span class="cf">function</span>(x) { x <span class="op">*</span><span class="st"> </span><span class="kw">tan.h</span>(<span class="kw">softplus</span>(x))}</a>
<a class="sourceLine" id="cb1798-13" data-line-number="13">softmax         &lt;-<span class="st"> </span><span class="cf">function</span>(x) { p =<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">1</span>, max); x =<span class="st"> </span>x <span class="op">-</span><span class="st"> </span>p; p =<span class="st"> </span><span class="kw">exp</span>(x) </a>
<a class="sourceLine" id="cb1798-14" data-line-number="14">                                 s =<span class="st"> </span><span class="kw">apply</span>(p, <span class="dv">1</span>, sum); <span class="kw">sweep</span>(p, <span class="dv">1</span>, s, <span class="st">&quot;/&quot;</span>) </a>
<a class="sourceLine" id="cb1798-15" data-line-number="15">                               }</a></code></pre></div>

<p>Note that there are many other variations of <strong>RELU</strong> such as <strong>Shifted RELU (SRELU)</strong>, <strong>Parameterized RELU (PRELU)</strong>, <strong>Scaled Exponential Linear Unit (SELU)</strong>, and others. We leave readers to investigate the variations.</p>
<p>Now, for every <strong>activation function</strong> we use, there is a corresponding <strong>gradient function</strong> and corresponding <strong>loss function</strong> required for <strong>backpropagation</strong>. Let us enumerate the gradient functions of a few of the <strong>activation functions</strong> where <span class="math inline">\(\frac{\partial\ a(x)}{\partial\ x} = a&#39;(x)\)</span> (no derivations included):</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{\nabla\ Sigmoid} = \sigma(x)(1 - \sigma(x)) \\ {} \\
\mathbf{\nabla\ RELU} = \begin{cases}
0 &amp; if\ x \le 0\\
1 &amp; if\ x &gt; 0
\end{cases} \\ {} \\
\mathbf{\nabla\  Softmax} = a(x_i)( \delta_{ij} - a(x_j)) \\ {} \\
\mathbf{\nabla\  Mish} =  \frac{exp(x) \times \omega}{\delta^2} 
\end{array}
\left|
\begin{array}{ll}
\nabla\ \mathbf{TanH} = 1 - a(x)^2\\ {} \\
\mathbf{\nabla\ Leaky\ RELU} = \begin{array}{l}
\begin{cases}
\alpha &amp; if\ x\le 0,\\ 
1 &amp; if\ x &gt; 0
\end{cases}\\ 
e.g. \alpha=0.01 \end{array}\\ {} \\
\mathbf{\nabla\  Swish} = a(x) + \text{sigmoid}(x)( 1 - a(x)) \\ {} \\
\mathbf{\nabla\  Softplus} =  \frac{1}{1 + exp(-x)}\ \leftarrow \text{(sigmoid)} 
\end{array}
\right.
\]</span></p>
<p>Note that <strong>activation functions</strong> have to be differentiable. For <strong>RELU</strong> and <strong>Leaky RELU</strong>, we may not have to be strictly mathematical; instead, we can produce a <strong>quasi-derivative</strong> for the respective functions like so:</p>
<p><span class="math display">\[\begin{align}
a&#39;(x) = \begin{cases}
0 &amp; if\ x &lt;= 0\\
1 &amp; if\ x &gt; 0
\end{cases}
\ \ \ \ \ \ \ \ \ \ \ \ \ 
a&#39;(x) = \begin{cases}
\alpha &amp; if\ x &lt;= 0\\
1 &amp; if\ x &gt; 0
\end{cases} \label{eqn:eqnnumber603}
\end{align}\]</span></p>
<p>For <strong>Mish</strong> <span class="citation">(Diganta M., <a href="bibliography.html#ref-ref972d">2019</a>)</span>, we have the following parameters for its derivatives:</p>
<p><span class="math display">\[\begin{align}
\omega = 4(x + 1) + 4e^{2x} + e^{3x} + e^x(4x + 6)
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\delta = 2e^x + e^{2x} + 2 
\end{align}\]</span></p>
<p>For <strong>Swish</strong> <span class="citation">(Prajit R. et. al, <a href="bibliography.html#ref-ref962p">2017</a>)</span>, the gradient is <span class="math inline">\(a(x) = x \times \text{sigmoid}(x)\)</span>. Additionally, <strong>Swish</strong> also can use an extra parameter, namely <span class="math inline">\(\beta\)</span>. For example:</p>
<p><span class="math display">\[\begin{align}
\text{Swish (Sigmoid)} = x \times \text{sigmoid}(\beta X)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\nabla \text{Swish (Sigmoid)} = \beta a(\beta x) + \text{sigmoid}(\beta x) \times ( 1 - \beta a(\beta x)) 
\end{align}\]</span></p>
<p>For <strong>Softmax</strong> (Ludwig Boltzmann, 1868), the derivative optionally uses <strong>Kronecker Delta</strong>, denoted by the symbol (<span class="math inline">\(\delta_{ij}\)</span>). Below is an example of applying the <strong>Kronecker Delta</strong> to a matrix that produces an <strong>Identity matrix</strong>:</p>
<p><span class="math display">\[
\left[
\begin{array}{lll}
1 &amp; 2 &amp; 3\\
4 &amp; 5 &amp; 6\\
7 &amp; 8 &amp; 9
\end{array}
\right]_X
\times
\left[
\begin{array}{lll}
1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 1
\end{array}
\right]_\delta = 
\left[
\begin{array}{lll}
1 &amp; 0 &amp; 0\\
0 &amp; 5 &amp; 0\\
0 &amp; 0 &amp; 9
\end{array}
\right]
\ \ \ \ where\ \ \ \
\delta_{jk} = 
\begin{cases}
1 &amp; \text{if j = k}\\
0 &amp; \text{if j }\ne\text{ k}
\end{cases}
\]</span></p>
<p>More discussion of <strong>Softmax</strong> to follow later ahead.</p>
<p>Below is a list of the implementation of the gradients:</p>

<div class="sourceCode" id="cb1799"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1799-1" data-line-number="1">gradient.sigmoid    &lt;-<span class="st"> </span><span class="cf">function</span>(o) {  o <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>o) } </a>
<a class="sourceLine" id="cb1799-2" data-line-number="2">gradient.tan.h      &lt;-<span class="st"> </span><span class="cf">function</span>(o) { <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>o<span class="op">^</span><span class="dv">2</span> }       </a>
<a class="sourceLine" id="cb1799-3" data-line-number="3">gradient.relu       &lt;-<span class="st"> </span><span class="cf">function</span>(o) { <span class="kw">pmax</span>(<span class="kw">sign</span>(o), <span class="dv">0</span>) }  </a>
<a class="sourceLine" id="cb1799-4" data-line-number="4">gradient.leaky.relu &lt;-<span class="st"> </span><span class="cf">function</span>(o, <span class="dt">a=</span><span class="fl">0.01</span>) { <span class="kw">pmax</span>(<span class="kw">sign</span>(o), a) }</a>
<a class="sourceLine" id="cb1799-5" data-line-number="5">gradient.softplus   &lt;-<span class="st"> </span><span class="cf">function</span>(o) { <span class="kw">sigmoid</span>(o) }  </a>
<a class="sourceLine" id="cb1799-6" data-line-number="6">gradient.swish.sigmoid  &lt;-<span class="st"> </span><span class="cf">function</span>(o) { a =<span class="st"> </span>o <span class="op">*</span><span class="st"> </span><span class="kw">sigmoid</span>(o);</a>
<a class="sourceLine" id="cb1799-7" data-line-number="7">                                         a <span class="op">+</span><span class="st"> </span><span class="kw">sigmoid</span>(o) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>a) }</a>
<a class="sourceLine" id="cb1799-8" data-line-number="8">gradient.mish.tan.h     &lt;-<span class="st"> </span><span class="cf">function</span>(o) { </a>
<a class="sourceLine" id="cb1799-9" data-line-number="9">                            w =<span class="st"> </span><span class="dv">4</span><span class="op">*</span>(o <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="dv">4</span><span class="op">*</span><span class="kw">exp</span>(<span class="dv">2</span><span class="op">*</span>o) <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="dv">3</span><span class="op">*</span>o) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1799-10" data-line-number="10"><span class="st">                            </span><span class="kw">exp</span>(o) <span class="op">*</span><span class="st"> </span>(<span class="dv">4</span><span class="op">*</span>o <span class="op">+</span><span class="st"> </span><span class="dv">6</span>)</a>
<a class="sourceLine" id="cb1799-11" data-line-number="11">                            d =<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">exp</span>(o) <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="dv">2</span><span class="op">*</span>o) <span class="op">+</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1799-12" data-line-number="12">                            (<span class="kw">exp</span>(o) <span class="op">*</span><span class="st"> </span>w) <span class="op">/</span><span class="st"> </span>d<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb1799-13" data-line-number="13">gradient.softmax   &lt;-<span class="st"> </span><span class="cf">function</span>(o) { J =<span class="st"> </span><span class="kw">list</span>();  N =<span class="st"> </span><span class="kw">nrow</span>(o)</a>
<a class="sourceLine" id="cb1799-14" data-line-number="14">                            <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1799-15" data-line-number="15">                                J[[i]] =<span class="st"> </span><span class="kw">diag</span>(o[i,]) <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1799-16" data-line-number="16"><span class="st">                                         </span><span class="kw">kronecker</span>(<span class="kw">t</span>(o[i,]), o[i,])</a>
<a class="sourceLine" id="cb1799-17" data-line-number="17">                            }; J }</a></code></pre></div>

<p>For a plot of the activation functions and corresponding derivatives, see Figure <a href="deeplearning1.html#fig:activationfunction">12.11</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:activationfunction"></span>
<img src="activationfunction.png" alt="Activation Function" width="100%" />
<p class="caption">
Figure 12.11: Activation Function
</p>
</div>
<p>It is possible to use a different <strong>activation function</strong> for hidden layers and the output layer. For example, for <strong>Logistic Regression</strong>, it may be common to use <strong>Rectified Linear Unit (RELU)</strong> as an <strong>activation functions</strong> for all <strong>hidden layers</strong> and opt to use <strong>sigmoid</strong> in the output layer. And for <strong>Multi Classification</strong>, we can use <strong>softmax function</strong> in the output layer.</p>
<p>Certain properties of an activation function can be reviewed to see if such a function qualifies to fit oneâs purposes: range, monotonicity, boundary (e.g., bounded below, unbounded above), and smoothness. Apart from the usual <strong>performance</strong> and <strong>accuracy</strong> goals, some of the activation functions, in certain conditions, may help to avoid the <strong>vanishing and exploding gradient</strong> conditions; thus, such properties are essential. The former condition, e.g., <strong>vanishing gradient</strong>, may manifest if the <strong>neural network</strong> has too many layers. For example, notice in Figure <a href="deeplearning1.html#fig:activationfunction">12.11</a> that the gradients of both <strong>Sigmoid</strong> and <strong>TanH</strong> reduce the effect of gradients as gradients approach zero and as their net inputs stretch outwards, e.g., <span class="math inline">\(\pm \infty\)</span>. In the figure, all other activation functions (apart from the linear and binary step) have a lower bound (e.g., zero) towards the negative direction and an infinite bound towards the positive direction. The latter condition, e.g., <strong>exploding gradient</strong>, may manifest if the <strong>neural network</strong> has connections that accumulate large weights due to large gradients. Whereas <strong>vanishing gradients</strong> tend towards <strong>-infinity</strong> (<span class="math inline">\(+\infty\)</span>), <strong>exploding gradients</strong> tend towards <strong>+infinity</strong> (<span class="math inline">\(+\infty\)</span>).</p>
<p>In terms of performance measures, each <strong>activation function</strong> may work best with its own <strong>loss function</strong>. For example, for both <strong>Relu</strong> and <strong>Leaky Relu</strong>, we have the following <strong>Least Squared Error</strong> formula:</p>
<p><span class="math display">\[\begin{align}
\mathcal{L} =  \frac{1}{N} \sum_{i=1}^N \left(t - o\right)^2
\end{align}\]</span></p>
<p>For both <strong>Sigmoid</strong> and <strong>Softmax</strong>, we can use <strong>Cross-Entropy (CE) Loss</strong>. Generally, we see the following equation, discussed in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>).</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}^{(CE)} =  
\underbrace{-\sum_x^X P_x \log_e \mathcal{Q}_x }_\text{Cross-Entropy Loss} 
= - \sum_x^X \left[  t_{x} \log_e \sigma(o_{(x)})\right]
\end{align}\]</span></p>
<p>Note that the idea of <strong>Cross-Entropy</strong> is about measuring the distance between two distributions - See Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>).</p>
<p>In essence, for <strong>Sigmoid</strong>, a <strong>Binary CE loss</strong> can be translated into a <strong>Binomial Logistic Loss</strong> like so:</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}^{(CE)} = 
\underbrace{ - \frac{1}{N} \sum_{i=1}^N\left[t_i \log_e(o_i) + (1 - t_i)\log_e(1 - o_i) \right]}_\text{Binomial Logistic Loss}
= 
\begin{cases}
-\log_e(o_i) &amp; \text{if }t_i\text{ = 1} \\
-\log_e(1 - o_i) &amp; \text{if }t_i\text{ = 0} \\
\end{cases} \label{eqn:eqnnumber604}
\end{align}\]</span></p>
<p>where <strong>N</strong> is the number of samples.</p>
<p>For <strong>Softmax</strong>, we may use a <strong>Multinomial Logistic Loss</strong> or <strong>Multiple Cross-Entropy Loss</strong> formula for <strong>Multi Classification</strong>:</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}^{(CE)} &amp;= \underbrace{ - \frac{1}{N} \sum_{i=1}^N\sum_{k=1}^K\left[t_{ik} \log_e(o_{ik}) + (1 - t_{ik})\log_e(1 - o_{ik}) \right]}_\text{Multinomial Logistic Loss} \\
&amp;= -\frac{1}{N}\sum_{N=1}^N\sum_{k=1}^Kt_{(ik)} \log_e(o_{(ik)})  
\end{align}\]</span></p>
<p>where <strong>N</strong> is the number of samples and <strong>K</strong> is the number of classes. A K-value of 2 makes the loss a <strong>Binary CE loss</strong>.</p>
<p>Note that the <strong>target</strong> (t) is a <strong>one-hot vector</strong>, and the <strong>output</strong> (o) is a vector of probabilities that sums up to 1.</p>
<p>Let us give a special look into <strong>Softmax</strong> in the context of backpropagation. Our discussion focuses on the derivative of <strong>CE loss</strong> with respect to an <strong>activation output</strong> - also called <strong>activation output</strong> in our own context.</p>
<p><strong>Sigmoid (Logistic) Loss</strong></p>
<p>It helps to start with the more simple <strong>Sigmoid</strong> activation function for <strong>Binomial Classification</strong>. Then we can generalize into <strong>Softmax</strong> for <strong>Multi Classification</strong>.</p>
<p>Using the following <strong>sigmoid</strong> function, we should be able to get the derivative of the <strong>sigmoid</strong> function with respect to the raw input (the <strong>logits</strong>).</p>
<p><span class="math display">\[\begin{align}
s(x) = \frac{1}{1 + exp(-x)}\ \ \ \ \ \ s&#39;(x) = \frac{\partial\ s(x)}{\partial x} = s(x)(1 - s(x))
\end{align}\]</span></p>
<p>From a <strong>Neural Network</strong> perspective, we have the equivalent notation in reference to Figure <a href="deeplearning1.html#fig:deltarule">12.8</a>:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial\ s(x)}{\partial x} = \frac{\partial\ \sigma(\hat{o})}{\partial \hat{o}}  =
\frac{\partial o}{\partial \hat{o}} = \sigma(\hat{o})(1 - \sigma(\hat{o})) = o ( 1 - o)
\end{align}\]</span></p>
<p>where our <strong>activation output</strong> (<strong>activation output</strong>) in the output layer is denoted by (<strong>o</strong>) and our <strong>logits input</strong> (<strong>net input</strong>) as (<span class="math inline">\(\mathbf{\hat{o}}\)</span>).</p>
<p>Additionally, the derivative of a <strong>Cross-Entropy Loss</strong> as our <strong>Loss Function</strong> for <strong>Sigmoid</strong> with respect to an <strong>activation output</strong> (<span class="math inline">\(\mathbf{o}\)</span>) is as follows (no derivations provided):</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}^{(CE)}}{\partial o} = - \frac{t}{o} + \frac{1 - t}{1 - o}
= \frac{o-t}{o( 1 - o)}
\end{align}\]</span></p>
<p>If we recall the <strong>Delta Rule</strong>, the derivative of the <strong>Cross-Entropy Loss</strong> for <strong>Sigmoid function</strong> with respect to a <strong>net input</strong> (<span class="math inline">\(\mathbf{\hat{o}}\)</span>) is as follows:</p>
<p><span class="math display">\[\begin{align}
\delta_{o}  = \frac{\partial \mathcal{L}^{(CE)}}{\partial \hat{o}} =  
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o}\right)
\left(\frac{\partial o}{\partial \hat{o}}\right)
= \left[\frac{o-t}{o( 1 - o)}\right] 
o ( 1 - o) = (o - t )
\end{align}\]</span></p>
<p>Then, the derivative of the <strong>Cross-Entropy Loss</strong> for <strong>Sigmoid function</strong> with respect to a <strong>weight</strong> (<span class="math inline">\(\omega_{(jk)}\)</span>) is as follows:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}^{(CE)}}{\partial \omega_{(jk)}} = 
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o}\right)
\left(\frac{\partial o}{\partial \hat{o}}\right)
\left(\frac{\partial \hat{o}}{\partial \omega_{(jk)}}\right) =  \delta_{o} \left(\frac{\partial \hat{o}}{\partial \omega_{(jk)}}\right) 
= \delta_{o}(h_j) = (o - t)(h_j) 
\end{align}\]</span></p>
<p><strong>Softmax Loss</strong></p>
<p>Now for <strong>Softmax</strong>, we should be able to get the derivative of the <strong>Softmax</strong> loss function with respect to the raw input.</p>
<p><span class="math display">\[\begin{align}
s(x_j) = \frac{exp(x_j)}{\sum_{k=1}^Kexp(x_k)}\ \ \ \ \ \ 
s&#39;(x_j) = \frac{\partial\ s(x_j)}{\partial x_j} = 
\begin{cases}
s(x_j)(1 - s(x_j)) &amp; \text{if j = k} \\
-s(x_j) s(x_k) &amp; \text{if j }\ne \text{k} \\
\end{cases} \label{eqn:eqnnumber605}
\end{align}\]</span></p>
<p>Here, from a <strong>Neural Network</strong> perspective, we have the equivalent notation in reference to Figure <a href="deeplearning1.html#fig:deltarule">12.8</a>:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial\ s(x_j)}{\partial x_j} = \frac{\partial\ \sigma(\hat{o}_j)}{\partial \hat{o}_j}  =
\frac{\partial o_j}{\partial \hat{o}_j} = 
\begin{cases}
\sigma(\hat{o}_j)(1 - \sigma(\hat{o}_j)) = o_j ( 1 - o_j)   &amp; \text{if j = k} \\
- \sigma(\hat{o}_j) \sigma(\hat{o}_k)  = -(o_j)(o_k)  &amp; \text{if j }\ne \text{k} \\
\end{cases} \label{eqn:eqnnumber606}
\end{align}\]</span></p>
<p>Figure <a href="deeplearning1.html#fig:softmaxgradient">12.12</a> shows a <strong>Jacobian matrix</strong> as reference when solving for the derivative of the <strong>activation output</strong> - an output from an <strong>activation function</strong> - with respect to the <strong>net input</strong>, e.g. <span class="math inline">\(\frac{\partial o_j}{\partial \hat{o}_j}\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:softmaxgradient"></span>
<img src="softmaxgradient.png" alt="SoftMax Gradient" width="100%" />
<p class="caption">
Figure 12.12: SoftMax Gradient
</p>
</div>
<p>To illustrate, suppose we have the following <strong>net input</strong> to be fed to a <strong>Softmax</strong> activation function (assume one sample and three target classes):</p>
<p><span class="math display">\[
\hat{o}_1 = 3 \ \ \ \ \ \ \ \ \ \ \ \ \
\hat{o}_1 = 4 \ \ \ \ \ \ \ \ \ \ \ \ \ \hat{o}_1 = 5
\]</span></p>
<p>Our <strong>Softmax</strong> yields the following:</p>

<div class="sourceCode" id="cb1800"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1800-1" data-line-number="1">o.hat =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>), <span class="dt">nrow=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1800-2" data-line-number="2">(<span class="dt">o =</span> <span class="kw">softmax</span>(o.hat))</a></code></pre></div>
<pre><code>##         [,1]   [,2]   [,3]
## [1,] 0.09003 0.2447 0.6652</code></pre>

<p>where sum equals one: <span class="math inline">\(\sum_i o_i =\)</span> 0.09 + 0.2447 + 0.6652 = 1.</p>
<p>Now, the <strong>Jacobian matrix</strong> representation of the derivatives of <strong>activation output</strong> with respect to the <strong>net input</strong> is as follows:</p>
<p><span class="math display">\[\begin{align}
\sigma(o_j)(\delta_{jk} - \sigma(o_k)) = 
\underbrace{\sigma(o_j)\delta_{jk}}_{\text{identity matrix}} - 
\underbrace{\sigma(o_j) \otimes \sigma(o_k)}_{\text{kronecker product}}
\ \ \ \ \  where\ \ \delta_{jk}\text{ is kronecker delta}
\end{align}\]</span></p>

<div class="sourceCode" id="cb1802"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1802-1" data-line-number="1">(<span class="dt">J =</span> <span class="kw">diag</span>(o) <span class="op">-</span><span class="st"> </span><span class="kw">kronecker</span>(<span class="kw">t</span>(o), o))</a></code></pre></div>
<pre><code>##         [,1]     [,2]     [,3]
## [1,] 0.08193  0.06800  0.03014
## [2,] 0.06800  0.03014 -0.07277
## [3,] 0.03014 -0.07277 -0.35251</code></pre>

<p>To validate, let us tackle the first derivative:</p>
<p><span class="math inline">\(\frac{\partial o_1}{\partial \hat{o}_1} = s(\hat{o}_1)(1 - s(\hat{o}_1)) = o_1 ( 1 - o_1)\)</span> = 0.09 <span class="math inline">\(\times (1-\)</span> 0.09<span class="math inline">\()\)</span> = 0.0819.</p>
<p><span class="math inline">\(\frac{\partial o_2}{\partial \hat{o}_2} = s(\hat{o}_2)(1 - s(\hat{o}_2)) = o_2 ( 1 - o_2)\)</span> = 0.2447 <span class="math inline">\(\times (1-\)</span> 0.2447<span class="math inline">\()\)</span> = 0.0301.</p>
<p><span class="math inline">\(\frac{\partial o_3}{\partial \hat{o}_3} = s(\hat{o}_3)(1 - s(\hat{o}_3)) = o_3 ( 1 - o_3)\)</span> = 0.6652 <span class="math inline">\(\times (1-\)</span> 0.6652<span class="math inline">\()\)</span> = -0.3525.</p>
<p><span class="math inline">\(\frac{\partial o_1}{\partial \hat{o}_2} = -s(\hat{o}_1) s(\hat{o}_2) = - o_1 \times o_2\)</span> = -0.09 <span class="math inline">\(\times\)</span> 0.2447 = 0.068.</p>
<p><span class="math inline">\(\frac{\partial o_1}{\partial \hat{o}_3} = -s(\hat{o}_1) s(\hat{o}_3) = - o_1 \times o_3\)</span> = -0.09 <span class="math inline">\(\times\)</span> 0.6652 = 0.0301.</p>
<p>â¦</p>
<p><span class="math inline">\(\frac{\partial o_3}{\partial \hat{o}_1} = -s(\hat{o}_3) s(\hat{o}_1) = - o_3 \times o_1\)</span> = -0.6652 <span class="math inline">\(\times\)</span> 0.09 = 0.0301.</p>
<p><span class="math inline">\(\frac{\partial o_3}{\partial \hat{o}_2} = -s(\hat{o}_3) s(\hat{o}_2) = - o_3 \times o_2\)</span> = -0.6652 <span class="math inline">\(\times\)</span> 0.2447 = -0.0728.</p>
<p>Next, the derivative of the <strong>Cross-Entropy Loss</strong> as our <strong>Loss Function</strong> for <strong>Softmax</strong> with respect to an <strong>activation output</strong> (<span class="math inline">\(\mathbf{o_j}\)</span>) is as follows:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}^{(CE)}}{\partial o_j} = 
\frac{\partial}{\partial o_j}  \left(-\sum_{k=1}^Kt_{k} \log_e(o_{k}) \right) = 
\underbrace{\frac{\partial}{\partial o_j}
\left(- t_j \log_e(o_j)\right)}_{\text{focus on one-hot encoding}} = -\frac{t_j}{o_j}
\end{align}\]</span></p>
<p>Note that <strong>t</strong> is a one-hot vector and only one element is equal to one. Assume therefore that if <span class="math inline">\(\mathbf{t_j= 1}\)</span>, it stands that <span class="math inline">\(-\frac{\partial \mathcal{L}^{(CE)}}{\partial o_j} = -\frac{\partial}{\partial o_j} \left((1)\log_e(o_j)\right)\)</span>, and given that <span class="math inline">\(f&#39;(ln(x)) = \frac{1}{x}\)</span>, therefore <span class="math inline">\(-\frac{\partial}{\partial o_j} \left(\log_e(o_j)\right) = -\frac{1}{o_j} = -\frac{t_j}{o_j}\)</span></p>
<p>For the <strong>Delta Rule</strong>, the derivative of the <strong>Cross-Entropy Loss</strong> function for <strong>Softmax function</strong> with respect to a <strong>net input</strong> (<span class="math inline">\(\mathbf{\hat{o}_j}\)</span>) is as follows:</p>
<p><span class="math display">\[\begin{align}
\delta_{o_j}  = \frac{\partial \mathcal{L}^{(CE)}}{\partial \hat{o}_j}
= \left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o_j}\right)
\left(\frac{\partial o_j}{\partial \hat{o}_j}\right) = 
\left(- \frac{t_k}{o_j} \right)
\left(
\begin{cases}
o_k ( 1 - o_k) &amp; \text{if j = k}\\
- o_j o_k    &amp; \text{if j }\ne \text{k}
\end{cases}
\right) = (o_j - t_j) \label{eqn:eqnnumber607}
\end{align}\]</span></p>
<p>Finally, the derivative of the <strong>Cross-Entropy Loss</strong> function for <strong>softmax function</strong> with respect to a <strong>weight</strong> (<span class="math inline">\(\omega_{(jk)}\)</span>) is as follows:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}^{(CE)}}{\partial \omega_{(jk)}} =  
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o_j}\right)
\left(\frac{\partial o_j}{\partial \hat{o}_k}\right)
\left(\frac{\partial \hat{o}_k}{\partial \omega_{(jk)}}\right) =
\delta_{o_k} \left(\frac{\partial \hat{o}_k}{\partial \omega_{(jk)}}\right) 
= \delta_{o_k} (h_j) = (o_k - t_k)(h_j) 
\end{align}\]</span></p>
<p>where <strong>J</strong> and <strong>K</strong> are the numbers of classes.</p>
<p>Therefore, if we carefully follow backpropagation using Figure <a href="deeplearning1.html#fig:deltarule">12.8</a>, the derivative of the <strong>Softmax</strong> loss with respect to the <strong>weight</strong> (<span class="math inline">\(\alpha_{1,1}\)</span>) is:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}^{(CE)}}{\partial \alpha_{1,1}} &amp;=
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o_1}\right) 
\left(\frac{\partial o_1}{\partial h_1}\right)
\left(\frac{\partial h_1}{\partial \alpha_{1,1}}\right)
&amp;+ 
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o_2}\right)
\left(\frac{\partial o_2}{\partial h_1}\right)
\left(\frac{\partial h_1}{\partial \alpha_{1,1}}\right)\\
\\
&amp;=
\left(-\mathbf{\frac{ t_1}{o_1}}\right) \left[o_1( 1 - o_1)\right] h_1
&amp;+ 
\left(-\mathbf{\frac{ t_2}{o_2}}\right) \left(-(o_2)(o_1)\right) h_1\\
&amp;=\left(t_2 o_1 - t_1 + t_1 o_1 \right) h_1\\
&amp;=\left((t_2 + t_1) o_1 - t_1 \right) h_1\\
&amp;=(o_1 - t_1) h_1 &amp; \text{where }\sum_i t_i = 1
\end{align}\]</span></p>
<p>Note that <strong>t</strong> is a one-hot vector. Therefore, in the case above, either we have (<span class="math inline">\(t_1 = 1, t_2 = 0\)</span>) or (<span class="math inline">\(t_1 = 0, t_2 = 1\)</span>).</p>
<p>To illustrate, suppose we have the following <strong>target</strong> (<strong>t</strong>) and <strong>softmax output</strong> (<strong>o</strong>):</p>

<div class="sourceCode" id="cb1804"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1804-1" data-line-number="1">t =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1804-2" data-line-number="2">o =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.49052134</span>, <span class="fl">0.50947866</span>)</a></code></pre></div>

<p>Below is the result of the unsimplified version:</p>

<div class="sourceCode" id="cb1805"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1805-1" data-line-number="1"><span class="op">-</span><span class="st"> </span>t[<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>o[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>o[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>o[<span class="dv">1</span>]) <span class="op">+</span><span class="st"> </span>(<span class="op">-</span>t[<span class="dv">2</span>]<span class="op">/</span>o[<span class="dv">2</span>]) <span class="op">*</span><span class="st"> </span>( <span class="op">-</span>o[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>o[<span class="dv">1</span>])</a></code></pre></div>
<pre><code>## [1] -0.5095</code></pre>

<p>And the simplified version:</p>

<div class="sourceCode" id="cb1807"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1807-1" data-line-number="1">o[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>t[<span class="dv">1</span>]</a></code></pre></div>
<pre><code>## [1] -0.5095</code></pre>

<p>Below, we have our implementation of the <strong>Loss functions</strong>, along with <strong>Helper functions</strong>:</p>

<div class="sourceCode" id="cb1809"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1809-1" data-line-number="1"><span class="co"># Helper Functions</span></a>
<a class="sourceLine" id="cb1809-2" data-line-number="2">ln                       &lt;-<span class="st"> </span><span class="cf">function</span>(x)    { <span class="kw">log</span>(x, <span class="kw">exp</span>(<span class="dv">1</span>))}</a>
<a class="sourceLine" id="cb1809-3" data-line-number="3">relu.loss                &lt;-<span class="st"> </span>leaky.relu.loss &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) { (t<span class="op">-</span>o)<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb1809-4" data-line-number="4">sigmoid.loss             &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) {  eps =<span class="st"> </span><span class="fl">1e-20</span> <span class="co"># Avoid NaN</span></a>
<a class="sourceLine" id="cb1809-5" data-line-number="5">                            <span class="op">-</span><span class="st"> </span>(t <span class="op">*</span><span class="st"> </span><span class="kw">ln</span>(o <span class="op">+</span><span class="st"> </span>eps) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>t) <span class="op">*</span><span class="st"> </span><span class="kw">ln</span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>o <span class="op">+</span><span class="st"> </span>eps))}</a>
<a class="sourceLine" id="cb1809-6" data-line-number="6">softmax.loss             &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) { eps =<span class="st"> </span><span class="fl">1e-20</span>; l =<span class="st"> </span>t <span class="op">*</span><span class="st"> </span><span class="kw">ln</span>(o <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1809-7" data-line-number="7">                            <span class="op">-</span><span class="kw">apply</span>(l, <span class="dv">1</span>, sum)  }</a>
<a class="sourceLine" id="cb1809-8" data-line-number="8">gradient.relu.loss       &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) { o<span class="op">-</span>t } <span class="co"># or -(t-o)</span></a>
<a class="sourceLine" id="cb1809-9" data-line-number="9">gradient.leaky.relu.loss &lt;-<span class="st"> </span>gradient.relu.loss</a>
<a class="sourceLine" id="cb1809-10" data-line-number="10">gradient.sigmoid.loss    &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) { (o<span class="op">-</span>t)<span class="op">/</span>(o <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>o)) }</a>
<a class="sourceLine" id="cb1809-11" data-line-number="11">gradient.softmax.loss    &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) { eps =<span class="st"> </span><span class="fl">1e-20</span>; <span class="op">-</span>t <span class="op">/</span><span class="st"> </span>(o <span class="op">+</span><span class="st"> </span>eps) }</a>
<a class="sourceLine" id="cb1809-12" data-line-number="12">activation               &lt;-<span class="st"> </span><span class="cf">function</span>(x, afunc)    { <span class="kw">afunc</span>(x) }</a>
<a class="sourceLine" id="cb1809-13" data-line-number="13">gradient.activation      &lt;-<span class="st"> </span><span class="cf">function</span>(o, afunc)    { <span class="kw">afunc</span>(o) }     </a>
<a class="sourceLine" id="cb1809-14" data-line-number="14">gradient.loss            &lt;-<span class="st"> </span><span class="cf">function</span>(t, o, afunc) { <span class="kw">afunc</span>(t, o) }</a></code></pre></div>

<p>For generating the <strong>loss function</strong>, we have:</p>

<div class="sourceCode" id="cb1810"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1810-1" data-line-number="1">get.loss &lt;-<span class="st"> </span><span class="cf">function</span>(target, layers, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>) {</a>
<a class="sourceLine" id="cb1810-2" data-line-number="2">    afunc  =<span class="st"> </span><span class="kw">get</span>(<span class="kw">paste0</span>(afunc, <span class="st">&quot;.loss&quot;</span>))</a>
<a class="sourceLine" id="cb1810-3" data-line-number="3">    L      =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb1810-4" data-line-number="4">    output =<span class="st"> </span>layers[[L]]<span class="op">$</span>output</a>
<a class="sourceLine" id="cb1810-5" data-line-number="5">    err    =<span class="st"> </span><span class="kw">afunc</span>(target, output)</a>
<a class="sourceLine" id="cb1810-6" data-line-number="6">    <span class="kw">mean</span>(err)</a>
<a class="sourceLine" id="cb1810-7" data-line-number="7">}</a></code></pre></div>

<p>We now move to the actual implementation of <strong>MLP</strong>.</p>
</div>
<div id="mlp-implementation" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.6</span> MLP Implementation<a href="deeplearning1.html#mlp-implementation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Following an <strong>MLP</strong> algorithm, we assume a <strong>complete Neural Network</strong> with the same setup as diagrammed in Figure <a href="deeplearning1.html#fig:deltarule">12.8</a>. Our supposition is that we have at least one hidden layer labeled as <strong>H</strong>.</p>
<p>Now, in the context of data structures, the implementation of <strong>MLP</strong> relies heavily on matrix manipulation, especially with multiple samples. We can see in Figure <a href="deeplearning1.html#fig:forwardpass">12.13</a> how the dot product of an input and its corresponding weight results in an output which is taken by the next layer as an input. In turn, the process continues with the following dot product.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:forwardpass"></span>
<img src="forwardpass.png" alt="Forward Pass" width="90%" />
<p class="caption">
Figure 12.13: Forward Pass
</p>
</div>
<p>Below is an <strong>MLP</strong> pseudocode for the <strong>Forward Pass</strong> algorithm. Here, we use <strong>sigmoid</strong> for activation function:</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{x_{(1,1)}, x_{(1,2)}, ..., x_{(n,p)}: x\ \in\ \mathbb{R}^{nxp}\}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rightarrow \text{(p is no. of features, n is no. of samples)}\\
\ \ \ \text{parameters}: \{\omega_{(1,1)}, \omega_{(1,2)}, ..., \omega_{(p,h)}: x\ \in\ \mathbb{R}^{pxh}\}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rightarrow \text{(p is no. of inputs, h is no. of target outputs)}\\
\ \ \ \text{activation function}: f(x) \rightarrow e.g.
\begin{cases} 
sigmoid(x) &amp; \text{if output layer}\\
leaky.relu(x) &amp; \text{if hidden layer}
\end{cases}\\
\mathbf{Algorithm}:\\
\ \ \ net.input = X\\
\ \ \ act.output = \{\} \\
\ \ \ \text{loop}\ L\ in\ 1:\ H\ \ \ \ \ \ \rightarrow \text{(H is no. of layers excluding input)}\\
\ \ \ \ \ \ \ weights = \omega^{(L)} \\
\ \ \ \ \ \ \ act.output^{(L)}  = f(net.input \cdot \omega^{(L)} )\ \ \ \ \ (act.output^{(L) }  \in \mathbb{R}^{nxh})\\
\ \ \ \ \ \ \ net.input = act.output^{(L)}\\
\ \ \ \text{end loop} \\
\mathbf{Output}:\\
\ \ \ act.output 
\end{array}
\]</span></p>
<p>Recall that our <strong>net input</strong> in the matrix includes the biases. Also, our bias constant connects to neurons in different layers with unique weights.</p>
<p>Note that the gradient of the resulting output can also be obtained in the same <strong>forward pass</strong> implementation.</p>
<p>Next, Figure <a href="deeplearning1.html#fig:backwardpass">12.14</a> shows the data structure used by <strong>BackPropagation</strong> and <strong>Backward Pass</strong> algorithms.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:backwardpass"></span>
<img src="backwardpass.png" alt="Backward Pass (Back Propagation)" width="90%" />
<p class="caption">
Figure 12.14: Backward Pass (Back Propagation)
</p>
</div>
<p>For all layers, excluding the input layer, a corresponding matrix table is produced for the <strong>deltas</strong>. Next, let us see an <strong>MLP</strong> algorithm for the <strong>BackPropagation</strong> that shows the equations used for the <strong>deltas</strong>:</p>

<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:  \text{(n is no. of samples)}\\
\ \ \ \text{dataset}: \{x_{(1,1)}, x_{(1,2)}, ..., x_{(n,p)}: x\ \in\ \mathbb{R}^{nxp}\}\ \rightarrow \text{(p is no. of features)}\\
\ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ : \{y_{(1,1)}, y_{(1,2)}, ..., y_{(n,o)}: y\ \in\ \mathbb{R}^{nxo}\}\ \rightarrow \text{(o is no. of neurons)}\\
\ \ \ \text{parameters}: \{\omega_{(1,1)}, \omega_{(1,2)}, ..., \omega_{(p,h)}: x\ \in\ \mathbb{R}^{pxh}\}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rightarrow \text{(p is no. of inputs, h is no. of target outputs)}\\
\ \ \ \text{forward-pass output}: act.output\\
\ \ \ \text{activation function}: f(x) \rightarrow e.g. \begin{cases} 
sigmoid(x) &amp; \text{if output layer}\\
leaky.relu(x) &amp; \text{if hidden layer}
\end{cases}\\
\ \ \ \text{gradient function}: g(f(x)) = f(x) (1 - f(x))\ \ \ \ \ \ \ \ \text{(e.g. sigmoid gradient)} \\
\mathbf{Algorithm}:\\
\ \ \ \text{loop}\ L\ in\ H:1 \\
\ \ \ \ \ \ \ \ \ gradient.output = act.output^{(L)}\\
\ \ \ \ \ \ \ \ \ net.input =\ \ \ \begin{cases}
X &amp;  \text{(if input layer)}\\
act.output^{(L-1)} &amp; \text{(otherwise)} 
\end{cases}\\
\ \ \ \ \ \ \ \ \  \frac{\partial \mathcal{L}}{\partial \omega} =\ \ \ \begin{cases}
(t - o) \times act.output^{(L)}  &amp; \text{(if output layer, where L=H)}\\
{\delta}^{(L+1)} \cdot transpose \left({\omega}^{(L+1)}\right) &amp; \text{(if hidden layer)}\\
\end{cases}\\
\ \ \ \ \ \ \ \ \ {\delta}^{(L)} =\ \ \frac{\partial \mathcal{L}}{\partial \omega} \times g(act.output^{(L)})\\
\ \ \ \ \ \ \ \ \ ({\nabla_{\omega}}\mathcal{L})^{(L)} = transpose(net.input) \cdot \delta^{(L)}\\
\ \ \ \text{end loop}\\
\mathbf{Output}: {\nabla_{\omega}} \mathcal{L}
\end{array}
\]</span>
</p>
<p>Note that <strong>Kronecker product</strong>, denoted by the symbol (<span class="math inline">\(\bigotimes\)</span>), also applies to <span class="math inline">\({\Delta _w}^{(L)}\)</span> where every element is multiplied to the matrix:</p>
<p><span class="math display">\[\begin{align}
\begin{array}{ll}
{\Delta _w}^{(L)}_{\mathbf{h \times o}} &amp;= transpose(net.input) \cdot \delta^{(L)}\\
&amp;= \sum_i^n 
\left\{reshape\left(net.input_{(i,)} \otimes {\delta}^{(L)}_{(i,)} 
\right)\right\} \in \mathbb{R}^{\mathbf{h \times o}}
\end{array} \label{eqn:eqnnumber608}
\end{align}\]</span></p>
<p>where <strong>âreshapeâ</strong> follows the shape of <span class="math inline">\(\omega\)</span>.</p>
<p>For example, using <strong>transpose</strong>, we have:</p>

<div class="sourceCode" id="cb1811"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1811-1" data-line-number="1">net.input =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1811-2" data-line-number="2">delta     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.1</span>,<span class="fl">0.2</span>,<span class="fl">0.3</span>,<span class="fl">0.4</span>,<span class="fl">0.5</span>,<span class="fl">0.6</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1811-3" data-line-number="3"><span class="kw">t</span>(net.input) <span class="op">%*%</span><span class="st"> </span>delta</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  3.5  4.4
## [2,]  4.4  5.6</code></pre>

<p>Using <strong>Kronecker product</strong>, we have:</p>

<div class="sourceCode" id="cb1813"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1813-1" data-line-number="1">reshape &lt;-<span class="st"> </span><span class="cf">function</span>(m, r, c) { <span class="kw">matrix</span>(m, <span class="dt">nrow=</span>r, <span class="dt">ncol=</span>c, <span class="dt">byrow=</span><span class="ot">TRUE</span>)}</a>
<a class="sourceLine" id="cb1813-2" data-line-number="2">s =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>), <span class="dt">nrow=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1813-3" data-line-number="3"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(net.input)) {</a>
<a class="sourceLine" id="cb1813-4" data-line-number="4">  s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span><span class="kw">reshape</span>( <span class="kw">kronecker</span>(net.input[i,], delta[i,]), <span class="dv">2</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1813-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1813-6" data-line-number="6">s</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  3.5  4.4
## [2,]  4.4  5.6</code></pre>

<p>For the <strong>Backward Pass</strong>, we have the following <strong>MLP</strong> pseudocode for the <strong>Gradient Descent</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{parameters}: \{\omega_{(1,1)}, \omega_{(1,2)}, ..., \omega_{(p,h)}: x\ \in\ \mathbb{R}^{pxh}\}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rightarrow \text{(p are inputs, h are  target outputs)}\\
\ \ \ \ \ \ \ \ \ \ \ \ \text{gradient}: \{\Delta \omega_{(1,1)}, \Delta \omega_{(1,2)}, ..., \Delta \omega_{(p,h)}: x\ \in\ \mathbb{R}^{pxh}\}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rightarrow \text{(p are inputs, h are target outputs)}\\
\ \ \ \text{learning rate}: \eta \\
\mathbf{Algorithm}:\\
\ \ \ \text{loop}\ L\ in\ 1:H \\
\ \ \ \ \ \ \ \ \ \omega^{(L)} = \omega^{(L)} - \eta {\nabla_{\omega}}\mathcal{L}^{(L)}\\
\ \ \ \text{end loop} \\
\mathbf{Output}:\\ 
\ \ \ \ \omega\ \ \ \ \ \  \text{(updated)}
\end{array}
\]</span></p>
<p>Now, let us review our example implementation of <strong>forward.pass(.)</strong>, <strong>backward.pass(.)</strong>, and <strong>back.propagation(.)</strong> functions for a <strong>vanilla neural network</strong>. As for the presence of the <strong>batchnorm</strong> functions and <strong>drop.out(.)</strong>, a detailed discussion is up ahead in later section.</p>


<p>For our <strong>Forward Feed</strong>, we use <strong>forward.pass(.)</strong> function implemented like so:</p>

<div class="sourceCode" id="cb1815"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1815-1" data-line-number="1">forward.pass &lt;-<span class="st"> </span><span class="cf">function</span>(X, layers, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>, <span class="dt">mode=</span><span class="st">&quot;train&quot;</span>) {</a>
<a class="sourceLine" id="cb1815-2" data-line-number="2">  afunc                  =<span class="st"> </span><span class="kw">get</span>(afunc)</a>
<a class="sourceLine" id="cb1815-3" data-line-number="3">  H                      =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb1815-4" data-line-number="4">  act.output             =<span class="st"> </span>X</a>
<a class="sourceLine" id="cb1815-5" data-line-number="5">  <span class="cf">for</span> (L <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>H) {</a>
<a class="sourceLine" id="cb1815-6" data-line-number="6">    <span class="co"># add constant (bias)</span></a>
<a class="sourceLine" id="cb1815-7" data-line-number="7">    layer =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb1815-8" data-line-number="8">    net.input            =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(X)), act.output)  </a>
<a class="sourceLine" id="cb1815-9" data-line-number="9">    net.input            =<span class="st"> </span>net.input <span class="op">%*%</span><span class="st"> </span>layer<span class="op">$</span>omega<span class="op">$</span>weight </a>
<a class="sourceLine" id="cb1815-10" data-line-number="10">    <span class="cf">if</span> (L <span class="op">==</span><span class="st"> </span>H) {          <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb1815-11" data-line-number="11">        act.output       =<span class="st"> </span><span class="kw">activation</span>(net.input, afunc)</a>
<a class="sourceLine" id="cb1815-12" data-line-number="12">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1815-13" data-line-number="13">        act.output =<span class="st"> </span>net.input</a>
<a class="sourceLine" id="cb1815-14" data-line-number="14">        <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(layer<span class="op">$</span>drop)) {</a>
<a class="sourceLine" id="cb1815-15" data-line-number="15">            act.output =<span class="st"> </span><span class="kw">drop.out</span>(act.output, <span class="dt">prob=</span>layer<span class="op">$</span>drop)</a>
<a class="sourceLine" id="cb1815-16" data-line-number="16">        }</a>
<a class="sourceLine" id="cb1815-17" data-line-number="17">        <span class="cf">if</span> (layer<span class="op">$</span>batchnorm <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1815-18" data-line-number="18">           <span class="cf">if</span> (mode <span class="op">==</span><span class="st"> &quot;train&quot;</span>) {</a>
<a class="sourceLine" id="cb1815-19" data-line-number="19">             normalized       =<span class="st"> </span><span class="kw">batchnorm.forward</span>(act.output,  layer)</a>
<a class="sourceLine" id="cb1815-20" data-line-number="20">             act.output       =<span class="st"> </span>normalized<span class="op">$</span>act.output</a>
<a class="sourceLine" id="cb1815-21" data-line-number="21">             layer<span class="op">$</span>moments    =<span class="st"> </span>normalized<span class="op">$</span>moments</a>
<a class="sourceLine" id="cb1815-22" data-line-number="22">           } <span class="cf">else</span> { <span class="co"># if test</span></a>
<a class="sourceLine" id="cb1815-23" data-line-number="23">             normalized       =<span class="st"> </span><span class="kw">batchnorm.prediction</span>(act.output,  layer)</a>
<a class="sourceLine" id="cb1815-24" data-line-number="24">             act.output       =<span class="st"> </span>normalized<span class="op">$</span>prediction</a>
<a class="sourceLine" id="cb1815-25" data-line-number="25">           }</a>
<a class="sourceLine" id="cb1815-26" data-line-number="26">        }</a>
<a class="sourceLine" id="cb1815-27" data-line-number="27">        act.output       =<span class="st"> </span><span class="kw">activation</span>(net.input, leaky.relu)</a>
<a class="sourceLine" id="cb1815-28" data-line-number="28">    } </a>
<a class="sourceLine" id="cb1815-29" data-line-number="29">    layer<span class="op">$</span>output =<span class="st"> </span>act.output</a>
<a class="sourceLine" id="cb1815-30" data-line-number="30">    layers[[L]]  =<span class="st"> </span>layer</a>
<a class="sourceLine" id="cb1815-31" data-line-number="31">  }</a>
<a class="sourceLine" id="cb1815-32" data-line-number="32">  <span class="kw">list</span>(<span class="st">&quot;layers&quot;</span> =<span class="st"> </span>layers)</a>
<a class="sourceLine" id="cb1815-33" data-line-number="33">}</a></code></pre></div>

<p>For our <strong>Back Propagation</strong>, we use <strong>back.propagation(.)</strong> function implemented like so:</p>

<div class="sourceCode" id="cb1816"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1816-1" data-line-number="1">back.propagation &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, model, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>) {</a>
<a class="sourceLine" id="cb1816-2" data-line-number="2">    afunc.loss                  =<span class="st"> </span><span class="kw">get</span>(<span class="kw">paste0</span>(<span class="st">&quot;gradient.&quot;</span>, afunc, <span class="st">&quot;.loss&quot;</span>))</a>
<a class="sourceLine" id="cb1816-3" data-line-number="3">    actfunc                     =<span class="st"> </span><span class="kw">get</span>(<span class="kw">paste0</span>(<span class="st">&quot;gradient.&quot;</span>, afunc))</a>
<a class="sourceLine" id="cb1816-4" data-line-number="4">    H                           =<span class="st"> </span><span class="kw">length</span>(model<span class="op">$</span>layers)  </a>
<a class="sourceLine" id="cb1816-5" data-line-number="5">    delta                       =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1816-6" data-line-number="6">    delta.params                =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1816-7" data-line-number="7">    layers                      =<span class="st"> </span>model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb1816-8" data-line-number="8">    <span class="cf">for</span> (L <span class="cf">in</span> H<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1816-9" data-line-number="9">        layer =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb1816-10" data-line-number="10">        delta.params[[L]] =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;omega&quot;</span>=<span class="st"> </span><span class="ot">NULL</span>, <span class="st">&quot;gamma&quot;</span>=<span class="st"> </span><span class="ot">NULL</span>, <span class="st">&quot;beta&quot;</span>=<span class="st"> </span><span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb1816-11" data-line-number="11">        act.output             =<span class="st"> </span>layer<span class="op">$</span>output</a>
<a class="sourceLine" id="cb1816-12" data-line-number="12">        <span class="cf">if</span> (L <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) { net.input =<span class="st"> </span>layers[[L<span class="dv">-1</span>]]<span class="op">$</span>output } <span class="cf">else</span> { net.input=X }</a>
<a class="sourceLine" id="cb1816-13" data-line-number="13">        <span class="co"># include constant for bias</span></a>
<a class="sourceLine" id="cb1816-14" data-line-number="14">        net.input               =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(X)), net.input)</a>
<a class="sourceLine" id="cb1816-15" data-line-number="15">        <span class="cf">if</span> (L <span class="op">==</span><span class="st"> </span>H) { <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb1816-16" data-line-number="16">            <span class="cf">if</span> (afunc <span class="op">==</span><span class="st"> &quot;sigmoid&quot;</span> <span class="op">||</span><span class="st"> </span>afunc <span class="op">==</span><span class="st"> &quot;softmax&quot;</span>) {  </a>
<a class="sourceLine" id="cb1816-17" data-line-number="17">               <span class="co"># the derivation of delta cancels out the activation gradient, </span></a>
<a class="sourceLine" id="cb1816-18" data-line-number="18">               <span class="co"># e.g. simplified version: delta = o - t</span></a>
<a class="sourceLine" id="cb1816-19" data-line-number="19">               delta[[L]]       =<span class="st"> </span><span class="kw">gradient.loss</span>(Y, act.output, </a>
<a class="sourceLine" id="cb1816-20" data-line-number="20">                                                gradient.relu.loss)</a>
<a class="sourceLine" id="cb1816-21" data-line-number="21">            } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1816-22" data-line-number="22">               gradient.loss    =<span class="st"> </span><span class="kw">gradient.loss</span>(Y, act.output, afunc.loss)</a>
<a class="sourceLine" id="cb1816-23" data-line-number="23">               gradient.output  =<span class="st"> </span><span class="kw">gradient.activation</span>(act.output, actfunc)</a>
<a class="sourceLine" id="cb1816-24" data-line-number="24">               delta[[L]]       =<span class="st"> </span>gradient.loss  <span class="op">*</span><span class="st"> </span>gradient.output  </a>
<a class="sourceLine" id="cb1816-25" data-line-number="25">            }</a>
<a class="sourceLine" id="cb1816-26" data-line-number="26">        } <span class="cf">else</span> {    </a>
<a class="sourceLine" id="cb1816-27" data-line-number="27">            net.weights         =<span class="st"> </span>layers[[L<span class="op">+</span><span class="dv">1</span>]]<span class="op">$</span>omega<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb1816-28" data-line-number="28">            gradient.loss       =<span class="st"> </span>delta[[L<span class="op">+</span><span class="dv">1</span>]] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(net.weights) </a>
<a class="sourceLine" id="cb1816-29" data-line-number="29">            gradient.loss       =<span class="st"> </span>gradient.loss[,<span class="op">-</span><span class="dv">1</span>] <span class="co"># exclude bias</span></a>
<a class="sourceLine" id="cb1816-30" data-line-number="30">            gradient.output     =<span class="st"> </span><span class="kw">gradient.activation</span>(act.output, </a>
<a class="sourceLine" id="cb1816-31" data-line-number="31">                                                      gradient.leaky.relu) </a>
<a class="sourceLine" id="cb1816-32" data-line-number="32">            bnorm               =<span class="st"> </span><span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb1816-33" data-line-number="33">            <span class="cf">if</span> (layers[[L]]<span class="op">$</span>batchnorm <span class="op">==</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1816-34" data-line-number="34">                normalized            =<span class="st"> </span><span class="kw">batchnorm.backward</span>( gradient.output, </a>
<a class="sourceLine" id="cb1816-35" data-line-number="35">                                          layer<span class="op">$</span>batch.gamma<span class="op">$</span>weight, </a>
<a class="sourceLine" id="cb1816-36" data-line-number="36">                                          layer<span class="op">$</span>moments)</a>
<a class="sourceLine" id="cb1816-37" data-line-number="37">                gradient.output         =<span class="st"> </span>normalized<span class="op">$</span>gradient.output</a>
<a class="sourceLine" id="cb1816-38" data-line-number="38">                delta.params[[L]]<span class="op">$</span>gamma =<span class="st"> </span>normalized<span class="op">$</span>delta.gamma</a>
<a class="sourceLine" id="cb1816-39" data-line-number="39">                delta.params[[L]]<span class="op">$</span>beta  =<span class="st"> </span>normalized<span class="op">$</span>delta.beta</a>
<a class="sourceLine" id="cb1816-40" data-line-number="40">            } </a>
<a class="sourceLine" id="cb1816-41" data-line-number="41"></a>
<a class="sourceLine" id="cb1816-42" data-line-number="42">            delta[[L]]           =<span class="st"> </span>gradient.loss  <span class="op">*</span><span class="st"> </span>gradient.output </a>
<a class="sourceLine" id="cb1816-43" data-line-number="43">        }</a>
<a class="sourceLine" id="cb1816-44" data-line-number="44">        delta.params[[L]]<span class="op">$</span>omega  =<span class="st"> </span><span class="kw">clipping</span>(<span class="kw">t</span>(net.input) <span class="op">%*%</span><span class="st"> </span>delta[[L]])</a>
<a class="sourceLine" id="cb1816-45" data-line-number="45">    }</a>
<a class="sourceLine" id="cb1816-46" data-line-number="46">    <span class="kw">list</span>( <span class="st">&quot;delta.output&quot;</span>  =<span class="st"> </span>delta, <span class="st">&quot;delta.params&quot;</span> =<span class="st"> </span>delta.params)</a>
<a class="sourceLine" id="cb1816-47" data-line-number="47">}</a></code></pre></div>

<p>For our <strong>updates</strong> of parameters as we propagate our gradients backward, we use <strong>backward.pass(.)</strong> function implemented like so:</p>

<div class="sourceCode" id="cb1817"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1817-1" data-line-number="1">backward.pass &lt;-<span class="st"> </span><span class="cf">function</span>(model, delta.params, eta, t, optimize) {</a>
<a class="sourceLine" id="cb1817-2" data-line-number="2">  eps =<span class="st"> </span><span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb1817-3" data-line-number="3">  layers =<span class="st"> </span>model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb1817-4" data-line-number="4">  H      =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb1817-5" data-line-number="5">  <span class="cf">for</span> (L <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>H) {</a>
<a class="sourceLine" id="cb1817-6" data-line-number="6">    layer =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb1817-7" data-line-number="7">    <span class="cf">if</span> (optimize <span class="op">==</span><span class="st"> &quot;sgd&quot;</span>) {</a>
<a class="sourceLine" id="cb1817-8" data-line-number="8">     layer<span class="op">$</span>omega<span class="op">$</span>weight =<span class="st"> </span>layer<span class="op">$</span>omega<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>delta.params[[L]]<span class="op">$</span>omega</a>
<a class="sourceLine" id="cb1817-9" data-line-number="9">    } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1817-10" data-line-number="10">    <span class="cf">if</span> (optimize <span class="op">==</span><span class="st"> &quot;adam&quot;</span>) {</a>
<a class="sourceLine" id="cb1817-11" data-line-number="11">      layer<span class="op">$</span>omega =<span class="st"> </span><span class="kw">adam</span>(layer<span class="op">$</span>omega, delta.params[[L]]<span class="op">$</span>omega, eta, t)</a>
<a class="sourceLine" id="cb1817-12" data-line-number="12">    }</a>
<a class="sourceLine" id="cb1817-13" data-line-number="13">    <span class="cf">if</span> (layer<span class="op">$</span>batchnorm <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1817-14" data-line-number="14">       <span class="cf">if</span> (optimize <span class="op">==</span><span class="st"> &quot;sgd&quot;</span>) {</a>
<a class="sourceLine" id="cb1817-15" data-line-number="15">         layer<span class="op">$</span>batch.gamma<span class="op">$</span>weight =<span class="st"> </span>layer<span class="op">$</span>batch.gamma<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1817-16" data-line-number="16"><span class="st">                                    </span>eta <span class="op">*</span><span class="st"> </span>delta.params[[L]]<span class="op">$</span>gamma</a>
<a class="sourceLine" id="cb1817-17" data-line-number="17">         layer<span class="op">$</span>batch.beta<span class="op">$</span>weight =<span class="st"> </span>layer<span class="op">$</span>batch.betya<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1817-18" data-line-number="18"><span class="st">                                    </span>eta <span class="op">*</span><span class="st"> </span>delta.params[[L]]<span class="op">$</span>beta</a>
<a class="sourceLine" id="cb1817-19" data-line-number="19">       } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1817-20" data-line-number="20">       <span class="cf">if</span> (optimize <span class="op">==</span><span class="st"> &quot;adam&quot;</span>) {</a>
<a class="sourceLine" id="cb1817-21" data-line-number="21">          layer<span class="op">$</span>batch.gamma =<span class="st"> </span><span class="kw">adam</span>(layer<span class="op">$</span>batch.gamma, </a>
<a class="sourceLine" id="cb1817-22" data-line-number="22">                                   delta.params[[L]]<span class="op">$</span>gamma, eta, t)</a>
<a class="sourceLine" id="cb1817-23" data-line-number="23">          layer<span class="op">$</span>batch.beta  =<span class="st"> </span><span class="kw">adam</span>(layer<span class="op">$</span>batch.beta, </a>
<a class="sourceLine" id="cb1817-24" data-line-number="24">                                   delta.params[[L]]<span class="op">$</span>beta, eta, t)</a>
<a class="sourceLine" id="cb1817-25" data-line-number="25">       }</a>
<a class="sourceLine" id="cb1817-26" data-line-number="26">       layer<span class="op">$</span>moving.mu       =<span class="st"> </span>layer<span class="op">$</span>moments<span class="op">$</span>moving.mu</a>
<a class="sourceLine" id="cb1817-27" data-line-number="27">       layer<span class="op">$</span>moving.variance =<span class="st"> </span>layer<span class="op">$</span>moments<span class="op">$</span>moving.variance</a>
<a class="sourceLine" id="cb1817-28" data-line-number="28">    }</a>
<a class="sourceLine" id="cb1817-29" data-line-number="29">    layers[[L]]<span class="op">$</span>omega           =<span class="st"> </span>layer<span class="op">$</span>omega</a>
<a class="sourceLine" id="cb1817-30" data-line-number="30">    layers[[L]]<span class="op">$</span>batch.gamma     =<span class="st"> </span>layer<span class="op">$</span>batch.gamma</a>
<a class="sourceLine" id="cb1817-31" data-line-number="31">    layers[[L]]<span class="op">$</span>batch.beta      =<span class="st"> </span>layer<span class="op">$</span>batch.beta</a>
<a class="sourceLine" id="cb1817-32" data-line-number="32">    layers[[L]]<span class="op">$</span>moving.mu       =<span class="st"> </span>layer<span class="op">$</span>moving.mu</a>
<a class="sourceLine" id="cb1817-33" data-line-number="33">    layers[[L]]<span class="op">$</span>moving.variance =<span class="st"> </span>layer<span class="op">$</span>moving.variance</a>
<a class="sourceLine" id="cb1817-34" data-line-number="34">  }</a>
<a class="sourceLine" id="cb1817-35" data-line-number="35">  layers</a>
<a class="sourceLine" id="cb1817-36" data-line-number="36">}</a></code></pre></div>

<p>Finally, we come down to the <strong>MLP</strong> implementation itself. Our implementation uses <strong>mini-batch SGD</strong> with <strong>sgd</strong> and <strong>adam</strong> optimization. Our dataset is split into mini batches using <strong>createFolds(.)</strong> function from <strong>caret</strong> library to generate mini batches.</p>

<div class="sourceCode" id="cb1818"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1818-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb1818-2" data-line-number="2">get.batch.dnn &lt;-<span class="st"> </span><span class="cf">function</span>(sample.data,  k, t) {  </a>
<a class="sourceLine" id="cb1818-3" data-line-number="3">   <span class="kw">set.seed</span>(t)</a>
<a class="sourceLine" id="cb1818-4" data-line-number="4">   n =<span class="st"> </span><span class="kw">nrow</span>(sample.data)</a>
<a class="sourceLine" id="cb1818-5" data-line-number="5">   shuffle =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dt">n =</span> n, <span class="dt">size=</span>n, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1818-6" data-line-number="6">   <span class="kw">createFolds</span>(shuffle, <span class="dt">k =</span> k, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1818-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb1818-8" data-line-number="8">my.MLP &lt;-<span class="st"> </span><span class="cf">function</span>(Xset, Yset, layers,  <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>, </a>
<a class="sourceLine" id="cb1818-9" data-line-number="9">                   <span class="dt">console=</span><span class="ot">FALSE</span>, <span class="dt">optimize=</span><span class="st">&quot;sgd&quot;</span>, <span class="dt">minibatch =</span> <span class="dv">30</span>, </a>
<a class="sourceLine" id="cb1818-10" data-line-number="10">                   <span class="dt">eta =</span> <span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">100</span>, <span class="dt">tol=</span><span class="fl">1e-10</span> ) {</a>
<a class="sourceLine" id="cb1818-11" data-line-number="11">  <span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">8</span>)  <span class="co"># 8 digits precision for our example</span></a>
<a class="sourceLine" id="cb1818-12" data-line-number="12">  eta =<span class="st"> </span><span class="kw">c</span>(eta)</a>
<a class="sourceLine" id="cb1818-13" data-line-number="13">  total.loss =<span class="st"> </span>old.delta.params =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1818-14" data-line-number="14">  old.loss =<span class="st"> </span><span class="ot">Inf</span></a>
<a class="sourceLine" id="cb1818-15" data-line-number="15">  k   =<span class="st"> </span><span class="kw">ceiling</span>(<span class="kw">nrow</span>(Xset) <span class="op">/</span><span class="st"> </span>minibatch)</a>
<a class="sourceLine" id="cb1818-16" data-line-number="16">  <span class="cf">if</span> (console<span class="op">==</span><span class="ot">TRUE</span>) { <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Batch Count:&quot;</span>, k)) }</a>
<a class="sourceLine" id="cb1818-17" data-line-number="17">  n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1818-18" data-line-number="18">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) { <span class="co"># one dataset pass per epoch</span></a>
<a class="sourceLine" id="cb1818-19" data-line-number="19">    n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1818-20" data-line-number="20">    batch.loss =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1818-21" data-line-number="21">    <span class="cf">for</span> (batch <span class="cf">in</span> <span class="kw">get.batch.dnn</span>(Xset, k, t)) {</a>
<a class="sourceLine" id="cb1818-22" data-line-number="22">       X        =<span class="st"> </span>Xset[batch,]</a>
<a class="sourceLine" id="cb1818-23" data-line-number="23">       Y        =<span class="st"> </span>Yset[batch,]</a>
<a class="sourceLine" id="cb1818-24" data-line-number="24">       model    =<span class="st"> </span><span class="kw">forward.pass</span>(X, layers, afunc)</a>
<a class="sourceLine" id="cb1818-25" data-line-number="25">       backprop =<span class="st"> </span><span class="kw">back.propagation</span>(X, Y, model,  afunc )</a>
<a class="sourceLine" id="cb1818-26" data-line-number="26">       layers   =<span class="st"> </span><span class="kw">backward.pass</span>(model,  backprop<span class="op">$</span>delta.params, eta, </a>
<a class="sourceLine" id="cb1818-27" data-line-number="27">                                (t <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>k <span class="op">+</span><span class="st"> </span>n,  optimize)</a>
<a class="sourceLine" id="cb1818-28" data-line-number="28">       loss     =<span class="st"> </span><span class="kw">get.loss</span>(Y, layers, afunc)</a>
<a class="sourceLine" id="cb1818-29" data-line-number="29">       batch.loss =<span class="st"> </span><span class="kw">c</span>(batch.loss, loss)</a>
<a class="sourceLine" id="cb1818-30" data-line-number="30">    }</a>
<a class="sourceLine" id="cb1818-31" data-line-number="31">    total.loss =<span class="st"> </span><span class="kw">c</span>(total.loss, <span class="kw">mean</span>(batch.loss))</a>
<a class="sourceLine" id="cb1818-32" data-line-number="32">    <span class="cf">if</span> (<span class="kw">is.na</span>(loss)) { <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;NaN loss encountered at &quot;</span>, t)); <span class="cf">break</span> }</a>
<a class="sourceLine" id="cb1818-33" data-line-number="33">    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.finite</span>(loss)) {</a>
<a class="sourceLine" id="cb1818-34" data-line-number="34">         <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Infinite Loss:&quot;</span>, loss)); <span class="cf">break</span>}</a>
<a class="sourceLine" id="cb1818-35" data-line-number="35">    <span class="cf">if</span> (<span class="kw">abs</span>(old.loss <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(batch.loss)) <span class="op">&lt;=</span><span class="st"> </span>tol) {</a>
<a class="sourceLine" id="cb1818-36" data-line-number="36">         <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Tolerance level reached:&quot;</span>, loss, <span class="st">&quot; at &quot;</span>, t)); <span class="cf">break</span> }</a>
<a class="sourceLine" id="cb1818-37" data-line-number="37">    old.loss         =<span class="st">  </span><span class="kw">mean</span>(batch.loss)</a>
<a class="sourceLine" id="cb1818-38" data-line-number="38">    old.delta.params =<span class="st"> </span>backprop<span class="op">$</span>delta.params</a>
<a class="sourceLine" id="cb1818-39" data-line-number="39">    <span class="cf">if</span> (n <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span> <span class="op">&amp;&amp;</span><span class="st"> </span>console<span class="op">==</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1818-40" data-line-number="40">      <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;epoch:&quot;</span>, t, <span class="st">&quot; loss:&quot;</span>, <span class="kw">mean</span>(batch.loss)))</a>
<a class="sourceLine" id="cb1818-41" data-line-number="41">      <span class="kw">flush.console</span>()</a>
<a class="sourceLine" id="cb1818-42" data-line-number="42">    }</a>
<a class="sourceLine" id="cb1818-43" data-line-number="43">  }</a>
<a class="sourceLine" id="cb1818-44" data-line-number="44">  L =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb1818-45" data-line-number="45">  model =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;layers&quot;</span>       =<span class="st"> </span>model<span class="op">$</span>layers,     <span class="st">&quot;cost&quot;</span>  =<span class="st"> </span>total.loss, </a>
<a class="sourceLine" id="cb1818-46" data-line-number="46">               <span class="st">&quot;delta.params&quot;</span> =<span class="st"> </span>old.delta.params, <span class="st">&quot;afunc&quot;</span> =<span class="st"> </span>afunc,</a>
<a class="sourceLine" id="cb1818-47" data-line-number="47">               <span class="st">&quot;last.iteration&quot;</span>  =<span class="st"> </span>t)</a>
<a class="sourceLine" id="cb1818-48" data-line-number="48">}</a></code></pre></div>

<p>Applying the implementation, we start with a simple dataset. Our dataset assumes having four samples with two features (X) and two targets (Y).</p>

<div class="sourceCode" id="cb1819"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1819-1" data-line-number="1">X =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>(<span class="fl">0.12</span>, <span class="fl">0.18</span>, <span class="fl">0.13</span>, <span class="fl">0.21</span>, <span class="fl">0.15</span>, <span class="fl">0.30</span>, <span class="fl">0.18</span>, <span class="fl">0.40</span>), </a>
<a class="sourceLine" id="cb1819-2" data-line-number="2">            <span class="dt">nrow=</span><span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1819-3" data-line-number="3">Y =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="fl">0.02</span>, <span class="fl">0.98</span>, <span class="fl">0.03</span>, <span class="fl">0.97</span>, <span class="fl">0.07</span>, <span class="fl">0.92</span>), </a>
<a class="sourceLine" id="cb1819-4" data-line-number="4">            <span class="dt">nrow=</span><span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a></code></pre></div>

<p>To be able to feed our data to <strong>MLP</strong>, we also construct a simple parameter structure using a learnable set of coefficients.</p>

<div class="sourceCode" id="cb1820"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1820-1" data-line-number="1">omega =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1820-2" data-line-number="2">omega[[<span class="dv">1</span>]]=<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.40</span>, <span class="fl">0.21</span>, <span class="fl">0.34</span>, <span class="fl">0.19</span>, <span class="fl">0.67</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1820-3" data-line-number="3">omega[[<span class="dv">2</span>]]=<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.18</span>, <span class="fl">0.27</span>, <span class="fl">0.09</span>, <span class="fl">0.06</span>, <span class="fl">0.30</span>, <span class="fl">0.15</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1820-4" data-line-number="4">omega[[<span class="dv">3</span>]]=<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.05</span>, <span class="fl">0.03</span>, <span class="fl">0.35</span>, <span class="fl">0.04</span>, <span class="fl">0.40</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<div class="sourceCode" id="cb1821"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1821-1" data-line-number="1">structure =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1821-2" data-line-number="2">di =<span class="st"> </span><span class="kw">dim</span>(omega[[<span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb1821-3" data-line-number="3"><span class="cf">for</span> (L <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>) { </a>
<a class="sourceLine" id="cb1821-4" data-line-number="4">        params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span>omega[[L]], <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, di), </a>
<a class="sourceLine" id="cb1821-5" data-line-number="5">                      <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, di) )</a>
<a class="sourceLine" id="cb1821-6" data-line-number="6">        structure[[L]] =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;size&quot;</span>  =<span class="st"> </span><span class="dv">2</span>, <span class="st">&quot;omega&quot;</span> =<span class="st"> </span>params, </a>
<a class="sourceLine" id="cb1821-7" data-line-number="7">                              <span class="st">&quot;batchnorm&quot;</span> =<span class="st"> </span><span class="ot">FALSE</span>  )</a>
<a class="sourceLine" id="cb1821-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb1821-9" data-line-number="9"><span class="kw">str</span>(structure)</a></code></pre></div>
<pre><code>## List of 3
##  $ :List of 3
##   ..$ size     : num 2
##   ..$ omega    :List of 3
##   .. ..$ weight: num [1:3, 1:2] 0.05 0.21 0.19 0.4 0.34 0.67
##   .. ..$ rho   : num [1:3, 1:2] 0 0 0 0 0 0
##   .. ..$ nu    : num [1:3, 1:2] 0 0 0 0 0 0
##   ..$ batchnorm: logi FALSE
##  $ :List of 3
##   ..$ size     : num 2
##   ..$ omega    :List of 3
##   .. ..$ weight: num [1:3, 1:2] 0.18 0.09 0.3 0.27 0.06 0.15
##   .. ..$ rho   : num [1:3, 1:2] 0 0 0 0 0 0
##   .. ..$ nu    : num [1:3, 1:2] 0 0 0 0 0 0
##   ..$ batchnorm: logi FALSE
##  $ :List of 3
##   ..$ size     : num 2
##   ..$ omega    :List of 3
##   .. ..$ weight: num [1:3, 1:2] 0.25 0.03 0.04 0.05 0.35 0.4
##   .. ..$ rho   : num [1:3, 1:2] 0 0 0 0 0 0
##   .. ..$ nu    : num [1:3, 1:2] 0 0 0 0 0 0
##   ..$ batchnorm: logi FALSE</code></pre>

<p>Let us now model an <strong>MLP</strong> using the data points and parameters above:</p>

<div class="sourceCode" id="cb1823"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1823-1" data-line-number="1">mlp.model.sigmoid =<span class="st"> </span><span class="kw">my.MLP</span>(X, Y, structure, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>, </a>
<a class="sourceLine" id="cb1823-2" data-line-number="2">                           <span class="dt">eta=</span><span class="fl">0.1</span>, <span class="dt">epoch=</span><span class="dv">100</span>, <span class="dt">console=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] &quot;Batch Count:1&quot;
## [1] &quot;epoch:10 loss:0.192948734912118&quot;
## [1] &quot;epoch:20 loss:0.181075498552071&quot;
## [1] &quot;epoch:30 loss:0.180865970971357&quot;
## [1] &quot;epoch:40 loss:0.180825075208516&quot;
## [1] &quot;epoch:50 loss:0.180799577608614&quot;
## [1] &quot;epoch:60 loss:0.180776485995595&quot;
## [1] &quot;epoch:70 loss:0.180754172643832&quot;
## [1] &quot;epoch:80 loss:0.180732413713138&quot;
## [1] &quot;epoch:90 loss:0.180711164234054&quot;
## [1] &quot;epoch:100 loss:0.180690402798621&quot;</code></pre>

<p>The final optimized parameters are obtained below after 100 <strong>epoch</strong>:</p>

<div class="sourceCode" id="cb1825"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1825-1" data-line-number="1">mlp.model =<span class="st"> </span>mlp.model.sigmoid</a>
<a class="sourceLine" id="cb1825-2" data-line-number="2">mlp.model<span class="op">$</span>layers[[<span class="dv">1</span>]]<span class="op">$</span>omega<span class="op">$</span>weight</a></code></pre></div>
<pre><code>##            [,1]       [,2]
## [1,] 0.18386740 0.87451275
## [2,] 0.22229200 0.38155529
## [3,] 0.20015635 0.69854820</code></pre>
<div class="sourceCode" id="cb1827"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1827-1" data-line-number="1">mlp.model<span class="op">$</span>layers[[<span class="dv">2</span>]]<span class="op">$</span>omega<span class="op">$</span>weight</a></code></pre></div>
<pre><code>##            [,1]       [,2]
## [1,] 0.74517894 0.85117204
## [2,] 0.17553396 0.14730975
## [3,] 0.69936942 0.55839558</code></pre>
<div class="sourceCode" id="cb1829"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1829-1" data-line-number="1">mlp.model<span class="op">$</span>layers[[<span class="dv">3</span>]]<span class="op">$</span>omega<span class="op">$</span>weight</a></code></pre></div>
<pre><code>##             [,1]       [,2]
## [1,] -0.84064501 0.69507475
## [2,] -0.74243477 0.73518925
## [3,] -0.72260868 0.78560822</code></pre>

<p>Compare that to the original values:</p>

<div class="sourceCode" id="cb1831"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1831-1" data-line-number="1">omega</a></code></pre></div>
<pre><code>## [[1]]
##      [,1] [,2]
## [1,] 0.05 0.40
## [2,] 0.21 0.34
## [3,] 0.19 0.67
## 
## [[2]]
##      [,1] [,2]
## [1,] 0.18 0.27
## [2,] 0.09 0.06
## [3,] 0.30 0.15
## 
## [[3]]
##      [,1] [,2]
## [1,] 0.25 0.05
## [2,] 0.03 0.35
## [3,] 0.04 0.40</code></pre>

<p>Note that the <strong>COST</strong> decreases as the parameters are optimized for each iteration.</p>

<div class="sourceCode" id="cb1833"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1833-1" data-line-number="1"><span class="kw">head</span>(mlp.model<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">15</span>)</a></code></pre></div>
<pre><code>##  [1] 0.69242946 0.58055032 0.48232118 0.39731471 0.32746102 0.27462355
##  [7] 0.23823521 0.21510574 0.20115424 0.19294873 0.18815312 0.18533726
## [13] 0.18366618 0.18266038 0.18204485</code></pre>

<p>Let us plot the <strong>COST</strong> (see Figure <a href="deeplearning1.html#fig:mlpplot">12.15</a>).</p>

<div class="sourceCode" id="cb1835"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1835-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.sigmoid<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1835-2" data-line-number="2">y =<span class="st"> </span>mlp.model.sigmoid<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1835-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1835-4" data-line-number="4">      <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;MLP Plot&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1835-5" data-line-number="5"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1835-6" data-line-number="6">color =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb1835-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col=</span>color[<span class="dv">1</span>], <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpplot"></span>
<img src="DS_files/figure-html/mlpplot-1.png" alt="MLP Plot" width="70%" />
<p class="caption">
Figure 12.15: MLP Plot
</p>
</div>

<p>In terms of prediction, we implement <strong>prediction(.)</strong> function to use the <strong>forward.pass(.)</strong> function like so:</p>

<div class="sourceCode" id="cb1836"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1836-1" data-line-number="1">my.predict &lt;-<span class="st"> </span><span class="cf">function</span>(X, model) {</a>
<a class="sourceLine" id="cb1836-2" data-line-number="2">  <span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">8</span>)  <span class="co"># 8 digits precision for our example</span></a>
<a class="sourceLine" id="cb1836-3" data-line-number="3">   response =<span class="st"> </span><span class="kw">forward.pass</span>(X, model<span class="op">$</span>layers, <span class="dt">afunc=</span>model<span class="op">$</span>afunc, <span class="dt">mode=</span><span class="st">&quot;test&quot;</span>)</a>
<a class="sourceLine" id="cb1836-4" data-line-number="4">   layers =<span class="st"> </span>response<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb1836-5" data-line-number="5">   L =<span class="st"> </span><span class="kw">length</span>(layers) </a>
<a class="sourceLine" id="cb1836-6" data-line-number="6">   predicted =<span class="st"> </span>layers[[L]]<span class="op">$</span>output</a>
<a class="sourceLine" id="cb1836-7" data-line-number="7">   <span class="kw">list</span>(<span class="st">&quot;prediction&quot;</span> =<span class="st"> </span>predicted)</a>
<a class="sourceLine" id="cb1836-8" data-line-number="8">}</a></code></pre></div>

<p>Now, for prediction, let us use a new set of data. Here, our <strong>MLP</strong> implementation requires our samples to be in matrix form:</p>

<div class="sourceCode" id="cb1837"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1837-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2019</span>)</a>
<a class="sourceLine" id="cb1837-2" data-line-number="2">new.X =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>), <span class="dt">nrow=</span><span class="dv">5</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1837-3" data-line-number="3"><span class="kw">my.predict</span>(new.X, mlp.model.sigmoid)</a></code></pre></div>
<pre><code>## $prediction
##             [,1]       [,2]
## [1,] 0.024982419 0.97394373
## [2,] 0.031984152 0.96638168
## [3,] 0.051672541 0.94489747
## [4,] 0.039240281 0.95849620
## [5,] 0.034823196 0.96330124</code></pre>

<p>Comparing the prediction with the target from the training set (to get an idea), we see a close match, though we may tend towards overfitting the closer we get.</p>

<div class="sourceCode" id="cb1839"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1839-1" data-line-number="1">Y</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,] 0.05 0.95
## [2,] 0.02 0.98
## [3,] 0.03 0.97
## [4,] 0.07 0.92</code></pre>

</div>
<div id="deep-neural-network-dnn" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.7</span> Deep Neural Network (DNN)  <a href="deeplearning1.html#deep-neural-network-dnn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, we have shown a simple <strong>neural network</strong>. In this section, let us improve the use of a network structure using our own <strong>deep.neural.layers(.)</strong> function. Note here that we are merely creating a structure of our network using a sequential list of a randomly generated matrix of weights (with default seed equals 2019).</p>

<div class="sourceCode" id="cb1841"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1841-1" data-line-number="1">deep.neural.layers  &lt;-<span class="st"> </span><span class="cf">function</span>(X, ...) {</a>
<a class="sourceLine" id="cb1841-2" data-line-number="2">    <span class="kw">set.seed</span>(<span class="dv">2019</span>)</a>
<a class="sourceLine" id="cb1841-3" data-line-number="3">    K          =<span class="st"> </span><span class="kw">ncol</span>(X)</a>
<a class="sourceLine" id="cb1841-4" data-line-number="4">    layers      =<span class="st"> </span><span class="kw">list</span>(...)</a>
<a class="sourceLine" id="cb1841-5" data-line-number="5">    parameters =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1841-6" data-line-number="6">    structure   =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1841-7" data-line-number="7">    l =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1841-8" data-line-number="8">    <span class="cf">for</span> (layer <span class="cf">in</span> layers) {</a>
<a class="sourceLine" id="cb1841-9" data-line-number="9">        l =<span class="st"> </span>l <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1841-10" data-line-number="10">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>batchnorm))  { layer<span class="op">$</span>batchnorm =<span class="st"> </span><span class="ot">FALSE</span> } </a>
<a class="sourceLine" id="cb1841-11" data-line-number="11">                                  <span class="cf">else</span> { layer<span class="op">$</span>batchnorm =<span class="st"> </span><span class="ot">TRUE</span>  }</a>
<a class="sourceLine" id="cb1841-12" data-line-number="12">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>drop)) { layer<span class="op">$</span>drop =<span class="st"> </span><span class="ot">NULL</span> }</a>
<a class="sourceLine" id="cb1841-13" data-line-number="13">        K =<span class="st"> </span>K <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="co"># include weights for the bias</span></a>
<a class="sourceLine" id="cb1841-14" data-line-number="14">        N =<span class="st"> </span>layer<span class="op">$</span>size</a>
<a class="sourceLine" id="cb1841-15" data-line-number="15">        O =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(K <span class="op">*</span><span class="st"> </span>N, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>), <span class="dt">nrow=</span>K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1841-16" data-line-number="16">        omega =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span>O,  <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">dim</span>(O)), </a>
<a class="sourceLine" id="cb1841-17" data-line-number="17">                     <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">dim</span>(O)) )</a>
<a class="sourceLine" id="cb1841-18" data-line-number="18">        batch.beta  =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N), <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N), </a>
<a class="sourceLine" id="cb1841-19" data-line-number="19">                           <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N) )</a>
<a class="sourceLine" id="cb1841-20" data-line-number="20">        batch.gamma =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, N), <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N), </a>
<a class="sourceLine" id="cb1841-21" data-line-number="21">                           <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N) )</a>
<a class="sourceLine" id="cb1841-22" data-line-number="22">        structure[[l]] =</a>
<a class="sourceLine" id="cb1841-23" data-line-number="23"><span class="st">           </span><span class="kw">list</span>(<span class="st">&quot;size&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>size, <span class="st">&quot;batchnorm&quot;</span> =<span class="st"> </span>layer<span class="op">$</span>batchnorm, </a>
<a class="sourceLine" id="cb1841-24" data-line-number="24">                <span class="st">&quot;drop&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>drop,     <span class="st">&quot;omega&quot;</span> =<span class="st"> </span>omega, </a>
<a class="sourceLine" id="cb1841-25" data-line-number="25">                <span class="st">&quot;batch.gamma&quot;</span> =<span class="st"> </span>batch.gamma,  <span class="st">&quot;batch.beta&quot;</span> =<span class="st"> </span>batch.beta,</a>
<a class="sourceLine" id="cb1841-26" data-line-number="26">                <span class="st">&quot;moving.variance&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, N), <span class="st">&quot;moving.mu&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb1841-27" data-line-number="27">                )</a>
<a class="sourceLine" id="cb1841-28" data-line-number="28">        parameters =<span class="st"> </span>parameters <span class="op">+</span><span class="st"> </span>K <span class="op">*</span><span class="st"> </span>N</a>
<a class="sourceLine" id="cb1841-29" data-line-number="29">        K =<span class="st"> </span>N</a>
<a class="sourceLine" id="cb1841-30" data-line-number="30">    }</a>
<a class="sourceLine" id="cb1841-31" data-line-number="31">    <span class="kw">list</span>(<span class="st">&quot;layers&quot;</span> =<span class="st"> </span>structure, <span class="st">&quot;parameters&quot;</span> =<span class="st"> </span>parameters)</a>
<a class="sourceLine" id="cb1841-32" data-line-number="32">}</a></code></pre></div>

<p>To use the function, consider a simple <strong>classification problem</strong>. Our recent implementation mainly covers the neural network with the <strong>sigmoid</strong> activation function for the output layer. Knowing that a <strong>sigmoid</strong> function squashes the output into a range between 0 and 1 can serve as a probability function for <strong>binomial classification</strong>. However, to generalize, we can use the <strong>softmax</strong> function for a <strong>multinomial classification</strong>.</p>
<p>In this section, let us demonstrate <strong>softmax</strong> for <strong>multinomial classification</strong> using a simple deep neural network. Our application requires us to convert our dataset labels to a set of one-hot encoded vectors. To help us generate a dataset with a one-hot encoded vector for our target, let us create a function called <strong>get.synthetic.samples(.)</strong> that can generate some random batch of the dataset with noise:</p>

<div class="sourceCode" id="cb1842"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1842-1" data-line-number="1">get.synthetic.samples &lt;-<span class="st"> </span><span class="cf">function</span>(N, <span class="dt">seed=</span><span class="dv">2019</span>) {</a>
<a class="sourceLine" id="cb1842-2" data-line-number="2"> <span class="kw">set.seed</span>(seed)</a>
<a class="sourceLine" id="cb1842-3" data-line-number="3">  Xset =<span class="st"> </span>Yset =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1842-4" data-line-number="4">  Xset[[<span class="dv">1</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.10</span>); Yset[[<span class="dv">1</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>) </a>
<a class="sourceLine" id="cb1842-5" data-line-number="5">  Xset[[<span class="dv">2</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.90</span>, <span class="fl">0.90</span>); Yset[[<span class="dv">2</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>) </a>
<a class="sourceLine" id="cb1842-6" data-line-number="6">  Xset[[<span class="dv">3</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.90</span>); Yset[[<span class="dv">3</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>) </a>
<a class="sourceLine" id="cb1842-7" data-line-number="7">  Xset[[<span class="dv">4</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.90</span>, <span class="fl">0.10</span>); Yset[[<span class="dv">4</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1842-8" data-line-number="8">  samples =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dt">n =</span> <span class="dv">4</span>, <span class="dt">size=</span>N, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1842-9" data-line-number="9">  X =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>); <span class="kw">colnames</span>(X) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>)</a>
<a class="sourceLine" id="cb1842-10" data-line-number="10">  Y =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1842-11" data-line-number="11">  <span class="kw">colnames</span>(Y) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Y1&quot;</span>, <span class="st">&quot;Y2&quot;</span>, <span class="st">&quot;Y3&quot;</span>, <span class="st">&quot;Y4&quot;</span>)</a>
<a class="sourceLine" id="cb1842-12" data-line-number="12">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) { </a>
<a class="sourceLine" id="cb1842-13" data-line-number="13">      X[i,] =<span class="st"> </span>Xset[[samples[[i]]]] </a>
<a class="sourceLine" id="cb1842-14" data-line-number="14">      Y[i,]  =<span class="st"> </span>Yset[[samples[[i]]]]</a>
<a class="sourceLine" id="cb1842-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb1842-16" data-line-number="16">  <span class="co"># Add some decent amount of noise</span></a>
<a class="sourceLine" id="cb1842-17" data-line-number="17">  X =<span class="st"> </span>X <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>N, <span class="dt">min=</span><span class="fl">0.001</span>, <span class="dt">max=</span><span class="fl">0.005</span>)</a>
<a class="sourceLine" id="cb1842-18" data-line-number="18">  <span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y)</a>
<a class="sourceLine" id="cb1842-19" data-line-number="19">}</a></code></pre></div>

<p>Barring <strong>interpretability</strong> of data at the moment while emphasizing the <strong>technicality</strong>, we concoct a <strong>synthetic</strong> dataset that can support <strong>Softmax</strong>. The idea is to have the ability to simulate unique and clear input patterns that can be mapped to unique output patterns and see how our model can correctly fit.</p>
<p>Let us split our dataset into train set and test set. Notice the pattern that we explicitly forced into the dataset.</p>

<div class="sourceCode" id="cb1843"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1843-1" data-line-number="1">train =<span class="st"> </span><span class="kw">get.synthetic.samples</span>(<span class="dt">N=</span><span class="dv">200</span>)</a>
<a class="sourceLine" id="cb1843-2" data-line-number="2">test  =<span class="st"> </span><span class="kw">get.synthetic.samples</span>(<span class="dt">N=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb1843-3" data-line-number="3">X =<span class="st"> </span>train<span class="op">$</span>X</a>
<a class="sourceLine" id="cb1843-4" data-line-number="4"><span class="kw">head</span>(<span class="kw">cbind</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y), <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##               X1         X2 Y1 Y2 Y3 Y4
##  [1,] 0.90179617 0.10179617  0  0  0  1
##  [2,] 0.10355535 0.90355535  0  0  1  0
##  [3,] 0.90421814 0.90421814  0  1  0  0
##  [4,] 0.10473834 0.90473834  0  0  1  0
##  [5,] 0.10198845 0.10198845  1  0  0  0
##  [6,] 0.10436652 0.10436652  1  0  0  0
##  [7,] 0.90298293 0.10298293  0  0  0  1
##  [8,] 0.10399414 0.10399414  1  0  0  0
##  [9,] 0.10132054 0.10132054  1  0  0  0
## [10,] 0.10137307 0.90137307  0  0  1  0</code></pre>

<p>Next, let us generate our initial neural network structure. Below, we have three layers. Two of the first hidden layers have two neurons, and the last output layer has four neural outputs.</p>

<div class="sourceCode" id="cb1845"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1845-1" data-line-number="1">dnn    =<span class="st"> </span><span class="kw">deep.neural.layers</span>(X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">4</span>))</a></code></pre></div>

<p>Using our trainset and parameters above, we train our <strong>DNN</strong> model for a <strong>Multi Classification</strong>.</p>

<div class="sourceCode" id="cb1846"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1846-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers,  <span class="dt">afunc=</span><span class="st">&quot;softmax&quot;</span>,  </a>
<a class="sourceLine" id="cb1846-2" data-line-number="2">                       <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>, <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">800</span>)</a></code></pre></div>

<p>Let us compare the prediction and the target using <strong>compare.outcome(.)</strong>:</p>

<div class="sourceCode" id="cb1847"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1847-1" data-line-number="1">compare.outcome &lt;-<span class="st"> </span><span class="cf">function</span>(t, o, <span class="dt">n=</span><span class="dv">10</span>) {</a>
<a class="sourceLine" id="cb1847-2" data-line-number="2">  c =<span class="st"> </span><span class="kw">ncol</span>(t)</a>
<a class="sourceLine" id="cb1847-3" data-line-number="3">  outc =<span class="st"> </span><span class="kw">cbind</span>(t, <span class="kw">round</span>(o,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1847-4" data-line-number="4">  <span class="kw">colnames</span>(outc) =<span class="st"> </span><span class="kw">c</span>(<span class="kw">paste0</span>(<span class="st">&quot;T&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,c)), <span class="kw">paste0</span>(<span class="st">&quot;O&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,c)))</a>
<a class="sourceLine" id="cb1847-5" data-line-number="5">  <span class="kw">print</span>(<span class="kw">head</span>(outc, <span class="dt">n=</span>n))</a>
<a class="sourceLine" id="cb1847-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb1847-7" data-line-number="7">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1847-8" data-line-number="8"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 row</span></a></code></pre></div>
<pre><code>##       T1 T2 T3 T4   O1   O2   O3   O4
##  [1,]  0  0  0  1 0.00 0.00 0.00 1.00
##  [2,]  0  0  1  0 0.00 0.00 1.00 0.00
##  [3,]  0  1  0  0 0.00 1.00 0.00 0.00
##  [4,]  0  0  1  0 0.00 0.00 1.00 0.00
##  [5,]  1  0  0  0 0.97 0.01 0.01 0.01
##  [6,]  1  0  0  0 0.97 0.01 0.01 0.01
##  [7,]  0  0  0  1 0.00 0.00 0.00 1.00
##  [8,]  1  0  0  0 0.97 0.01 0.01 0.01
##  [9,]  1  0  0  0 0.97 0.01 0.01 0.01
## [10,]  0  0  1  0 0.00 0.00 1.00 0.00</code></pre>

<p>We can see that the trained model matches the target by reviewing the result. We can perform prediction and measure performance using <strong>ROC</strong> and other related metrics relevant to <strong>precision</strong> and <strong>recall</strong>.</p>
</div>
<div id="vanishing-and-exploding-gradient" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.8</span> Vanishing and Exploding Gradient  <a href="deeplearning1.html#vanishing-and-exploding-gradient" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, let us simulate conditions in which gradients tend to have extremely negative or positive values. Here, we edit <strong>get.synthetic.samples(.)</strong> function to generate dataset intended for <strong>DNN</strong> with <strong>RELU</strong> output.</p>

<div class="sourceCode" id="cb1849"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1849-1" data-line-number="1">get.synthetic.samples &lt;-<span class="st"> </span><span class="cf">function</span>(N, <span class="dt">seed=</span><span class="dv">2019</span>) {</a>
<a class="sourceLine" id="cb1849-2" data-line-number="2"> <span class="kw">set.seed</span>(seed)</a>
<a class="sourceLine" id="cb1849-3" data-line-number="3">  Xset =<span class="st"> </span>Yset =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1849-4" data-line-number="4">  Xset[[<span class="dv">1</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.90</span>);  Yset[[<span class="dv">1</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.90</span>, <span class="fl">0.10</span>) </a>
<a class="sourceLine" id="cb1849-5" data-line-number="5">  Xset[[<span class="dv">2</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.20</span>, <span class="fl">0.80</span>);  Yset[[<span class="dv">2</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.80</span>, <span class="fl">0.20</span>) </a>
<a class="sourceLine" id="cb1849-6" data-line-number="6">  Xset[[<span class="dv">3</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.30</span>, <span class="fl">0.70</span>);  Yset[[<span class="dv">3</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.70</span>, <span class="fl">0.30</span>) </a>
<a class="sourceLine" id="cb1849-7" data-line-number="7">  Xset[[<span class="dv">4</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.40</span>, <span class="fl">0.60</span>);  Yset[[<span class="dv">4</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.60</span>, <span class="fl">0.40</span>) </a>
<a class="sourceLine" id="cb1849-8" data-line-number="8">  Xset[[<span class="dv">5</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.50</span>);  Yset[[<span class="dv">5</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.50</span>) </a>
<a class="sourceLine" id="cb1849-9" data-line-number="9">  Xset[[<span class="dv">6</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.60</span>, <span class="fl">0.40</span>);  Yset[[<span class="dv">6</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.40</span>, <span class="fl">0.60</span>) </a>
<a class="sourceLine" id="cb1849-10" data-line-number="10">  Xset[[<span class="dv">7</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.70</span>, <span class="fl">0.30</span>);  Yset[[<span class="dv">7</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.30</span>, <span class="fl">0.70</span>) </a>
<a class="sourceLine" id="cb1849-11" data-line-number="11">  Xset[[<span class="dv">8</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.80</span>, <span class="fl">0.20</span>);  Yset[[<span class="dv">8</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.20</span>, <span class="fl">0.80</span>) </a>
<a class="sourceLine" id="cb1849-12" data-line-number="12">  Xset[[<span class="dv">9</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.90</span>, <span class="fl">0.10</span>);  Yset[[<span class="dv">9</span>]] =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.90</span>) </a>
<a class="sourceLine" id="cb1849-13" data-line-number="13">  samples =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dt">n =</span> <span class="dv">9</span>, <span class="dt">size=</span>N, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1849-14" data-line-number="14">  X =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>); <span class="kw">colnames</span>(X) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>)</a>
<a class="sourceLine" id="cb1849-15" data-line-number="15">  Y =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>); <span class="kw">colnames</span>(Y) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Y1&quot;</span>, <span class="st">&quot;Y2&quot;</span>)</a>
<a class="sourceLine" id="cb1849-16" data-line-number="16">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) { </a>
<a class="sourceLine" id="cb1849-17" data-line-number="17">      X[i,] =<span class="st"> </span>Xset[[samples[[i]]]] </a>
<a class="sourceLine" id="cb1849-18" data-line-number="18">      Y[i,]  =<span class="st"> </span>Yset[[samples[[i]]]]</a>
<a class="sourceLine" id="cb1849-19" data-line-number="19">  }</a>
<a class="sourceLine" id="cb1849-20" data-line-number="20">  <span class="co"># Add some decent amount of noise</span></a>
<a class="sourceLine" id="cb1849-21" data-line-number="21">  X =<span class="st"> </span>X <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>N, <span class="dt">min=</span><span class="fl">0.01</span>, <span class="dt">max=</span><span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb1849-22" data-line-number="22">  Y =<span class="st"> </span>Y <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>N, <span class="dt">min=</span><span class="fl">0.01</span>, <span class="dt">max=</span><span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb1849-23" data-line-number="23">  <span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y)</a>
<a class="sourceLine" id="cb1849-24" data-line-number="24">}</a>
<a class="sourceLine" id="cb1849-25" data-line-number="25">train =<span class="st"> </span><span class="kw">get.synthetic.samples</span>(<span class="dt">N=</span><span class="dv">50</span>)</a>
<a class="sourceLine" id="cb1849-26" data-line-number="26">test  =<span class="st"> </span><span class="kw">get.synthetic.samples</span>(<span class="dt">N=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb1849-27" data-line-number="27">X =<span class="st"> </span>train<span class="op">$</span>X</a>
<a class="sourceLine" id="cb1849-28" data-line-number="28"><span class="kw">head</span>(<span class="kw">cbind</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y), <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##               X1         X2         Y1         Y2
##  [1,] 0.72217088 0.32217088 0.32915548 0.72915548
##  [2,] 0.71349361 0.31349361 0.34168717 0.74168717
##  [3,] 0.31142471 0.71142471 0.72626247 0.32626247
##  [4,] 0.62385755 0.42385755 0.44342895 0.64342895
##  [5,] 0.13064818 0.93064818 0.92470781 0.12470781
##  [6,] 0.12493056 0.92493056 0.93566241 0.13566241
##  [7,] 0.84983223 0.24983223 0.21899916 0.81899916
##  [8,] 0.14664758 0.94664758 0.93292818 0.13292818
##  [9,] 0.11212189 0.91212189 0.91777823 0.11777823
## [10,] 0.61198812 0.41198812 0.41205500 0.61205500</code></pre>

<p>Here, we use a <strong>DNN</strong> that is wide or shallow - meaning that we have a small number of layers with a large number of neurons. For example, below, we create three hidden layers with 100 neurons for the first two layers and a size of 2 neurons for the third layer.</p>

<div class="sourceCode" id="cb1851"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1851-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1851-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">100</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">100</span>),<span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>) )</a>
<a class="sourceLine" id="cb1851-3" data-line-number="3">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)</a></code></pre></div>

<p>The number of parameters generated for our <strong>DNN</strong> is 10602 parameters. Let us now train our network. Note that we use the whole dataset as our minibatch to force only one iterative pass and be able to capture the error in training.</p>

<div class="sourceCode" id="cb1852"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1852-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">50</span>,</a>
<a class="sourceLine" id="cb1852-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">200</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>
<pre><code>## [1] &quot;Infinite Loss:Inf&quot;</code></pre>

<p>The model results in an early stop and does not complete the iteration. We also notice that <strong>COST</strong> becomes extremely large.</p>

<div class="sourceCode" id="cb1854"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1854-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 4</code></pre>
<div class="sourceCode" id="cb1856"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1856-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>## [1] 6.5924684e+06 7.4100188e+19 1.3648587e+85           Inf</code></pre>

<p>Also, we begin to see signs of <strong>exploding gradient</strong> given the extremely large positive values. See below:</p>

<div class="sourceCode" id="cb1858"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1858-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>
<pre><code>##             [,1]          [,2]          [,3]
##    1.2259206e+72 1.1754450e+72 1.1790274e+72
## X1 6.0578526e+71 5.8084292e+71 5.8261314e+71
## X2 6.9268430e+71 6.6416402e+71 6.6618818e+71</code></pre>

<p>If we evaluate the performance of our prediction, the outcome results in <strong>NaN</strong>. Otherwise, it results in extremely large negative number (e.g. <span class="math inline">\(-\infty\)</span>).</p>

<div class="sourceCode" id="cb1860"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1860-1" data-line-number="1">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1860-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 rows</span></a></code></pre></div>
<pre><code>##               T1         T2              O1              O2
##  [1,] 0.32623900 0.72623900 -4.6659318e+205 -5.9787324e+205
##  [2,] 0.33325993 0.73325993 -4.6520722e+205 -5.9609733e+205
##  [3,] 0.74788943 0.34788943 -4.6806885e+205 -5.9976410e+205
##  [4,] 0.41684077 0.61684077 -4.6142492e+205 -5.9125084e+205
##  [5,] 0.91527475 0.11527475 -4.7837296e+205 -6.1296736e+205
##  [6,] 0.91044131 0.11044131 -4.7762117e+205 -6.1200405e+205
##  [7,] 0.23370034 0.83370034 -4.5557140e+205 -5.8375039e+205
##  [8,] 0.94793236 0.14793236 -4.7844383e+205 -6.1305817e+205
##  [9,] 0.93496225 0.13496225 -4.7431521e+205 -6.0776792e+205
## [10,] 0.42822478 0.62822478 -4.6490108e+205 -5.9570505e+205</code></pre>

<p>Similarly, let us now try to use a <strong>DNN</strong> that is thin or deep - meaning that we have increased the number of layers with a smaller number of neurons.</p>

<div class="sourceCode" id="cb1862"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1862-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1862-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1862-3" data-line-number="3">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1862-4" data-line-number="4">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1862-5" data-line-number="5">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)  </a></code></pre></div>

<p>The number of parameters generated for our <strong>DNN</strong> is 237 parameters. Let us now train our network.</p>

<div class="sourceCode" id="cb1863"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1863-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">50</span>,</a>
<a class="sourceLine" id="cb1863-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">200</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>
<pre><code>## [1] &quot;Infinite Loss:Inf&quot;</code></pre>

<p>The model results in an early stop and does not complete the iteration. We also notice that the <strong>COST</strong> becomes extremely large.</p>

<div class="sourceCode" id="cb1865"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1865-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb1867"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1867-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>## [1] 2.6917433e+06 4.5512916e+70           Inf</code></pre>

<p>Also, we begin to see signs of <strong>exploding gradient</strong> given the extremely large negative values. See below:</p>

<div class="sourceCode" id="cb1869"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1869-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>
<pre><code>##              [,1]           [,2]           [,3]
##    -1.8063676e+67 -2.1878042e+67 -6.3399875e+66
## X1 -8.9154808e+66 -1.0798094e+67 -3.1291548e+66
## X2 -1.0217233e+67 -1.2374726e+67 -3.5860435e+66</code></pre>

<p>If we evaluate the performance of our prediction, the outcome results in <strong>NaN</strong>. Otherwise, it results in an extremely large positive number (e.g. <span class="math inline">\(\infty\)</span>).</p>

<div class="sourceCode" id="cb1871"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1871-1" data-line-number="1">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1871-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 rows</span></a></code></pre></div>
<pre><code>##               T1         T2  O1  O2
##  [1,] 0.32623900 0.72623900 Inf Inf
##  [2,] 0.33325993 0.73325993 Inf Inf
##  [3,] 0.74788943 0.34788943 Inf Inf
##  [4,] 0.41684077 0.61684077 Inf Inf
##  [5,] 0.91527475 0.11527475 Inf Inf
##  [6,] 0.91044131 0.11044131 Inf Inf
##  [7,] 0.23370034 0.83370034 Inf Inf
##  [8,] 0.94793236 0.14793236 Inf Inf
##  [9,] 0.93496225 0.13496225 Inf Inf
## [10,] 0.42822478 0.62822478 Inf Inf</code></pre>

<p>This section shows that a very thin or very deep network can cause an <strong>exploding gradient</strong>. However, for <strong>vanishing gradient</strong>, it helps to use the <strong>sigmoid</strong> activation function to simulate the phenomenon. We know that a sigmoid output is squashed between 0 and 1. Asymptotically, the output may never reach zero or one. It is this asymptotic nature of <strong>sigmoid</strong> that the more layers or more neurons we use, the product of such fractions gets smaller to the point that the effect on learning slows down.</p>
</div>
<div id="dead-relu" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.9</span> Dead Relu <a href="deeplearning1.html#dead-relu" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We know that a <strong>RELU</strong> activation function drops to a flat zero towards the right, eventually manifesting the so-called <strong>Dying RELU</strong> because certain gradients end up with zero values throughout the training. In contrast to the effect of <strong>vanishing gradient</strong> in which learning slows down, a <strong>Dead Relu</strong> can completely stop learning.</p>
<p>One of the many experiments to try in this section is to simulate a <strong>Dying Relu</strong>. To do that, we use our original <strong>forward.pass(.)</strong> function and <strong>back.propagation(.)</strong> function such that we default the activation functions of the hidden layers to <strong>RELU</strong> instead of using <strong>Leaky RELU</strong>.</p>

<div class="sourceCode" id="cb1873"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1873-1" data-line-number="1">forward.pass &lt;-<span class="st"> </span><span class="cf">function</span>(X, layers, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>, <span class="dt">mode=</span><span class="st">&quot;train&quot;</span>) {</a>
<a class="sourceLine" id="cb1873-2" data-line-number="2">  ...</a>
<a class="sourceLine" id="cb1873-3" data-line-number="3">  act.output       =<span class="st"> </span><span class="kw">activation</span>(net.input, relu)</a>
<a class="sourceLine" id="cb1873-4" data-line-number="4">  ...</a>
<a class="sourceLine" id="cb1873-5" data-line-number="5">}</a></code></pre></div>
<div class="sourceCode" id="cb1874"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1874-1" data-line-number="1">back.propagation &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, model, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>) {</a>
<a class="sourceLine" id="cb1874-2" data-line-number="2">  ...</a>
<a class="sourceLine" id="cb1874-3" data-line-number="3">  gradient.output     =<span class="st"> </span><span class="kw">gradient.activation</span>(act.output, gradient.relu)</a>
<a class="sourceLine" id="cb1874-4" data-line-number="4">  ...</a>
<a class="sourceLine" id="cb1874-5" data-line-number="5">}</a></code></pre></div>

<p>If we then use a very thin or very deep network model, we should begin to see some of our <strong>gradients</strong> always set to zero throughout the training.</p>

<div class="sourceCode" id="cb1875"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1875-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>

<p>We leave readers to experiment on this topic.</p>
<p>In the following sections, we discuss some tune-ups to overcome such conditions as <strong>vanishing gradients</strong>, <strong>exploding gradients</strong>, and <strong>overfitting</strong>. </p>
</div>
<div id="gradient-clipping-gc" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.10</span> Gradient Clipping (GC) <a href="deeplearning1.html#gradient-clipping-gc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Clipping</strong> is a technique that <strong>clips</strong> a value within a minimum and maximum range. Similar to <strong>Batch Normalization</strong>, we can use such a technique to avoid <strong>exploding gradients</strong> as our case allows while at the same time achieving some level of performance improvement.</p>
<p>To illustrate, let us review our original <strong>back.propagation(.)</strong> function with emphasis on clipping:</p>

<div class="sourceCode" id="cb1876"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1876-1" data-line-number="1">back.propagation &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, model, params, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>) {</a>
<a class="sourceLine" id="cb1876-2" data-line-number="2">    ...</a>
<a class="sourceLine" id="cb1876-3" data-line-number="3">        <span class="co"># gradient clipping</span></a>
<a class="sourceLine" id="cb1876-4" data-line-number="4">        delta.omega[[L]]        =<span class="st"> </span><span class="kw">clipping</span>( <span class="kw">t</span>(net.input) <span class="op">%*%</span><span class="st"> </span>delta[[L]] )</a>
<a class="sourceLine" id="cb1876-5" data-line-number="5">    ...</a>
<a class="sourceLine" id="cb1876-6" data-line-number="6">}</a></code></pre></div>

<p>A simple implementation of clipping is written as such:</p>
<p>To disable clipping:</p>

<div class="sourceCode" id="cb1877"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1877-1" data-line-number="1">clipping &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">x.min=</span><span class="op">-</span><span class="ot">Inf</span>, <span class="dt">x.max=</span><span class="ot">Inf</span>) { </a>
<a class="sourceLine" id="cb1877-2" data-line-number="2">  <span class="kw">pmax</span>( <span class="kw">pmin</span>(x, x.max),  x.min)</a>
<a class="sourceLine" id="cb1877-3" data-line-number="3">}</a></code></pre></div>

<p>To enable clipping:</p>

<div class="sourceCode" id="cb1878"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1878-1" data-line-number="1">clipping &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">x.min=</span><span class="op">-</span><span class="dv">1</span>, <span class="dt">x.max=</span><span class="dv">1</span>) { </a>
<a class="sourceLine" id="cb1878-2" data-line-number="2">  <span class="kw">pmax</span>( <span class="kw">pmin</span>(x, x.max),  x.min)</a>
<a class="sourceLine" id="cb1878-3" data-line-number="3">}</a></code></pre></div>

<p>Using the following layers as before, let us now see if we are able to avoid the <strong>exploding gradient</strong> condition.</p>

<div class="sourceCode" id="cb1879"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1879-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1879-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1879-3" data-line-number="3">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1879-4" data-line-number="4">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1879-5" data-line-number="5">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)  </a></code></pre></div>

<p>Given that, let us train our model:</p>

<div class="sourceCode" id="cb1880"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1880-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">10</span>,</a>
<a class="sourceLine" id="cb1880-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">1800</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>The model does not result in an early stop and completes the iteration. We also notice that the <strong>COST</strong> becomes stable.</p>

<div class="sourceCode" id="cb1881"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1881-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 1800</code></pre>
<div class="sourceCode" id="cb1883"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1883-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.075484215 0.073736657 0.074051346 0.074300869 0.074657774
##  [6] 0.074209654 0.074944662 0.073766241 0.074005804 0.074472293</code></pre>

<p>Also, we do not see extremely large values. See below:</p>

<div class="sourceCode" id="cb1885"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1885-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>
<pre><code>##             [,1]          [,2]            [,3]
##    -0.0022213189 -0.0063910013 -0.000016248965
## X1 -0.0069608285 -0.0201426031 -0.000046744422
## X2  0.0046131129  0.0133853472  0.000029664716</code></pre>

<p>See a plot of the <strong>COST</strong> for the model in Figure <a href="deeplearning1.html#fig:mlpplotdc">12.16</a>.</p>

<div class="sourceCode" id="cb1887"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1887-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.deep<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1887-2" data-line-number="2">y =<span class="st"> </span>mlp.model.deep<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1887-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   </a>
<a class="sourceLine" id="cb1887-4" data-line-number="4">     <span class="dt">main=</span><span class="st">&quot;DNN Plot (Gradient Clipping)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1887-5" data-line-number="5"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1887-6" data-line-number="6">color =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb1887-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col=</span>color[<span class="dv">1</span>], <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpplotdc"></span>
<img src="DS_files/figure-html/mlpplotdc-1.png" alt="DNN Plot (Gradient Clipping)" width="70%" />
<p class="caption">
Figure 12.16: DNN Plot (Gradient Clipping)
</p>
</div>

<p>Reviewing the performance, we see that the predicted result closely matches the target.</p>

<div class="sourceCode" id="cb1888"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1888-1" data-line-number="1">output     =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1888-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) </a></code></pre></div>
<pre><code>##               T1         T2   O1   O2
##  [1,] 0.32623900 0.72623900 0.56 0.50
##  [2,] 0.33325993 0.73325993 0.56 0.50
##  [3,] 0.74788943 0.34788943 0.56 0.50
##  [4,] 0.41684077 0.61684077 0.56 0.50
##  [5,] 0.91527475 0.11527475 0.56 0.50
##  [6,] 0.91044131 0.11044131 0.56 0.50
##  [7,] 0.23370034 0.83370034 0.56 0.50
##  [8,] 0.94793236 0.14793236 0.56 0.50
##  [9,] 0.93496225 0.13496225 0.56 0.49
## [10,] 0.42822478 0.62822478 0.56 0.50</code></pre>

<p>The important note is that we can avoid the <strong>exploding gradient</strong> by clipping our <strong>gradients</strong>. At the same time, we should still be able to obtain higher performance by adjusting the learning rate, epoch limit, and tolerance level. Additionally, notice that our <strong>minimum and maximum thresholds</strong> are hand-selected for our clips. The chosen values are heuristically obtained. However, other literature may provide better ways to derive such thresholds automatically. We leave readers to investigate such techniques.</p>
</div>
<div id="parameter-initialization" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.11</span> Parameter Initialization <a href="deeplearning1.html#parameter-initialization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Deciding on the appropriate parameter initialization is an essential step in <strong>DNN</strong> that cannot be taken lightly without leading to failed convergence or very slow convergence. Here, let us introduce three initialization methods used:</p>
<ul>
<li><strong>Xavier (Glorot) Initialization</strong> - This initialization is introduced by Xavier Glorot and Yoshua Bengio in <span class="citation">(<a href="bibliography.html#ref-ref1188x">2010</a>)</span>. The idea behind the initialization is to generate random values for all weights based on a choice of <strong>uniform</strong> distribution or <strong>normal distribution</strong> using the below formulation (derivation is not included):  </li>
</ul>
<p><span class="math display">\[\begin{align}
\text{glorot.init} = \frac{1}{n^{[l]} + n^{[l-1]}},
\ \ \ \ \ \sigma = \sqrt{(2 \times \text{glorot.init})}
\ \ \ \ \ \ limit = \sqrt{(6 \times glorot.init)}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathbf{W}^{(l)} \sim \mathcal{N}\left(\mu = 0,\sigma =  \sigma\right)
\ \ \ \ \ \ 
\mathbf{W}^{(l)} \sim \mathcal{U}\left(\text{min = -limit, max = +limit}\right)
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{n^{(l)}}\)</span> is the number of input neurons, <span class="math inline">\(\mathbf{n^{(l-1)}}\)</span> is the number of output neurons, and <strong>l</strong> refers to a layer. The emphasis is on scaling variance to constrain weights from starting with very small or very large values.</p>
<p>Such initialization is used for <strong>Sigmoid</strong> and <strong>Softmax</strong> activations.</p>
<ul>
<li><strong>Kaiming/He Initialization</strong> - the initialization is a variant of <strong>Xavier initialization</strong> introduced by <strong>Kaiming He</strong> et al. <span class="citation">(<a href="bibliography.html#ref-ref1355k">2015</a>)</span>, which considers only the number of input neurons:  </li>
</ul>
<p><span class="math display">\[\begin{align}
\text{he.init} = \frac{1}{n^{[l]}  },
\ \ \ \ \ \sigma = \sqrt{(2 \times \text{he.init})}
\ \ \ \ \ \ limit = \sqrt{(6 \times he.init)}
\end{align}\]</span></p>
<p>Such initialization is used for <strong>Relu</strong> activation.</p>
<ul>
<li><strong>LeCun Initialization</strong> - the initialization introduced by <strong>Yann Lecun</strong> has the following variant: </li>
</ul>
<p><span class="math display">\[\begin{align}
\text{lecun.init} = \frac{1}{n^{[l]}  },
\ \ \ \ \ \sigma = \sqrt{(3 \times \text{lecun.init})}
\ \ \ \ \ \ limit = \sqrt{(1 \times lecun.init)}
\end{align}\]</span></p>
<p>Such initialization is used for <strong>TanH</strong> activation.</p>
<p>Below is our example implementation of the initialization methods:</p>

<div class="sourceCode" id="cb1890"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1890-1" data-line-number="1">net.initialization &lt;-<span class="st"> </span><span class="cf">function</span>(size, ni, no, <span class="dt">itype =</span> <span class="st">&quot;glorot&quot;</span>, </a>
<a class="sourceLine" id="cb1890-2" data-line-number="2">                               <span class="dt">dist=</span><span class="st">&quot;normal&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-3" data-line-number="3">   <span class="cf">if</span> (itype <span class="op">==</span><span class="st"> &quot;glorot&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-4" data-line-number="4">     glorot.init =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(ni <span class="op">+</span><span class="st"> </span>no); sd =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>glorot.init)</a>
<a class="sourceLine" id="cb1890-5" data-line-number="5">     lim =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">6</span> <span class="op">*</span><span class="st"> </span>glorot.init)</a>
<a class="sourceLine" id="cb1890-6" data-line-number="6">   } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1890-7" data-line-number="7">   <span class="cf">if</span> (itype <span class="op">==</span><span class="st"> &quot;he&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-8" data-line-number="8">     he.init =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(ni); sd =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>he.init)</a>
<a class="sourceLine" id="cb1890-9" data-line-number="9">     lim =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">6</span> <span class="op">*</span><span class="st"> </span>he.init)</a>
<a class="sourceLine" id="cb1890-10" data-line-number="10">   } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1890-11" data-line-number="11">   <span class="cf">if</span> (itype <span class="op">==</span><span class="st"> &quot;lecun&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-12" data-line-number="12">     lecun.init =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(ni); sd =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">3</span> <span class="op">*</span><span class="st"> </span>lecun.init)</a>
<a class="sourceLine" id="cb1890-13" data-line-number="13">     lim =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">1</span> <span class="op">*</span><span class="st"> </span>lecun.init)</a>
<a class="sourceLine" id="cb1890-14" data-line-number="14">   }</a>
<a class="sourceLine" id="cb1890-15" data-line-number="15">   <span class="cf">if</span> (dist <span class="op">==</span><span class="st"> &quot;normal&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-16" data-line-number="16">     init =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd =</span> sd)</a>
<a class="sourceLine" id="cb1890-17" data-line-number="17">   } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1890-18" data-line-number="18">   <span class="cf">if</span> (dist <span class="op">==</span><span class="st"> &quot;uniform&quot;</span>) {</a>
<a class="sourceLine" id="cb1890-19" data-line-number="19">     init =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>size, <span class="dt">min=</span><span class="op">-</span>lim, <span class="dt">max=</span>lim)</a>
<a class="sourceLine" id="cb1890-20" data-line-number="20">   }</a>
<a class="sourceLine" id="cb1890-21" data-line-number="21">   init</a>
<a class="sourceLine" id="cb1890-22" data-line-number="22">}</a></code></pre></div>

<p>Let us modify our <strong>deep.neural.layers(.)</strong> function to use the <strong>net.initialization(.)</strong> function. To do that, we change from:</p>

<div class="sourceCode" id="cb1891"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1891-1" data-line-number="1">deep.neural.layers &lt;-<span class="st"> </span><span class="cf">function</span>(X, ...) {</a>
<a class="sourceLine" id="cb1891-2" data-line-number="2">  ...</a>
<a class="sourceLine" id="cb1891-3" data-line-number="3">  O =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(K <span class="op">*</span><span class="st"> </span>N, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>), <span class="dt">nrow=</span>K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1891-4" data-line-number="4">  ...</a>
<a class="sourceLine" id="cb1891-5" data-line-number="5">}</a></code></pre></div>

<p>to:</p>

<div class="sourceCode" id="cb1892"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1892-1" data-line-number="1">deep.neural.layers &lt;-<span class="st"> </span><span class="cf">function</span>(X, ...) {</a>
<a class="sourceLine" id="cb1892-2" data-line-number="2">   ...</a>
<a class="sourceLine" id="cb1892-3" data-line-number="3">   weights =<span class="st"> </span><span class="kw">net.initialization</span>( K <span class="op">*</span><span class="st"> </span>N, K, N, <span class="dt">itype=</span><span class="st">&quot;glorot&quot;</span>, <span class="dt">dist=</span><span class="st">&quot;normal&quot;</span> )</a>
<a class="sourceLine" id="cb1892-4" data-line-number="4">   O =<span class="st"> </span><span class="kw">matrix</span>(weights, <span class="dt">nrow=</span>K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1892-5" data-line-number="5">   ...</a>
<a class="sourceLine" id="cb1892-6" data-line-number="6">}</a></code></pre></div>



<p>To see the effect of the <strong>Glorot</strong> initialization scheme, let us follow the same line of thought using a wide <strong>DNN</strong> to demonstrate the benefit, then followed by using a thin <strong>DNN</strong> after:</p>

<div class="sourceCode" id="cb1893"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1893-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1893-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">100</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">100</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1893-3" data-line-number="3">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)   </a></code></pre></div>

<p>The number of parameters generated for our <strong>DNN</strong> is 10602 parameters. Let us now train our network.</p>

<div class="sourceCode" id="cb1894"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1894-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">50</span>,</a>
<a class="sourceLine" id="cb1894-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">500</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>This time, the model does not result in an early stop though it reaches our tolerance level. Furthermore, the <strong>COST</strong> is tamed.</p>

<div class="sourceCode" id="cb1895"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1895-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 500</code></pre>
<div class="sourceCode" id="cb1897"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1897-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.00017286656 0.00017277647 0.00017268774 0.00017260033
##  [5] 0.00017251487 0.00017243049 0.00017234708 0.00017226500
##  [9] 0.00017218433 0.00017210568</code></pre>

<p>In addition, we find no sign of vanishing gradient. See below:</p>

<div class="sourceCode" id="cb1899"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1899-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>
<pre><code>##              [,1]           [,2]          [,3]
##     1.2549305e-06 0.001745049023 -0.0023383996
## X1 -1.6533580e-05 0.001748552505 -0.0045640682
## X2  1.0058240e-05 0.000019985832  0.0017300846</code></pre>

<p>If we evaluate the performance of our prediction, the outcome results in values that marginally match the target.</p>

<div class="sourceCode" id="cb1901"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1901-1" data-line-number="1">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1901-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 rows</span></a></code></pre></div>
<pre><code>##               T1         T2   O1   O2
##  [1,] 0.32623900 0.72623900 0.33 0.74
##  [2,] 0.33325993 0.73325993 0.33 0.74
##  [3,] 0.74788943 0.34788943 0.73 0.31
##  [4,] 0.41684077 0.61684077 0.42 0.64
##  [5,] 0.91527475 0.11527475 0.93 0.13
##  [6,] 0.91044131 0.11044131 0.93 0.13
##  [7,] 0.23370034 0.83370034 0.23 0.83
##  [8,] 0.94793236 0.14793236 0.93 0.13
##  [9,] 0.93496225 0.13496225 0.93 0.13
## [10,] 0.42822478 0.62822478 0.43 0.64</code></pre>

<p>On the other hand, let us also try to use a <strong>DNN</strong> that is thin.</p>

<div class="sourceCode" id="cb1903"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1903-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1903-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1903-3" data-line-number="3">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">5</span>),</a>
<a class="sourceLine" id="cb1903-4" data-line-number="4">                       <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1903-5" data-line-number="5">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)  </a></code></pre></div>

<p>The number of parameters generated for our <strong>DNN</strong> is 237 parameters. Let us now train our network.</p>

<div class="sourceCode" id="cb1904"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1904-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">50</span>,</a>
<a class="sourceLine" id="cb1904-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">500</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>Similarly, the model does not result in an early stop and the iteration also completes.</p>

<div class="sourceCode" id="cb1905"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1905-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 500</code></pre>
<div class="sourceCode" id="cb1907"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1907-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.074299949 0.074299945 0.074299942 0.074299939 0.074299936
##  [6] 0.074299933 0.074299929 0.074299926 0.074299923 0.074299920</code></pre>

<p>However, while it does show that <strong>COST</strong> is stable, we start to see the <strong>gradients</strong> getting much smaller.</p>

<div class="sourceCode" id="cb1909"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1909-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a></code></pre></div>
<pre><code>##              [,1]           [,2]           [,3]
##     2.6874065e-09 -2.5275332e-07 -3.9684473e-07
## X1  2.5275477e-05 -2.3778705e-03 -3.7350164e-03
## X2 -2.5381638e-05  2.3878567e-03  3.7506992e-03</code></pre>

<p>If we evaluate the performance of our prediction, the outcome results in values that are way off compared to the target.</p>

<div class="sourceCode" id="cb1911"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1911-1" data-line-number="1">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1911-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 rows</span></a></code></pre></div>
<pre><code>##               T1         T2   O1  O2
##  [1,] 0.32623900 0.72623900 0.56 0.5
##  [2,] 0.33325993 0.73325993 0.56 0.5
##  [3,] 0.74788943 0.34788943 0.56 0.5
##  [4,] 0.41684077 0.61684077 0.56 0.5
##  [5,] 0.91527475 0.11527475 0.56 0.5
##  [6,] 0.91044131 0.11044131 0.56 0.5
##  [7,] 0.23370034 0.83370034 0.56 0.5
##  [8,] 0.94793236 0.14793236 0.56 0.5
##  [9,] 0.93496225 0.13496225 0.56 0.5
## [10,] 0.42822478 0.62822478 0.56 0.5</code></pre>

<p>This is the effect of having a <strong>DNN</strong> that is too deep.</p>
<p>Apart from merely using initialization to mitigate vanishing and exploding gradient, it should be apparent that a <strong>DNN</strong> that is too wide or too thin may not give any advantage. In architecting a <strong>DNN</strong>, it sometimes helps to design a decent and simple one.</p>
<p>To illustrate, suppose we adjust the number of layers used in our example above and come up with a decent number of layers, e.g., only two hidden layers instead of the 100 hidden layers we used prior, each corresponding to a select number of neurons: (5,5,2) - the first two layers are the <strong>hidden</strong> layers while the last layer represents the <strong>output</strong> layer.</p>

<div class="sourceCode" id="cb1913"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1913-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1913-2" data-line-number="2">dnn =<span class="st"> </span><span class="kw">deep.neural.layers</span>(X, <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>), </a>
<a class="sourceLine" id="cb1913-3" data-line-number="3">                            <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">2</span>))  </a></code></pre></div>

<p>Let us continue to use the same configurations as before to train our network.</p>

<div class="sourceCode" id="cb1914"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1914-1" data-line-number="1">mlp.model.deep=<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">10</span>,</a>
<a class="sourceLine" id="cb1914-2" data-line-number="2">                       <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">500</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>Here, the training completes with <strong>no early stop</strong>.</p>

<div class="sourceCode" id="cb1915"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1915-1" data-line-number="1">mlp.model.deep<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 500</code></pre>
<div class="sourceCode" id="cb1917"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1917-1" data-line-number="1"><span class="kw">head</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>) <span class="co"># Reviewing first few MSEs</span></a></code></pre></div>
<pre><code>##  [1] 0.35822684 0.35964301 0.35890920 0.35693836 0.35793780 0.35811155
##  [7] 0.36048859 0.35749239 0.35851598 0.35929211</code></pre>
<div class="sourceCode" id="cb1919"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1919-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.deep<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>) <span class="co"># Reviewing last few MSEs</span></a></code></pre></div>
<pre><code>##  [1] 0.00020791055 0.00020416317 0.00020717125 0.00020579111
##  [5] 0.00020944271 0.00020917991 0.00019817092 0.00020185509
##  [9] 0.00020208305 0.00019340582</code></pre>

<p>Reviewing our <strong>gradients</strong>, the values are not extreme. See below:</p>

<div class="sourceCode" id="cb1921"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1921-1" data-line-number="1">mlp.model.deep<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega</a></code></pre></div>
<pre><code>##               [,1]          [,2]          [,3]           [,4]
##     0.000055319940 -0.0061976654 -0.0072942866 -0.00254141681
## X1 -0.000090601529 -0.0025536053 -0.0059808568 -0.00251385332
## X2  0.000158613103 -0.0053286353 -0.0033576009 -0.00074866972
##            [,5]
##    0.0079363948
## X1 0.0066501195
## X2 0.0035133448</code></pre>

<p>Let us plot the error (see Figure <a href="deeplearning1.html#fig:mlpplotd">12.17</a>).</p>

<div class="sourceCode" id="cb1923"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1923-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.deep<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1923-2" data-line-number="2">y =<span class="st"> </span>mlp.model.deep<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1923-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1923-4" data-line-number="4">      <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;DNN Plot&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1923-5" data-line-number="5"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1923-6" data-line-number="6">color =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb1923-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col=</span>color[<span class="dv">1</span>], <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpplotd"></span>
<img src="DS_files/figure-html/mlpplotd-1.png" alt="DNN Plot" width="70%" />
<p class="caption">
Figure 12.17: DNN Plot
</p>
</div>

<p>Let us now use our trained model to test using our test set:</p>

<div class="sourceCode" id="cb1924"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1924-1" data-line-number="1">output =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.deep)</a>
<a class="sourceLine" id="cb1924-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) <span class="co"># display first 10 rows</span></a></code></pre></div>
<pre><code>##               T1         T2   O1   O2
##  [1,] 0.32623900 0.72623900 0.34 0.73
##  [2,] 0.33325993 0.73325993 0.34 0.73
##  [3,] 0.74788943 0.34788943 0.72 0.33
##  [4,] 0.41684077 0.61684077 0.43 0.64
##  [5,] 0.91527475 0.11527475 0.92 0.12
##  [6,] 0.91044131 0.11044131 0.92 0.12
##  [7,] 0.23370034 0.83370034 0.23 0.85
##  [8,] 0.94793236 0.14793236 0.92 0.12
##  [9,] 0.93496225 0.13496225 0.92 0.12
## [10,] 0.42822478 0.62822478 0.44 0.63</code></pre>

<p>We see that the set of predicted outcomes is marginally equivalent to the set of targets.</p>
</div>
<div id="regularization-by-dropouts" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.12</span> Regularization by Dropouts <a href="deeplearning1.html#regularization-by-dropouts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Neural Networks</strong> are not immune from <strong>overfitting</strong>. As we recall in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>), we introduce <strong>L1-loss</strong> and <strong>L2-loss</strong> to reduce overfitting by amplifying or diminishing the effect of the <strong>loss function</strong> on the coefficients (weights). Here, in <strong>Neural Network</strong>, we introduce <strong>Dropout</strong> to <strong>regularize</strong> <strong>neural networks</strong> (Nitish Srivastava, Geoffrey Hinton, et al., <span class="citation">(<a href="bibliography.html#ref-ref1196n">2014</a>)</span>).</p>
<p>The idea is to disconnect incoming and outgoing connections to and from a neuron, effectively dropping out the neuron from the network. For example, in Figure <a href="deeplearning1.html#fig:dropout">12.18</a>, there are three dropout neurons in <strong>H1 layer</strong>, one dropout neuron in <strong>H2 layer</strong>, two dropout neurons in <strong>H3 layer</strong>, and so on.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dropout"></span>
<img src="dropout.png" alt="Dropout" width="90%" />
<p class="caption">
Figure 12.18: Dropout
</p>
</div>
<p>To use <strong>dropouts</strong>, we apply the modification to our forward pass equation:</p>
<p><span class="math display">\[\begin{align}
H = activation( X \cdotp \omega + b) \circ \mathbf{r}\ \ \ \ \ \leftarrow H = activation( X \cdotp  \omega + b)
\end{align}\]</span></p>
<p>where <strong>r</strong> is a Bernoulli distribution, e.g. <span class="math inline">\(r \sim Bernoulli(p)\)</span>, with <strong>p</strong> probability set preferably greater than or equal to 0.50. A higher probability indicates a lesser dropout. The random dropout is also scaled by <span class="math inline">\(1/p\)</span> to compensate for dropping some neurons. For example, below is our implementation of the <strong>drop.out(.)</strong> function motivated by a python code segment from CS231n PPT by Fei Fei et. al.</p>

<div class="sourceCode" id="cb1926"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1926-1" data-line-number="1">drop.out &lt;-<span class="st"> </span><span class="cf">function</span>(data, prob) {  </a>
<a class="sourceLine" id="cb1926-2" data-line-number="2">    v =<span class="st"> </span>data</a>
<a class="sourceLine" id="cb1926-3" data-line-number="3">    di  =<span class="st"> </span><span class="kw">length</span>(v)</a>
<a class="sourceLine" id="cb1926-4" data-line-number="4">    is.dim =<span class="st"> </span><span class="kw">is.matrix</span>(data) <span class="op">||</span><span class="st"> </span><span class="kw">is.array</span>(data)</a>
<a class="sourceLine" id="cb1926-5" data-line-number="5">    <span class="cf">if</span> (is.dim) {</a>
<a class="sourceLine" id="cb1926-6" data-line-number="6">        di =<span class="st"> </span><span class="kw">dim</span>(data); v =<span class="st"> </span><span class="kw">c</span>(data)</a>
<a class="sourceLine" id="cb1926-7" data-line-number="7">    }</a>
<a class="sourceLine" id="cb1926-8" data-line-number="8">    len =<span class="st"> </span><span class="kw">prod</span>(di)</a>
<a class="sourceLine" id="cb1926-9" data-line-number="9">    p =<span class="st"> </span>(<span class="kw">runif</span>(<span class="dt">n =</span> len, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>) <span class="op">&lt;</span><span class="st"> </span>prob) <span class="op">/</span><span class="st"> </span>prob  </a>
<a class="sourceLine" id="cb1926-10" data-line-number="10">    <span class="kw">array</span>(v <span class="op">*</span><span class="st"> </span>p, di)</a>
<a class="sourceLine" id="cb1926-11" data-line-number="11">}</a></code></pre></div>

<p>To illustrate further, we modify our original implementation of <strong>forward.pass(.)</strong> and <strong>my.MLP(.)</strong> to accommodate our <strong>dropout</strong> functionality. All other code and functions in our <strong>MLP implementation</strong> are unchanged. See below:</p>

<div class="sourceCode" id="cb1927"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1927-1" data-line-number="1">forward.pass &lt;-<span class="cf">function</span>(X, params, <span class="dt">afunc=</span><span class="st">&quot;sigmoid&quot;</span>, <span class="dt">drop=</span><span class="ot">NULL</span>, </a>
<a class="sourceLine" id="cb1927-2" data-line-number="2">                        <span class="dt">batchnorm=</span><span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb1927-3" data-line-number="3">  ...</a>
<a class="sourceLine" id="cb1927-4" data-line-number="4">  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(drop)) {</a>
<a class="sourceLine" id="cb1927-5" data-line-number="5">     <span class="cf">if</span> (<span class="kw">length</span>(drop) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) { prob =<span class="st"> </span>drop[L]} <span class="cf">else</span> { prob =<span class="st"> </span>drop }</a>
<a class="sourceLine" id="cb1927-6" data-line-number="6">  } <span class="cf">else</span> ...</a>
<a class="sourceLine" id="cb1927-7" data-line-number="7">  ...</a>
<a class="sourceLine" id="cb1927-8" data-line-number="8">}</a></code></pre></div>

<p>Let us use some arbitrary number of layers like so without dropouts, then train our model.</p>

<div class="sourceCode" id="cb1928"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1928-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1928-2" data-line-number="2">dnn =<span class="st"> </span><span class="kw">deep.neural.layers</span>(X, <span class="kw">list</span>(<span class="st">&quot;size&quot;</span>=<span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span>=<span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span>=<span class="dv">2</span>))  </a></code></pre></div>

<p>Here, let us train a model without dropout and another model with a 50% dropout.</p>

<div class="sourceCode" id="cb1929"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1929-1" data-line-number="1">mlp.model.nodrop =<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">10</span>, </a>
<a class="sourceLine" id="cb1929-2" data-line-number="2">                   <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>,<span class="dt">eta=</span><span class="fl">0.001</span>,  <span class="dt">epoch=</span><span class="dv">200</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>Now, let us the same number of layers but with 50% dropout, then train our model.</p>

<div class="sourceCode" id="cb1930"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1930-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1930-2" data-line-number="2">dnn =<span class="st"> </span><span class="kw">deep.neural.layers</span>(X, <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>, <span class="st">&quot;drop&quot;</span>=<span class="fl">0.50</span>), </a>
<a class="sourceLine" id="cb1930-3" data-line-number="3">                            <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>, <span class="st">&quot;drop&quot;</span>=<span class="fl">0.50</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">2</span>))</a></code></pre></div>
<div class="sourceCode" id="cb1931"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1931-1" data-line-number="1">mlp.model.withdrop =<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, </a>
<a class="sourceLine" id="cb1931-2" data-line-number="2">                            <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">200</span>,  <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a></code></pre></div>

<p>Using the same trainset as before, we can see that our model with dropout can reach our <strong>epoch</strong> limit with <strong>no</strong> early stop. Also, notice how the <strong>COST</strong> oscillates.</p>

<div class="sourceCode" id="cb1932"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1932-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;Epoch (no Dropouts)&quot;</span> =<span class="st"> </span>mlp.model.nodrop<span class="op">$</span>last.iteration,</a>
<a class="sourceLine" id="cb1932-2" data-line-number="2">  <span class="st">&quot;Epoch (with Dropouts)&quot;</span> =<span class="st"> </span>mlp.model.withdrop<span class="op">$</span>last.iteration )</a></code></pre></div>
<pre><code>##   Epoch (no Dropouts) Epoch (with Dropouts) 
##                   200                   200</code></pre>
<div class="sourceCode" id="cb1934"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1934-1" data-line-number="1"><span class="kw">head</span>(mlp.model.nodrop<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.35804328 0.34882459 0.32788790 0.30374957 0.28486765 0.26892971
##  [7] 0.25912319 0.24184753 0.23954416 0.23575337</code></pre>
<div class="sourceCode" id="cb1936"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1936-1" data-line-number="1"><span class="kw">head</span>(mlp.model.withdrop<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.35844878 0.35822198 0.35845522 0.35818881 0.35801963 0.35856555
##  [7] 0.35818907 0.35812269 0.35810622 0.35872444</code></pre>

<p>Let us plot the results in which we can see a change in the landscape for <strong>COST</strong> and a noticeable oscillation for the model with <strong>dropout</strong>. See Figure <a href="deeplearning1.html#fig:mlpplot11">12.19</a>.</p>

<div class="sourceCode" id="cb1938"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1938-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.nodrop<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1938-2" data-line-number="2"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1938-3" data-line-number="3">y =<span class="st"> </span>mlp.model.nodrop<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1938-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1938-5" data-line-number="5">     <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;MLP (No Dropouts)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1938-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1938-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1938-8" data-line-number="8">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.withdrop<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1938-9" data-line-number="9">y =<span class="st"> </span>mlp.model.withdrop<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1938-10" data-line-number="10"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1938-11" data-line-number="11">    <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;MLP (With Dropouts)&quot;</span>, <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1938-12" data-line-number="12"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1938-13" data-line-number="13"><span class="kw">lines</span>(x, y, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpplot11"></span>
<img src="DS_files/figure-html/mlpplot11-1.png" alt="MLP (Dropouts)" width="70%" />
<p class="caption">
Figure 12.19: MLP (Dropouts)
</p>
</div>

<p>Note that <strong>dropouts</strong> may best suit <strong>fully connected neural networks</strong> in which neurons in hidden layers can be turned on and off randomly. This behavior somehow <strong>simulates</strong> the nature of <strong>missing data</strong> or <strong>perturbation</strong>; thus, it generalizes the model, effectively resisting an overfit.</p>
<p>Note that other <strong>Neural networks</strong> may not be that accommodating towards <strong>dropouts</strong> especially for those that are not <strong>fully connected</strong>. While this may be true on a case-by-case basis, we leave readers to investigate <strong>CNN</strong> and <strong>RNN</strong> (which we cover later) as these <strong>Neural networks</strong> support datasets with <strong>Spatial Relationships</strong> or <strong>Time-Series</strong> properties.</p>
</div>
<div id="batch-normalization" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.13</span> Batch Normalization <a href="deeplearning1.html#batch-normalization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Internal Covariance shift</strong> is a phenomenon noted by Serge Loffe and Christian Szegedy in their <span class="citation">(<a href="bibliography.html#ref-ref986s">2019</a>)</span> paper. The intuition behind the shift emphasizes the data distribution of the activation output being fed to the next layer. Inconsistent distribution in scale (<strong>variance</strong>) and shift (<strong>mean</strong>) may manifest across <strong>mini-batches</strong> that are processed. <strong>Batch Normalization</strong> intends to reduce the <strong>shifts</strong>. In other words, it attempts to compose consistency in the data distribution across <strong>mini-batches</strong>. On the other hand, a paper published by Shibani Santunkar, Dimitris Tsipras, et al.Â in <span class="citation">(<a href="bibliography.html#ref-ref1002s">2018</a>)</span> explains that <strong>Batch Normalization</strong> works because it lays out a smoother landscape in the direction of the gradients traveling from layer to layer instead of a more coarse or rugged terrain.</p>
<p>To illustrate, using Figure <a href="deeplearning1.html#fig:forwardpass">12.13</a>, let us take the <strong>mean</strong> and <strong>variance</strong> of an <strong>activation output</strong> as starting point.</p>
<p><span class="math display">\[\begin{align}
\vec{\mu}_B - \frac{1}{m}\sum_{i=1}^m h_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\vec{\sigma^2}_B = \frac{1}{m}\sum_{i=1}^m (h_i - \vec{\mu}_B)^2
\end{align}\]</span></p>
<p>where: <span class="math inline">\(\vec{\mu}_B \in \mathbb{R}^p\ and\ \vec{\sigma^2} \in \mathbb{R}^p\)</span>.</p>
<p>Recall that <strong>activation output</strong> is an output from an <strong>activation function</strong>. Here, for now, let us represent the <strong>activation output</strong> as <strong>h</strong> and <strong>m</strong> as the number of samples in a <strong>mini-batch</strong>. Also, <strong>p</strong> is represented as number of <strong>activation neurons</strong>. Now, the normalized version is expressed as:</p>
<p><span class="math display">\[\begin{align}
h^{(norm)}_i = \frac{h_i - \vec{\mu}_B}{\sqrt{\vec{\sigma^2}_B + \epsilon}} 
\end{align}\]</span></p>
<p>We then use the normalized version and scale it down using two learnable hyperparameters: gamma (<span class="math inline">\(\gamma\)</span>) and beta (<span class="math inline">\(\beta\)</span>).</p>
<p><span class="math display">\[\begin{align}
\hat{h}_i = \gamma * h^{(norm)}_i   + \beta = BN_{\gamma,\beta}(h)
\end{align}\]</span></p>
<p>where <span class="math inline">\(h^{(norm)} \in \mathbb{R}^{nxp}\)</span> and <span class="math inline">\(i = 1 ... m\)</span>.</p>
<p>In R, the use of <strong>var(.)</strong> gives sample variance using <strong>Besselâs correction</strong>, <span class="math inline">\(\frac{1}{(n-1)}\)</span>. Here, we use our <strong>batch variance</strong> (a.l.a population variance):</p>

<div class="sourceCode" id="cb1939"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1939-1" data-line-number="1"><span class="co"># not using the Bessel&#39;s correction ( for sample variance).</span></a>
<a class="sourceLine" id="cb1939-2" data-line-number="2">batch.variance &lt;-<span class="st"> </span><span class="cf">function</span>(h) { <span class="kw">mean</span>( (h <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(h))<span class="op">^</span><span class="dv">2</span>  ) }</a></code></pre></div>

<p>Let us obtain the <strong>mean</strong> (<span class="math inline">\(\vec{\mu}_B\)</span>) and <strong>variance</strong> (<span class="math inline">\(\vec{\sigma^2}_B\)</span>) given a simple dataset like so:</p>

<div class="sourceCode" id="cb1940"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1940-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">digits=</span><span class="dv">8</span>)</a>
<a class="sourceLine" id="cb1940-2" data-line-number="2">(<span class="dt">Input =</span> <span class="kw">matrix</span>( <span class="kw">c</span>(<span class="fl">0.12</span>, <span class="fl">0.18</span>, <span class="fl">0.13</span>, <span class="fl">0.21</span>, <span class="fl">0.15</span>, <span class="fl">0.30</span>, <span class="fl">0.18</span>, <span class="fl">0.40</span>), </a>
<a class="sourceLine" id="cb1940-3" data-line-number="3">                 <span class="dt">nrow=</span><span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,] 0.12 0.18
## [2,] 0.13 0.21
## [3,] 0.15 0.30
## [4,] 0.18 0.40</code></pre>
<div class="sourceCode" id="cb1942"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1942-1" data-line-number="1">(<span class="dt">mu =</span> <span class="kw">apply</span>(Input, <span class="dv">2</span>, mean))</a></code></pre></div>
<pre><code>## [1] 0.1450 0.2725</code></pre>
<div class="sourceCode" id="cb1944"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1944-1" data-line-number="1">(<span class="dt">variance  =</span> <span class="kw">apply</span>(Input, <span class="dv">2</span>, batch.variance )) </a></code></pre></div>
<pre><code>## [1] 0.00052500 0.00736875</code></pre>

<p>Given all that, below is our example implementation of a <strong>batchnorm</strong> forward function:</p>

<div class="sourceCode" id="cb1946"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1946-1" data-line-number="1">batchnorm.forward &lt;-<span class="st"> </span><span class="cf">function</span>(H, layer, <span class="dt">eps=</span><span class="fl">1e-8</span>, <span class="dt">momentum =</span> <span class="fl">0.90</span>,</a>
<a class="sourceLine" id="cb1946-2" data-line-number="2">                              <span class="dt">rmax=</span><span class="dv">1</span>, <span class="dt">dmax=</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1946-3" data-line-number="3">    gamma             =<span class="st"> </span>layer<span class="op">$</span>batch.gamma<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb1946-4" data-line-number="4">    beta              =<span class="st"> </span>layer<span class="op">$</span>batch.beta<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb1946-5" data-line-number="5">    moving.mu         =<span class="st"> </span>layer<span class="op">$</span>moving.mu</a>
<a class="sourceLine" id="cb1946-6" data-line-number="6">    moving.variance   =<span class="st"> </span>layer<span class="op">$</span>moving.variance</a>
<a class="sourceLine" id="cb1946-7" data-line-number="7">    <span class="co">## For Training</span></a>
<a class="sourceLine" id="cb1946-8" data-line-number="8">    <span class="co">#</span></a>
<a class="sourceLine" id="cb1946-9" data-line-number="9">    mu                =<span class="st"> </span><span class="kw">apply</span>(H, <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1946-10" data-line-number="10">    H.mu              =<span class="st"> </span><span class="kw">sweep</span>(H, <span class="dv">2</span>, mu, <span class="st">&quot;-&quot;</span>)</a>
<a class="sourceLine" id="cb1946-11" data-line-number="11">    var               =<span class="st"> </span><span class="kw">apply</span>( H.mu<span class="op">^</span><span class="dv">2</span> , <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1946-12" data-line-number="12">    istd              =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(var <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb1946-13" data-line-number="13">    H.norm            =<span class="st"> </span><span class="kw">sweep</span>( H.mu, <span class="dv">2</span>, istd, <span class="st">&quot;*&quot;</span> ) </a>
<a class="sourceLine" id="cb1946-14" data-line-number="14">    <span class="co">## For Inference</span></a>
<a class="sourceLine" id="cb1946-15" data-line-number="15">    moving.mu         =<span class="st"> </span>momentum <span class="op">*</span><span class="st"> </span>moving.mu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>momentum) <span class="op">*</span><span class="st"> </span>mu</a>
<a class="sourceLine" id="cb1946-16" data-line-number="16">    moving.variance   =<span class="st"> </span>momentum <span class="op">*</span><span class="st"> </span>moving.variance <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>momentum) <span class="op">*</span><span class="st"> </span>var</a>
<a class="sourceLine" id="cb1946-17" data-line-number="17">    <span class="co">## Now generate the act.output</span></a>
<a class="sourceLine" id="cb1946-18" data-line-number="18">    H.hat             =<span class="st"> </span>H.norm <span class="op">*</span><span class="st"> </span>gamma <span class="op">+</span><span class="st"> </span>beta</a>
<a class="sourceLine" id="cb1946-19" data-line-number="19">    moments           =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;H.norm&quot;</span>          =<span class="st"> </span>H.norm,</a>
<a class="sourceLine" id="cb1946-20" data-line-number="20">                             <span class="st">&quot;H.mu&quot;</span>            =<span class="st"> </span>H.mu,</a>
<a class="sourceLine" id="cb1946-21" data-line-number="21">                             <span class="st">&quot;istd&quot;</span>            =<span class="st"> </span>istd,</a>
<a class="sourceLine" id="cb1946-22" data-line-number="22">                             <span class="st">&quot;moving.mu&quot;</span>       =<span class="st"> </span>moving.mu,</a>
<a class="sourceLine" id="cb1946-23" data-line-number="23">                             <span class="st">&quot;moving.variance&quot;</span> =<span class="st"> </span>moving.variance)</a>
<a class="sourceLine" id="cb1946-24" data-line-number="24">    <span class="kw">list</span>(<span class="st">&quot;act.output&quot;</span> =<span class="st"> </span>H.hat, <span class="st">&quot;moments&quot;</span> =<span class="st"> </span>moments)</a>
<a class="sourceLine" id="cb1946-25" data-line-number="25">}</a></code></pre></div>
<div class="sourceCode" id="cb1947"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1947-1" data-line-number="1">batchnorm.prediction &lt;-<span class="st"> </span><span class="cf">function</span>(H, layer, <span class="dt">eps=</span><span class="fl">1e-8</span>) {</a>
<a class="sourceLine" id="cb1947-2" data-line-number="2">    gamma             =<span class="st"> </span>layer<span class="op">$</span>batch.gamma<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb1947-3" data-line-number="3">    beta              =<span class="st"> </span>layer<span class="op">$</span>batch.beta<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb1947-4" data-line-number="4">    moving.mu         =<span class="st"> </span>layer<span class="op">$</span>moving.mu</a>
<a class="sourceLine" id="cb1947-5" data-line-number="5">    moving.variance   =<span class="st"> </span>layer<span class="op">$</span>moving.variance</a>
<a class="sourceLine" id="cb1947-6" data-line-number="6">    istd              =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(moving.variance   <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb1947-7" data-line-number="7">    H.mu              =<span class="st"> </span><span class="kw">sweep</span>(H, <span class="dv">2</span>, moving.mu, <span class="st">&quot;-&quot;</span>)</a>
<a class="sourceLine" id="cb1947-8" data-line-number="8">    H.norm            =<span class="st"> </span><span class="kw">sweep</span>( H.mu, <span class="dv">2</span>, istd, <span class="st">&quot;*&quot;</span> )</a>
<a class="sourceLine" id="cb1947-9" data-line-number="9">    H.hat             =<span class="st"> </span>(H.norm <span class="op">*</span><span class="st"> </span>gamma <span class="op">+</span><span class="st"> </span>beta)</a>
<a class="sourceLine" id="cb1947-10" data-line-number="10">    <span class="kw">list</span>(<span class="st">&quot;prediction&quot;</span> =<span class="st"> </span>H.hat)</a>
<a class="sourceLine" id="cb1947-11" data-line-number="11">}</a></code></pre></div>

<p>Notice the inclusion of three parameters, namely <strong>running average</strong>, <strong>running variance</strong>, and <strong>momentum</strong>. We need these parameters for our <strong>inference</strong> which we cover in our later discussion.</p>
<p>From here, it helps to review <strong>forward.pass(.)</strong> function in <strong>MLP Implementation</strong> section once more in order to see how we use the <strong>batchnorm.forward(.)</strong> function.</p>
<p>Note that two learnable parameters are passed to the <strong>forward.pass(.)</strong> and <strong>batchnorm.forward(.)</strong> functions, namely one with the symbol <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) and the other one with symbol <strong>beta</strong> (<span class="math inline">\(\beta\)</span>). Both correspondingly represent variance (for scaling) and mean (for shifting) such that their convergence ultimately produces the same original (un-normalized) <strong>activation output</strong> - in other words, a gamma of 1 and beta of 0 readily dissolve the effect to the original distribution. Note that each activation (each neuron) in each layer requires a pair of these scalar parameters.</p>
<p>The following list of derivatives is formulated to obtain the deltas we need for our update rules, especially for the <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) and <strong>beta</strong> (<span class="math inline">\(\beta\)</span>) parameters (note that we are excluding derivations of the derivatives):</p>
<p><strong>First</strong>, we obtain the derivative of the <strong>total loss</strong> with respect to the normalized <strong>output</strong> - or <strong>input</strong> to a layer:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}} = 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}}\right) 
\left(\frac{\partial \hat{h}}{\partial h^{(norm)}}\right) = \frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}} \cdotp \gamma  
\end{align}\]</span></p>
<p><strong>Second</strong>, we obtain the derivative of the <strong>total loss</strong> with respect to <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) and <strong>beta</strong> (<span class="math inline">\(\beta\)</span>) respectively:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \gamma} =  
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}_i}\right)
\left(\frac{\partial \hat{h}_i}{\partial \gamma}\right) =  
\sum_{i=1}^m\frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}_i} \times h^{(norm)}_i 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \beta} = 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}_i}\right)
\left(\frac{\partial \hat{h}_i}{\partial \beta}\right) =
\sum_{i=1}^m \frac{\partial \mathcal{L}_{(total)}}{\partial \hat{h}_i}   
\ \ \ \  where\ \left(\frac{\partial \hat{h}_i}{\partial \beta}\right) = 1
\end{align}\]</span></p>
<p><strong>Third</strong>, we obtain the derivative of the <strong>total loss</strong> with respect to <strong>variance</strong> (<span class="math inline">\(\vec{\sigma^2}_B\)</span>):</p>
<p><span class="math display">\[\begin{align}
\begin{array}{ll} 
\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\sigma}^2_B}  
&amp;= \left(\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}}\right) 
\left(\frac{\partial h^{(norm)}}{\partial \vec{\sigma}^2_B}\right)\\
&amp; = 
 \sum_{i=1}^m \left[\frac{\partial \mathcal{L}_{(total)}}{\partial  h^{(norm)}_i}  \times
 \left(h_i - \vec{\mu}_B\right) \right]\times \frac{-1}{2} \left(\frac{1}{\sqrt{\vec{\sigma^2}_B + \epsilon}}\right)^3 \\
\end{array} \label{eqn:eqnnumber633}
\end{align}\]</span></p>
<p><strong>Fourth</strong>, we obtain the derivative of the <strong>total loss</strong> with respect to <strong>mean</strong> (<span class="math inline">\(\vec{\mu}_B\)</span>):</p>

<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\mu}_B}  
&amp;= 
\left[
\sum_{i=1}^m\left(\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}_i}\right) 
\left(\frac{\partial h^{(norm)}_i}{\partial \vec{\mu}_B}\right)\right] + 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial  \vec{\sigma^2}_B}\right) 
\left(\frac{\partial  \vec{\sigma^2}_B}{\partial \vec{\mu}_B}\right) \\
&amp;= \left[\sum_{i=1}^m\left(\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}_i}\right)  \left(\frac{-1}{\sqrt{\vec{\sigma^2}_B + \epsilon}}\right)\right] + 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\sigma^2}_B}\right)
 \left( \frac{\sum_{i=1}^m -2 (h_i - \vec{\mu}_B)}{m}\right)
\end{align}\]</span>
</p>
<p><strong>Fifth</strong>, we obtain the derivative of the <strong>total loss</strong> with respect to <span class="math inline">\(\mathbf{h_i}\)</span>:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}_{(total)}}{\partial h_i} 
&amp;= 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}_i}\right)
\left(\frac{\partial h^{(norm)}_i}{\partial h_i}\right) + 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\sigma^2}_B}\right)
\left(\frac{\partial \vec{\sigma^2}_B}{\partial h_i}\right) + \nonumber \\
&amp;\ \ \ \left(\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\mu}_B}\right)
\left(\frac{\partial \vec{\mu}_B}{\partial h_i}\right) \\
&amp;= 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial h^{(norm)}_i}\right)
\left(\frac{1}{\sqrt{\vec{\sigma^2}_B + \epsilon}}\right) + 
\left(\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\sigma^2}_B}\right)
\left(\frac{2(h_i - \vec{\mu}_B)}{m}\right) + \nonumber \\ 
&amp;\ \ \ \left(\frac{\partial \mathcal{L}_{(total)}}{\partial \vec{\mu}_B}\right)
\left(\frac{1}{m}\right)
\end{align}\]</span></p>
<p><strong>Finally</strong>, given all that, we now implement a backward pass for our learnable parameters with the following example implementation of <strong>batchnorm.backward(.)</strong>:</p>

<div class="sourceCode" id="cb1948"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1948-1" data-line-number="1">batchnorm.backward &lt;-<span class="st"> </span><span class="cf">function</span>(Dout, gamma, moments) {</a>
<a class="sourceLine" id="cb1948-2" data-line-number="2">   H.norm         =<span class="st"> </span>moments<span class="op">$</span>H.norm</a>
<a class="sourceLine" id="cb1948-3" data-line-number="3">   H.mu           =<span class="st"> </span>moments<span class="op">$</span>H.mu </a>
<a class="sourceLine" id="cb1948-4" data-line-number="4">   istd           =<span class="st"> </span>moments<span class="op">$</span>istd </a>
<a class="sourceLine" id="cb1948-5" data-line-number="5">   m              =<span class="st"> </span><span class="kw">apply</span>(H.norm, <span class="dv">2</span>, length)</a>
<a class="sourceLine" id="cb1948-6" data-line-number="6">   delta.gamma    =<span class="st"> </span><span class="kw">apply</span>(Dout <span class="op">*</span><span class="st"> </span>H.norm, <span class="dv">2</span>, sum)</a>
<a class="sourceLine" id="cb1948-7" data-line-number="7">   delta.beta     =<span class="st"> </span><span class="kw">apply</span>(Dout, <span class="dv">2</span>, sum)</a>
<a class="sourceLine" id="cb1948-8" data-line-number="8">   delta.H.norm   =<span class="st"> </span>Dout <span class="op">*</span><span class="st"> </span>gamma</a>
<a class="sourceLine" id="cb1948-9" data-line-number="9">   delta.std      =<span class="st"> </span><span class="kw">apply</span>(delta.H.norm <span class="op">*</span><span class="st"> </span>H.mu, <span class="dv">2</span>, sum) </a>
<a class="sourceLine" id="cb1948-10" data-line-number="10">   delta.var      =<span class="st"> </span>delta.std <span class="op">*</span><span class="st"> </span><span class="fl">-0.5</span> <span class="op">*</span><span class="st"> </span>istd<span class="op">^</span><span class="dv">3</span> </a>
<a class="sourceLine" id="cb1948-11" data-line-number="11">   delta.Hmu1     =<span class="st"> </span><span class="kw">apply</span>( delta.H.norm <span class="op">*</span><span class="st"> </span><span class="op">-</span>istd, <span class="dv">2</span>, sum)</a>
<a class="sourceLine" id="cb1948-12" data-line-number="12">   delta.Hmu2     =<span class="st"> </span>delta.var <span class="op">*</span><span class="st"> </span><span class="kw">apply</span>( <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>H.mu,<span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1948-13" data-line-number="13">   delta.mu       =<span class="st"> </span>delta.Hmu1 <span class="op">+</span><span class="st"> </span>delta.Hmu2</a>
<a class="sourceLine" id="cb1948-14" data-line-number="14">   delta.out      =<span class="st"> </span><span class="kw">sweep</span>(delta.H.norm <span class="op">*</span><span class="st"> </span>istd <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1948-15" data-line-number="15"><span class="st">                    </span>delta.var <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>H.mu) <span class="op">/</span><span class="st"> </span>m, <span class="dv">2</span>, (delta.mu <span class="op">/</span><span class="st"> </span>m), <span class="st">&#39;+&#39;</span>)</a>
<a class="sourceLine" id="cb1948-16" data-line-number="16">   <span class="kw">list</span>(<span class="st">&quot;gradient.output&quot;</span> =<span class="st"> </span>delta.out, </a>
<a class="sourceLine" id="cb1948-17" data-line-number="17">        <span class="st">&quot;delta.gamma&quot;</span> =<span class="st"> </span>delta.gamma, <span class="st">&quot;delta.beta&quot;</span> =<span class="st"> </span>delta.beta)</a>
<a class="sourceLine" id="cb1948-18" data-line-number="18">}</a></code></pre></div>

<p>From here, it also helps to review <strong>back.propagation(.)</strong> function in the same <strong>MLP Implementation</strong> section in order to see the use of <strong>batchnorm.backward(.)</strong> function. Additionally, we may have to review <strong>backward.pass(.)</strong> function which enforces the update rules for hyperparameters, along with the use of <strong>Adam</strong> optimization:</p>

<div class="sourceCode" id="cb1949"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1949-1" data-line-number="1">optimize.adam =<span class="st"> </span>adam &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, t) {</a>
<a class="sourceLine" id="cb1949-2" data-line-number="2">    beta1 =<span class="st"> </span><span class="fl">0.90</span>; beta2 =<span class="st"> </span><span class="fl">0.999</span>; eps=<span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb1949-3" data-line-number="3">    param<span class="op">$</span>rho    =<span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>rho <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1) <span class="op">*</span><span class="st"> </span>gradient</a>
<a class="sourceLine" id="cb1949-4" data-line-number="4">    param<span class="op">$</span>nu     =<span class="st"> </span>beta2 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta2) <span class="op">*</span><span class="st"> </span>gradient<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1949-5" data-line-number="5">    rho.hat      =<span class="st"> </span>param<span class="op">$</span>rho <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1949-6" data-line-number="6">    nu.hat       =<span class="st"> </span>param<span class="op">$</span>nu <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta2<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1949-7" data-line-number="7">    phi          =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(nu.hat) <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb1949-8" data-line-number="8">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>phi <span class="op">*</span><span class="st"> </span>rho.hat</a>
<a class="sourceLine" id="cb1949-9" data-line-number="9">    param</a>
<a class="sourceLine" id="cb1949-10" data-line-number="10">}</a></code></pre></div>

<p>Lastly, the <strong>deep.neural.layers(.)</strong> function incorporates initialization of the two new hyperparameters, namely <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) and <strong>beta</strong> (<span class="math inline">\(\beta\)</span>) for optimization.</p>
<p>To illustrate, let us continue to use the following layers.</p>

<div class="sourceCode" id="cb1950"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1950-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1950-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1950-3" data-line-number="3">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)</a></code></pre></div>

<p>That gives us about 57 parameters (including biases).</p>
<p>Now, let us try modeling DNN without batch normalization.</p>

<div class="sourceCode" id="cb1951"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1951-1" data-line-number="1">mlp.model.noBN =<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">10</span>, </a>
<a class="sourceLine" id="cb1951-2" data-line-number="2">                 <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>, <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">100</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a>
<a class="sourceLine" id="cb1951-3" data-line-number="3">mlp.model.noBN<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb1953"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1953-1" data-line-number="1"><span class="kw">head</span>(mlp.model.noBN<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.35804328 0.34882459 0.32788790 0.30374957 0.28486765 0.26892971
##  [7] 0.25912319 0.24184753 0.23954416 0.23575337</code></pre>
<div class="sourceCode" id="cb1955"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1955-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.noBN<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.00023908686 0.00022170241 0.00021220855 0.00020488970
##  [5] 0.00019650187 0.00018757701 0.00018158010 0.00017569945
##  [9] 0.00017344208 0.00016758827</code></pre>
<div class="sourceCode" id="cb1957"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1957-1" data-line-number="1">mlp.model.noBN<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega</a></code></pre></div>
<pre><code>##              [,1]         [,2]         [,3]          [,4]
##    -0.00034719214  0.008882285  0.002632272  0.0027318712
## X1 -0.00018229795 -0.004817867 -0.011059380 -0.0029795080
## X2 -0.00017847808  0.012816578  0.012179239  0.0052451620
##             [,5]
##    -0.0043827349
## X1  0.0102768353
## X2 -0.0131979135</code></pre>

<p>Next, we model <strong>DNN</strong> with batch normalization. For now, let us keep the same learning rate - eta symbol (<span class="math inline">\(\eta\)</span>).</p>

<div class="sourceCode" id="cb1959"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1959-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1959-2" data-line-number="2">layers =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>, <span class="st">&quot;batchnorm&quot;</span> =<span class="st"> </span><span class="ot">TRUE</span>), </a>
<a class="sourceLine" id="cb1959-3" data-line-number="3">                       <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">5</span>), <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1959-4" data-line-number="4">dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, layers)</a></code></pre></div>
<div class="sourceCode" id="cb1960"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1960-1" data-line-number="1">mlp.model.BN =<span class="st"> </span><span class="kw">my.MLP</span>(train<span class="op">$</span>X, train<span class="op">$</span>Y, dnn<span class="op">$</span>layers, <span class="dt">minibatch=</span><span class="dv">10</span>, </a>
<a class="sourceLine" id="cb1960-2" data-line-number="2">               <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>, <span class="dt">eta=</span><span class="fl">0.001</span>, <span class="dt">epoch=</span><span class="dv">100</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>)</a>
<a class="sourceLine" id="cb1960-3" data-line-number="3">mlp.model.BN<span class="op">$</span>last.iteration</a></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb1962"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1962-1" data-line-number="1"><span class="kw">head</span>(mlp.model.BN<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.35807389 0.35782997 0.34387280 0.32203210 0.30398898 0.28888063
##  [7] 0.27927648 0.26171434 0.25589441 0.24918166</code></pre>
<div class="sourceCode" id="cb1964"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1964-1" data-line-number="1"><span class="kw">tail</span>(mlp.model.BN<span class="op">$</span>cost, <span class="dt">n=</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] 0.051191994 0.050035899 0.050687903 0.052175072 0.050586036
##  [6] 0.050977164 0.050522264 0.049580550 0.051174961 0.050063166</code></pre>
<div class="sourceCode" id="cb1966"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1966-1" data-line-number="1">mlp.model.BN<span class="op">$</span>delta.params[[<span class="dv">1</span>]]<span class="op">$</span>omega</a></code></pre></div>
<pre><code>##           [,1] [,2] [,3]        [,4] [,5]
##    0.026829669   -1   -1 -0.47626824    1
## X1 0.011910041   -1   -1 -0.18591176    1
## X2 0.016637621   -1   -1 -0.32358615    1</code></pre>

<p>We can observe that the model with <strong>Batch Normalization</strong> seems to have a higher <strong>COST</strong> at every <strong>epoch</strong> compared to one without normalization. However, if we see the plot, the landscape of the <strong>cost</strong> function indeed follows a more comfortable trajectory. See Figure <a href="deeplearning1.html#fig:mlpplot2">12.20</a>.</p>

<div class="sourceCode" id="cb1968"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1968-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.noBN<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1968-2" data-line-number="2"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1968-3" data-line-number="3">y =<span class="st"> </span>mlp.model.noBN<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1968-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1968-5" data-line-number="5">      <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;MLP (No BatchNorm)&quot;</span>, <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1968-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1968-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1968-8" data-line-number="8">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(mlp.model.BN<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb1968-9" data-line-number="9">y =<span class="st"> </span>mlp.model.BN<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb1968-10" data-line-number="10"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb1968-11" data-line-number="11">    <span class="dt">xlab=</span><span class="st">&quot;ITERATION&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;COST&quot;</span>,   <span class="dt">main=</span><span class="st">&quot;MLP (With BatchNorm)&quot;</span>, <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1968-12" data-line-number="12"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1968-13" data-line-number="13"><span class="kw">lines</span>(x, y, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlpplot2"></span>
<img src="DS_files/figure-html/mlpplot2-1.png" alt="MLP (Batch Normalization)" width="70%" />
<p class="caption">
Figure 12.20: MLP (Batch Normalization)
</p>
</div>

<p>Now, in terms of <strong>inference</strong>, we use <strong>moving average</strong> and <strong>moving variance</strong> to keep track of the <strong>population average</strong> of all mini-batches throughout the training time - granting the choice of normalization is <strong>batch normalization</strong> versus <strong>layer normalization</strong>. Such parameters are then used at test time. See our <strong>batchnorm.forward(.)</strong> function and <strong>batchnorm.prediction(.)</strong> function. Also, we use <strong>momentum</strong> denoted by <strong>alpha</strong> (<span class="math inline">\(\alpha\)</span>) which serves as a <strong>decaying rate</strong> for the moments (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>) during training. The default we use in our implementation is 0.90.</p>
<p><span class="math display">\[\begin{align}
\mu_{(moving)} &amp;= \alpha \times \mu_{(moving)} + (1 - \alpha) \times \mu_B\\
\sigma^2_{(moving)} &amp;= \alpha \times \sigma^2_{(moving)} + (1 - \alpha) \times \sigma^2_{B}
\end{align}\]</span></p>
<p>To illustrate, we compare the result of a model without <strong>BN</strong> (recall that our dataset is concocted to have a <strong>RELU</strong> pattern for output):</p>

<div class="sourceCode" id="cb1969"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1969-1" data-line-number="1">output.noBN   =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.noBN)</a>
<a class="sourceLine" id="cb1969-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output.noBN<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>)  </a></code></pre></div>
<pre><code>##               T1         T2   O1   O2
##  [1,] 0.32623900 0.72623900 0.34 0.73
##  [2,] 0.33325993 0.73325993 0.34 0.73
##  [3,] 0.74788943 0.34788943 0.72 0.33
##  [4,] 0.41684077 0.61684077 0.43 0.64
##  [5,] 0.91527475 0.11527475 0.92 0.13
##  [6,] 0.91044131 0.11044131 0.92 0.13
##  [7,] 0.23370034 0.83370034 0.23 0.84
##  [8,] 0.94793236 0.14793236 0.92 0.13
##  [9,] 0.93496225 0.13496225 0.92 0.13
## [10,] 0.42822478 0.62822478 0.43 0.63</code></pre>

<p>and the result of a model with <strong>BN</strong></p>

<div class="sourceCode" id="cb1971"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1971-1" data-line-number="1">output.BN     =<span class="st"> </span><span class="kw">my.predict</span>(test<span class="op">$</span>X, mlp.model.BN)</a>
<a class="sourceLine" id="cb1971-2" data-line-number="2"><span class="kw">compare.outcome</span>(test<span class="op">$</span>Y, output.BN<span class="op">$</span>prediction, <span class="dt">n =</span><span class="dv">10</span>) </a></code></pre></div>
<pre><code>##               T1         T2   O1   O2
##  [1,] 0.32623900 0.72623900 0.53 0.53
##  [2,] 0.33325993 0.73325993 0.53 0.54
##  [3,] 0.74788943 0.34788943 0.59 0.47
##  [4,] 0.41684077 0.61684077 0.54 0.53
##  [5,] 0.91527475 0.11527475 0.63 0.42
##  [6,] 0.91044131 0.11044131 0.63 0.42
##  [7,] 0.23370034 0.83370034 0.50 0.57
##  [8,] 0.94793236 0.14793236 0.63 0.42
##  [9,] 0.93496225 0.13496225 0.62 0.43
## [10,] 0.42822478 0.62822478 0.54 0.52</code></pre>

<p>Note that while the prediction for the normalization seems a bit off in our particular case, we leave readers to try to use a learning rate of 0.01 for improvement. Additionally, the learning speed for batch normalization is more visible in the next section when we deal with <strong>CNN</strong> against the <strong>cifar-10</strong> dataset.</p>
<p>Also, the <strong>batch normalization</strong> step is positioned before the activation function in our implementation. In a section up ahead, dealing with <strong>CNN</strong>, we shall try to position the <strong>batch normalization</strong> after the activation function at the convolution layers. We leave readers to investigate the effectiveness of <strong>batch normalization</strong> based on whether it should be before or after the activation function.</p>
</div>
<div id="optimization" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.14</span> Optimization <a href="deeplearning1.html#optimization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our recent discussions put emphasis on <strong>vanishing gradients</strong> and <strong>exploding gradients</strong>. Certain techniques are introduced, such as <strong>Parameter initialization</strong>, <strong>Gradient Clipping</strong>, and designing a <strong>Decent Architecture</strong> in terms of the number of layers and neurons to use.</p>
<p>In this section, our emphasis is on <strong>overfitting</strong>, <strong>underfitting</strong>, and avoiding <strong>local minima</strong>. We begin to show the importance of adjusting a combination of some performance knobs such as <strong>floating-point precision</strong>, <strong>learning rate</strong>, <strong>epoch limit</strong>, and <strong>tolerance level</strong>. If we use a very small <strong>learning rate</strong>, we may end up with an <strong>underfit</strong> model. If we use a very large <strong>learning rate</strong>, we may end up with an <strong>overfit</strong> model, <strong>overshooting</strong> the <strong>target</strong>. If our <strong>epoch limit</strong> is too small (e.g., inducing an <strong>early stop</strong>), we may <strong>underfit</strong>, if it is too large and we do not hit a <strong>tolerance threshold</strong>, we may <strong>overfit</strong>. Heuristically, we also learn that <strong>Batch Normalization</strong> and <strong>Gradient Clipping</strong> can increase performance. This affects the effectiveness of the hyperparameters. Thus they require adjusting along with the use of the mentioned techniques. For <strong>Batch Normalization</strong>, it helps to tune the <strong>learning rate</strong> to balance down the learning to a more optimal speed. So far, what we have done heuristically is to find the best value for the learning rate. However, this trial and error approach may not be as effective if we are not careful.</p>
<p>Also, in this section, let us delve deeper into the <strong>update rule</strong> of the vanilla <strong>Gradient Descent</strong> algorithm, which is implemented in our <strong>backward.pass(.)</strong> function.</p>
<p><span class="math display">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \eta\times g^{(t)} \ \ \ \ \ \ where\ \ \ \ \ g^{(t)} = \nabla \omega \mathcal{L}
\end{align}\]</span></p>
<p>Note that the <strong>omega</strong> symbol (<span class="math inline">\(\omega\)</span>) represents coefficients or weights (also called parameters) that dictate the degrees of freedom (df) with which our model is fitted. Such parameters are to be tuned <strong>optimally</strong> to fit the model <strong>generally</strong> - we call this training the model (or, in other words, machine learning). One of the important components or elements of all these is the <strong>learning rate</strong> denoted by the <strong>eta</strong> symbol (<span class="math inline">\(\eta\)</span>). Specifically, we aim to find the most <strong>optimal</strong> <strong>learning rate</strong>. With that, there are enhancements to optimizing the <strong>Gradient Descent</strong> algorithm.  </p>
<p>Here, we list about eight optimizers that are regarded as state-of-art optimizers, whether currently or at one point in time in their rights. Note that, in principle, the aim of the different optimizers is to avoid local minima, avoid overfitting, avoid underfitting, and improve performance. Therefore, without covering the derivations, let us briefly explore and review the equations and implementation of each optimizer.</p>
<p><strong>First</strong>, let us introduce <strong>Momentum</strong> which is represented by the <strong>nu</strong> (<span class="math inline">\(\nu\)</span>) symbol added to <strong>Gradient Descent</strong>, in particular, to the <strong>Stochastic Gradient Descent (SGD)</strong>. This is also known as <strong>SGD with Momentum</strong>. The enhancement is as follows:   </p>
<p><span class="math display">\[\begin{align}
\nu^{(t+1)} = \gamma \times \nu^{(t)} + \eta \times  g^{(t)} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\omega^{(t+1)} =\omega^{(t)} - \nu^{(t+1)}
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1973"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1973-1" data-line-number="1">v =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1973-2" data-line-number="2"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1973-3" data-line-number="3">    v =<span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>v <span class="op">+</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span><span class="kw">gradient</span>( ... ) </a>
<a class="sourceLine" id="cb1973-4" data-line-number="4">    w =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>v</a>
<a class="sourceLine" id="cb1973-5" data-line-number="5">}</a></code></pre></div>

<p>Here, we use <strong>momentum</strong> to complement our <strong>learning rate</strong> as a way to apply <strong>moving average</strong> to the gradient. Additionally, it also comes with a <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) hyperparameter, a decaying sum that controls the <strong>momentum</strong>. A starting value to try for <strong>gamma</strong> (<span class="math inline">\(\gamma\)</span>) is 0.99.</p>
<p><strong>Second</strong>, let us introduce <strong>Adaptive Gradient (AdaGrad)</strong> optimizer formulated by John Duchi, Elad Hazan, and Yoram Singer in <span class="citation">(<a href="bibliography.html#ref-ref1013d">2011</a>)</span>. The enhancement is expressed as follows: </p>
<p><span class="math display">\[\begin{align}
\nu^{(t+1)} = \nu^{(t)} + (g^{(t)})^2 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi \times g^{(t)}
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\sqrt{v^{(t+1)} + \epsilon}}
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1974"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1974-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1974-2" data-line-number="2">    g =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1974-3" data-line-number="3">    v  =<span class="st"> </span>v <span class="op">+</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1974-4" data-line-number="4">    pi =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1974-5" data-line-number="5">    w  =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1974-6" data-line-number="6">}</a></code></pre></div>

<p>Similarly here, a <strong>momentum</strong> hyperparameter is enforced as a <strong>decaying rate</strong> denoted by the <strong>nu</strong> (<span class="math inline">\(\nu\)</span>) symbol to control the <strong>Learning Rate</strong> (<span class="math inline">\(\eta\)</span>). It should be apparent that if the <strong>denominator</strong> is smaller, it makes the <strong>Learning Rate</strong> larger (thus more aggressively fast). Therefore, there is a risk of <strong>overshooting</strong>. On the other hand, the further into the iteration we get, the smaller and slower the learning rate becomes.</p>
<p>Note that the <strong>epsilon</strong> (<span class="math inline">\(\epsilon\)</span>) symbol takes an extremely small number that exists only to prevent the division from zero, e.g., <span class="math inline">\(\epsilon =\)</span> 1e-10.</p>
<p><strong>Third</strong>, let us introduce the <strong>Root Mean Square Propagation (RMSProp)</strong> optimizer lectured by Tijmen Tieleman and Geoffrey Hinton in <span class="citation">(<a href="bibliography.html#ref-ref1022t">2012</a>)</span>. The enhancement is expressed as follows: </p>
<p><span class="math display">\[\begin{align}
\nu^{(t+1)} = \beta \times \nu^{(t)} + (1 - \beta) (g^{(t)})^2  
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi\ \times g^{(t)}
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\sqrt{v^{(t+1)} + \epsilon}}  
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1975"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1975-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1975-2" data-line-number="2">    g  =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1975-3" data-line-number="3">    v  =<span class="st"> </span>B <span class="op">*</span><span class="st"> </span>v <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B) <span class="op">*</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1975-4" data-line-number="4">    pi =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1975-5" data-line-number="5">    w  =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1975-6" data-line-number="6">}</a></code></pre></div>

<p><strong>RMSProp</strong> is a variant of <strong>AdaGrad</strong>. The presence of the <strong>Beta</strong> (<span class="math inline">\(\beta\)</span>) hyperparameter is to control the decay growth of the <strong>denominator</strong> in the case the <strong>Decay Rate</strong> grows exponentially.</p>
<p><strong>Fourth</strong>, let us introduce the <strong>Adaptive Delta (AdaDelta)</strong> optimizer formulated by Matthew Zeiler in <span class="citation">(<a href="bibliography.html#ref-ref1011m">2012</a>)</span>. This optimizer calculates <strong>Root Means Square (RMS)</strong>. The enhancement is expressed as follows: </p>
<p><span class="math display">\[\begin{align}
s^{(t+1)}  = \rho  \times s^{(t)}   + (1 - \rho)(g^{(t+1)})^2
\ \ \ \ \ where\ s = \mathbb{E}[g^{(t)})^2]
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\pi = \frac{\text{RMS}[\Delta \omega^{(t)}]^{(t)}}{\text{RMS}[g^{(t)}]^{(t+1)}}  g^{(t)} 
\ \ \ \ \ \ \ \ \ where\ \ \ \ \text{RMS}[g^{(t)}]^{(t+1)} = \sqrt{s^{(t+1)} + \epsilon}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi  
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\Delta \omega^{(t+1)} = \rho \times \Delta \omega^{(t)} + ( 1 - \rho) \times  \pi^2
\end{align}\]</span></p>
<p>where the <strong>rho</strong> (<span class="math inline">\(\rho\)</span>) symbol represents the <strong>Decay Rate</strong>. A good starting point to try for the empirical analysis is a value of 0.90.</p>
<p>The corresponding implementation is as such:</p>

<div class="sourceCode" id="cb1976"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1976-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1976-2" data-line-number="2">    g      =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1976-3" data-line-number="3">    s      =<span class="st"> </span>rho <span class="op">*</span><span class="st"> </span>s <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rho) <span class="op">*</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1976-4" data-line-number="4">    pi     =<span class="st"> </span><span class="kw">sqrt</span>(delta <span class="op">+</span><span class="st"> </span>eps) <span class="op">*</span><span class="st"> </span>( g <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(s <span class="op">+</span><span class="st"> </span>eps) )</a>
<a class="sourceLine" id="cb1976-5" data-line-number="5">    w      =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi  </a>
<a class="sourceLine" id="cb1976-6" data-line-number="6">    delta  =<span class="st"> </span>rho <span class="op">*</span><span class="st"> </span>delta <span class="op">+</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rho) <span class="op">*</span><span class="st"> </span>pi<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1976-7" data-line-number="7">}</a></code></pre></div>

<p>Note that <strong>s</strong> carries the average of the first moment of the gradient, and <span class="math inline">\(\Delta \omega\)</span> carries the average of the second moment.</p>
<p><strong>Fifth</strong>, let us introduce <strong>Adaptive Moment Estimation (Adam)</strong> optimizer formulated by Diederik P. Kingma and Jimmy Lei Ba in <span class="citation">(<a href="bibliography.html#ref-ref1003d">2015</a>)</span>. The enhancement is expressed as follows: </p>
<p><span class="math display">\[\begin{align}
\tau ^{(t+1)} = \beta_1 \times \tau^{(t)} + (1 - \beta_1) g^{(t)}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\nu^{(t+1)} = \beta_2 \times \nu^{(t)}+ (1 - \beta_2) (g^{(t)})^2 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\hat{\tau} = \frac{\tau^{(t+1)}}{1 - {(\beta_1)}^t}
\ \ \ \ \ \ \ \ \ \ 
\hat{\nu} = \frac{\nu^{(t+1)}}{1 - {(\beta_2)}^t}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi\ \times \hat{\tau}
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\sqrt{\hat{v} + \epsilon}}
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1977"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1977-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1977-2" data-line-number="2">    g      =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1977-3" data-line-number="3">    r      =<span class="st"> </span>B1 <span class="op">*</span><span class="st"> </span>r <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1) <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1977-4" data-line-number="4">    v      =<span class="st"> </span>B2 <span class="op">*</span><span class="st"> </span>v <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B2) <span class="op">*</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1977-5" data-line-number="5">    r.hat  =<span class="st"> </span>r <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1<span class="op">^</span>t)  </a>
<a class="sourceLine" id="cb1977-6" data-line-number="6">    v.hat  =<span class="st"> </span>v <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B2<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1977-7" data-line-number="7">    pi     =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v.hat <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1977-8" data-line-number="8">    w      =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>r.hat</a>
<a class="sourceLine" id="cb1977-9" data-line-number="9">}</a></code></pre></div>

<p><strong>Adam</strong> is one of the popular optimizers used in <strong>DNN</strong>. Apart from using the same concept of moving averages as that of <strong>SGD with Momentum</strong>, here, we use two <strong>moments</strong> in the form of the <strong>rho</strong> (<span class="math inline">\(\rho\)</span>) and <strong>nu</strong> (<span class="math inline">\(\nu\)</span>) symbols. The former is a moving average, and the latter is an exponential moving average for the respective gradients, along with their respective <strong>bias corrections</strong> denoted by <strong>rho-hat</strong> (<span class="math inline">\(\hat{\rho}\)</span>) and <strong>nu-hat</strong> (<span class="math inline">\(\hat{\nu}\)</span>) symbols. Additionally, the moving averages are controlled by two other extra hyperparameters as <strong>Decay Rate</strong> for the moments in the form of the <strong>beta</strong> (<span class="math inline">\(\beta\)</span>) symbols. Empirically and in typical practice, the (<span class="math inline">\(\beta\)</span>) values default to 0.90 and 0.999, respectively, with a learning rate (<span class="math inline">\(\eta\)</span>) of 0.001.</p>
<p><strong>Sixth</strong>, let us introduce a variant of <strong>Adam</strong> called <strong>AMSGrad</strong> optimizer formulated by Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar in <span class="citation">(<a href="bibliography.html#ref-ref1014s">2018</a>)</span>. The enhancement is expressed as follows: </p>
<p><span class="math display">\[\begin{align}
\tau ^{(t+1)} = \beta_1 \times \tau^{(t)} + (1 - \beta_1) g^{(t)}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\nu^{(t+1)} = \beta_2 \times \nu^{(t)}+ (1 - \beta_2) (g^{(t)})^2 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\hat{\nu}^{(t+1)} = max \left(\nu^{(t+1)} , \hat{\nu}^{(t)} \right)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi\ \times \tau^{(t+1)}
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\sqrt{\hat{v}^{(t+1)} + \epsilon}}
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1978"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1978-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1978-2" data-line-number="2">    g     =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1978-3" data-line-number="3">    r     =<span class="st"> </span>B1 <span class="op">*</span><span class="st"> </span>r <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1) <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1978-4" data-line-number="4">    v     =<span class="st"> </span>B2 <span class="op">*</span><span class="st"> </span>v <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B2) <span class="op">*</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1978-5" data-line-number="5">    v.hat =<span class="st"> </span><span class="kw">max</span>(v, v.hat)</a>
<a class="sourceLine" id="cb1978-6" data-line-number="6">    pi    =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v.hat <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1978-7" data-line-number="7">    w     =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb1978-8" data-line-number="8">}</a></code></pre></div>

<p>Like <strong>Adam</strong>, two <strong>moments</strong> are also used by <strong>AMSGrad</strong> for <strong>moving averages</strong>; however, unlike <strong>Adam</strong>, the <strong>bias corrections</strong> are eliminated. In their place, the maximum between current and past gradients is considered for an update, replacing the original formulation of <strong>nu-hat</strong> (<span class="math inline">\(\hat{\nu}\)</span>).</p>
<p><strong>Seventh</strong>, let us introduce a variant of <strong>Adam</strong> called <strong>Adaptive Max Pooling (AdaMax)</strong> optimizer, also formulated by Diederik P. Kingma and Jimmy Lei Ba in the same work in <span class="citation">(<a href="bibliography.html#ref-ref1003d">2015</a>)</span>. The enhancement is expressed as follows: </p>
<p><span class="math display">\[\begin{align}
\tau ^{(t+1)} = \beta_1 \times \tau^{(t)} + (1 - \beta_1) g^{(t)} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\nu^{(t+1)} =\max\left(\beta_2 \times \nu^{(t)}, |g^{(t)}|\right)  
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\hat{\tau}^{(t+1)} = \frac{\tau^{(t+1)}}{1 - {(\beta_1)}^t}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi\ \times \hat{\tau}^{(t+1)}
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\nu^{(t+1)} + \epsilon }
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1979"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1979-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1979-2" data-line-number="2">    g     =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1979-3" data-line-number="3">    r     =<span class="st"> </span>B1 <span class="op">*</span><span class="st"> </span>r <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1) <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1979-4" data-line-number="4">    v     =<span class="st"> </span><span class="kw">max</span>(B<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>v, <span class="kw">abs</span>(g) )</a>
<a class="sourceLine" id="cb1979-5" data-line-number="5">    r.hat =<span class="st"> </span>r <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1979-6" data-line-number="6">    pi    =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v <span class="op">+</span><span class="st"> </span>eps) </a>
<a class="sourceLine" id="cb1979-7" data-line-number="7">    w     =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>r.hat</a>
<a class="sourceLine" id="cb1979-8" data-line-number="8">}</a></code></pre></div>

<p>While <strong>Adam</strong> optimization uses <strong>L2-norm</strong> (euclidean), the <strong>AdaMax</strong> optimization uses <strong>max-norm</strong>, also called <strong>infinity norm</strong>. This is apparent in the equation used. Empirically, and in common practice, the (<span class="math inline">\(\beta\)</span>) values also default to 0.90 and 0.999 respectively with a learning rate (<span class="math inline">\(\eta\)</span>) of 0.001.</p>
<p><strong>Eight</strong>, let us introduce <strong>Nesterov-Accelerated Adaptive Moment (Nadam)</strong> optimizer formulated by Timothy Dozat in <span class="citation">(<a href="bibliography.html#ref-ref1057t">2016</a>)</span>. The enhancement is expressed as follows:  </p>
<p><span class="math display">\[\begin{align}
\tau ^{(t+1)} = \beta_1 \times \tau^{(t)} + (1 - \beta_1) g^{(t)}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\nu^{(t+1)} = \beta_2 \times \nu^{(t)}+ (1 - \beta_2) (g^{(t)})^2  
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\hat{\tau}^{(t+1)} = \frac{\tau^{(t+1)}}{1 - {(\beta_1)}^t} + 
\mathbf{\frac{(1 - \beta_1) \times (g)^{(t)}}{1 - {(\beta_1)}^t}}
\ \ \ \ \ \ \ \ \ \ 
\hat{\nu}^{(t+1)} = \frac{\nu^{(t+1)}}{1 - {(\beta_2)}^t} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\omega^{(t+1)} = \omega^{(t)} - \pi\ \times \hat{\tau}^{(t+1)} 
\ \ \ \ \ \ \ \ where\ \pi = \frac{\eta}{\sqrt{\hat{v}^{(t+1)} + \epsilon}}
\end{align}\]</span></p>
<p>with a corresponding implementation:</p>

<div class="sourceCode" id="cb1980"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1980-1" data-line-number="1"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb1980-2" data-line-number="2">    g      =<span class="st"> </span><span class="kw">gradient</span>( ... )</a>
<a class="sourceLine" id="cb1980-3" data-line-number="3">    r      =<span class="st"> </span>B1 <span class="op">*</span><span class="st"> </span>r <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1) <span class="op">*</span><span class="st"> </span>g</a>
<a class="sourceLine" id="cb1980-4" data-line-number="4">    v      =<span class="st"> </span>B2 <span class="op">*</span><span class="st"> </span>v <span class="op">+</span><span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B2) <span class="op">*</span><span class="st"> </span>g<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1980-5" data-line-number="5">    r.hat  =<span class="st"> </span>r <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1<span class="op">^</span>t) <span class="op">+</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1) <span class="op">*</span><span class="st"> </span>g <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B1<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1980-6" data-line-number="6">    v.hat  =<span class="st"> </span>v <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>B2<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb1980-7" data-line-number="7">    pi     =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v.hat <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb1980-8" data-line-number="8">    w      =<span class="st"> </span>w <span class="op">-</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>r.hat</a>
<a class="sourceLine" id="cb1980-9" data-line-number="9">}</a></code></pre></div>

<p><strong>Nadam</strong> optimizer expresses identical equations as <strong>Adam</strong> with one slight change to the <strong>rho-hat</strong> (<span class="math inline">\(\hat{\rho}\)</span>) - the addition of <strong>Nesterov Momentum</strong>. As stated in the title of Dozatâs paper <span class="citation">(<a href="bibliography.html#ref-ref1057t">2016</a>)</span>, the idea is to incorporate <strong>Nesterov Momentum</strong> into <strong>Adam</strong>.</p>
<p><strong>Finally</strong>, apart from the eight optimizers we discussed, other strategies can influence <strong>SGD</strong> learning speed. Let us present two of them.</p>
<p><strong>First</strong>, one strategy that allows a non-static learning rate is what we call <strong>Step Decay Schedule</strong> <span class="citation">(Rong Ge et al. <a href="bibliography.html#ref-ref1070r">2019</a>)</span>. Instead of just being static, the idea here is to reduce progressively (or decay) the <strong>learning rate</strong> (e.g., by half) at a given interval up to the <strong>epoch</strong> limit. An example formula for the <strong>learning rate</strong> is: </p>
<p><span class="math display">\[\begin{align}
\eta^{(t+1)} = \eta^{(initial)} \times DF^{floor\left(\frac{t}{step size}\right)}
\end{align}\]</span></p>
<p>where <strong>t</strong> is the epoch and <span class="math inline">\(\mathbf{\eta}\)</span> is the learning rate.</p>
<p>We cover this more on <strong>CNN</strong>.</p>
<p><strong>Second</strong>, another strategy that allows a non-static learning rate is what we call <strong>Cyclical Learning Rate</strong> <span class="citation">(Leslie N. Smith <a href="bibliography.html#ref-ref1082l">2017</a>)</span>. Instead of a <strong>step-wise decay</strong>, the idea is to vary the value of the <strong>learning rate</strong> within a <strong>reasonable</strong> minimum and maximum boundary. We define a <strong>cycle</strong> as an interval based on certain choices, e.g., at fix interval or every batch update. We then oscillate the learning rate by allowing it to increase up to the maximum boundary monotonically, and then at the next cycle, we monotonically decrease up to a minimum boundary. </p>
<p><strong>Lastly</strong>, we now come down to a more general perspective of <strong>learning rate</strong>. The onus is upon us to find the most optimal learning rate during training. We know that it takes trial and error to find a value for our learning rate if we use a static value throughout the training. Alternatively, we can resort to a more adaptive mechanism by performing <strong>simulated annealing</strong> (or in the case of <strong>DNN</strong>, we call it <strong>learning rate annealing</strong>) - that is to say, not only do we control the speed of the decay but also at which stage or cycle in training to vary the decay.</p>
<p>We leave readers to investigate other proposals for optimizing the <strong>learning rate</strong>.</p>
<p>In terms of implementation, we also leave readers to modify our <strong>backward.pass(.)</strong> function and experiment on the different optimizers recently discussed.</p>
</div>
<div id="interpretability" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.3.15</span> Interpretability<a href="deeplearning1.html#interpretability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The assumption, particularly in our early examples in <strong>DNN</strong>, is that our two output neurons in the output layer are mutually exclusive (e.g., linearly independent). Particularly for the output layer, the first output neuron produces a sigmoid value ranging between 0 and 1, irrespective of values from the other output neuron. It is essential to note here that if we contrive our samples so that we have a carefully crafted input and target output as shown in Figure <a href="deeplearning1.html#fig:interpretability">12.21</a> using a modified version of our <strong>get.synthetic.samples(.)</strong> function, we hope to see predictable output patterns as a way to validate our <strong>DNN</strong>. Indeed, Figure <a href="deeplearning1.html#fig:interpretability">12.21</a> shows that if we adjust the <strong>epoch</strong> count or the new <strong>X</strong> values, the predicted output expectedly follows the intended pattern.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:interpretability"></span>
<img src="interpretability.png" alt="Interpretability of Predicted Output" width="90%" />
<p class="caption">
Figure 12.21: Interpretability of Predicted Output
</p>
</div>
<p>If we can simulate a synthetic dataset (both input and output) in a very controlled fashion with precise and unique patterns, then we should be able to also yield precise and unique patterns for inference, no matter if we also induce a synthetic perturbation and a set of outliers. However, that is because we understand our data - we crafted it. Likewise, our implementation of <strong>DNN</strong> using <strong>my.MLP(.)</strong> makes it possible for us to tailor the complexity of our network (e.g., number of layers, number of neurons). We can make it in such a way as to avoid the perception of a <strong>black-box</strong> model. However, the context and its interpretation are essential. Somehow, our <strong>synthetic</strong> data has to simulate a real-world process that produces our samples naturally in the first place, like a toss of a coin. As easy as it may sound, real-world data, in actuality, is a lot more diluted with natural patterns and noise that we may not be able to simulate. We can contrive <strong>synthetic</strong> samples by any holistic and stochastic means, but it requires domain expertise to qualify a comprehensive sample and representative of a domainâs entire population sample.</p>
<p>Nevertheless, the point to take here is that as long as there is indeed a pattern, no matter how inconspicuous it may be, upon which we can infer, then <strong>DNN</strong> can train. Our <strong>mini.batch(.)</strong> function can provide such pattern that may simulate a <strong>gaussian</strong> distribution. This is useful for simple empirical purposes to allow our <strong>DNN</strong> to yield some expected pattern for training and testing, thereby having the ability to quantify the correctness and mistakes.</p>
<p>However, we may also have to account for one intended to prevent <strong>DNN</strong> from the ability to train rather than make mistakes even. One, in particular, that may offset the ability for <strong>DNN</strong> to train is the so-called <strong>Adversarial Sample</strong>, which has no pattern whatsoever. Another challenge that a <strong>Neural Network</strong> finds difficult to train is the so-called <strong>Bongard Problem</strong> which roots in puzzle-based samples.</p>
<p>Lastly, we also have to account for the data types. For example, data with spatial or sequential relationships calls for a different kind of <strong>Neural Network</strong> which we cover in the next section.</p>
</div>
</div>
<div id="convolutional-neural-network-cnn" class="section level2 hasAnchor">
<h2><span class="header-section-number">12.4</span> Convolutional Neural Network (CNN)  <a href="deeplearning1.html#convolutional-neural-network-cnn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the 1980s, Yann Lecun laid out the underpinnings of what is known today as the <strong>Convolutional Neural Network (CNN)</strong> used for <strong>Image Recognition and Classification</strong>.</p>
<div id="computer-graphics" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.1</span> Computer Graphics<a href="deeplearning1.html#computer-graphics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The main goal of working on <strong>CNN</strong> is to process and classify images. However, to classify images, we need to know first the type of computer graphics used.</p>
<p>There are essentially two types of computer graphics: <strong>vector graphics</strong> and <strong>raster graphics</strong>:</p>
<p><strong>Raster graphics</strong> are composed of pixels (whether greyscaled or colored). For example, the left image in Figure <a href="deeplearning1.html#fig:raster">12.22</a> represents the lower right portion of a circle projected to show the individual pixels.</p>
<p><strong>Vector graphics</strong> are rendered based on vertices and paths. For example, the right image in the figure shows a rectangle with four vertices (points) and four paths (lines) connecting the vertices.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:raster"></span>
<img src="raster.png" alt="Raster and Vector Graphics" width="70%" />
<p class="caption">
Figure 12.22: Raster and Vector Graphics
</p>
</div>
<p>For our purposes, we use a rasterized image in the format of <strong>jpeg</strong> (<strong>png</strong> and <strong>gif</strong> are as equally popular). Here, we use image operations made available in <strong>R</strong> by third-party libraries such as <strong>jpeg</strong>. Below is the code to read an image. See Figure <a href="deeplearning1.html#fig:appleorange">12.23</a> for the actual image.</p>

<div class="sourceCode" id="cb1981"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1981-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;jpeg&quot;</span>)</a>
<a class="sourceLine" id="cb1981-2" data-line-number="2">image      =<span class="st"> </span>jpeg<span class="op">::</span><span class="kw">readJPEG</span>(<span class="dt">source =</span> <span class="st">&quot;appleorange.jpg&quot;</span>)</a>
<a class="sourceLine" id="cb1981-3" data-line-number="3">image.copy =<span class="st"> </span>image</a>
<a class="sourceLine" id="cb1981-4" data-line-number="4">draw.image &lt;-<span class="st"> </span><span class="cf">function</span>(image, <span class="dt">main=</span><span class="st">&quot;Apple, Oranges, and Banana&quot;</span> ) {</a>
<a class="sourceLine" id="cb1981-5" data-line-number="5">  di =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb1981-6" data-line-number="6">  img =<span class="st"> </span><span class="kw">array</span>(image, <span class="kw">c</span>(di[<span class="dv">1</span>], di[<span class="dv">2</span>], di[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb1981-7" data-line-number="7">  sz =<span class="st"> </span><span class="kw">ncol</span>(img)</a>
<a class="sourceLine" id="cb1981-8" data-line-number="8">  <span class="kw">par</span>(<span class="dt">pty=</span><span class="st">&quot;s&quot;</span>) </a>
<a class="sourceLine" id="cb1981-9" data-line-number="9">  <span class="kw">plot</span>(<span class="ot">NA</span>, <span class="dt">type=</span><span class="st">&quot;n&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">1</span>, sz), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">1</span>, sz), </a>
<a class="sourceLine" id="cb1981-10" data-line-number="10">     <span class="dt">xlab=</span><span class="st">&quot;Image Width&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Image Height&quot;</span>,  <span class="dt">main=</span>main)</a>
<a class="sourceLine" id="cb1981-11" data-line-number="11">  <span class="kw">rasterImage</span>(img,<span class="dt">xleft=</span><span class="dv">1</span>, <span class="dt">ybottom=</span>sz, <span class="dt">xright=</span>sz, <span class="dt">ytop=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1981-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb1981-13" data-line-number="13"><span class="kw">draw.image</span>(image)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange"></span>
<img src="plotappleorange.png" alt="Apple, Oranges, and Banana" width="50%" />
<p class="caption">
Figure 12.23: Apple, Oranges, and Banana
</p>
</div>

<p>We use the <strong>rasterImage(.)</strong> function to project the image to a pixelized format - this is also called <strong>rasterization</strong>.</p>
<p>While our scope does not cover <strong>image processing</strong>, it helps to leave readers to investigate <strong>Graphics Pipelines</strong>, which is covered in <strong>Data Visualization</strong> courses. In addition, the topic touches on <strong>vertex processing</strong> such as <strong>tesselation</strong>, <strong>transformation</strong>, <strong>clipping</strong>, <strong>thresholding</strong>, and <strong>filtering or masking</strong>, which are mostly the processes we may use for illustration purposes when we get to <strong>Kernels</strong>.</p>
<p>Additionally, we know that <strong>3D Games</strong> perform heavy <strong>image processing</strong> and thus rely heavily on <strong>Graphics Processing Unit (GPU)</strong> for faster processing. Similarly, some, if not most, <strong>CNN</strong> standard packages are already built to have <strong>GPU</strong> support.</p>
<p>Part of <strong>Computer Graphics</strong> also touches on <strong>Object Segmentation</strong>, <strong>Object Detection or Localization</strong>, and <strong>Object Selection</strong>, which are essential when dealing with <strong>Autonomous Vehicles</strong> in a more dynamic time-series fashion (which we cover in the <strong>RNN</strong> section). In the same line of thinking, recall our discussion on <strong>Lidar Sensors</strong> and <strong>Kalman Filters</strong> in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>) for dynamic systems.</p>
<p>Now, going back to our image of a handful of fruits, to manipulate the image, we first have to note that the image is stored in a 3D tensor form with a 500x500x3 dimension. It can easily be shown like so:</p>

<div class="sourceCode" id="cb1982"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1982-1" data-line-number="1"><span class="kw">dim</span>(image)</a></code></pre></div>
<pre><code>## [1] 500 500   3</code></pre>

<p>The three dimensions correspondingly represent the height, width, and depth of the image. The depth in a <strong>JPEG</strong> image also represents the three channels of the RGB color scheme. Each element in the tensor represents the color intensity of an RGB color scheme which is normalized by a max value of 255. For example, we can get the hexadecimal equivalent of red color like so:</p>

<div class="sourceCode" id="cb1984"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1984-1" data-line-number="1"><span class="kw">rgb</span>(<span class="dv">255</span><span class="op">/</span><span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [1] &quot;#FF0000&quot;</code></pre>

<p>To change the color of the image in order to show only the red color scheme, we can manipulate the tensor like so:</p>

<div class="sourceCode" id="cb1986"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1986-1" data-line-number="1">image.copy =<span class="st"> </span>image; image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)] =<span class="st"> </span><span class="dv">0</span>  <span class="co"># red</span></a>
<a class="sourceLine" id="cb1986-2" data-line-number="2"><span class="kw">draw.image</span>(image.copy)</a></code></pre></div>

<p>We can apply the same changes to isolate the green and blue color schemes of the image.</p>

<div class="sourceCode" id="cb1987"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1987-1" data-line-number="1">image.copy =<span class="st"> </span>image; image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)] =<span class="st"> </span><span class="dv">0</span>  <span class="co"># green</span></a>
<a class="sourceLine" id="cb1987-2" data-line-number="2"><span class="kw">draw.image</span>(image.copy)</a>
<a class="sourceLine" id="cb1987-3" data-line-number="3"></a>
<a class="sourceLine" id="cb1987-4" data-line-number="4">image.copy =<span class="st"> </span>image; image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)] =<span class="st"> </span><span class="dv">0</span>  <span class="co"># blue</span></a>
<a class="sourceLine" id="cb1987-5" data-line-number="5"><span class="kw">draw.image</span>(image.copy)</a></code></pre></div>

<p>Figure <a href="deeplearning1.html#fig:appleorange1">12.24</a> gives us the original image and the three images in separate RGB color schemes.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange1"></span>
<img src="appleorange1.png" alt="Apple, Oranges, and Banana" width="80%" />
<p class="caption">
Figure 12.24: Apple, Oranges, and Banana
</p>
</div>
<p>Equivalently, Figure <a href="deeplearning1.html#fig:appleorange2">12.25</a> gives us an enhanced version of the original image and three images in separate grayscale color schemes using the following tricks:</p>

<div class="sourceCode" id="cb1988"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1988-1" data-line-number="1">image.copy =<span class="st"> </span>image</a>
<a class="sourceLine" id="cb1988-2" data-line-number="2">image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,] =<span class="st"> </span><span class="kw">ifelse</span>(image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.50</span>, <span class="dv">0</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1988-3" data-line-number="3"><span class="kw">draw.image</span>(image.copy)</a>
<a class="sourceLine" id="cb1988-4" data-line-number="4"></a>
<a class="sourceLine" id="cb1988-5" data-line-number="5">image.copy =<span class="st"> </span>image</a>
<a class="sourceLine" id="cb1988-6" data-line-number="6">image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,] =<span class="st"> </span>image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span>]  <span class="op">*</span><span class="st"> </span>( <span class="dv">250</span> <span class="op">/</span><span class="st"> </span><span class="dv">255</span> )</a>
<a class="sourceLine" id="cb1988-7" data-line-number="7"><span class="kw">draw.image</span>(image.copy)</a>
<a class="sourceLine" id="cb1988-8" data-line-number="8"></a>
<a class="sourceLine" id="cb1988-9" data-line-number="9">image.copy =<span class="st"> </span>image</a>
<a class="sourceLine" id="cb1988-10" data-line-number="10">image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,] =<span class="st"> </span>image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">2</span>]  <span class="op">*</span><span class="st"> </span>( <span class="dv">250</span> <span class="op">/</span><span class="st"> </span><span class="dv">255</span> )</a>
<a class="sourceLine" id="cb1988-11" data-line-number="11"><span class="kw">draw.image</span>(image.copy)</a>
<a class="sourceLine" id="cb1988-12" data-line-number="12"></a>
<a class="sourceLine" id="cb1988-13" data-line-number="13">image.copy =<span class="st"> </span>image</a>
<a class="sourceLine" id="cb1988-14" data-line-number="14">image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,] =<span class="st"> </span>image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">3</span>]  <span class="op">*</span><span class="st"> </span>( <span class="dv">250</span> <span class="op">/</span><span class="st"> </span><span class="dv">255</span> )</a>
<a class="sourceLine" id="cb1988-15" data-line-number="15"><span class="kw">draw.image</span>(image.copy)</a></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange2"></span>
<img src="appleorange2.png" alt="Apple, Oranges, and Banana" width="80%" />
<p class="caption">
Figure 12.25: Apple, Oranges, and Banana
</p>
</div>
<p>Given the ability to manipulate the image tensor by hand, there are other more creative ways to apply effects to the image. In the context of <strong>CNN</strong>, we use <strong>Filters</strong> to manipulate images. The purpose is to <strong>highlight</strong> image properties that are readily identifiable and classifiable. This is where we discuss <strong>Convolution</strong>.</p>
</div>
<div id="convolution" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.2</span> Convolution <a href="deeplearning1.html#convolution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>convolutional</strong> nature of <strong>CNN</strong> comes with the fact that <strong>CNN</strong> assumes an input image, performs an <strong>element-wise</strong> multiplication with a <strong>filter</strong>, then sums the product to produce a <strong>feature map</strong>. We demonstrate the operation using Figure <a href="deeplearning1.html#fig:convolution">12.26</a>. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:convolution"></span>
<img src="convolution.png" alt="Convolution" width="90%" />
<p class="caption">
Figure 12.26: Convolution
</p>
</div>
<p>The idea is to use a <strong>Filter</strong>, also called <strong>Kernel</strong>, which is a 2D matrix (in the case of Figure <a href="deeplearning1.html#fig:convolution">12.26</a>). We slide the Filter over the input image in the direction from left to right and top to bottom, one step at a time. A <strong>step</strong> is called a <strong>stride</strong>, which dictates the pace or number of <strong>pixels</strong> to skip horizontally and vertically. As the <strong>Kernel</strong> lands on a <strong>patch</strong> which has the same dimension as the Kernel, we perform an <strong>element-wise</strong> multiplication and then sum the product. For example, the first patch (2x2) consists of the following pixels (1,2,5,6). We then multiply the patch with the Kernel like so:</p>
<p><span class="math display">\[
1 \times 1 + 2\times 0 + 5 \times 0 + 6 \times 1  = 7
\]</span></p>
<p>A <strong>Patch</strong> may also reference the terms <strong>receptive field</strong>, <strong>image patch</strong>, and <strong>convolution patch</strong>. The next patch (2x2), after 1st stride, consists of the following pixels (5,6,9,10). We then multiply the patch with the kernel like so:   </p>
<p><span class="math display">\[
5 \times 1 + 6 \times 0 + 9 \times 0 + 10 \times 1  = 15
\]</span></p>
<p>We perform the same operation for subsequent patches until we form a new matrix called <strong>Feature Map</strong>, also called <strong>Convolved Feature</strong>. </p>
<p>We can repeat the process by performing the same steps, yielding a new <strong>Feature Map</strong>. For example, in Figure <a href="deeplearning1.html#fig:convolved">12.27</a>, our <strong>Feature Map</strong> becomes an input and goes through another round of convolution using a smaller filter (2x2) with one stride.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:convolved"></span>
<img src="convolved.png" alt="Convolution (Repeated)" width="70%" />
<p class="caption">
Figure 12.27: Convolution (Repeated)
</p>
</div>
<p>It is also possible to perform convolution, resulting in a new <strong>Feature Map</strong> with the same dimension as the original Input Image. To achieve this, where the kernel crosses over the edges of an image, we use <strong>padding</strong> with zero values. See Figure <a href="deeplearning1.html#fig:convolved1">12.28</a>. The operation uses one padding and one stride. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:convolved1"></span>
<img src="convolved1.png" alt="Repeated Convolution with Padding" width="70%" />
<p class="caption">
Figure 12.28: Repeated Convolution with Padding
</p>
</div>
<p>Convolution is expressed using the following general formulation (with padding and a stride of one):</p>
<p><span class="math display">\[\begin{align}
M_{(\text{feature.map})} = \sum_d^D \sum_{(i=-p)}^{(H+p)} \sum_{(j=-p)}^{(W+p)} I(h_s: h_e, w_s : w_e, d) * K(,,d)
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>H</strong>, <strong>,W</strong>, <strong>,D</strong> are the dimensions of an image (e.g.Â height, width, depth) ,</li>
<li><strong>k</strong> is the size of the kernel,</li>
<li><strong>I</strong> is the input image with dim. (<span class="math inline">\(H \times W \times D\)</span>),</li>
<li><strong>K</strong> is the kernel with dim. (<span class="math inline">\(kr \times kc \times D\)</span>),</li>
<li><strong>p</strong> is the padding,</li>
<li><strong>s</strong> is the stride.</li>
</ul>
<p>For the rest of <strong>CNN</strong> discussions, we assume <strong>symmetric or equal</strong> paddings along the edges if required.</p>
<p>The dimension (<span class="math inline">\(O \times O\)</span>) of the <strong>feature map</strong> has the <strong>O</strong> size calculated as such <span class="citation">(Dumoulin V., Visin F. <a href="bibliography.html#ref-ref1144v">2018</a>)</span>:</p>
<p><span class="math display">\[\begin{align}
O = (W - K + 2p) / s + 1
\end{align}\]</span></p>
<p>For our purposes, especially if implemented in <strong>R</strong>, we assume one (and not zero) as starting index. Therefore, to construct the <strong>receptive field</strong>, we use the following pseudocode and formulation:</p>
<p><span class="math display">\[\begin{align}
\{\ h_i = (s + 1) \times i \ \}_{i=1}^{(H+2p)} \ \ \ \ \ \ \ \ \  \{\ w_j = (s + 1) \times j \ \}_{j=1}^{(W+2p)}
\end{align}\]</span></p>
<p><span class="math display">\[
\begin{array}{ll}
\text{for i in }\{1, ..., O\}: \\
\ \ \ \ \ \text{for j in }\{1, ..., O\}: \\
\ \ \ \ \ \ \ \ \ \  h_s=h_i; \ \ \ \ he = (hs + r - 1)\\
\ \ \ \ \ \ \ \ \ \ w_s=w_j;\ \ \ \ we = (ws + r - 1)  \\ 
\ \ \ \ \ \ \ \ \ \ ...\  I[h_s: h_e, w_s, w_e]\ ...
\end{array}
\]</span></p>
<p>The <strong>receptive field</strong> is bounded by the four hyperparameters, namely <span class="math inline">\(\mathbf{h_s}\)</span>, <span class="math inline">\(\mathbf{h_e}\)</span>, <span class="math inline">\(\mathbf{w_s}\)</span>, and <span class="math inline">\(\mathbf{w_e}\)</span> which respectively represent the starting and ending indices of its height and width.</p>
<p>Finally, let us review our example implementation of <strong>Convolution</strong>. Our implementation allows the ability to dilate both the image and the kernel and cache the indices and size that form the receptive fields. This capability is essential during backpropagation in the <strong>Pooling Layer</strong>, discussed later.</p>

<div class="sourceCode" id="cb1989"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1989-1" data-line-number="1"><span class="kw">library</span>(dequer)</a>
<a class="sourceLine" id="cb1989-2" data-line-number="2">convolution &lt;-<span class="st"> </span><span class="cf">function</span>(images, filter, </a>
<a class="sourceLine" id="cb1989-3" data-line-number="3">                 <span class="dt">cur.fmaps =</span> <span class="ot">NULL</span>, <span class="dt">dw.kernel =</span> <span class="ot">NULL</span>, <span class="dt">pw.kernel =</span> <span class="ot">NULL</span>,</a>
<a class="sourceLine" id="cb1989-4" data-line-number="4">                 <span class="dt">Dout=</span><span class="ot">NULL</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding =</span> <span class="dv">0</span>, <span class="dt">dil_rate=</span><span class="dv">0</span>, <span class="dt">dil_input=</span><span class="dv">0</span>, </a>
<a class="sourceLine" id="cb1989-5" data-line-number="5">                 <span class="dt">autopad =</span> <span class="ot">NULL</span>, <span class="dt">auto.pad=</span><span class="dv">0</span>, <span class="dt">normalize=</span><span class="ot">FALSE</span>, <span class="dt">clipping=</span><span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb1989-6" data-line-number="6">                 <span class="dt">afunc=</span><span class="ot">NULL</span>, <span class="dt">pool.cache =</span> <span class="ot">NULL</span>, <span class="dt">pool =</span> <span class="ot">NULL</span>, <span class="dt">bias =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1989-7" data-line-number="7">                 <span class="dt">dw.bias =</span> <span class="ot">NULL</span>, <span class="dt">pw.bias =</span> <span class="ot">NULL</span>, <span class="dt">ptype=</span><span class="st">&quot;convolv&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-8" data-line-number="8">  gaps      =<span class="st"> </span>dil_input</a>
<a class="sourceLine" id="cb1989-9" data-line-number="9">  d         =<span class="st"> </span><span class="kw">dim</span>(images)</a>
<a class="sourceLine" id="cb1989-10" data-line-number="10">  img.h     =<span class="st"> </span>d[<span class="dv">1</span>]; img.w =<span class="st"> </span>d[<span class="dv">2</span>]; img.d =<span class="st"> </span>d[<span class="dv">3</span>]; img.s =<span class="st"> </span>d[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb1989-11" data-line-number="11">  padded    =<span class="st"> </span><span class="kw">pad</span>(<span class="kw">dilate</span>(images, dil_input), filter, padding)</a>
<a class="sourceLine" id="cb1989-12" data-line-number="12">  image     =<span class="st"> </span>padded<span class="op">$</span>image</a>
<a class="sourceLine" id="cb1989-13" data-line-number="13">  padding   =<span class="st"> </span>padded<span class="op">$</span>padding</a>
<a class="sourceLine" id="cb1989-14" data-line-number="14">  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(autopad)) {</a>
<a class="sourceLine" id="cb1989-15" data-line-number="15">      extra     =<span class="st"> </span><span class="kw">autopad</span>(image, filter, padding, stride, <span class="dt">edge=</span>autopad,</a>
<a class="sourceLine" id="cb1989-16" data-line-number="16">                          auto.pad)</a>
<a class="sourceLine" id="cb1989-17" data-line-number="17">      image     =<span class="st"> </span>extra<span class="op">$</span>image</a>
<a class="sourceLine" id="cb1989-18" data-line-number="18">      auto.pad  =<span class="st"> </span>extra<span class="op">$</span>pad</a>
<a class="sourceLine" id="cb1989-19" data-line-number="19">  } </a>
<a class="sourceLine" id="cb1989-20" data-line-number="20">  di =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb1989-21" data-line-number="21">  new.h =<span class="st"> </span>di[<span class="dv">1</span>]; new.w =<span class="st"> </span>di[<span class="dv">2</span>]; new.d =<span class="st"> </span>di[<span class="dv">3</span>]; new.s =<span class="st"> </span>di[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb1989-22" data-line-number="22">  K         =<span class="st"> </span><span class="kw">dilate</span>(filter, dil_rate)</a>
<a class="sourceLine" id="cb1989-23" data-line-number="23">  r         =<span class="st"> </span><span class="kw">nrow</span>(K)</a>
<a class="sourceLine" id="cb1989-24" data-line-number="24">  c         =<span class="st"> </span><span class="kw">ncol</span>(K)</a>
<a class="sourceLine" id="cb1989-25" data-line-number="25">  bkprop.cache =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1989-26" data-line-number="26">  O =<span class="st">   </span><span class="kw">floor</span>( (new.w <span class="op">-</span><span class="st"> </span>c) <span class="op">/</span><span class="st"> </span>stride <span class="op">+</span><span class="st"> </span><span class="dv">1</span> ) </a>
<a class="sourceLine" id="cb1989-27" data-line-number="27">  <span class="cf">if</span> (new.h <span class="op">&lt;=</span><span class="st"> </span>r) <span class="kw">error</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb1989-28" data-line-number="28">  <span class="cf">if</span> (O <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>) { <span class="kw">return</span>(<span class="ot">NULL</span>)}</a>
<a class="sourceLine" id="cb1989-29" data-line-number="29">  h =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">1</span>, <span class="dt">to=</span>new.h, <span class="dt">by=</span>stride)</a>
<a class="sourceLine" id="cb1989-30" data-line-number="30">  w =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">1</span>, <span class="dt">to=</span>new.w, <span class="dt">by=</span>stride)</a>
<a class="sourceLine" id="cb1989-31" data-line-number="31">  I =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1989-32" data-line-number="32">  n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1989-33" data-line-number="33">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;maxpool&quot;</span> <span class="op">||</span><span class="st"> </span>ptype <span class="op">==</span><span class="st"> &quot;avgpool&quot;</span>) </a>
<a class="sourceLine" id="cb1989-34" data-line-number="34">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb1989-35" data-line-number="35">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb1989-36" data-line-number="36">        n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1989-37" data-line-number="37">        hs =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1989-38" data-line-number="38">        ws =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1989-39" data-line-number="39">        I[[n]] =<span class="st"> </span><span class="kw">array</span>(image[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,], <span class="kw">c</span>(r, c, di[<span class="dv">3</span>], di[<span class="dv">4</span>])) </a>
<a class="sourceLine" id="cb1989-40" data-line-number="40">    }</a>
<a class="sourceLine" id="cb1989-41" data-line-number="41">  }</a>
<a class="sourceLine" id="cb1989-42" data-line-number="42">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;convolv&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-43" data-line-number="43">      feature.map =<span class="st"> </span><span class="kw">convolve.image</span>(image, dw.kernel, pw.kernel, </a>
<a class="sourceLine" id="cb1989-44" data-line-number="44">                                   dw.bias, pw.bias, bias, dil_rate, </a>
<a class="sourceLine" id="cb1989-45" data-line-number="45">                                   O, h, w, r, c)</a>
<a class="sourceLine" id="cb1989-46" data-line-number="46">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1989-47" data-line-number="47">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;gradient.I.wrt.K&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-48" data-line-number="48">      feature.map =<span class="st"> </span><span class="kw">gradient.I.wrt.K</span>(image, K, O, h, w, r, c)</a>
<a class="sourceLine" id="cb1989-49" data-line-number="49">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1989-50" data-line-number="50">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;gradient.K.wrt.I&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-51" data-line-number="51">      feature.map =<span class="st"> </span><span class="kw">gradient.K.wrt.I</span>(image, Dout, cur.fmaps, pw.kernel, </a>
<a class="sourceLine" id="cb1989-52" data-line-number="52">                                     dil_rate, O, h, w)</a>
<a class="sourceLine" id="cb1989-53" data-line-number="53">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1989-54" data-line-number="54">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;gradient.I.wrt.P&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-55" data-line-number="55">      feature.map =<span class="st"> </span><span class="kw">gradient.I.wrt.P</span>(image, Dout, K, O, pool, h, w,</a>
<a class="sourceLine" id="cb1989-56" data-line-number="56">                                     pool.cache)</a>
<a class="sourceLine" id="cb1989-57" data-line-number="57">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1989-58" data-line-number="58">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;maxpool&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-59" data-line-number="59">      di =<span class="st"> </span><span class="kw">dim</span>(I[[<span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb1989-60" data-line-number="60">      img.s   =<span class="st"> </span>di[<span class="dv">4</span>] </a>
<a class="sourceLine" id="cb1989-61" data-line-number="61">      im2col  =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(I), <span class="kw">c</span>( di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>],  di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s <span class="op">*</span><span class="st"> </span>n))</a>
<a class="sourceLine" id="cb1989-62" data-line-number="62">      im2max  =<span class="st"> </span><span class="kw">apply</span>(im2col, <span class="dv">2</span>, max)</a>
<a class="sourceLine" id="cb1989-63" data-line-number="63">      pool.cache =<span class="st"> </span><span class="kw">apply</span>(im2col, <span class="dv">2</span>, which.max)</a>
<a class="sourceLine" id="cb1989-64" data-line-number="64">      im2col  =<span class="st"> </span><span class="kw">array</span>( im2max, <span class="kw">c</span>(di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s, O, O))</a>
<a class="sourceLine" id="cb1989-65" data-line-number="65">      max.channel =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1989-66" data-line-number="66">      <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s)) {</a>
<a class="sourceLine" id="cb1989-67" data-line-number="67">          max.channel[[d]] =<span class="st"> </span><span class="kw">t</span>(im2col[d,,])</a>
<a class="sourceLine" id="cb1989-68" data-line-number="68">      }</a>
<a class="sourceLine" id="cb1989-69" data-line-number="69">      max.channel =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(max.channel), <span class="kw">c</span>(O, O, di[<span class="dv">3</span>], img.s))</a>
<a class="sourceLine" id="cb1989-70" data-line-number="70">      feature.map =<span class="st"> </span>max.channel</a>
<a class="sourceLine" id="cb1989-71" data-line-number="71">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1989-72" data-line-number="72">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;avgpool&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-73" data-line-number="73">      di      =<span class="st"> </span><span class="kw">dim</span>(I[[<span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb1989-74" data-line-number="74">      img.s   =<span class="st"> </span>di[<span class="dv">4</span>] </a>
<a class="sourceLine" id="cb1989-75" data-line-number="75">      im2col  =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(I), <span class="kw">c</span>( di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>],  di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s <span class="op">*</span><span class="st"> </span>n))</a>
<a class="sourceLine" id="cb1989-76" data-line-number="76">      im2mean =<span class="st"> </span>pool.cache =<span class="st"> </span><span class="kw">colMeans</span>(im2col)</a>
<a class="sourceLine" id="cb1989-77" data-line-number="77">      im2col  =<span class="st"> </span><span class="kw">array</span>( im2mean, <span class="kw">c</span>(di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s, O, O))</a>
<a class="sourceLine" id="cb1989-78" data-line-number="78">      mean.channel =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1989-79" data-line-number="79">      <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s)) {</a>
<a class="sourceLine" id="cb1989-80" data-line-number="80">          mean.channel[[d]] =<span class="st"> </span><span class="kw">t</span>(im2col[d,,])</a>
<a class="sourceLine" id="cb1989-81" data-line-number="81">      }</a>
<a class="sourceLine" id="cb1989-82" data-line-number="82">      mean.channel =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(mean.channel), <span class="kw">c</span>(O, O, di[<span class="dv">3</span>], img.s))</a>
<a class="sourceLine" id="cb1989-83" data-line-number="83">      feature.map =<span class="st"> </span>mean.channel</a>
<a class="sourceLine" id="cb1989-84" data-line-number="84">  } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1989-85" data-line-number="85">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;mask&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-86" data-line-number="86">      img.copy =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, di)</a>
<a class="sourceLine" id="cb1989-87" data-line-number="87">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb1989-88" data-line-number="88">        <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb1989-89" data-line-number="89">            hs =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1989-90" data-line-number="90">            ws =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1989-91" data-line-number="91">            img.copy[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,] =<span class="st"> </span></a>
<a class="sourceLine" id="cb1989-92" data-line-number="92"><span class="st">               </span><span class="kw">sum</span>( <span class="kw">array</span>(image[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,], <span class="kw">c</span>(r, c, di[<span class="dv">3</span>], di[<span class="dv">4</span>])) <span class="op">*</span><span class="st"> </span>K)</a>
<a class="sourceLine" id="cb1989-93" data-line-number="93">        }</a>
<a class="sourceLine" id="cb1989-94" data-line-number="94">      }</a>
<a class="sourceLine" id="cb1989-95" data-line-number="95">      feature.map =<span class="st"> </span>img.copy</a>
<a class="sourceLine" id="cb1989-96" data-line-number="96">  } </a>
<a class="sourceLine" id="cb1989-97" data-line-number="97">  <span class="cf">if</span> (normalize<span class="op">==</span><span class="ot">TRUE</span>) {  </a>
<a class="sourceLine" id="cb1989-98" data-line-number="98">     mx =<span class="st"> </span><span class="kw">max</span>(feature.map)</a>
<a class="sourceLine" id="cb1989-99" data-line-number="99">     mn =<span class="st"> </span><span class="kw">min</span>(feature.map)</a>
<a class="sourceLine" id="cb1989-100" data-line-number="100">     feature.map =<span class="st"> </span>(feature.map <span class="op">-</span><span class="st"> </span>mn) <span class="op">/</span><span class="st"> </span>(mx <span class="op">-</span><span class="st"> </span>mn)   </a>
<a class="sourceLine" id="cb1989-101" data-line-number="101">  } </a>
<a class="sourceLine" id="cb1989-102" data-line-number="102">  <span class="cf">if</span> (clipping<span class="op">==</span><span class="ot">TRUE</span>) { </a>
<a class="sourceLine" id="cb1989-103" data-line-number="103">     feature.map =<span class="st">  </span><span class="kw">pmax</span>(<span class="kw">pmin</span>( feature.map, <span class="dv">1</span>), <span class="dv">0</span>) </a>
<a class="sourceLine" id="cb1989-104" data-line-number="104">  }</a>
<a class="sourceLine" id="cb1989-105" data-line-number="105">  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(afunc)) {</a>
<a class="sourceLine" id="cb1989-106" data-line-number="106">     feature.map =<span class="st"> </span><span class="kw">activation</span>(feature.map, <span class="kw">get</span>(afunc))</a>
<a class="sourceLine" id="cb1989-107" data-line-number="107">  }</a>
<a class="sourceLine" id="cb1989-108" data-line-number="108">  bkprop.cache =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;stride&quot;</span>   =<span class="st"> </span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1989-109" data-line-number="109">      <span class="st">&quot;padding&quot;</span>    =<span class="st"> </span><span class="kw">max</span>(r <span class="op">-</span><span class="st"> </span>(padding <span class="op">+</span><span class="st"> </span>auto.pad <span class="op">+</span><span class="st"> </span><span class="dv">1</span>), <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb1989-110" data-line-number="110">      <span class="st">&quot;dil_rate&quot;</span>   =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;dil_input&quot;</span>  =<span class="st"> </span>stride<span class="dv">-1</span>, <span class="st">&quot;auto.pad&quot;</span> =<span class="st"> </span>auto.pad,</a>
<a class="sourceLine" id="cb1989-111" data-line-number="111">      <span class="st">&quot;autopad&quot;</span>    =<span class="st"> &quot;left&quot;</span>,</a>
<a class="sourceLine" id="cb1989-112" data-line-number="112">      <span class="st">&quot;wstride&quot;</span>    =<span class="st"> </span><span class="dv">1</span>,   <span class="st">&quot;wpadding&quot;</span> =<span class="st"> </span>padding, <span class="st">&quot;wdil_rate&quot;</span> =<span class="st"> </span>stride <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb1989-113" data-line-number="113">      <span class="st">&quot;wdil_input&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;wauto.pad&quot;</span>  =<span class="st"> </span>auto.pad,</a>
<a class="sourceLine" id="cb1989-114" data-line-number="114">      <span class="st">&quot;wautopad&quot;</span>   =<span class="st"> </span><span class="kw">ifelse</span>(auto.pad <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;right.backprop&quot;</span>, <span class="st">&quot;left.backprop&quot;</span>)</a>
<a class="sourceLine" id="cb1989-115" data-line-number="115">         )</a>
<a class="sourceLine" id="cb1989-116" data-line-number="116">  <span class="kw">list</span>(<span class="st">&quot;feature.map&quot;</span> =<span class="st"> </span>feature.map, <span class="st">&quot;bkprop.cache&quot;</span> =<span class="st"> </span>bkprop.cache, </a>
<a class="sourceLine" id="cb1989-117" data-line-number="117">       <span class="st">&quot;pool.cache&quot;</span>  =<span class="st"> </span>pool.cache, <span class="st">&quot;auto.pad&quot;</span> =<span class="st"> </span>auto.pad, <span class="st">&quot;image&quot;</span> =<span class="st"> </span>image)</a>
<a class="sourceLine" id="cb1989-118" data-line-number="118">}</a></code></pre></div>

<p>Note that in the context of graphical images in which we need to respect the color intensity boundary, we implement the option of normalizing or scaling the feature map back to the allowable range between 0 and 1 after convolution and after the option of clipping to a boundary. This option is only for visual or graphical interpretation - for example, if only just because we need to see the visual effect experimentally. On the other hand, when we discuss <strong>feed-forward</strong> in <strong>CNN</strong>, we primarily use <strong>RELU</strong> or <strong>Leaky RELU</strong> as activation function right after the convolution operation - another way of clipping a value to a lower bound.</p>
</div>
<div id="stride-and-padding" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.3</span> Stride and Padding  <a href="deeplearning1.html#stride-and-padding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In determining the optimal stride and padding for our <strong>CNN</strong>, we need to consider how we should avoid losing information which happens in a case in which an input of size <span class="math inline">\(4\times 4\)</span> and a filter of size <span class="math inline">\(3\times 3\)</span> force us to drop the right side edge and bottom edge of the input if using stride equal to two. Ideally, we can pad the edges with zeroes. Figure <a href="deeplearning1.html#fig:paddingstride">12.29</a> shows an example of a convolution with no padding and two convolutions with paddings.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:paddingstride"></span>
<img src="paddingstride.png" alt="Stride and Padding" width="80%" />
<p class="caption">
Figure 12.29: Stride and Padding
</p>
</div>
<p>Based on the figure, each choice of padding and stride affects the size of the feature map and thus also the information it produces. We may opt to use a stride equal to one with no padding in our case. That gives us a smaller <span class="math inline">\(2 \times 2\)</span> feature map while preserving information.</p>
<p>Below is our example implementation of <strong>Convolution</strong> with <strong>Padding</strong> following the equation. In the implementation, we include <strong>Auto Padding</strong> and <strong>Dilation</strong>, which we cover in a section up ahead.</p>

<div class="sourceCode" id="cb1990"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1990-1" data-line-number="1"><span class="co"># Assumes symmetric matrix for the kernels</span></a>
<a class="sourceLine" id="cb1990-2" data-line-number="2">pad &lt;-<span class="st"> </span><span class="cf">function</span>(image, filter, padding) {</a>
<a class="sourceLine" id="cb1990-3" data-line-number="3">  <span class="cf">if</span> (padding <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span>) { <span class="kw">return</span> ( <span class="kw">list</span>(<span class="st">&quot;image&quot;</span> =<span class="st"> </span>image, <span class="st">&quot;padding&quot;</span> =<span class="st"> </span>padding))}</a>
<a class="sourceLine" id="cb1990-4" data-line-number="4">  di        =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb1990-5" data-line-number="5">  fi        =<span class="st"> </span><span class="kw">dim</span>(filter)</a>
<a class="sourceLine" id="cb1990-6" data-line-number="6">  img.h     =<span class="st"> </span>di[<span class="dv">1</span>]; kernel.h =<span class="st"> </span>fi[<span class="dv">1</span>]  </a>
<a class="sourceLine" id="cb1990-7" data-line-number="7">  img.w     =<span class="st"> </span>di[<span class="dv">2</span>]; kernel.w =<span class="st"> </span>fi[<span class="dv">2</span>] </a>
<a class="sourceLine" id="cb1990-8" data-line-number="8">  img.d     =<span class="st"> </span>di[<span class="dv">3</span>]; kernel.d =<span class="st"> </span>fi[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb1990-9" data-line-number="9">  img.s     =<span class="st"> </span>di[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb1990-10" data-line-number="10">  <span class="cf">if</span> (padding <span class="op">&gt;=</span><span class="st"> </span>kernel.h) { <span class="co"># limit paddings</span></a>
<a class="sourceLine" id="cb1990-11" data-line-number="11">      padding =<span class="st"> </span>kernel.h <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1990-12" data-line-number="12">  } </a>
<a class="sourceLine" id="cb1990-13" data-line-number="13">  h =<span class="st"> </span>(img.h <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>padding)  </a>
<a class="sourceLine" id="cb1990-14" data-line-number="14">  w =<span class="st"> </span>(img.w <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>padding)  </a>
<a class="sourceLine" id="cb1990-15" data-line-number="15">  img =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(h,w, img.d, img.s))</a>
<a class="sourceLine" id="cb1990-16" data-line-number="16">  img[(<span class="dv">1</span><span class="op">+</span>padding)<span class="op">:</span>(img.h<span class="op">+</span>padding),(<span class="dv">1</span><span class="op">+</span>padding)<span class="op">:</span>(img.w<span class="op">+</span>padding),,] =<span class="st"> </span>image </a>
<a class="sourceLine" id="cb1990-17" data-line-number="17">  <span class="kw">list</span>(<span class="st">&quot;image&quot;</span> =<span class="st"> </span>img, <span class="st">&quot;padding&quot;</span> =<span class="st"> </span>padding)</a>
<a class="sourceLine" id="cb1990-18" data-line-number="18">}</a></code></pre></div>

<p>In our implementation, we restrict the number of zero paddings allowed in our <strong>CNN</strong> such that the number of zero paddings cannot be greater than the size of the kernel size, which is a waste of space.</p>
<p><span class="math display">\[\begin{align}
p = \begin{cases}
kr - 1 &amp; p \ge kr\\
p &amp; otherwise
\end{cases} \label{eqn:eqnnumber609}
\end{align}\]</span></p>
<p>Additionally, to preserve information, we automatically add extra paddings to the right and bottom of the image to ensure that we can filter pixels along the edges of the image. In our example implementation below, we allow forcing extra paddings to the right or left edges of the image (as required by backpropagation, for example). Furthermore, if the overall zero paddings exceed the size of the kernel, then we disregard the extra paddings.</p>

<div class="sourceCode" id="cb1991"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1991-1" data-line-number="1">autopad &lt;-<span class="st"> </span><span class="cf">function</span>(image, filter, padding, stride, <span class="dt">edge=</span><span class="st">&quot;right&quot;</span>, </a>
<a class="sourceLine" id="cb1991-2" data-line-number="2">                    <span class="dt">auto.pad =</span> <span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1991-3" data-line-number="3">  di        =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb1991-4" data-line-number="4">  fi        =<span class="st"> </span><span class="kw">dim</span>(filter)</a>
<a class="sourceLine" id="cb1991-5" data-line-number="5">  img.h     =<span class="st"> </span>di[<span class="dv">1</span>]; kernel.h =<span class="st"> </span>fi[<span class="dv">1</span>]  </a>
<a class="sourceLine" id="cb1991-6" data-line-number="6">  img.w     =<span class="st"> </span>di[<span class="dv">2</span>]; kernel.w =<span class="st"> </span>fi[<span class="dv">2</span>] </a>
<a class="sourceLine" id="cb1991-7" data-line-number="7">  img.d     =<span class="st"> </span>di[<span class="dv">3</span>]; kernel.d =<span class="st"> </span>fi[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb1991-8" data-line-number="8">  img.s     =<span class="st"> </span>di[<span class="dv">4</span>];</a>
<a class="sourceLine" id="cb1991-9" data-line-number="9">  <span class="cf">if</span> (edge <span class="op">==</span><span class="st"> &quot;right&quot;</span>) {</a>
<a class="sourceLine" id="cb1991-10" data-line-number="10">        extra.pad =<span class="st"> </span>(img.h <span class="op">-</span><span class="st"> </span>kernel.h) <span class="op">%%</span><span class="st"> </span>stride</a>
<a class="sourceLine" id="cb1991-11" data-line-number="11">        extra.pad =<span class="st"> </span><span class="kw">ifelse</span>(extra.pad <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, stride <span class="op">-</span><span class="st"> </span>extra.pad, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1991-12" data-line-number="12">  } <span class="cf">else</span>  {</a>
<a class="sourceLine" id="cb1991-13" data-line-number="13">        extra.pad =<span class="st"> </span>auto.pad</a>
<a class="sourceLine" id="cb1991-14" data-line-number="14">  }</a>
<a class="sourceLine" id="cb1991-15" data-line-number="15">  <span class="cf">if</span> (extra.pad <span class="op">==</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>extra.pad <span class="op">+</span><span class="st"> </span>padding <span class="op">&gt;=</span><span class="st"> </span>kernel.h) {</a>
<a class="sourceLine" id="cb1991-16" data-line-number="16">    <span class="kw">return</span>(<span class="kw">list</span>(<span class="st">&quot;image&quot;</span> =<span class="st"> </span>image, <span class="st">&quot;pad&quot;</span> =<span class="st"> </span><span class="dv">0</span>))  </a>
<a class="sourceLine" id="cb1991-17" data-line-number="17">  }     </a>
<a class="sourceLine" id="cb1991-18" data-line-number="18">  h =<span class="st"> </span>img.h <span class="op">+</span><span class="st"> </span>extra.pad</a>
<a class="sourceLine" id="cb1991-19" data-line-number="19">  w =<span class="st"> </span>img.w <span class="op">+</span><span class="st"> </span>extra.pad</a>
<a class="sourceLine" id="cb1991-20" data-line-number="20">  img =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(h, w, img.d, img.s))</a>
<a class="sourceLine" id="cb1991-21" data-line-number="21">  <span class="cf">if</span> (edge <span class="op">==</span><span class="st"> &quot;right&quot;</span> <span class="op">||</span><span class="st"> </span>edge<span class="op">==</span><span class="st">&quot;right.backprop&quot;</span>) {</a>
<a class="sourceLine" id="cb1991-22" data-line-number="22">    img[<span class="dv">1</span><span class="op">:</span>img.h,<span class="dv">1</span><span class="op">:</span>img.w,,] =<span class="st"> </span>image    </a>
<a class="sourceLine" id="cb1991-23" data-line-number="23">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1991-24" data-line-number="24">    img[(<span class="dv">1</span><span class="op">+</span>extra.pad)<span class="op">:</span>(img.h<span class="op">+</span>extra.pad),(<span class="dv">1</span><span class="op">+</span>extra.pad)<span class="op">:</span>(img.w<span class="op">+</span>extra.pad),,]=<span class="st"> </span></a>
<a class="sourceLine" id="cb1991-25" data-line-number="25"><span class="st">      </span>image</a>
<a class="sourceLine" id="cb1991-26" data-line-number="26">  }</a>
<a class="sourceLine" id="cb1991-27" data-line-number="27">  <span class="kw">list</span>(<span class="st">&quot;image&quot;</span> =<span class="st"> </span>img, <span class="st">&quot;pad&quot;</span> =<span class="st"> </span>extra.pad)</a>
<a class="sourceLine" id="cb1991-28" data-line-number="28">}</a></code></pre></div>

</div>
<div id="kernels-and-filters" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.4</span> Kernels And Filters<a href="deeplearning1.html#kernels-and-filters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>filter</strong> is a composition of kernels. A filter may only consist of one Kernel, and thus perhaps the terms are interchangeable in that respect based on such cases. Applying a kernel or filter to an image is called <strong>masking</strong> or <strong>filtering</strong>.</p>
<p>In this section, we cover a few of the <strong>masking or filtering</strong> tricks based on the type of <strong>Kernel</strong>.</p>
<p>While graphically, we use <strong>kernels</strong> to mask an image, which results in <strong>blurring</strong>, <strong>smoothing</strong>, <strong>sharpening</strong>, <strong>embossing</strong>, <strong>edge detection</strong>, or even <strong>noise reduction</strong>, among many other effects, in <strong>CNN</strong>, we use <strong>kernels</strong> to preserve the <strong>spatial relationship</strong> of individual pixels, knowing that during convolution, the process may reduce the dimension of the image if we choose to in our architecture design.</p>
<p>To illustrate, let us form a list of <strong>kernels</strong> to use. Below, we list a few examples of a <strong>kernel</strong>:</p>

<p><span class="math display">\[
 \underbrace{\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 1 \\ 2 &amp; 0 &amp; -2 \\ 1 &amp; 0 &amp; -1 \end{array} \right]}_{\text{Sobel Kernel}}
 \ \ \ \ \ 
\underbrace{\left[ \begin{array}{rrr} -1 &amp; 0 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ -1 &amp; 0 &amp; -1 \end{array} \right]}_{\text{Negative Photo }}
\ \ \ \ \ \
\underbrace{ \left[ \begin{array}{rrr} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \end{array} \right]}_{\text{Embossed (Raised) }}
\ \ \ \ \ \ 
\underbrace{ \left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ -1 &amp; -1 &amp; -1 \end{array} \right]}_{\text{Embossed (Recessed) }}
\]</span>
</p>
<p>We can implement the kernels as follows:</p>

<div class="sourceCode" id="cb1992"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1992-1" data-line-number="1">sobel.kernel    =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>,  <span class="dv">1</span>, <span class="dv">-2</span>, <span class="dv">0</span>,  <span class="dv">2</span>, <span class="dv">-1</span>,  <span class="dv">0</span>,  <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1992-2" data-line-number="2">negative.photo  =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">-1</span>,  <span class="dv">0</span>, <span class="dv">0</span>,  <span class="dv">0</span>, <span class="dv">-1</span>,  <span class="dv">0</span>, <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb1992-3" data-line-number="3">raised.emboss   =<span class="st"> </span><span class="kw">c</span>( <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">-1</span>,  <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">-1</span>,  <span class="dv">1</span>,  <span class="dv">0</span>, <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb1992-4" data-line-number="4">recessed.emboss =<span class="st"> </span><span class="kw">c</span>( <span class="dv">1</span>, <span class="dv">1</span>,  <span class="dv">1</span>,  <span class="dv">0</span>, <span class="dv">0</span>,  <span class="dv">0</span>, <span class="dv">-1</span>, <span class="dv">-1</span>, <span class="dv">-1</span>)</a></code></pre></div>

<p>Finally, let us demonstrate the use of the <strong>convolution(.)</strong> function with a kernel. Below, we perform convolution against the image using different kernels (see the following ). Recall that because convolution may result in a color intensity beyond the range of 0 and 1, we use normalization as an option to scale the feature map back to the same range as required by a <strong>JPEG</strong> format; otherwise, we also can use clipping.</p>

<div class="sourceCode" id="cb1993"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1993-1" data-line-number="1">kernel       =<span class="st"> </span>sobel.kernel</a>
<a class="sourceLine" id="cb1993-2" data-line-number="2">filter       =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-3" data-line-number="3">apple.image  =<span class="st"> </span><span class="kw">array</span>(image, <span class="kw">c</span>(<span class="dv">500</span>,<span class="dv">500</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-4" data-line-number="4">sobel.img    =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter,  <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1993-5" data-line-number="5">                           <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb1993-6" data-line-number="6"><span class="co"># crude way of converting greyscale to white-black colors.</span></a>
<a class="sourceLine" id="cb1993-7" data-line-number="7">sobel.img [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,,] =<span class="st"> </span><span class="kw">ifelse</span>(sobel.img [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,,] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.50</span>, <span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1993-8" data-line-number="8"></a>
<a class="sourceLine" id="cb1993-9" data-line-number="9">kernel       =<span class="st"> </span>negative.photo</a>
<a class="sourceLine" id="cb1993-10" data-line-number="10">filter       =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-11" data-line-number="11">apple.image  =<span class="st"> </span><span class="kw">array</span>(image, <span class="kw">c</span>(<span class="dv">500</span>,<span class="dv">500</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-12" data-line-number="12">negative.img =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter,  <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1993-13" data-line-number="13">                           <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb1993-14" data-line-number="14"></a>
<a class="sourceLine" id="cb1993-15" data-line-number="15">kernel =<span class="st"> </span>raised.emboss</a>
<a class="sourceLine" id="cb1993-16" data-line-number="16">filter =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-17" data-line-number="17">raised.img =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter,  <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1993-18" data-line-number="18">                                <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb1993-19" data-line-number="19"></a>
<a class="sourceLine" id="cb1993-20" data-line-number="20">kernel       =<span class="st"> </span>recessed.emboss</a>
<a class="sourceLine" id="cb1993-21" data-line-number="21">filter       =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-22" data-line-number="22">recessed.img =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter, <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1993-23" data-line-number="23">                       <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a></code></pre></div>

<p>See Figure <a href="deeplearning1.html#fig:appleorange3">12.30</a> for the effects of the kernels ordered as follows: Sobel kernel, negative photo, embossed (raised), embossed (recessed).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange3"></span>
<img src="appleorange3.png" alt="Apple, Oranges, and Banana" width="80%" />
<p class="caption">
Figure 12.30: Apple, Oranges, and Banana
</p>
</div>
<p>Notice how the <strong>Sobel kernel</strong> can be used for <strong>edge detection</strong>; albeit, there may be other kernels that can mask just the same. For example, test the following kernel:</p>

<div class="sourceCode" id="cb1994"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1994-1" data-line-number="1">edge.kernel =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">-1</span>, <span class="dv">0</span>, <span class="dv">-1</span>, <span class="dv">4</span>, <span class="dv">-1</span>, <span class="dv">0</span>, <span class="dv">-1</span>, <span class="dv">0</span>)</a></code></pre></div>

<p>One of the important techniques in <strong>CNN</strong> is the ability to perform multiple convolution operations, each using a different kernel like so:</p>

<div class="sourceCode" id="cb1995"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1995-1" data-line-number="1">kernel       =<span class="st"> </span>sobel.kernel</a>
<a class="sourceLine" id="cb1995-2" data-line-number="2">filter       =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1995-3" data-line-number="3">apple.image  =<span class="st"> </span><span class="kw">array</span>(image, <span class="kw">c</span>(<span class="dv">500</span>,<span class="dv">500</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1995-4" data-line-number="4">sobel.img    =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter,  <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1995-5" data-line-number="5">                           <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb1995-6" data-line-number="6"></a>
<a class="sourceLine" id="cb1995-7" data-line-number="7">kernel       =<span class="st"> </span>negative.photo</a>
<a class="sourceLine" id="cb1995-8" data-line-number="8">filter       =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1995-9" data-line-number="9">negative.img =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter,  <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1995-10" data-line-number="10">                           <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb1995-11" data-line-number="11"></a>
<a class="sourceLine" id="cb1995-12" data-line-number="12">kernel   =<span class="st"> </span>edge.kernel</a>
<a class="sourceLine" id="cb1995-13" data-line-number="13">filter   =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1995-14" data-line-number="14">edge.img =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter, <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1995-15" data-line-number="15">                       <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a></code></pre></div>

<p>We can then combine the convolved result to form a new input image like so:</p>

<div class="sourceCode" id="cb1996"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1996-1" data-line-number="1">d =<span class="st"> </span><span class="kw">dim</span>(edge.img)</a>
<a class="sourceLine" id="cb1996-2" data-line-number="2">new.input.image =<span class="st"> </span><span class="kw">array</span>( <span class="kw">cbind</span>(sobel.img, negative.img, edge.img), </a>
<a class="sourceLine" id="cb1996-3" data-line-number="3">                         <span class="kw">c</span>(d[<span class="dv">1</span>], d[<span class="dv">2</span>], <span class="dv">3</span>, <span class="dv">1</span>))</a></code></pre></div>

<p>which yields the following dimension.</p>

<div class="sourceCode" id="cb1997"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1997-1" data-line-number="1"><span class="kw">dim</span>(new.input.image)</a></code></pre></div>
<pre><code>## [1] 500 500   3   1</code></pre>

<p>Along with the new image, we can also concoct another separate filter:</p>

<div class="sourceCode" id="cb1999"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1999-1" data-line-number="1">pattern =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>( <span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dv">15</span>), <span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>), <span class="dv">15</span>)), <span class="dt">nrow=</span><span class="dv">30</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1999-2" data-line-number="2">new.filter =<span class="st"> </span><span class="kw">array</span>( <span class="kw">cbind</span>(pattern, pattern, pattern), <span class="kw">c</span>(<span class="dv">30</span>, <span class="dv">30</span>, <span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1999-3" data-line-number="3"><span class="kw">dim</span>(new.filter)</a></code></pre></div>
<pre><code>## [1] 30 30  3  1</code></pre>

<p>Finally, we perform convolution against the new input image with the new filter (see Figure <a href="deeplearning1.html#fig:appleorange4">12.31</a>).</p>

<div class="sourceCode" id="cb2001"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2001-1" data-line-number="1">new.image =<span class="st"> </span><span class="kw">convolution</span>(new.input.image , <span class="dt">filter =</span> new.filter,  </a>
<a class="sourceLine" id="cb2001-2" data-line-number="2">                    <span class="dt">normalize=</span><span class="ot">TRUE</span>, <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb2001-3" data-line-number="3"><span class="kw">draw.image</span>(new.image)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange4"></span>
<img src="appleorange4.png" alt="Apple, Oranges, and Banana" width="60%" />
<p class="caption">
Figure 12.31: Apple, Oranges, and Banana
</p>
</div>

<p>Note that while we use a <span class="math inline">\(30 \times 30\)</span> kernel size for illustration purposes, literatures tend to use <span class="math inline">\(3 \times 3\)</span>, <span class="math inline">\(5 \times 5\)</span>, and <span class="math inline">\(7 \times 7\)</span> kernel sizes. Determining an optimal kernel size can be done heuristically, or it can be done by automatic or adaptive means. However, a notable emphasis is that as long as the kernel can preserve some level of correlation within local features (e.g., highlighting specific features of an image such as edges and others), then we can perhaps say that we are on the right track, at least for the sake of classification or pattern recognition. Hereafter, let us begin to use <strong>highlight</strong> to also refer to a <strong>feature</strong>.</p>
<p>On the other hand, we may start losing the highlights if we are not careful in choosing the correct kernel (let alone its size) - as an example of this is shown in Figure <a href="deeplearning1.html#fig:appleorange4">12.31</a> in which the image starts to blur too much. Irrespective of that, visualization becomes irrelevant as we start training <strong>CNN</strong> to optimize our kernels.</p>
</div>
<div id="dilation" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.5</span> Dilation <a href="deeplearning1.html#dilation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Dilation</strong> is the method of stretching or inflating a <strong>receptive field</strong> such that it skips or leaves gaps in between elements. For example, in Figure <a href="deeplearning1.html#fig:dilating">12.32</a>, we use a <strong>dilate rate</strong> of two, resulting in a feature map element of 453 after convolution. In general, a <strong>dilate rate</strong> of one is equivalent to a normal convolution with no gaps. Moreover, a <strong>dilate rate</strong> of two corresponds to a one-pixel gap.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dilating"></span>
<img src="dilating.png" alt="Dilation" width="80%" />
<p class="caption">
Figure 12.32: Dilation
</p>
</div>
<p>Below is our example implementation of <strong>Dilation</strong>. Note that for our own purposes only, we use a <strong>dilate rate</strong> equal to zero for normal convolution with no gaps. And a rate of one corresponds to a one-pixel gap.</p>

<div class="sourceCode" id="cb2002"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2002-1" data-line-number="1">dilate  &lt;-<span class="st"> </span><span class="cf">function</span>(image, <span class="dt">dil_rate=</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb2002-2" data-line-number="2">   <span class="cf">if</span> (dil_rate <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>) { <span class="kw">return</span>(image) }  </a>
<a class="sourceLine" id="cb2002-3" data-line-number="3">   d       =<span class="st"> </span><span class="kw">dim</span>(image) </a>
<a class="sourceLine" id="cb2002-4" data-line-number="4">   img.h   =<span class="st"> </span>d[<span class="dv">1</span>]; img.w =<span class="st"> </span>d[<span class="dv">2</span>]; img.d =<span class="st"> </span>d[<span class="dv">3</span>]; img.s =<span class="st"> </span>d[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb2002-5" data-line-number="5">   h =<span class="st"> </span>img.h <span class="op">+</span><span class="st"> </span>dil_rate <span class="op">*</span><span class="st"> </span>(img.h <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2002-6" data-line-number="6">   w =<span class="st"> </span>img.w <span class="op">+</span><span class="st"> </span>dil_rate <span class="op">*</span><span class="st"> </span>(img.w <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2002-7" data-line-number="7">   img =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(h,w, img.d, img.s))</a>
<a class="sourceLine" id="cb2002-8" data-line-number="8">   <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>img.h) {</a>
<a class="sourceLine" id="cb2002-9" data-line-number="9">     <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>img.w) {</a>
<a class="sourceLine" id="cb2002-10" data-line-number="10">       img[i<span class="op">*</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,j<span class="op">*</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,,] =<span class="st"> </span>image[i,j,,]</a>
<a class="sourceLine" id="cb2002-11" data-line-number="11">     }</a>
<a class="sourceLine" id="cb2002-12" data-line-number="12">   }</a>
<a class="sourceLine" id="cb2002-13" data-line-number="13">   img</a>
<a class="sourceLine" id="cb2002-14" data-line-number="14">}</a></code></pre></div>
<div class="sourceCode" id="cb2003"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2003-1" data-line-number="1">dilate.filters &lt;-<span class="st"> </span><span class="cf">function</span>(filters, <span class="dt">dil_rate =</span> <span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb2003-2" data-line-number="2">   <span class="cf">if</span> (dil_rate <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>) { <span class="kw">return</span>(filters) }  </a>
<a class="sourceLine" id="cb2003-3" data-line-number="3">   len =<span class="st"> </span><span class="kw">length</span>(filters)</a>
<a class="sourceLine" id="cb2003-4" data-line-number="4">   <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2003-5" data-line-number="5">       filter =<span class="st"> </span>filters[[l]]</a>
<a class="sourceLine" id="cb2003-6" data-line-number="6">       d       =<span class="st"> </span><span class="kw">dim</span>(filter) </a>
<a class="sourceLine" id="cb2003-7" data-line-number="7">       img.h   =<span class="st"> </span>d[<span class="dv">1</span>]; img.w =<span class="st"> </span>d[<span class="dv">2</span>]; img.d =<span class="st"> </span>d[<span class="dv">3</span>]; img.s =<span class="st"> </span>d[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb2003-8" data-line-number="8">       h =<span class="st"> </span>img.h <span class="op">+</span><span class="st"> </span>dil_rate <span class="op">*</span><span class="st"> </span>(img.h <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2003-9" data-line-number="9">       w =<span class="st"> </span>img.w <span class="op">+</span><span class="st"> </span>dil_rate <span class="op">*</span><span class="st"> </span>(img.w <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2003-10" data-line-number="10">       img =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(h,w, img.d, img.s))</a>
<a class="sourceLine" id="cb2003-11" data-line-number="11">       <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>img.h) {</a>
<a class="sourceLine" id="cb2003-12" data-line-number="12">         <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>img.w) {</a>
<a class="sourceLine" id="cb2003-13" data-line-number="13">           img[i<span class="op">*</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,j<span class="op">*</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,,] =<span class="st"> </span>image[i,j,,]</a>
<a class="sourceLine" id="cb2003-14" data-line-number="14">         }</a>
<a class="sourceLine" id="cb2003-15" data-line-number="15">       }</a>
<a class="sourceLine" id="cb2003-16" data-line-number="16">       filters[[l]] =<span class="st"> </span>img</a>
<a class="sourceLine" id="cb2003-17" data-line-number="17">   }</a>
<a class="sourceLine" id="cb2003-18" data-line-number="18">   filters</a>
<a class="sourceLine" id="cb2003-19" data-line-number="19">}</a></code></pre></div>

<p>A simple example of using the <strong>dilate(.)</strong> function is shown below:</p>

<div class="sourceCode" id="cb2004"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2004-1" data-line-number="1">size =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2004-2" data-line-number="2">m =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>,size <span class="op">*</span><span class="st"> </span>size), <span class="kw">c</span>(size, size, <span class="dv">1</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2004-3" data-line-number="3">m[,,,<span class="dv">1</span>]</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    4    7
## [2,]    2    5    8
## [3,]    3    6    9</code></pre>
<div class="sourceCode" id="cb2006"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2006-1" data-line-number="1"><span class="kw">dilate</span>(m, <span class="dt">dil_rate=</span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>## , , 1, 1
## 
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    0    4    0    7
## [2,]    0    0    0    0    0
## [3,]    2    0    5    0    8
## [4,]    0    0    0    0    0
## [5,]    3    0    6    0    9</code></pre>

<p>To illustrate, let us use the edge kernel, but this time, we pass a new hyperparameter to the <strong>convolution(.)</strong> function called <strong>Dilation Rate</strong> with a value equal to one.</p>

<div class="sourceCode" id="cb2008"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2008-1" data-line-number="1">kernel =<span class="st"> </span>edge.kernel</a>
<a class="sourceLine" id="cb2008-2" data-line-number="2">filter =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2008-3" data-line-number="3">new.image =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter, <span class="dt">dil_rate=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb2008-4" data-line-number="4">                    <span class="dt">normalize=</span><span class="ot">TRUE</span>, <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a></code></pre></div>

<p>Figure <a href="deeplearning1.html#fig:appleorange5">12.33</a> shows the new image using a dilated filter.</p>

<div class="sourceCode" id="cb2009"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2009-1" data-line-number="1"><span class="kw">draw.image</span>(new.image)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange5"></span>
<img src="appleorange5.png" alt="Apple, Oranges, and Banana" width="60%" />
<p class="caption">
Figure 12.33: Apple, Oranges, and Banana
</p>
</div>

</div>
<div id="pooling" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.6</span> Pooling <a href="deeplearning1.html#pooling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Pooling</strong> is also regarded as <strong>SubSampling</strong> or <strong>DownSampling</strong>, of which the primary intent is for dimensionality reduction by summarizing the feature map.</p>
<p>Similar to convolution, pooling is an operation that slides a <span class="math inline">\(P \times P\)</span> filter over a feature map in the direction from left to right and top to bottom. We then perform one of two common pooling methods, namely <strong>maximum pooling</strong> and <strong>average pooling</strong> (see Figure <a href="deeplearning1.html#fig:pooling">12.34</a>).  </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pooling"></span>
<img src="pooling.png" alt="Pooling" width="70%" />
<p class="caption">
Figure 12.34: Pooling
</p>
</div>
<p>In Figure <a href="deeplearning1.html#fig:pooling">12.34</a>, we compute the value of the first cell in the pool for either the maximum pooling or average pooling. Below is the result:</p>

<div class="sourceCode" id="cb2010"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2010-1" data-line-number="1">p1 =<span class="st"> </span><span class="kw">max</span>(<span class="kw">c</span>(<span class="dv">224</span>, <span class="dv">161</span>, <span class="dv">325</span>, <span class="dv">83</span>))</a>
<a class="sourceLine" id="cb2010-2" data-line-number="2">p2 =<span class="st"> </span><span class="kw">mean</span>(<span class="kw">c</span>(<span class="dv">224</span>, <span class="dv">161</span>,<span class="dv">325</span>, <span class="dv">83</span>))</a>
<a class="sourceLine" id="cb2010-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;MaxPool&quot;</span> =<span class="st"> </span>p1, <span class="st">&quot;AvePool&quot;</span> =<span class="st"> </span>p2)</a></code></pre></div>
<pre><code>## MaxPool AvePool 
##  325.00  198.25</code></pre>

<p>We then slide the filter to the next patch to perform the same calculation. We repeat the process until we slide to the last patch.</p>

<div class="sourceCode" id="cb2012"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2012-1" data-line-number="1">p1 =<span class="st"> </span><span class="kw">max</span>(<span class="kw">c</span>(<span class="dv">161</span>, <span class="dv">164</span>, <span class="dv">83</span>, <span class="dv">65</span>))</a>
<a class="sourceLine" id="cb2012-2" data-line-number="2">p2 =<span class="st"> </span><span class="kw">mean</span>(<span class="kw">c</span>(<span class="dv">161</span>, <span class="dv">164</span>, <span class="dv">83</span>, <span class="dv">65</span>))</a>
<a class="sourceLine" id="cb2012-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;MaxPool&quot;</span> =<span class="st"> </span>p1, <span class="st">&quot;AvePool&quot;</span> =<span class="st"> </span>p2)</a></code></pre></div>
<pre><code>## MaxPool AvePool 
##  164.00  118.25</code></pre>

<p>Notice that the resulting matrix is reduced in size to a smaller dimension. It means a reduction in the number of parameters and computation. Note that for a tensor, the depth is preserved.</p>
<p>Our implementation of <strong>Pooling</strong> piggybacks on <strong>Convolution</strong> in our case. Therefore, we need a corresponding function, e.g., <strong>pooling(.)</strong>. It is notable to mention that other implementations may avoid overlapping, which can be done by adjusting the strides.</p>

<div class="sourceCode" id="cb2014"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2014-1" data-line-number="1">pooling &lt;-<span class="cf">function</span>(<span class="dt">ptype =</span> <span class="st">&quot;maxpool&quot;</span>, ...) { <span class="kw">convolution</span>( <span class="dt">ptype=</span>ptype, ...)}</a>
<a class="sourceLine" id="cb2014-2" data-line-number="2">pool.backprop &lt;-<span class="st"> </span>pooling</a></code></pre></div>

<p>As an example of its application:</p>

<div class="sourceCode" id="cb2015"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2015-1" data-line-number="1">m =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">48</span>), <span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">3</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2015-2" data-line-number="2">f =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">27</span>), <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2015-3" data-line-number="3"><span class="kw">dim</span>(m)</a></code></pre></div>
<pre><code>## [1] 4 4 3 1</code></pre>
<div class="sourceCode" id="cb2017"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2017-1" data-line-number="1">m[,,,<span class="dv">1</span>]</a></code></pre></div>
<pre><code>## , , 1
## 
##      [,1] [,2] [,3] [,4]
## [1,]    1    5    9   13
## [2,]    2    6   10   14
## [3,]    3    7   11   15
## [4,]    4    8   12   16
## 
## , , 2
## 
##      [,1] [,2] [,3] [,4]
## [1,]   17   21   25   29
## [2,]   18   22   26   30
## [3,]   19   23   27   31
## [4,]   20   24   28   32
## 
## , , 3
## 
##      [,1] [,2] [,3] [,4]
## [1,]   33   37   41   45
## [2,]   34   38   42   46
## [3,]   35   39   43   47
## [4,]   36   40   44   48</code></pre>
<div class="sourceCode" id="cb2019"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2019-1" data-line-number="1">p =<span class="st"> </span><span class="kw">pooling</span>(<span class="dt">image=</span>m, <span class="dt">filter=</span>f, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2019-2" data-line-number="2"><span class="kw">dim</span>(p<span class="op">$</span>feature.map)</a></code></pre></div>
<pre><code>## [1] 2 2 3 1</code></pre>
<div class="sourceCode" id="cb2021"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2021-1" data-line-number="1">p<span class="op">$</span>feature.map[,,,<span class="dv">1</span>]  <span class="co"># default: maxpool</span></a></code></pre></div>
<pre><code>## , , 1
## 
##      [,1] [,2]
## [1,]   11   15
## [2,]   12   16
## 
## , , 2
## 
##      [,1] [,2]
## [1,]   27   31
## [2,]   28   32
## 
## , , 3
## 
##      [,1] [,2]
## [1,]   43   47
## [2,]   44   48</code></pre>

<p>There are other types of pooling used in other literature. We leave readers to investigate the others.</p>
<p>In the next section, we now discuss how convolution works in a deep convolutional neural network.</p>
</div>
<div id="cnn-architectures" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.7</span> CNN Architectures<a href="deeplearning1.html#cnn-architectures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A few <strong>CNN</strong> architectures have earned their rightful place and can keep their respective prestigious names, namely <strong>LeNet-5</strong>, <strong>AlexNet</strong>, <strong>VGG-16</strong>, and <strong>ResNet</strong>. Such architectures have become classic models for object and image detection in popular competitions - one competition, in particular, is the <strong>ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</strong> which promotes annual competition, eyeing for deserving architectures that may find their place in the top ranks based on performance, accuracy, and other criteria.    </p>
<p>Designing <strong>CNN</strong> architecture involves determining the number of convolution layers to use, the number of filters to use, the number of pooling layers to use, the size of the minibatch to use, and the use of batch normalization and dropouts, among many other factors. Ultimately, we desire to reduce computation costs while achieving reasonable stability and accuracy across general cases.</p>
<p>In this section, let us use a <strong>toy architecture</strong> to discuss the major components of a <strong>CNN</strong> architecture as shown in Figure <a href="deeplearning1.html#fig:cnnarch">12.35</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cnnarch"></span>
<img src="cnnarchitecture.png" alt="CNN (Toy Architecture)" width="90%" />
<p class="caption">
Figure 12.35: CNN (Toy Architecture)
</p>
</div>
<p>For <strong>CNN</strong> to work, we ensure it knows how to learn (to be trained) first. We go through the same methods such as <strong>FeedForward</strong> and <strong>BackPropagation</strong> as covered in previous sections about <strong>MLP</strong>. Moreover, we go through the possible use of optimizers such as <strong>AdaGrad</strong>, <strong>Adam</strong>, and <strong>Batch Normalization</strong>.</p>
</div>
<div id="forward-feed-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.8</span> Forward Feed <a href="deeplearning1.html#forward-feed-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We mostly cover the <strong>Forward Feed</strong> technique in the <strong>MLP</strong> section. It is indeed a straightforward process. Recall that in the <strong>Forward Pass</strong> portion of the algorithm, we are given the option to choose which activation function to use. In the context of <strong>CNN</strong>, we have additional options to choose from, a couple of them being the size and number of kernels. Essentially, this dictates the complexity of our <strong>CNN</strong>, driven by the number of learnable parameters to use. There are two considerations to note in determining the size and number of kernels:</p>
<p><strong>Local Connectivity</strong> </p>
<p>The core premise of <strong>convolution</strong> is to form a local grouping of pixels that may carry unique highlights or patterns. Such a group of pixels is projected in a summarized form to what we call <strong>activation map</strong>; otherwise known as <strong>feature map</strong>. Each cell in a <strong>feature map</strong> corresponds to a single <strong>neuron</strong> due to convolving a portion of an input image using a filter. As the filter slides over the input image, we form a collection of <strong>neurons</strong> in the form of the <strong>activation map</strong>. The term <strong>local connectivity</strong> emphasizes that a <strong>neuron</strong> is built upon (or is connected to) only a subset (a small region) of an input image. It should be noted that the effect of such <strong>local connectivity</strong> unavoidably diminishes the quality of the original image as we move deeper down the neural network via <strong>feed-forward</strong>. This effect can be regarded as smoothing and a way to offset overfitting. On the other hand, <strong>local connectivity</strong> tries to discover and preserve unique patterns per neuron in a <strong>feature map</strong>. The entire <strong>feature map</strong> is a collection of unique local patterns, forming the important, relevant components of a class - possibly a unique unified global information that can be classified. Preserving while siphoning this information from one layer to the next is challenging. For intuition, the ears, eyes, or nose of a facial image are all highlights at a high level, produced by a convolution of the facial image with different kernels. It is an onus upon us to design a <strong>CNN</strong> that effectively allows the highlights to percolate through the network down a final classification layer without losing relevant information. This final layer is the fully connected (FC) neural network layer, as shown in Figure <a href="deeplearning1.html#fig:cnnarch">12.35</a> responsible for performing softmax for classification. It is at the output layer of the fully connected neural network where we also connect all highlights in a more global setting to finally classify the image in its entirety based on softmax probability. </p>
<p><strong>Weight Sharing</strong> </p>
<p><strong>Activation maps</strong> contain neurons (cells) that share common <strong>weights</strong> in the form of <strong>kernels</strong>. Similar to neurons in <strong>MLP</strong>, <strong>activation maps</strong> are the <strong>net output</strong> of a <strong>dot product</strong> between a set of inputs and weights. Here, <strong>kernels</strong> - being the <strong>weights</strong> - hold learnable parameters that need to be optimized during <strong>BackPropagation</strong>. Therefore, if our kernel size is <span class="math inline">\(3 \times 3\)</span>, we have nine learnable parameters to tune. Likewise, if we build a filter of <span class="math inline">\(3 \times 3\)</span> kernels with a depth size of 10, we have ninety learnable parameters.</p>
<p><strong>Feedforward</strong></p>
<p>Now, in terms of <strong>FeedForward</strong>, we need to be able to construct a set of randomly initialized kernels. Similar to <strong>deep.neural.layer(.)</strong> based on our <strong>MLP</strong> implementation, let us also write a <strong>CNN</strong> version named <strong>deep.cnn.layer(.)</strong>. Both contain a structure of learnable <strong>weights</strong>, which are trained and updated during <strong>Backpropagation</strong> and <strong>Backward Pass</strong>.</p>

<div class="sourceCode" id="cb2023"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2023-1" data-line-number="1">error &lt;-<span class="st"> </span><span class="cf">function</span>(e) {</a>
<a class="sourceLine" id="cb2023-2" data-line-number="2">    <span class="cf">if</span> (e <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2023-3" data-line-number="3">       <span class="kw">stop</span>(<span class="kw">paste0</span>(<span class="st">&quot;Feature map convolvs into 1x1.&quot;</span>,</a>
<a class="sourceLine" id="cb2023-4" data-line-number="4">                  <span class="st">&quot; Input size is too small or kernel size is too large&quot;</span>,</a>
<a class="sourceLine" id="cb2023-5" data-line-number="5">                  <span class="st">&quot; or there are too many layers.&quot;</span>)) </a>
<a class="sourceLine" id="cb2023-6" data-line-number="6">    } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb2023-7" data-line-number="7">    <span class="cf">if</span> (e <span class="op">==</span><span class="st"> </span><span class="dv">2</span> ) {</a>
<a class="sourceLine" id="cb2023-8" data-line-number="8">        <span class="kw">stop</span>(<span class="kw">paste0</span>(<span class="st">&quot;Backprop Convolution does not trace to correct path.&quot;</span>))</a>
<a class="sourceLine" id="cb2023-9" data-line-number="9">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2023-10" data-line-number="10">    <span class="cf">if</span> (e <span class="op">==</span><span class="st"> </span><span class="dv">3</span> ) {</a>
<a class="sourceLine" id="cb2023-11" data-line-number="11">        <span class="kw">stop</span>(<span class="kw">paste0</span>(<span class="st">&quot;Full Convolution does not trace to correct path.&quot;</span>))</a>
<a class="sourceLine" id="cb2023-12" data-line-number="12">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2023-13" data-line-number="13">    <span class="cf">if</span> (e <span class="op">==</span><span class="st"> </span><span class="dv">4</span>) {</a>
<a class="sourceLine" id="cb2023-14" data-line-number="14">        <span class="kw">stop</span>(<span class="kw">paste0</span>(</a>
<a class="sourceLine" id="cb2023-15" data-line-number="15">          <span class="st">&quot;Size of kernel cannot be the same or greater than input size.&quot;</span>))</a>
<a class="sourceLine" id="cb2023-16" data-line-number="16">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2023-17" data-line-number="17">    <span class="cf">if</span> (e <span class="op">==</span><span class="st"> </span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb2023-18" data-line-number="18">        <span class="kw">stop</span>(<span class="kw">paste0</span>(</a>
<a class="sourceLine" id="cb2023-19" data-line-number="19">          <span class="st">&quot;Size of Stride cannot be the same or greater than kernel size.&quot;</span>))</a>
<a class="sourceLine" id="cb2023-20" data-line-number="20">    }</a>
<a class="sourceLine" id="cb2023-21" data-line-number="21">}</a></code></pre></div>
<div class="sourceCode" id="cb2024"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2024-1" data-line-number="1">deep.cnn.layers &lt;-<span class="st"> </span><span class="cf">function</span>(X,  ...) { </a>
<a class="sourceLine" id="cb2024-2" data-line-number="2">   di        =<span class="st"> </span><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb2024-3" data-line-number="3">   <span class="cf">if</span> (<span class="kw">length</span>(di) <span class="op">&lt;</span><span class="st"> </span><span class="dv">4</span>) {</a>
<a class="sourceLine" id="cb2024-4" data-line-number="4">     di =<span class="st"> </span><span class="kw">c</span>(di, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2024-5" data-line-number="5">     X =<span class="st"> </span><span class="kw">array</span>(X, di)</a>
<a class="sourceLine" id="cb2024-6" data-line-number="6">   }</a>
<a class="sourceLine" id="cb2024-7" data-line-number="7">   input.size  =<span class="st"> </span>di[<span class="dv">1</span>]; img.s =<span class="st"> </span>di[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb2024-8" data-line-number="8">   depth       =<span class="st"> </span>di[<span class="dv">3</span>] </a>
<a class="sourceLine" id="cb2024-9" data-line-number="9">   layers      =<span class="st"> </span><span class="kw">list</span>(...)</a>
<a class="sourceLine" id="cb2024-10" data-line-number="10">   structure   =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2024-11" data-line-number="11">   l         =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2024-12" data-line-number="12">   <span class="cf">for</span> (layer <span class="cf">in</span> layers) {</a>
<a class="sourceLine" id="cb2024-13" data-line-number="13">      l =<span class="st"> </span>l <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2024-14" data-line-number="14">      <span class="cf">if</span> (layer<span class="op">$</span>type <span class="op">==</span><span class="st"> &quot;convolv&quot;</span>) { </a>
<a class="sourceLine" id="cb2024-15" data-line-number="15">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>stride))     { layer<span class="op">$</span>stride     =<span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb2024-16" data-line-number="16">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>padding))    { layer<span class="op">$</span>padding    =<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb2024-17" data-line-number="17">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>dil_rate))   { layer<span class="op">$</span>dil_rate   =<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb2024-18" data-line-number="18">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>filters))    { layer<span class="op">$</span>filters    =<span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb2024-19" data-line-number="19">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>auto.pad))   { layer<span class="op">$</span>auto.pad   =<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb2024-20" data-line-number="20">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>afunc))      { layer<span class="op">$</span>afunc      =<span class="st"> </span><span class="ot">NULL</span> }  </a>
<a class="sourceLine" id="cb2024-21" data-line-number="21">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>drop))       { layer<span class="op">$</span>drop       =<span class="st"> </span><span class="ot">NULL</span> }</a>
<a class="sourceLine" id="cb2024-22" data-line-number="22">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>bias))       { layer<span class="op">$</span>bias       =<span class="st"> </span><span class="ot">TRUE</span> }</a>
<a class="sourceLine" id="cb2024-23" data-line-number="23">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>normalize))  { layer<span class="op">$</span>normalize  =<span class="st"> </span><span class="ot">FALSE</span> }</a>
<a class="sourceLine" id="cb2024-24" data-line-number="24">        <span class="cf">if</span> (layer<span class="op">$</span>size <span class="op">&lt;=</span><span class="st"> </span>layer<span class="op">$</span>stride) { <span class="kw">error</span>(<span class="dv">5</span>) }</a>
<a class="sourceLine" id="cb2024-25" data-line-number="25">           </a>
<a class="sourceLine" id="cb2024-26" data-line-number="26">        batch.gamma      =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2024-27" data-line-number="27">        batch.beta       =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2024-28" data-line-number="28">        layer<span class="op">$</span>batchnorm  =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;gamma&quot;</span>  =<span class="st"> </span>batch.gamma, <span class="st">&quot;beta&quot;</span> =<span class="st"> </span>batch.beta, </a>
<a class="sourceLine" id="cb2024-29" data-line-number="29">                                <span class="st">&quot;moving.mu&quot;</span>  =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(layer<span class="op">$</span>filters,<span class="dv">1</span>)),</a>
<a class="sourceLine" id="cb2024-30" data-line-number="30">                                <span class="st">&quot;moving.var&quot;</span> =<span class="st"> </span><span class="kw">array</span>(<span class="dv">1</span>, <span class="kw">c</span>(layer<span class="op">$</span>filters,<span class="dv">1</span>)),</a>
<a class="sourceLine" id="cb2024-31" data-line-number="31">                                <span class="st">&quot;normalize&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>normalize)  </a>
<a class="sourceLine" id="cb2024-32" data-line-number="32">        <span class="co"># For Depthwise Separable convolution</span></a>
<a class="sourceLine" id="cb2024-33" data-line-number="33">        weights =<span class="st"> </span><span class="kw">net.initialization</span>(layer<span class="op">$</span>size <span class="op">*</span><span class="st"> </span>layer<span class="op">$</span>size, di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>],</a>
<a class="sourceLine" id="cb2024-34" data-line-number="34">                                 layer<span class="op">$</span>size <span class="op">*</span><span class="st"> </span>layer<span class="op">$</span>size, <span class="dt">itype=</span><span class="st">&quot;he&quot;</span>, </a>
<a class="sourceLine" id="cb2024-35" data-line-number="35">                                 <span class="dt">dist=</span><span class="st">&quot;uniform&quot;</span>)</a>
<a class="sourceLine" id="cb2024-36" data-line-number="36">        layer<span class="op">$</span>dw.kernel  =<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb2024-37" data-line-number="37">                <span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">array</span>(weights, <span class="kw">c</span>(layer<span class="op">$</span>size, layer<span class="op">$</span>size, depth)), </a>
<a class="sourceLine" id="cb2024-38" data-line-number="38">                <span class="st">&quot;rho&quot;</span>    =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(layer<span class="op">$</span>size, layer<span class="op">$</span>size, depth)),</a>
<a class="sourceLine" id="cb2024-39" data-line-number="39">                <span class="st">&quot;nu&quot;</span>     =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(layer<span class="op">$</span>size, layer<span class="op">$</span>size, depth)) )</a>
<a class="sourceLine" id="cb2024-40" data-line-number="40">        layer<span class="op">$</span>dw.bias    =<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb2024-41" data-line-number="41">                <span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>depth, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb2024-42" data-line-number="42">                <span class="st">&quot;rho&quot;</span>    =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, depth),</a>
<a class="sourceLine" id="cb2024-43" data-line-number="43">                <span class="st">&quot;nu&quot;</span>     =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, depth) )     </a>
<a class="sourceLine" id="cb2024-44" data-line-number="44">        <span class="co"># For Pointwise convolution</span></a>
<a class="sourceLine" id="cb2024-45" data-line-number="45">        weights =<span class="st"> </span><span class="kw">net.initialization</span>(depth <span class="op">*</span><span class="st"> </span>layer<span class="op">$</span>filters, di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>], </a>
<a class="sourceLine" id="cb2024-46" data-line-number="46">                                 depth <span class="op">*</span><span class="st"> </span>layer<span class="op">$</span>filters, <span class="dt">itype=</span><span class="st">&quot;he&quot;</span>, </a>
<a class="sourceLine" id="cb2024-47" data-line-number="47">                                 <span class="dt">dist=</span><span class="st">&quot;uniform&quot;</span>)</a>
<a class="sourceLine" id="cb2024-48" data-line-number="48">        layer<span class="op">$</span>pw.kernel =<span class="st"> </span></a>
<a class="sourceLine" id="cb2024-49" data-line-number="49"><span class="st">                     </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">array</span>(weights, <span class="kw">c</span>(depth, layer<span class="op">$</span>filters)),</a>
<a class="sourceLine" id="cb2024-50" data-line-number="50">                          <span class="st">&quot;rho&quot;</span>    =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(depth, layer<span class="op">$</span>filters)),</a>
<a class="sourceLine" id="cb2024-51" data-line-number="51">                          <span class="st">&quot;nu&quot;</span>     =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(depth, layer<span class="op">$</span>filters)) )</a>
<a class="sourceLine" id="cb2024-52" data-line-number="52">        layer<span class="op">$</span>pw.bias   =<span class="st"> </span></a>
<a class="sourceLine" id="cb2024-53" data-line-number="53"><span class="st">                     </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>layer<span class="op">$</span>filters, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb2024-54" data-line-number="54">                          <span class="st">&quot;rho&quot;</span>    =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, layer<span class="op">$</span>filters),</a>
<a class="sourceLine" id="cb2024-55" data-line-number="55">                           <span class="st">&quot;nu&quot;</span>    =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, layer<span class="op">$</span>filters) )  </a>
<a class="sourceLine" id="cb2024-56" data-line-number="56"></a>
<a class="sourceLine" id="cb2024-57" data-line-number="57">        input.size         =<span class="st"> </span>layer<span class="op">$</span>size</a>
<a class="sourceLine" id="cb2024-58" data-line-number="58">        depth              =<span class="st"> </span>layer<span class="op">$</span>filters</a>
<a class="sourceLine" id="cb2024-59" data-line-number="59">        structure[[l]]     =<span class="st"> </span></a>
<a class="sourceLine" id="cb2024-60" data-line-number="60"><span class="st">          </span><span class="kw">list</span>(<span class="st">&quot;stride&quot;</span>    =<span class="st"> </span>layer<span class="op">$</span>stride,    <span class="st">&quot;padding&quot;</span>    =<span class="st"> </span>layer<span class="op">$</span>padding,   </a>
<a class="sourceLine" id="cb2024-61" data-line-number="61">               <span class="st">&quot;dil_rate&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>dil_rate,  <span class="st">&quot;auto.pad&quot;</span>   =<span class="st"> </span>layer<span class="op">$</span>auto.pad,   </a>
<a class="sourceLine" id="cb2024-62" data-line-number="62">               <span class="st">&quot;afunc&quot;</span>     =<span class="st"> </span>layer<span class="op">$</span>afunc,     <span class="st">&quot;istype&quot;</span>     =<span class="st"> </span>layer<span class="op">$</span>type,</a>
<a class="sourceLine" id="cb2024-63" data-line-number="63">               <span class="st">&quot;dw.kernel&quot;</span> =<span class="st"> </span>layer<span class="op">$</span>dw.kernel, <span class="st">&quot;pw.kernel&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>pw.kernel,</a>
<a class="sourceLine" id="cb2024-64" data-line-number="64">               <span class="st">&quot;dw.bias&quot;</span>   =<span class="st"> </span>layer<span class="op">$</span>dw.bias,   <span class="st">&quot;pw.bias&quot;</span>    =<span class="st"> </span>layer<span class="op">$</span>pw.bias,</a>
<a class="sourceLine" id="cb2024-65" data-line-number="65">               <span class="st">&quot;batchnorm&quot;</span> =<span class="st"> </span>layer<span class="op">$</span>batchnorm, <span class="st">&quot;drop&quot;</span>       =<span class="st"> </span>layer<span class="op">$</span>drop,</a>
<a class="sourceLine" id="cb2024-66" data-line-number="66">               <span class="st">&quot;bias&quot;</span>      =<span class="st"> </span>layer<span class="op">$</span>bias)</a>
<a class="sourceLine" id="cb2024-67" data-line-number="67">     } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2024-68" data-line-number="68">     <span class="cf">if</span> (layer<span class="op">$</span>type <span class="op">==</span><span class="st"> &quot;pooling&quot;</span>) {</a>
<a class="sourceLine" id="cb2024-69" data-line-number="69">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>stride))  { layer<span class="op">$</span>stride  =<span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb2024-70" data-line-number="70">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>padding)) { layer<span class="op">$</span>padding =<span class="st"> </span><span class="dv">0</span> }   </a>
<a class="sourceLine" id="cb2024-71" data-line-number="71">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>dil_rate))   { layer<span class="op">$</span>dil_rate   =<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb2024-72" data-line-number="72">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>dil_input))  { layer<span class="op">$</span>dil_input  =<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb2024-73" data-line-number="73">        wsiz =<span class="st"> </span>layer<span class="op">$</span>size</a>
<a class="sourceLine" id="cb2024-74" data-line-number="74">        window =<span class="st"> </span><span class="kw">array</span>(<span class="kw">rep</span>(<span class="dv">1</span>, wsiz <span class="op">*</span><span class="st"> </span>wsiz), <span class="kw">c</span>(wsiz, wsiz, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2024-75" data-line-number="75">        structure[[l]] =<span class="st"> </span></a>
<a class="sourceLine" id="cb2024-76" data-line-number="76"><span class="st">          </span><span class="kw">list</span>(<span class="st">&quot;window&quot;</span>    =<span class="st"> </span>window,         <span class="st">&quot;stride&quot;</span>    =<span class="st"> </span>layer<span class="op">$</span>stride, </a>
<a class="sourceLine" id="cb2024-77" data-line-number="77">               <span class="st">&quot;padding&quot;</span>   =<span class="st"> </span>layer<span class="op">$</span>padding,  <span class="st">&quot;auto.pad&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>auto.pad,</a>
<a class="sourceLine" id="cb2024-78" data-line-number="78">               <span class="st">&quot;dil_rate&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>dil_rate, <span class="st">&quot;dil_input&quot;</span> =<span class="st"> </span>layer<span class="op">$</span>dil_input,</a>
<a class="sourceLine" id="cb2024-79" data-line-number="79">               <span class="st">&quot;ptype&quot;</span>     =<span class="st"> </span>layer<span class="op">$</span>ptype,    <span class="st">&quot;istype&quot;</span>    =<span class="st"> </span>layer<span class="op">$</span>type)</a>
<a class="sourceLine" id="cb2024-80" data-line-number="80">      } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2024-81" data-line-number="81">      <span class="cf">if</span> (layer<span class="op">$</span>type <span class="op">==</span><span class="st"> &quot;dense&quot;</span>) {</a>
<a class="sourceLine" id="cb2024-82" data-line-number="82">       structure[[l]] =<span class="st"> </span><span class="kw">list</span>( <span class="st">&quot;fc.layers&quot;</span>     =<span class="st"> </span>layer[layer <span class="op">!=</span><span class="st"> </span>layer<span class="op">$</span>type],</a>
<a class="sourceLine" id="cb2024-83" data-line-number="83">                              <span class="st">&quot;init&quot;</span>          =<span class="st"> </span><span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb2024-84" data-line-number="84">                              <span class="st">&quot;istype&quot;</span>        =<span class="st"> </span>layer<span class="op">$</span>type)</a>
<a class="sourceLine" id="cb2024-85" data-line-number="85">      }</a>
<a class="sourceLine" id="cb2024-86" data-line-number="86">  }</a>
<a class="sourceLine" id="cb2024-87" data-line-number="87">  structure</a>
<a class="sourceLine" id="cb2024-88" data-line-number="88">}</a></code></pre></div>

<p>To illustrate, let us use a <strong>synthetic</strong> image with the following dimension.</p>

<div class="sourceCode" id="cb2025"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2025-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb2025-2" data-line-number="2">size =<span class="st"> </span><span class="dv">7</span></a>
<a class="sourceLine" id="cb2025-3" data-line-number="3">depth =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2025-4" data-line-number="4">input =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb2025-5" data-line-number="5">mx =<span class="st"> </span>size <span class="op">*</span><span class="st"> </span>size <span class="op">*</span><span class="st"> </span>depth</a>
<a class="sourceLine" id="cb2025-6" data-line-number="6">X =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>,mx), <span class="kw">c</span>(size,size,depth, input))</a>
<a class="sourceLine" id="cb2025-7" data-line-number="7">cnn.layers =<span class="st"> </span><span class="kw">deep.cnn.layers</span>( X, </a>
<a class="sourceLine" id="cb2025-8" data-line-number="8">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">2</span>, <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>),</a>
<a class="sourceLine" id="cb2025-9" data-line-number="9">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;pooling&quot;</span>, <span class="dt">size=</span><span class="dv">2</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">ptype=</span><span class="st">&quot;maxpool&quot;</span>),</a>
<a class="sourceLine" id="cb2025-10" data-line-number="10">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;dense&quot;</span>, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">3</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">3</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">10</span>))</a>
<a class="sourceLine" id="cb2025-11" data-line-number="11">        )</a>
<a class="sourceLine" id="cb2025-12" data-line-number="12"><span class="kw">str</span>(cnn.layers, <span class="dt">strict.width=</span><span class="st">&quot;wrap&quot;</span>)</a></code></pre></div>
<pre><code>## List of 3
## $ :List of 13
## ..$ stride : num 1
## ..$ padding : num 0
## ..$ dil_rate : num 0
## ..$ auto.pad : num 0
## ..$ afunc : NULL
## ..$ istype : chr &quot;convolv&quot;
## ..$ dw.kernel:List of 3
## .. ..$ weight: num [1:3, 1:3, 1:3] 0.236 0.215 -0.293
##    -0.183 0.326 ...
## .. ..$ rho : num [1:3, 1:3, 1:3] 0 0 0 0 0 0 0 0 0 0
##    ...
## .. ..$ nu : num [1:3, 1:3, 1:3] 0 0 0 0 0 0 0 0 0 0 ...
## ..$ pw.kernel:List of 3
## .. ..$ weight: num [1:3, 1:2] -0.0192 0.0585 -0.1033
##    -0.33 0.3465 ...
## .. ..$ rho : num [1:3, 1:2] 0 0 0 0 0 0
## .. ..$ nu : num [1:3, 1:2] 0 0 0 0 0 0
## ..$ dw.bias :List of 3
## .. ..$ weight: num [1:3] 0.84 0.397 0.393
## .. ..$ rho : num [1:3] 0 0 0
## .. ..$ nu : num [1:3] 0 0 0
## ..$ pw.bias :List of 3
## .. ..$ weight: num [1:2] 0.552 0.102
## .. ..$ rho : num [1:2] 0 0
## .. ..$ nu : num [1:2] 0 0
## ..$ batchnorm:List of 5
## .. ..$ gamma :List of 3
## .. .. ..$ weight: num 1
## .. .. ..$ rho : num 0
## .. .. ..$ nu : num 0
## .. ..$ beta :List of 3
## .. .. ..$ weight: num 0
## .. .. ..$ rho : num 0
## .. .. ..$ nu : num 0
## .. ..$ moving.mu : num [1:2, 1] 0 0
## .. ..$ moving.var: num [1:2, 1] 1 1
## .. ..$ normalize : chr &quot;batch&quot;
## ..$ drop : NULL
## ..$ bias : logi TRUE
## $ :List of 8
## ..$ window : num [1:2, 1:2, 1] 1 1 1 1
## ..$ stride : num 1
## ..$ padding : num 0
## ..$ auto.pad : NULL
## ..$ dil_rate : num 0
## ..$ dil_input: num 0
## ..$ ptype : chr &quot;maxpool&quot;
## ..$ istype : chr &quot;pooling&quot;
## $ :List of 3
## ..$ fc.layers:List of 3
## .. ..$ :List of 1
## .. .. ..$ size: num 3
## .. ..$ :List of 1
## .. .. ..$ size: num 3
## .. ..$ :List of 1
## .. .. ..$ size: num 10
## ..$ init : logi TRUE
## ..$ istype : chr &quot;dense&quot;</code></pre>

<p>Here, we use a depth size of three, corresponding to the <strong>RGB channels</strong>. The kernels then inherit the input depth in the first layer in our <strong>CNN</strong>. Now, suppose that we use only two convolutional layers in building our <strong>CNN</strong>. For the first layer, we use a kernel size of <span class="math inline">\(3 \times 3\)</span>. We also specify three kernels (based on the <strong>kernels</strong> parameter). After convolution, it will produce a feature map with a depth of three, based on specifying three kernels. The second layer also uses a kernel size of <span class="math inline">\(3 \times 3\)</span> but with only two kernels. In other words, the first <strong>feature map</strong> to be generated will have a depth of 3, and the second <strong>feature map</strong> will have a depth of two. Let us use our <strong>deep.cnn.layers(.)</strong> function to construct the structure. Here, we assume a default stride of 1 and padding of 0. The first layer uses <strong>Leaky RELU</strong> at the end of the convolution.</p>

<div class="sourceCode" id="cb2027"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2027-1" data-line-number="1">cnn.layers =<span class="st"> </span><span class="kw">deep.cnn.layers</span>( X,</a>
<a class="sourceLine" id="cb2027-2" data-line-number="2">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">kernels=</span><span class="dv">3</span>),</a>
<a class="sourceLine" id="cb2027-3" data-line-number="3">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">kernels=</span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb2027-4" data-line-number="4">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;pooling&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">ptype=</span><span class="st">&quot;maxpool&quot;</span>),</a>
<a class="sourceLine" id="cb2027-5" data-line-number="5">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;dense&quot;</span>, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">3</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">3</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">10</span>))</a>
<a class="sourceLine" id="cb2027-6" data-line-number="6">        )</a></code></pre></div>

<p>With the structure of the <strong>CNN</strong> layers in place, let us now have our example implementation of <strong>forward pass</strong> for our <strong>CNN</strong>, passing the input and the filters, and yielding a flattened vector as output.</p>

<div class="sourceCode" id="cb2028"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2028-1" data-line-number="1">forward.pass.cnn &lt;-<span class="st"> </span><span class="cf">function</span>(X, layers,  <span class="dt">train=</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb2028-2" data-line-number="2">  cnn.output   =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2028-3" data-line-number="3">  cache.output =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2028-4" data-line-number="4">  pool.cache   =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2028-5" data-line-number="5">  feature.map =<span class="st"> </span>X</a>
<a class="sourceLine" id="cb2028-6" data-line-number="6">  H =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb2028-7" data-line-number="7">  <span class="cf">for</span> (L <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>H) {</a>
<a class="sourceLine" id="cb2028-8" data-line-number="8">    layer =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb2028-9" data-line-number="9">    <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;convolv&quot;</span>) {</a>
<a class="sourceLine" id="cb2028-10" data-line-number="10">      fmaps   =<span class="st"> </span><span class="ot">NULL</span>; v =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2028-11" data-line-number="11">      conv.output      =<span class="st"> </span><span class="kw">convolution</span>(feature.map,                   </a>
<a class="sourceLine" id="cb2028-12" data-line-number="12">             <span class="dt">filter    =</span> layer<span class="op">$</span>dw.kernel<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2028-13" data-line-number="13">             <span class="dt">dw.kernel =</span> layer<span class="op">$</span>dw.kernel<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2028-14" data-line-number="14">             <span class="dt">pw.kernel =</span> layer<span class="op">$</span>pw.kernel<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2028-15" data-line-number="15">             <span class="dt">dw.bias   =</span> layer<span class="op">$</span>dw.bias<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2028-16" data-line-number="16">             <span class="dt">pw.bias   =</span> layer<span class="op">$</span>pw.bias<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2028-17" data-line-number="17">             <span class="dt">bias      =</span> layer<span class="op">$</span>bias,</a>
<a class="sourceLine" id="cb2028-18" data-line-number="18">             <span class="dt">stride    =</span> layer<span class="op">$</span>stride, </a>
<a class="sourceLine" id="cb2028-19" data-line-number="19">             <span class="dt">padding   =</span> layer<span class="op">$</span>padding,</a>
<a class="sourceLine" id="cb2028-20" data-line-number="20">             <span class="dt">dil_rate  =</span> layer<span class="op">$</span>dil_rate,</a>
<a class="sourceLine" id="cb2028-21" data-line-number="21">             <span class="dt">autopad   =</span> <span class="st">&quot;right&quot;</span>,</a>
<a class="sourceLine" id="cb2028-22" data-line-number="22">             <span class="dt">auto.pad  =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb2028-23" data-line-number="23">             <span class="dt">afunc     =</span> <span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb2028-24" data-line-number="24">      feature.map =<span class="st"> </span>conv.output<span class="op">$</span>feature.map<span class="op">$</span>pw.fmap</a>
<a class="sourceLine" id="cb2028-25" data-line-number="25">      <span class="co"># Dropout Before Activation</span></a>
<a class="sourceLine" id="cb2028-26" data-line-number="26">      <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(layer<span class="op">$</span>drop) <span class="op">&amp;&amp;</span><span class="st"> </span>train<span class="op">==</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb2028-27" data-line-number="27">          feature.map =<span class="st"> </span><span class="kw">drop.out</span>(feature.map, layer<span class="op">$</span>drop)</a>
<a class="sourceLine" id="cb2028-28" data-line-number="28">      }</a>
<a class="sourceLine" id="cb2028-29" data-line-number="29">      <span class="co"># Activation Before Normalization</span></a>
<a class="sourceLine" id="cb2028-30" data-line-number="30">      <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(layer<span class="op">$</span>afunc)) {</a>
<a class="sourceLine" id="cb2028-31" data-line-number="31">        feature.map =<span class="st"> </span><span class="kw">activation</span>(feature.map, <span class="kw">get</span>(layer<span class="op">$</span>afunc))</a>
<a class="sourceLine" id="cb2028-32" data-line-number="32">      }</a>
<a class="sourceLine" id="cb2028-33" data-line-number="33">      <span class="co"># Normalization After Activation</span></a>
<a class="sourceLine" id="cb2028-34" data-line-number="34">      <span class="cf">if</span> (layer<span class="op">$</span>batchnorm<span class="op">$</span>normalize <span class="op">!=</span><span class="st"> </span><span class="ot">FALSE</span>) { </a>
<a class="sourceLine" id="cb2028-35" data-line-number="35">        normalize    =<span class="st"> </span><span class="kw">normalize.forward</span>(feature.map, layer, train)</a>
<a class="sourceLine" id="cb2028-36" data-line-number="36">        feature.map  =<span class="st"> </span>normalize<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb2028-37" data-line-number="37">        layer        =<span class="st"> </span>normalize<span class="op">$</span>layer</a>
<a class="sourceLine" id="cb2028-38" data-line-number="38">        layers[[L]]  =<span class="st"> </span>layer</a>
<a class="sourceLine" id="cb2028-39" data-line-number="39">      }</a>
<a class="sourceLine" id="cb2028-40" data-line-number="40">      conv.output<span class="op">$</span>featuremap<span class="op">$</span>pw.fmap =<span class="st"> </span>feature.map</a>
<a class="sourceLine" id="cb2028-41" data-line-number="41">      cnn.output[[L]]   =<span class="st"> </span>conv.output<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb2028-42" data-line-number="42"></a>
<a class="sourceLine" id="cb2028-43" data-line-number="43">      cache.output[[L]] =<span class="st"> </span>conv.output<span class="op">$</span>bkprop.cache</a>
<a class="sourceLine" id="cb2028-44" data-line-number="44">    } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb2028-45" data-line-number="45">    <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;pooling&quot;</span>) {</a>
<a class="sourceLine" id="cb2028-46" data-line-number="46">      conv.output =<span class="st"> </span><span class="kw">pooling</span>(feature.map, </a>
<a class="sourceLine" id="cb2028-47" data-line-number="47">                           <span class="dt">filter  =</span> layer<span class="op">$</span>window,</a>
<a class="sourceLine" id="cb2028-48" data-line-number="48">                           <span class="dt">stride  =</span> layer<span class="op">$</span>stride, </a>
<a class="sourceLine" id="cb2028-49" data-line-number="49">                           <span class="dt">padding =</span> layer<span class="op">$</span>padding,</a>
<a class="sourceLine" id="cb2028-50" data-line-number="50">                           <span class="dt">ptype   =</span> layer<span class="op">$</span>ptype)</a>
<a class="sourceLine" id="cb2028-51" data-line-number="51">      pmaps             =<span class="st"> </span>conv.output<span class="op">$</span>feature.map </a>
<a class="sourceLine" id="cb2028-52" data-line-number="52">      feature.map       =<span class="st"> </span>pmaps</a>
<a class="sourceLine" id="cb2028-53" data-line-number="53">      cnn.output[[L]]   =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;pw.fmap&quot;</span> =<span class="st"> </span>feature.map )</a>
<a class="sourceLine" id="cb2028-54" data-line-number="54">      cache.output[[L]] =<span class="st"> </span>conv.output<span class="op">$</span>bkprop.cache</a>
<a class="sourceLine" id="cb2028-55" data-line-number="55">      pool.cache[[L]]   =<span class="st"> </span>conv.output<span class="op">$</span>pool.cache</a>
<a class="sourceLine" id="cb2028-56" data-line-number="56">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2028-57" data-line-number="57">    <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;dense&quot;</span>) {</a>
<a class="sourceLine" id="cb2028-58" data-line-number="58">      di =<span class="st"> </span><span class="kw">dim</span>(feature.map)</a>
<a class="sourceLine" id="cb2028-59" data-line-number="59">      feature.map =<span class="st">  </span><span class="kw">t</span>(<span class="kw">array</span>(feature.map, <span class="kw">c</span>( di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">3</span>], di[<span class="dv">4</span>])))</a>
<a class="sourceLine" id="cb2028-60" data-line-number="60">      <span class="cf">if</span> (layer<span class="op">$</span>init <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb2028-61" data-line-number="61">          fclayers             =<span class="st"> </span>layer<span class="op">$</span>fc.layers</a>
<a class="sourceLine" id="cb2028-62" data-line-number="62">          fclayers[[<span class="st">&quot;X&quot;</span>]]      =<span class="st"> </span>feature.map</a>
<a class="sourceLine" id="cb2028-63" data-line-number="63">          dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, fclayers)</a>
<a class="sourceLine" id="cb2028-64" data-line-number="64">          layers[[L]]<span class="op">$</span>fc.layers =<span class="st"> </span>dnn<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2028-65" data-line-number="65">          layers[[L]]<span class="op">$</span>init      =<span class="st"> </span><span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb2028-66" data-line-number="66">      } </a>
<a class="sourceLine" id="cb2028-67" data-line-number="67">      fc.layers =<span class="st"> </span>layers[[L]]<span class="op">$</span>fc.layers</a>
<a class="sourceLine" id="cb2028-68" data-line-number="68">      fc.model  =<span class="st"> </span><span class="kw">forward.pass</span>(feature.map, fc.layers, <span class="dt">afunc=</span><span class="st">&quot;softmax&quot;</span>) </a>
<a class="sourceLine" id="cb2028-69" data-line-number="69">    }</a>
<a class="sourceLine" id="cb2028-70" data-line-number="70">  }  </a>
<a class="sourceLine" id="cb2028-71" data-line-number="71">  <span class="kw">list</span>(<span class="st">&quot;cnn.output&quot;</span>   =<span class="st"> </span>cnn.output,        <span class="st">&quot;flattened.output&quot;</span> =<span class="st"> </span>feature.map,</a>
<a class="sourceLine" id="cb2028-72" data-line-number="72">       <span class="st">&quot;cache.output&quot;</span> =<span class="st"> </span>cache.output,      <span class="st">&quot;pool.cache&quot;</span>       =<span class="st"> </span>pool.cache,</a>
<a class="sourceLine" id="cb2028-73" data-line-number="73">       <span class="st">&quot;fc.model&quot;</span>    =<span class="st"> </span>fc.model ,          <span class="st">&quot;layers&quot;</span>           =<span class="st"> </span>layers)</a>
<a class="sourceLine" id="cb2028-74" data-line-number="74">}</a></code></pre></div>

<p>Assume a tiny <span class="math inline">\(15 \times 15\)</span> image of depth 3 (RGB channels). Let us run a forward pass using the generated layers.</p>

<div class="sourceCode" id="cb2029"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2029-1" data-line-number="1">image =<span class="st"> </span><span class="kw">array</span>(<span class="kw">runif</span>(<span class="dv">675</span>, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2029-2" data-line-number="2"><span class="kw">dim</span>(image)</a></code></pre></div>
<pre><code>## [1] 15 15  3  1</code></pre>
<div class="sourceCode" id="cb2031"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2031-1" data-line-number="1">model =<span class="st"> </span><span class="kw">forward.pass.cnn</span>(image, cnn.layers)</a>
<a class="sourceLine" id="cb2031-2" data-line-number="2">len   =<span class="st"> </span><span class="kw">length</span>(model<span class="op">$</span>fc.model<span class="op">$</span>layers)</a>
<a class="sourceLine" id="cb2031-3" data-line-number="3"><span class="kw">str</span>(model<span class="op">$</span>fc.model<span class="op">$</span>layers[[len]]<span class="op">$</span>output)</a></code></pre></div>
<pre><code>##  num [1, 1:10] 0.0246 0.0714 0.0825 0.3705 0.1133 ...</code></pre>

<p>The final set of <strong>feature maps</strong> is flattened as one vector, and the vector is fed to the fully connected layer. Thus, if we have <span class="math inline">\(15 \times 15 \times 3\)</span> feature maps, all three maps are flattened sequentially into one vector.</p>
</div>
<div id="backpropagation-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.9</span> BackPropagation <a href="deeplearning1.html#backpropagation-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We start the discussion of <strong>CNN backpropagation</strong> using Figure <a href="deeplearning1.html#fig:cnnbackprop">12.36</a>. Note that the diagram uses <strong>Max Pooling</strong>. <strong>Average Pooling</strong> is another option to use.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cnnbackprop"></span>
<img src="cnnbackprop.png" alt="CNN (BackPropagation)" width="100%" />
<p class="caption">
Figure 12.36: CNN (BackPropagation)
</p>
</div>
<p>Here, <strong>CNN backpropagation</strong> consists of two parts. The first part performs backpropagation from a regular <strong>MLP (dense network)</strong> referred to as a fully connected neural network. The second part takes the gradient output from the <strong>dense network</strong> and uses that as input to perform the <strong>convolutional backpropagation</strong>.</p>
<p>Let us have a quick review of the first part.</p>
<p><strong>First</strong>, we take the derivative of the <strong>Loss Function</strong> with respect to the outputs in the output layer:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial o_1} = o_1 - t_1
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\frac{\partial \mathcal{L}}{\partial o_2} = o_2 - t_2
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\frac{\partial \mathcal{L}}{\partial o_3} = o_3 - t_3
\end{align}\]</span></p>
<p><strong>Second</strong>, we then calculate for the <strong>deltas</strong> (<span class="math inline">\(\delta o\)</span>):</p>
<p><span class="math display">\[\begin{align}
\delta o_1 = \frac{\partial \mathcal{L}}{\partial o_1} \left( \frac{\partial o_1}{\partial \hat{o}_1}\right)
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\delta o_2 = \frac{\partial \mathcal{L}}{\partial o_2} \left( \frac{\partial o_2}{\partial \hat{o}_2}\right)
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\delta o_3 = \frac{\partial \mathcal{L}}{\partial o_3} \left( \frac{\partial o_3}{\partial \hat{o}_3}\right)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\delta_o = \sum_i \frac{\partial \mathcal{L}}{\partial o_i} \left( \frac{\partial o_i}{\partial \hat{o}_i}\right) =  \delta o_1 + \delta o_2 + \delta o_3
\end{align}\]</span></p>
<p><strong>Third</strong>, we calculate the derivative of the <strong>Loss Function</strong> with respect to weights:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial \varphi_{1,1}} = \delta o_1 \left( \frac{\partial \hat{o}_1}{\partial \varphi_{1,1}}\right) = \delta {o_1} h_1
\ \ \ \ \ \ \ 
...
\ \ \ \ \ \ \ 
\frac{\partial \mathcal{L}}{\partial \varphi_{2,3}} = \delta o_3 \left( \frac{\partial \hat{o}_3}{\partial \varphi_{2,3}}\right) = \delta {o_3} h_2
\end{align}\]</span></p>
<p><strong>Fourth</strong>, we calculate the derivative of the <strong>Loss Function</strong> with respect to <strong>activation output</strong>:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial h_{1}} =  \sum_i 
 \left(
  \frac{\partial \mathcal{L}_i}{\partial o_i}
  \frac{\partial o_i}{\partial \hat{o}_i}
 \frac{\partial \hat{o}_i}{\partial h_1}
 \right) = \sum_i \delta o_i \varphi_{1,i} =
 \delta o_1 \varphi_{1,1} + \delta o_2 \varphi_{1,2} + \delta o_3 \varphi_{1,3}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial h_{2}} =  \sum_i 
 \left(
  \frac{\partial \mathcal{L}_i}{\partial o_i}
  \frac{\partial o_i}{\partial \hat{o}_i}
 \frac{\partial \hat{o}_i}{\partial h_2}
 \right)  = \sum_i \delta o_i \varphi_{2,i} =
 \delta o_1 \varphi_{2,1} + \delta o_2 \varphi_{2,2}  + \delta o_3 \varphi_{2,3} 
\end{align}\]</span></p>
<p><strong>Fifth</strong>, we continue to the next hidden FC layer and perform the same, taking derivative with respect to next previous weight:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial \omega_{1,1}} = \delta h_1 \left(\frac{\partial \hat{h}_1}{\partial \omega_{1,1}}\right ) = \delta h_1\ p_1 
\ \ \ \ \ \ \ 
...
\ \ \ \ \ \ \ 
\frac{\partial \mathcal{L}}{\partial \omega_{8,2}} = \delta h_2 \left(\frac{\partial \hat{h}_2}{\partial \omega_{8,2}}\right )  = \delta h_2\ p_8 
\end{align}\]</span></p>
<p>From:</p>
<p><span class="math display">\[\begin{align}
\delta {h_1} = \frac{\partial \mathcal{L}}{\partial h_{1}} \frac{\partial h_1}{\partial \hat{h}_1} = \left(\sum_i \delta o_i \varphi_{1,i} \right) \left(\nabla \text{RELU} = \begin{cases} 
0 &amp; if\ h_1 \le 0 \\
1 &amp; if\ h_1 &gt; 0
\end{cases} \right) \label{eqn:eqnnumber610}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\delta {h_2} = \frac{\partial \mathcal{L}}{\partial h_{2}} \frac{\partial h_2}{\partial \hat{h}_2} = \left(\sum_i \delta o_i \varphi_{2,i} \right)  \left(\nabla \text{RELU} = \begin{cases} 
0 &amp; if\ h_2 \le 0 \\
1 &amp; if\ h_2 &gt; 0
\end{cases} \right) \label{eqn:eqnnumber611}
\end{align}\]</span></p>
<p><strong>Sixth</strong>, we use the flattened vector - this is the common <strong>X input</strong> as we know it in <strong>MLP</strong>. The flattened vector shows eight input features (p1, â¦, p8) - equivalent to (x1, â¦, x8). We take the derivative of the <strong>Loss function</strong> with respect to <span class="math inline">\(\mathbf{x_1}\)</span>. In our case, that is the <span class="math inline">\(\mathbf{p_1}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial p_1} = \sum_i \left( \delta h_i\  \frac{\partial \hat{h}_i}{\partial p_1}\right)
=  \sum_i \left( \delta h_i\  \omega_{1,i} \right)
= (\delta h_1 \omega_{1,1} + \delta \hat{h}_2 \omega_{1,2} ) 
\end{align}\]</span></p>
<p><span class="math display">\[
...
\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial p_8} = \sum_i \left( \delta h_i\  \frac{\partial \hat{h}_i}{\partial p_8}\right)
=  \sum_i \left( \delta h_i\  \omega_{8,i} \right)
= (\delta h_1 \omega_{8,1} + \delta \hat{h}_2 \omega_{8,2} ) 
\end{align}\]</span></p>
<p><strong>Seventh</strong>, we move to the second part of the <strong>CNN backpropagation</strong>, from the <strong>FC</strong> layer and into the <strong>Max Pooling</strong> layer. Here, we take the gradients as output from the <strong>FC layer</strong> and use them as backpropagation input to the <strong>Max Pooling</strong> layer. Each gradient output corresponds to a maximum value derived from individual <strong>Patches</strong> in subsequent previous <strong>feature maps</strong>. For example, let us consider the first gradient output from the previous layer, e.g. <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial p_1}\)</span> or <span class="math inline">\(\nabla_{p_1}\mathcal{L}\)</span>. This gradient represents the effect or influence of <span class="math inline">\(\mathbf{p_1}\)</span> to the <strong>Loss function</strong>. At the same time, <span class="math inline">\(\mathbf{p_1}\)</span> also represents the maximum value, e.g., 409, derived from the <strong>1st Patch</strong> of the succeeding previous <strong>first feature map</strong> produced by the convolution with the first kernel set (still in reference to Figure <a href="deeplearning1.html#fig:cnnbackprop">12.36</a>).</p>
<p><span class="math display">\[\begin{align}
\left[ \begin{array}{ll}  f_1 = 307 &amp; f_2 = 226\\ f_4 = 409 &amp; f_5 = 170 \end{array}
\right]\ \ \ \ \rightarrow 
\left[ \begin{array}{ll}  \frac{\partial \mathcal{L}}{\partial f_1} = 0 &amp; \frac{\partial \mathcal{L}}{\partial f_2} = 0\\ \frac{\partial \mathcal{L}}{\partial f_4} = \frac{\partial \mathcal{L}}{\partial p_1} &amp; \frac{\partial \mathcal{L}}{\partial f_5} = 0 \end{array}
\right] \rightarrow \text{(influence to }\mathcal{L} )   \label{eqn:eqnnumber612}
\end{align}\]</span></p>
<p>Let us take another example. We review the <strong>4th Patch</strong> of the <strong>2nd feature map</strong> produced by the convolution with the second kernel set. It has 158 as the maximum value.</p>
<p><span class="math display">\[\begin{align}
\left[ \begin{array}{ll}  f_{14} = 4 &amp; f_{15} = 88\\ f_{17} = 140 &amp; f_{18} = 158 \end{array}
\right]\ \ \ \ \rightarrow 
\left[ \begin{array}{ll}  \frac{\partial \mathcal{L}}{\partial  f_{14}} = 0 &amp; \frac{\partial \mathcal{L}}{\partial f_{15} } = 0\\ \frac{\partial \mathcal{L}}{\partial f_{17}} = 0  &amp; \frac{\partial \mathcal{L}}{\partial f_{18}} = \frac{\partial \mathcal{L}}{\partial p_8} \end{array}
\right] \rightarrow \text{(influence to }\mathcal{L} )    \label{eqn:eqnnumber613}
\end{align}\]</span></p>
<p>We, therefore, should be able to construct the gradients of the <strong>Loss Function</strong> with respect to all the <strong>activation outputs</strong> in the two feature maps based on the <strong>Max Pool</strong>.</p>
<p><span class="math display">\[\begin{align}
\left[ \begin{array}{lll}  
\frac{\partial \mathcal{L}}{\partial  f_{1}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{2} } &amp; 
\frac{\partial \mathcal{L}}{\partial f_{3} }\\ 
\frac{\partial \mathcal{L}}{\partial  f_{4}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{5} } &amp; 
\frac{\partial \mathcal{L}}{\partial f_{6} }\\ 
\frac{\partial \mathcal{L}}{\partial  f_{7}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{8} } &amp;
\frac{\partial \mathcal{L}}{\partial f_{9} }\\ 
\end{array}
\right] = 
\left[ \begin{array}{lll}  
0 &amp; 0 &amp; \frac{\partial \mathcal{L}}{\partial p_{2} }\\ 
\left(
\frac{\partial \mathcal{L}}{\partial  p_{1}} + \frac{\partial \mathcal{L}}{\partial  p_{3}}\right) &amp; 0 &amp; 0\\ 
0 &amp; 0 &amp;  \frac{\partial \mathcal{L}}{\partial p_{4} }\\ 
\end{array}
\right]  \label{eqn:eqnnumber614}
\end{align}\]</span></p>
<p>Notice that the gradients with respect to <span class="math inline">\(\mathbf{p_1}\)</span> and <span class="math inline">\(\mathbf{p_3}\)</span> are overlapping as they both claim the same maximum value for <span class="math inline">\(\mathbf{f_4}\)</span>, but each comes from separate patches. For this case of overlap, we aggregate the gradients by addition.</p>
<p>The second feature map has the following corresponding gradients.</p>
<p><span class="math display">\[\begin{align}
\left[ \begin{array}{lll}  
\frac{\partial \mathcal{L}}{\partial  f_{10}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{11} } &amp; 
\frac{\partial \mathcal{L}}{\partial f_{12} }\\ 
\frac{\partial \mathcal{L}}{\partial  f_{13}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{14} } &amp; 
\frac{\partial \mathcal{L}}{\partial f_{15} }\\ 
\frac{\partial \mathcal{L}}{\partial  f_{16}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{17} } &amp; 
\frac{\partial \mathcal{L}}{\partial f_{18} }\\ 
\end{array}
\right] = 
\left[ \begin{array}{lll}  
0 &amp; 0 &amp; 0\\ 
0 &amp; \frac{\partial \mathcal{L}}{\partial  p_{5}}  &amp; \frac{\partial \mathcal{L}}{\partial  p_{6}} \\ 
0 &amp; \frac{\partial \mathcal{L}}{\partial  p_{7}}  &amp;  \frac{\partial \mathcal{L}}{\partial p_{8} }\\ 
\end{array}
\right]  \label{eqn:eqnnumber615}
\end{align}\]</span></p>
<p>Overall, the individual contribution for each cell to the <strong>Loss function</strong> can be expressed in this simple equation:</p>
<p><span class="math display">\[\begin{align}
\nabla f_i \mathcal{L} = \sum_{j}
\begin{cases}  
\frac{\partial \mathcal{L}}{\partial p_j} &amp; if\ f_i =  p_j \\
0 &amp; otherwise 
\end{cases}
\ \ \ \ \ \ \
 where\ p_j = \text{max}(\text{patch}_j\text{)}  \label{eqn:eqnnumber616}
\end{align}\]</span></p>
<p>Our implementation of the <strong>Pooling Layer</strong> uses cached indices from the convolution operation to trace the image pixels (or neurons from previous feature maps) responsible for bearing the influence (see the use of <strong>pool.cache</strong> from <strong>convolution(.)</strong> function). Each neuron from the <strong>gradient map</strong> corresponds to a patch region in the image.</p>
<p>Now, it is important to emphasize that if we do not have a <strong>Pooling layer</strong> between a <strong>Convolutional layer</strong> and an <strong>FC layer</strong>, then the <strong>gradient</strong> with respect to each element in the <strong>flattened vector</strong> maps directly to the <strong>first previous</strong> <strong>feature map</strong> without having to deal with <strong>maximum or average values</strong>.</p>
<p>Nonetheless, the <strong>output</strong> in <strong>CNN</strong> from a <strong>convolution</strong> operation (which may include using an <strong>activation function</strong> such as <strong>RELU</strong> or <strong>Leaky RELU</strong>) maps to a cell in a <strong>feature map</strong>. Such a cell can also be regarded as <strong>neuron</strong> receiving an <strong>activation output</strong> similar to that in <strong>MLP</strong>. Here, the gradient of the <strong>Loss</strong> with respect to the <strong>activation output</strong> is backpropagated.</p>
<p><strong>Eight</strong>, we then depend on the gradients above to solve for the gradient of the <strong>Loss function</strong> with respect to the input image (or previous feature maps). Now, when dealing with <strong>backpropagation</strong>, we need to trace the influence of each component to the <strong>Loss Function</strong> from layer to layer backward. Sometimes, the needed operations are quite involved, requiring some effort. However, from time to time, a few tricks are discovered to allow certain operations to use computations that are mathematically convenient, especially, as an example, when computing for the gradients of the weights. Here, we can use <strong>Full Convolution</strong> and <span class="math inline">\(\mathbf{180^{\circ}}\)</span> matrix rotation as introduced in other literature. To illustrate, let us use Figure <a href="deeplearning1.html#fig:fullconvolv0">12.37</a>. The convolution in the figure forms a <span class="math inline">\(2 \times 2\)</span> feature map.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fullconvolv0"></span>
<img src="fullconvolv0.png" alt="CNN (Full Convolution without Padding)" width="90%" />
<p class="caption">
Figure 12.37: CNN (Full Convolution without Padding)
</p>
</div>
<p>Below is the complete influence of I to individual <strong>neurons</strong> when multiplied to weights:</p>
<p><span class="math display">\[\begin{align}
\begin{array}{ll}
f_{11} &amp;= I_{11} \times k_1 + I_{12} \times k_2  + I_{21} \times k_3  + I_{22} \times k_4 \\
f_{12} &amp;= I_{12} \times k_1 + I_{13} \times k_2  + I_{22} \times k_3  + I_{23} \times k_4 \\
f_{21} &amp;= I_{21} \times k_1 + I_{22} \times k_2  + I_{31} \times k_3  + I_{32} \times k_4 \\
f_{22} &amp;= I_{22} \times k_1 + I_{23} \times k_2  + I_{32} \times k_3  + I_{33} \times k_4 \\
\end{array}  \label{eqn:eqnnumber617}
\end{align}\]</span></p>
<p>We can observe in the formulations above that the influence of one specific image feature, namely <span class="math inline">\(I_{11}\)</span>, only applies to <span class="math inline">\(f_{11}\)</span>. In terms of gradient, we can therefore show that as:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial I_{11}}  = \frac{\partial \mathcal{L}}{\partial f_{11}} \frac{\partial f_{11}}{\partial I_{11}} = \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_1 
\end{align}\]</span></p>
<p>The other image features, such as <span class="math inline">\(I_{13}\)</span>, <span class="math inline">\(I_{31}\)</span>, and <span class="math inline">\(I_{33}\)</span>, follow the same gradient formulation. Additionally, <span class="math inline">\(I_{12}\)</span> influences two neurons, and so the gradients are aggregated like so:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial I_{12}}  = \frac{\partial \mathcal{L}}{\partial f_{11}} \frac{\partial f_{11}}{\partial I_{12}} + \frac{\partial \mathcal{L}}{\partial f_{12}} \frac{\partial f_{12}}{\partial I_{12}} = \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_2 + \frac{\partial \mathcal{L}}{\partial f_{12}} \times k_1 
\end{align}\]</span></p>
<p>The others follow the same formulation.</p>
<p>Now, to derive them all, we use <strong>Full Convolution</strong> for the gradients of all <strong>activation outputs</strong>, e.g. <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial f_{11}}, ..., \frac{\partial \mathcal{L}}{\partial f_{18}}\)</span>, with the corresponding kernels which are rotated about <span class="math inline">\(\mathbf{180}^{\circ}\)</span>. From there, we end up with the following:</p>
<p><span class="math display">\[\begin{align}
\begin{array}{ll}
\frac{\partial \mathcal{L}}{\partial I_{11}} &amp;= \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_1  \\ 
\frac{\partial \mathcal{L}}{\partial I_{13}} &amp;= \frac{\partial \mathcal{L}}{\partial f_{12}} \times k_2  \\ 
\frac{\partial \mathcal{L}}{\partial I_{31}} &amp;= \frac{\partial \mathcal{L}}{\partial f_{21}} \times k_3  \\ 
\frac{\partial \mathcal{L}}{\partial I_{33}} &amp;= \frac{\partial \mathcal{L}}{\partial f_{22}} \times k_4  \\ 
\end{array}
\ \ \ \ \ \ \ \ 
\begin{array}{ll}
\frac{\partial \mathcal{L}}{\partial I_{12}} &amp;=  \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_2 + \frac{\partial \mathcal{L}}{f_{12}} \times k_1 \\ 
\frac{\partial \mathcal{L}}{\partial I_{21}} &amp;=  \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_3 + \frac{\partial \mathcal{L}}{f_{21}} \times k_1 \\ 
\frac{\partial \mathcal{L}}{\partial I_{23}} &amp;=  \frac{\partial \mathcal{L}}{\partial f_{12}} \times k_4 + \frac{\partial \mathcal{L}}{f_{22}} \times k_2 \\ 
\frac{\partial \mathcal{L}}{\partial I_{32}} &amp;=  \frac{\partial \mathcal{L}}{\partial f_{21}} \times k_4 + \frac{\partial \mathcal{L}}{f_{22}} \times k_3  \\ 
\end{array}  \label{eqn:eqnnumber618}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial I_{22}} =  \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_4 + \frac{\partial \mathcal{L}}{\partial f_{12}} \times k_3  + \frac{\partial \mathcal{L}}{\partial f_{21}} \times k_2  + \frac{\partial \mathcal{L}}{\partial f_{22}} \times k_1
\end{align}\]</span></p>
<p>Notice that the gradients of individual <strong>activation output</strong> with respect to image features result in individual weights:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial f_{11}} {\partial I_{11}} = k_1
\ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\frac{\partial f_{11}} {\partial I_{12}} =  k_2
\ \ \ \ \ \ \ 
...
\ \ \ \ \ \ \ 
\frac{\partial f_{22}} {\partial I_{33}} = k_4
\end{align}\]</span></p>
<p>Equivalently, from individual neuron perspective, the gradient of an <strong>activation output</strong>, e.g. <span class="math inline">\(\mathbf{f_{11}}\)</span>, with respect to a weight, e.g. <span class="math inline">\(\mathbf{k_1}\)</span>, in the filter is:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial f_{11}} {\partial k_1} = I_{11}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\frac{\partial f_{11}} {\partial k_2} = I_{12}
\ \ \ \ \ \ \ 
...
\ \ \ \ \ \ \ 
\frac{\partial f_{22}} {\partial k_4} = I_{33}
\end{align}\]</span></p>
<p>Let us take another case in which our convolution includes padding equal to 1. See Figure <a href="deeplearning1.html#fig:fullconvolv1">12.38</a>. The convolution in Figure <a href="deeplearning1.html#fig:fullconvolv1">12.38</a> forms a <span class="math inline">\(4 \times 4\)</span> feature map because of the extra padding.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fullconvolv1"></span>
<img src="fullconvolv1.png" alt="CNN (Full Convolution with Padding)" width="90%" />
<p class="caption">
Figure 12.38: CNN (Full Convolution with Padding)
</p>
</div>
<p>Notice this time that the <strong>Full Convolution</strong> does not require padding during backpropagation, whereas during the <strong>feedforward</strong>, we have padding set to 1.</p>
<p>Therefore, we should notice that performing a <strong>Full Convolution</strong> with a rotated kernel at <span class="math inline">\(180^\circ\)</span> requires careful tracing of the influence of every neuron and weight to the <strong>Loss Function</strong>. With the many permutations of image size, kernel size, and the number of stride and paddings, it requires a bit of heuristic iteration to arrive at the correct combination of knobs to use (in terms of stride, padding, dilation, auto padding) for our full convolution. Figure <a href="deeplearning1.html#fig:heuristicbp">12.39</a> illustrates a combination table as a result of heuristically iterating over a few adjustments of the hyperparameters.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:heuristicbp"></span>
<img src="heuristicbp.png" alt="BackProp Combination (Heuristic)" width="70%" />
<p class="caption">
Figure 12.39: BackProp Combination (Heuristic)
</p>
</div>
<p>For example, if we use stride=1 with padding=0, e.g.Â 1/0, against an image of size <span class="math inline">\(4\times4\)</span> with kernel size <span class="math inline">\(3 \times 3\)</span>, e.g.Â 4/3, then our <strong>full convolution</strong> should use <strong>1200</strong> (stride=1, padding=2, dilated image=0, auto.pad=0). Programmatically, that is equivalent to the following code:</p>

<div class="sourceCode" id="cb2033"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2033-1" data-line-number="1"><span class="kw">convolution</span>(<span class="dt">image   =</span> Dout,   <span class="dt">filter  =</span> rotated.filters, </a>
<a class="sourceLine" id="cb2033-2" data-line-number="2">            <span class="dt">stride  =</span> <span class="dv">1</span>,      <span class="dt">padding =</span> <span class="dv">2</span>,  <span class="dt">dil_input =</span> <span class="dv">0</span>,  <span class="dt">auto.pad =</span> <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb2033-3" data-line-number="3">            <span class="dt">autopad =</span> <span class="st">&quot;left&quot;</span>, <span class="dt">ptype   =</span> <span class="st">&quot;gradient.I.wrt.K&quot;</span>)</a></code></pre></div>

<p>To automatically determine the proper combination, a few patterns in the table expose the below set of formulas:</p>
<p><span class="math display">\[\begin{align}
\begin{array}{ll}
\text{stride} &amp;= 1\\
\text{padding} &amp;= \text{max}(\text{kernel.ht} - (\text{padding} + \text{auto.pad}  + 1), 0)\\
\text{dilate}\_\text{input} &amp;= \text{stride} - 1 \\
\text{auto.pad} &amp;= \text{auto.pad} \\
\text{auto.pad} &amp;= \text{left}
\end{array}  \label{eqn:eqnnumber619}
\end{align}\]</span></p>
<p>Note here that we can pass <strong>dilation rate</strong> (e.g., dil_input) to dilate the jacobian matrix of the gradients, and we can also pass <strong>dilation rate</strong> hyperparameter (e.g., dil_rate) to dilate receptive fields (which is always set to zero). Additionally, auto padding is done to the left and top edges.</p>
<p>Also, our brief experimentation does show that an image size of 4 and a kernel size of 3 does not produce a combination for our full convolution if our stride and paddings are set to 2, as shown in Figure <a href="deeplearning1.html#fig:heuristicbp">12.39</a>. This experiment tells us that while the formulas above work in reasonably general cases, there are configurations that may not apply. Here, a decent general case implies that a kernel size of 3 and stride of 1 is a good choice versus a stride of 4, which leaves a gap between receptive fields, causing loss of information. Thus, in our later implementation of <strong>CNN</strong>, we constrain the stride size not to be equal to or greater than the kernel size. We leave readers to investigate this combination and re-evaluate the general case above.</p>
<p><strong>Ninth</strong>, in terms of solving for the gradient of the <strong>Loss function</strong> with respect to the individual weights (parameters in the kernels) in a <strong>convolutional layer</strong>, we take one of the learnable weights as an example, e.g., <span class="math inline">\(\mathbf{k_1}\)</span>, from Figure <a href="deeplearning1.html#fig:cnnbackprop">12.36</a>. In such a case, we have the following equation:</p>
<p><span class="math display">\[\begin{align}
\nabla_{k_1} \mathcal{L} = \frac{\partial \mathcal{L}} {\partial k_1} = 
\left(\frac{\partial \mathcal{L}}{\partial f_1}\right)
\left(\frac{\partial f_1} {\partial k_1}\right)  
\end{align}\]</span></p>
<p>If an <strong>activation function</strong> such as <strong>RELU</strong> or <strong>Leaky RELU</strong> is involved, then we can also use the following equation:</p>
<p><span class="math display">\[\begin{align}
\nabla_{k_1} \mathcal{L} = \frac{\partial \mathcal{L}} {\partial k_1} = 
\left(\frac{\partial \mathcal{L}}{\partial f_1}\right)
\left(\frac{\partial f_1} {\partial \hat{f_1}}\right)  
\left(\frac{\partial \hat{f_1}} {\partial k_1}\right) 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\hat{f}_i\)</span> is a <strong>net input</strong> as result of the convolution prior to invoking an <strong>activation function</strong>.</p>
<p>Here, we use <strong>convolution</strong> this time to perform backpropagation. Similar to <strong>full convolution</strong>, we need the correct knobs to use. Below is a set of formulas that can be applied in general cases.</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\text{stride} &amp;= 1\\
\text{padding} &amp;= \text{padding}\\
\text{dilate}\_\text{rate} &amp;= \text{stride} - 1\\
\text{auto.pad} &amp;= \text{auto.pad} \\
\text{autapad} &amp;= \begin{cases}
\text{right} &amp; \text{auto.pad} &gt; 0  \\
\text{left} &amp; \text{otherwise}
\end{cases}
\end{array} 
\end{align*}\]</span></p>
<p>For example, suppose we use a stride=2 and padding=0 against an image with size <span class="math inline">\(4 \times 4\)</span> and a kernel with size <span class="math inline">\(3 \times 3\)</span>, we arrive at five knobs, e.g. (stride=1, padding=0, dilate_rate=1, auto.pad = 0, autopad=right), to adjust for our backpropagation that also goes through its convolution process (which we tag it with type equal to <strong>gradient.K.wrt.I</strong> in our implementation). Programmatically, that can be written as such:</p>

<div class="sourceCode" id="cb2034"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2034-1" data-line-number="1">cv =<span class="st"> </span><span class="kw">convolution</span>(<span class="dt">image=</span>image, <span class="dt">filters=</span>Dout, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding=</span><span class="dv">0</span>, <span class="dt">dil_rate=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2034-2" data-line-number="2">     <span class="dt">dil_input=</span><span class="dv">0</span>, <span class="dt">auto.pad =</span> <span class="dv">1</span>, <span class="dt">autopad =</span> <span class="st">&quot;right&quot;</span>, <span class="dt">ptype=</span><span class="st">&quot;gradient.K.wrt.I&quot;</span>)</a></code></pre></div>

<p>Note here that we can pass the <strong>dilation rate</strong> parameter (e.g., dil_rate) to dilate receptive fields, and we can also pass <strong>dilation rate</strong> (e.g., dil_input) to dilate images/feature maps (which is always set to zero).</p>
<p>Let us now see our example implementation of <strong>CNN Backpropagation</strong>. For that, let us show a summarized form of backpropagation (see Figure <a href="deeplearning1.html#fig:cnnbackpropsummary">12.40</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cnnbackpropsummary"></span>
<img src="cnnbackpropsummary.png" alt="CNN (Backpropagation Summary)" width="100%" />
<p class="caption">
Figure 12.40: CNN (Backpropagation Summary)
</p>
</div>

<div class="sourceCode" id="cb2035"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2035-1" data-line-number="1">get.gradient &lt;-<span class="st"> </span><span class="cf">function</span>(map, afunc) { <span class="co"># @P1@^P1</span></a>
<a class="sourceLine" id="cb2035-2" data-line-number="2">    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(afunc)) {</a>
<a class="sourceLine" id="cb2035-3" data-line-number="3">        di =<span class="st"> </span><span class="kw">dim</span>(map)</a>
<a class="sourceLine" id="cb2035-4" data-line-number="4">        afunc =<span class="st"> </span><span class="kw">get</span>(afunc)</a>
<a class="sourceLine" id="cb2035-5" data-line-number="5">        gradient =<span class="st"> </span>map</a>
<a class="sourceLine" id="cb2035-6" data-line-number="6">        <span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">4</span>])</a>
<a class="sourceLine" id="cb2035-7" data-line-number="7">        <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">3</span>]) {</a>
<a class="sourceLine" id="cb2035-8" data-line-number="8">            gradient[,,d,s] =<span class="st">  </span><span class="kw">gradient.activation</span>(map[,,d,s], afunc) </a>
<a class="sourceLine" id="cb2035-9" data-line-number="9">        }</a>
<a class="sourceLine" id="cb2035-10" data-line-number="10">        gradient =<span class="st"> </span><span class="kw">array</span>(gradient, di) </a>
<a class="sourceLine" id="cb2035-11" data-line-number="11">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2035-12" data-line-number="12">        gradient =<span class="st"> </span>map  </a>
<a class="sourceLine" id="cb2035-13" data-line-number="13">    } </a>
<a class="sourceLine" id="cb2035-14" data-line-number="14">    gradient  </a>
<a class="sourceLine" id="cb2035-15" data-line-number="15">}</a></code></pre></div>
<div class="sourceCode" id="cb2036"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2036-1" data-line-number="1">back.propagation.cnn &lt;-<span class="cf">function</span> (X, Y, model) {</a>
<a class="sourceLine" id="cb2036-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2036-3" data-line-number="3">    layers    =<span class="st"> </span>model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2036-4" data-line-number="4">    H         =<span class="st"> </span><span class="kw">length</span>(layers) <span class="op">-</span><span class="st"> </span><span class="dv">1</span> <span class="co">#remove FC layer and focus on CONV layers</span></a>
<a class="sourceLine" id="cb2036-5" data-line-number="5">    fc.layers =<span class="st"> </span>model<span class="op">$</span>fc.model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2036-6" data-line-number="6">    backprop  =<span class="st"> </span><span class="kw">back.propagation</span>(model<span class="op">$</span>flattened.output, Y, model<span class="op">$</span>fc.model, </a>
<a class="sourceLine" id="cb2036-7" data-line-number="7">                                       <span class="dt">afunc=</span><span class="st">&quot;softmax&quot;</span>) </a>
<a class="sourceLine" id="cb2036-8" data-line-number="8">    Dout      =<span class="st"> </span>backprop<span class="op">$</span>delta.output[[<span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb2036-9" data-line-number="9">    di        =<span class="st"> </span><span class="kw">dim</span>(model<span class="op">$</span>cnn.output[[H]]<span class="op">$</span>pw.fmap)  </a>
<a class="sourceLine" id="cb2036-10" data-line-number="10">    gradient.loss =<span class="st"> </span>Dout <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(fc.layers[[<span class="dv">1</span>]]<span class="op">$</span>omega<span class="op">$</span>weight) <span class="co"># @L/@P1</span></a>
<a class="sourceLine" id="cb2036-11" data-line-number="11">    gradient.loss =<span class="st"> </span>gradient.loss[,<span class="op">-</span><span class="dv">1</span>] <span class="co">#remove bias</span></a>
<a class="sourceLine" id="cb2036-12" data-line-number="12"></a>
<a class="sourceLine" id="cb2036-13" data-line-number="13">    loss.I           =<span class="st"> </span><span class="kw">array</span>(<span class="kw">t</span>(gradient.loss), di) </a>
<a class="sourceLine" id="cb2036-14" data-line-number="14">    delta.normparams =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2036-15" data-line-number="15">    delta.dws        =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2036-16" data-line-number="16">    delta.pws        =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2036-17" data-line-number="17">    delta.dw.biases  =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2036-18" data-line-number="18">    delta.pw.biases  =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2036-19" data-line-number="19">    <span class="cf">for</span> (L <span class="cf">in</span> H<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2036-20" data-line-number="20">        layer     =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb2036-21" data-line-number="21">        cache     =<span class="st"> </span>model<span class="op">$</span>cache.output[[L]]</a>
<a class="sourceLine" id="cb2036-22" data-line-number="22">        cur.fmaps =<span class="st"> </span>model<span class="op">$</span>cnn.output[[L]]</a>
<a class="sourceLine" id="cb2036-23" data-line-number="23">        <span class="cf">if</span> (L <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2036-24" data-line-number="24">            prev.fmaps =<span class="st"> </span>model<span class="op">$</span>cnn.output[[L<span class="dv">-1</span>]]<span class="op">$</span>pw.fmap</a>
<a class="sourceLine" id="cb2036-25" data-line-number="25">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2036-26" data-line-number="26">        <span class="cf">if</span> (L<span class="op">==</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2036-27" data-line-number="27">            prev.fmaps =<span class="st"> </span>X</a>
<a class="sourceLine" id="cb2036-28" data-line-number="28">        }</a>
<a class="sourceLine" id="cb2036-29" data-line-number="29">        <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;pooling&quot;</span>) {</a>
<a class="sourceLine" id="cb2036-30" data-line-number="30">            conv.output =<span class="st"> </span><span class="kw">pool.backprop</span>(prev.fmaps, </a>
<a class="sourceLine" id="cb2036-31" data-line-number="31">                           <span class="dt">filter  =</span> layer<span class="op">$</span>window,</a>
<a class="sourceLine" id="cb2036-32" data-line-number="32">                           <span class="dt">Dout    =</span> loss.I,</a>
<a class="sourceLine" id="cb2036-33" data-line-number="33">                           <span class="dt">stride  =</span> layer<span class="op">$</span>stride, </a>
<a class="sourceLine" id="cb2036-34" data-line-number="34">                           <span class="dt">pool.cache =</span> model<span class="op">$</span>pool.cache[[L]],</a>
<a class="sourceLine" id="cb2036-35" data-line-number="35">                           <span class="dt">ptype   =</span> <span class="st">&quot;gradient.I.wrt.P&quot;</span>,</a>
<a class="sourceLine" id="cb2036-36" data-line-number="36">                           <span class="dt">pool    =</span> layer<span class="op">$</span>ptype)</a>
<a class="sourceLine" id="cb2036-37" data-line-number="37">           loss.I             =<span class="st"> </span>conv.output<span class="op">$</span>feature.map </a>
<a class="sourceLine" id="cb2036-38" data-line-number="38">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2036-39" data-line-number="39">        <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;convolv&quot;</span>) { </a>
<a class="sourceLine" id="cb2036-40" data-line-number="40">           <span class="co"># activation gradient first before normalization</span></a>
<a class="sourceLine" id="cb2036-41" data-line-number="41">           <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(layer<span class="op">$</span>afunc)) { </a>
<a class="sourceLine" id="cb2036-42" data-line-number="42">                gradient.I =<span class="st"> </span><span class="kw">get.gradient</span>(cur.fmaps<span class="op">$</span>pw.fmap, layer<span class="op">$</span>afunc)</a>
<a class="sourceLine" id="cb2036-43" data-line-number="43">                Dout       =<span class="st"> </span>loss.I <span class="op">*</span><span class="st"> </span>gradient.I </a>
<a class="sourceLine" id="cb2036-44" data-line-number="44">            } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2036-45" data-line-number="45">                Dout       =<span class="st"> </span>loss.I</a>
<a class="sourceLine" id="cb2036-46" data-line-number="46">            }</a>
<a class="sourceLine" id="cb2036-47" data-line-number="47">            <span class="cf">if</span> (layer<span class="op">$</span>batchnorm<span class="op">$</span>normalize <span class="op">!=</span><span class="st"> </span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb2036-48" data-line-number="48">                normalize             =<span class="st"> </span><span class="kw">normalize.backward</span>(Dout, layer)</a>
<a class="sourceLine" id="cb2036-49" data-line-number="49">                Dout                  =<span class="st"> </span>normalize<span class="op">$</span>gradient.loss</a>
<a class="sourceLine" id="cb2036-50" data-line-number="50">                delta.normparams[[L]] =<span class="st"> </span>normalize<span class="op">$</span>params</a>
<a class="sourceLine" id="cb2036-51" data-line-number="51">            }</a>
<a class="sourceLine" id="cb2036-52" data-line-number="52"></a>
<a class="sourceLine" id="cb2036-53" data-line-number="53">            conv.output   =<span class="st"> </span><span class="kw">convolution</span>(prev.fmaps, </a>
<a class="sourceLine" id="cb2036-54" data-line-number="54">                        <span class="dt">filter    =</span> Dout, <span class="co"># used only to calculate shape</span></a>
<a class="sourceLine" id="cb2036-55" data-line-number="55">                        <span class="dt">Dout      =</span> Dout, </a>
<a class="sourceLine" id="cb2036-56" data-line-number="56">                        <span class="dt">cur.fmaps =</span> cur.fmaps,</a>
<a class="sourceLine" id="cb2036-57" data-line-number="57">                        <span class="dt">pw.kernel =</span> layer<span class="op">$</span>pw.kernel<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2036-58" data-line-number="58">                        <span class="dt">stride    =</span> cache<span class="op">$</span>wstride, </a>
<a class="sourceLine" id="cb2036-59" data-line-number="59">                        <span class="dt">padding   =</span> cache<span class="op">$</span>wpadding,  </a>
<a class="sourceLine" id="cb2036-60" data-line-number="60">                        <span class="dt">dil_rate  =</span> cache<span class="op">$</span>wdil_rate,</a>
<a class="sourceLine" id="cb2036-61" data-line-number="61">                        <span class="dt">dil_input =</span> cache<span class="op">$</span>wdil_input,</a>
<a class="sourceLine" id="cb2036-62" data-line-number="62">                        <span class="dt">auto.pad  =</span> cache<span class="op">$</span>wauto.pad,</a>
<a class="sourceLine" id="cb2036-63" data-line-number="63">                        <span class="dt">autopad   =</span> cache<span class="op">$</span>wautopad,</a>
<a class="sourceLine" id="cb2036-64" data-line-number="64">                        <span class="dt">ptype     =</span> <span class="st">&quot;gradient.K.wrt.I&quot;</span>)</a>
<a class="sourceLine" id="cb2036-65" data-line-number="65">            delta.dws[[L]] =<span class="st"> </span>conv.output<span class="op">$</span>feature.map<span class="op">$</span>delta.dw</a>
<a class="sourceLine" id="cb2036-66" data-line-number="66">            delta.pws[[L]] =<span class="st"> </span>conv.output<span class="op">$</span>feature.map<span class="op">$</span>delta.pw</a>
<a class="sourceLine" id="cb2036-67" data-line-number="67">            dw.Dout        =<span class="st"> </span>conv.output<span class="op">$</span>feature.map<span class="op">$</span>dw.Dout</a>
<a class="sourceLine" id="cb2036-68" data-line-number="68">            pw.Dout        =<span class="st"> </span>conv.output<span class="op">$</span>feature.map<span class="op">$</span>pw.Dout</a>
<a class="sourceLine" id="cb2036-69" data-line-number="69">            </a>
<a class="sourceLine" id="cb2036-70" data-line-number="70">            <span class="co"># Compute Biases</span></a>
<a class="sourceLine" id="cb2036-71" data-line-number="71">            di.dw          =<span class="st"> </span><span class="kw">dim</span>(dw.Dout)</a>
<a class="sourceLine" id="cb2036-72" data-line-number="72">            di.pw          =<span class="st"> </span><span class="kw">dim</span>(pw.Dout)</a>
<a class="sourceLine" id="cb2036-73" data-line-number="73">            delta.dw.bias  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, di.dw[<span class="dv">3</span>])</a>
<a class="sourceLine" id="cb2036-74" data-line-number="74">            delta.pw.bias  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, di.pw[<span class="dv">3</span>])</a>
<a class="sourceLine" id="cb2036-75" data-line-number="75">            <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di.dw[<span class="dv">3</span>]) { delta.dw.bias[d] =<span class="st"> </span><span class="kw">sum</span>(dw.Dout[,,d,]) }</a>
<a class="sourceLine" id="cb2036-76" data-line-number="76">            <span class="cf">for</span> (f <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di.pw[<span class="dv">3</span>]) { delta.pw.bias[f] =<span class="st"> </span><span class="kw">sum</span>(pw.Dout[,,f,]) }</a>
<a class="sourceLine" id="cb2036-77" data-line-number="77"></a>
<a class="sourceLine" id="cb2036-78" data-line-number="78">            delta.dw.biases[[L]] =<span class="st"> </span>delta.dw.bias</a>
<a class="sourceLine" id="cb2036-79" data-line-number="79">            delta.pw.biases[[L]] =<span class="st"> </span>delta.pw.bias</a>
<a class="sourceLine" id="cb2036-80" data-line-number="80"></a>
<a class="sourceLine" id="cb2036-81" data-line-number="81">            rotated.kernel =<span class="st"> </span><span class="kw">rotate.matrix.180</span>(layer<span class="op">$</span>dw.kernel<span class="op">$</span>weight)</a>
<a class="sourceLine" id="cb2036-82" data-line-number="82">            conv.output   =<span class="st"> </span><span class="kw">convolution</span>(dw.Dout, </a>
<a class="sourceLine" id="cb2036-83" data-line-number="83">                        <span class="dt">filter    =</span> rotated.kernel, </a>
<a class="sourceLine" id="cb2036-84" data-line-number="84">                        <span class="dt">stride    =</span> cache<span class="op">$</span>stride, </a>
<a class="sourceLine" id="cb2036-85" data-line-number="85">                        <span class="dt">padding   =</span> cache<span class="op">$</span>padding,  </a>
<a class="sourceLine" id="cb2036-86" data-line-number="86">                        <span class="dt">dil_rate  =</span> cache<span class="op">$</span>dil_rate,</a>
<a class="sourceLine" id="cb2036-87" data-line-number="87">                        <span class="dt">dil_input =</span> cache<span class="op">$</span>dil_input,</a>
<a class="sourceLine" id="cb2036-88" data-line-number="88">                        <span class="dt">auto.pad  =</span> cache<span class="op">$</span>auto.pad,</a>
<a class="sourceLine" id="cb2036-89" data-line-number="89">                        <span class="dt">autopad   =</span> cache<span class="op">$</span>autopad,</a>
<a class="sourceLine" id="cb2036-90" data-line-number="90">                        <span class="dt">ptype     =</span> <span class="st">&quot;gradient.I.wrt.K&quot;</span>)</a>
<a class="sourceLine" id="cb2036-91" data-line-number="91">            loss.I =<span class="st"> </span>conv.output<span class="op">$</span>feature.map             </a>
<a class="sourceLine" id="cb2036-92" data-line-number="92">        }</a>
<a class="sourceLine" id="cb2036-93" data-line-number="93">    } </a>
<a class="sourceLine" id="cb2036-94" data-line-number="94">    <span class="kw">list</span>(<span class="st">&quot;fc.delta.params&quot;</span>      =<span class="st"> </span>backprop<span class="op">$</span>delta.params,</a>
<a class="sourceLine" id="cb2036-95" data-line-number="95">         <span class="st">&quot;cnn.delta.normparams&quot;</span> =<span class="st"> </span>delta.normparams,</a>
<a class="sourceLine" id="cb2036-96" data-line-number="96">         <span class="st">&quot;cnn.delta.dws&quot;</span>        =<span class="st"> </span>delta.dws,</a>
<a class="sourceLine" id="cb2036-97" data-line-number="97">         <span class="st">&quot;cnn.delta.pws&quot;</span>        =<span class="st"> </span>delta.pws,</a>
<a class="sourceLine" id="cb2036-98" data-line-number="98">         <span class="st">&quot;cnn.delta.dw.biases&quot;</span>  =<span class="st"> </span>delta.dw.biases,</a>
<a class="sourceLine" id="cb2036-99" data-line-number="99">         <span class="st">&quot;cnn.delta.pw.biases&quot;</span>  =<span class="st"> </span>delta.pw.biases)</a>
<a class="sourceLine" id="cb2036-100" data-line-number="100">}</a></code></pre></div>

<p>Our implementation of <strong>full convolution</strong> rotates the filter. Below is a script to perform matrix rotation.</p>

<div class="sourceCode" id="cb2037"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2037-1" data-line-number="1">rotate<span class="fl">.90</span> &lt;-<span class="st"> </span><span class="cf">function</span>(filters) {</a>
<a class="sourceLine" id="cb2037-2" data-line-number="2">    len =<span class="st"> </span><span class="kw">length</span>(filters)</a>
<a class="sourceLine" id="cb2037-3" data-line-number="3">    <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2037-4" data-line-number="4">       a =<span class="st"> </span>filters[[l]]</a>
<a class="sourceLine" id="cb2037-5" data-line-number="5">       dl =<span class="st"> </span><span class="kw">dim</span>(a); h =<span class="st"> </span>dl[<span class="dv">1</span>]; w =<span class="st"> </span>dl[<span class="dv">2</span>]; d =<span class="st"> </span>dl[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2037-6" data-line-number="6">       mx =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2037-7" data-line-number="7">       <span class="cf">for</span> (depth <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>d) {</a>
<a class="sourceLine" id="cb2037-8" data-line-number="8">         m =<span class="st"> </span>a[,,depth]</a>
<a class="sourceLine" id="cb2037-9" data-line-number="9">         mx =<span class="st"> </span><span class="kw">cbind</span>(mx, <span class="kw">t</span>(m[<span class="kw">nrow</span>(m)<span class="op">:</span><span class="dv">1</span>,,<span class="dt">drop =</span> <span class="ot">FALSE</span>]))</a>
<a class="sourceLine" id="cb2037-10" data-line-number="10">       }</a>
<a class="sourceLine" id="cb2037-11" data-line-number="11">       filters[[l]] =<span class="st"> </span><span class="kw">array</span>(mx, <span class="kw">c</span>(h, w, d))</a>
<a class="sourceLine" id="cb2037-12" data-line-number="12">    }</a>
<a class="sourceLine" id="cb2037-13" data-line-number="13">    filters</a>
<a class="sourceLine" id="cb2037-14" data-line-number="14">}</a></code></pre></div>
<div class="sourceCode" id="cb2038"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2038-1" data-line-number="1">rotate<span class="fl">.180</span> &lt;-<span class="st"> </span><span class="cf">function</span>(a) { <span class="kw">rotate.90</span>(<span class="kw">rotate.90</span>(a)) }</a>
<a class="sourceLine" id="cb2038-2" data-line-number="2">rotate.matrix<span class="fl">.90</span> &lt;-<span class="st"> </span><span class="cf">function</span>(a) {</a>
<a class="sourceLine" id="cb2038-3" data-line-number="3">  dl =<span class="st"> </span><span class="kw">dim</span>(a)</a>
<a class="sourceLine" id="cb2038-4" data-line-number="4">  mx =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2038-5" data-line-number="5">  <span class="cf">for</span> (depth <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>dl[<span class="dv">3</span>]) {</a>
<a class="sourceLine" id="cb2038-6" data-line-number="6">    m =<span class="st"> </span>a[,,depth]</a>
<a class="sourceLine" id="cb2038-7" data-line-number="7">    mx =<span class="st"> </span><span class="kw">cbind</span>(mx, <span class="kw">t</span>(m[<span class="kw">nrow</span>(m)<span class="op">:</span><span class="dv">1</span>,,<span class="dt">drop =</span> <span class="ot">FALSE</span>]))</a>
<a class="sourceLine" id="cb2038-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb2038-9" data-line-number="9">  <span class="kw">array</span>(mx, dl)</a>
<a class="sourceLine" id="cb2038-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb2038-11" data-line-number="11">rotate.matrix<span class="fl">.180</span> &lt;-<span class="st"> </span><span class="cf">function</span>(a) {</a>
<a class="sourceLine" id="cb2038-12" data-line-number="12">    <span class="kw">rotate.matrix.90</span>(<span class="kw">rotate.matrix.90</span>(a)) </a>
<a class="sourceLine" id="cb2038-13" data-line-number="13">}</a></code></pre></div>

</div>
<div id="optimization-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.10</span> Optimization<a href="deeplearning1.html#optimization-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Finally</strong>, reviewing Figure <a href="deeplearning1.html#fig:cnnbackprop">12.36</a> once again, we come down to the update rules. For the <strong>Backward Pass</strong>, let us use one of the kernel weights, e.g. <span class="math inline">\(k_1\)</span>, to illustrate.</p>
<p><span class="math display">\[\begin{align}
k_1 = k_1 - \eta \times \nabla k_1 \mathcal{L}
\ \ \ \ \ \ \ 
...
\ \ \ \ \ \ \ 
k_4 = k_4 - \eta \times \nabla k_4 \mathcal{L}
\end{align}\]</span></p>
<p>Our implementation of the backward pass is encapsulated into the <strong>optimizer(.)</strong> function. Recall our discussion of <strong>optimization</strong> in <strong>MLP</strong> section. Here, we implement a few of the known optimizers. See below:</p>

<div class="sourceCode" id="cb2039"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2039-1" data-line-number="1">optimizer   &lt;-<span class="st"> </span><span class="cf">function</span>(backprop, layers, <span class="dt">optimize=</span><span class="st">&quot;momentum&quot;</span>, t, eta) {</a>
<a class="sourceLine" id="cb2039-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2039-3" data-line-number="3">  optimize.sgd =<span class="st"> </span>sgd &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, <span class="dt">t =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb2039-4" data-line-number="4">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>gradient</a>
<a class="sourceLine" id="cb2039-5" data-line-number="5">    param</a>
<a class="sourceLine" id="cb2039-6" data-line-number="6">  }</a>
<a class="sourceLine" id="cb2039-7" data-line-number="7">         </a>
<a class="sourceLine" id="cb2039-8" data-line-number="8">  optimize.momentum =<span class="st"> </span>momentum &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, <span class="dt">t =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb2039-9" data-line-number="9">    gamma        =<span class="st"> </span><span class="fl">0.90</span></a>
<a class="sourceLine" id="cb2039-10" data-line-number="10">    param<span class="op">$</span>nu     =<span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>gradient</a>
<a class="sourceLine" id="cb2039-11" data-line-number="11">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>param<span class="op">$</span>nu</a>
<a class="sourceLine" id="cb2039-12" data-line-number="12">    param</a>
<a class="sourceLine" id="cb2039-13" data-line-number="13">  }</a>
<a class="sourceLine" id="cb2039-14" data-line-number="14">                  </a>
<a class="sourceLine" id="cb2039-15" data-line-number="15">  optimize.rmsprop =<span class="st"> </span>rmsprop &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, <span class="dt">t =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb2039-16" data-line-number="16">    beta1 =<span class="st"> </span><span class="fl">0.90</span> ; eps=<span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb2039-17" data-line-number="17">    param<span class="op">$</span>nu     =<span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1) <span class="op">*</span><span class="st"> </span>gradient<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb2039-18" data-line-number="18">    phi          =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(param<span class="op">$</span>nu) <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb2039-19" data-line-number="19">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>phi <span class="op">*</span><span class="st"> </span>gradient</a>
<a class="sourceLine" id="cb2039-20" data-line-number="20">    param</a>
<a class="sourceLine" id="cb2039-21" data-line-number="21">  }</a>
<a class="sourceLine" id="cb2039-22" data-line-number="22">         </a>
<a class="sourceLine" id="cb2039-23" data-line-number="23">optimize.adam =<span class="st"> </span>adam &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, t) {</a>
<a class="sourceLine" id="cb2039-24" data-line-number="24">    beta1 =<span class="st"> </span><span class="fl">0.90</span>; beta2 =<span class="st"> </span><span class="fl">0.999</span>; eps=<span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb2039-25" data-line-number="25">    param<span class="op">$</span>rho    =<span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>rho <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1) <span class="op">*</span><span class="st"> </span>gradient</a>
<a class="sourceLine" id="cb2039-26" data-line-number="26">    param<span class="op">$</span>nu     =<span class="st"> </span>beta2 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta2) <span class="op">*</span><span class="st"> </span>gradient<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb2039-27" data-line-number="27">    rho.hat      =<span class="st"> </span>param<span class="op">$</span>rho <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb2039-28" data-line-number="28">    nu.hat       =<span class="st"> </span>param<span class="op">$</span>nu <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta2<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb2039-29" data-line-number="29">    phi          =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(nu.hat) <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb2039-30" data-line-number="30">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>phi <span class="op">*</span><span class="st"> </span>rho.hat</a>
<a class="sourceLine" id="cb2039-31" data-line-number="31">    param</a>
<a class="sourceLine" id="cb2039-32" data-line-number="32">}</a>
<a class="sourceLine" id="cb2039-33" data-line-number="33">             </a>
<a class="sourceLine" id="cb2039-34" data-line-number="34">  optimize.adadelta =<span class="st"> </span>adadelta &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, t) {</a>
<a class="sourceLine" id="cb2039-35" data-line-number="35">    rho =<span class="st"> </span><span class="fl">0.90</span>; eps=<span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb2039-36" data-line-number="36">    param<span class="op">$</span>rho    =<span class="st"> </span>rho <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>rho <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rho) <span class="op">*</span><span class="st"> </span>gradient<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb2039-37" data-line-number="37">    phi          =<span class="st"> </span><span class="kw">sqrt</span>(param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>eps) <span class="op">*</span><span class="st"> </span>(gradient <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(param<span class="op">$</span>rho <span class="op">+</span><span class="st"> </span>eps))</a>
<a class="sourceLine" id="cb2039-38" data-line-number="38">    param<span class="op">$</span>nu     =<span class="st"> </span>rho <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rho) <span class="op">*</span><span class="st"> </span>phi<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb2039-39" data-line-number="39">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>phi</a>
<a class="sourceLine" id="cb2039-40" data-line-number="40">    param</a>
<a class="sourceLine" id="cb2039-41" data-line-number="41">  }</a>
<a class="sourceLine" id="cb2039-42" data-line-number="42"></a>
<a class="sourceLine" id="cb2039-43" data-line-number="43">  optimizing &lt;-<span class="st"> </span><span class="cf">function</span>(func, param, gradient, eta, t) {</a>
<a class="sourceLine" id="cb2039-44" data-line-number="44">      func =<span class="st"> </span><span class="kw">get</span>(func)</a>
<a class="sourceLine" id="cb2039-45" data-line-number="45">      <span class="kw">func</span>(param, gradient, eta, t)</a>
<a class="sourceLine" id="cb2039-46" data-line-number="46">  }</a>
<a class="sourceLine" id="cb2039-47" data-line-number="47">    </a>
<a class="sourceLine" id="cb2039-48" data-line-number="48">  delta.params     =<span class="st"> </span>backprop<span class="op">$</span>fc.delta.params</a>
<a class="sourceLine" id="cb2039-49" data-line-number="49">  delta.normparams =<span class="st"> </span>backprop<span class="op">$</span>cnn.delta.normparams</a>
<a class="sourceLine" id="cb2039-50" data-line-number="50">  delta.dws        =<span class="st"> </span>backprop<span class="op">$</span>cnn.delta.dws</a>
<a class="sourceLine" id="cb2039-51" data-line-number="51">  delta.pws        =<span class="st"> </span>backprop<span class="op">$</span>cnn.delta.pws</a>
<a class="sourceLine" id="cb2039-52" data-line-number="52">  delta.dw.biases  =<span class="st"> </span>backprop<span class="op">$</span>cnn.delta.dw.biases</a>
<a class="sourceLine" id="cb2039-53" data-line-number="53">  delta.pw.biases  =<span class="st"> </span>backprop<span class="op">$</span>cnn.delta.pw.biases</a>
<a class="sourceLine" id="cb2039-54" data-line-number="54">  H =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb2039-55" data-line-number="55"></a>
<a class="sourceLine" id="cb2039-56" data-line-number="56">  <span class="cf">for</span> (L <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>H) {</a>
<a class="sourceLine" id="cb2039-57" data-line-number="57">     layer =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb2039-58" data-line-number="58">     <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;dense&quot;</span>) {</a>
<a class="sourceLine" id="cb2039-59" data-line-number="59">        fc.layers =<span class="st"> </span>layer<span class="op">$</span>fc.layers</a>
<a class="sourceLine" id="cb2039-60" data-line-number="60">        len  =<span class="st"> </span><span class="kw">length</span>(delta.params)</a>
<a class="sourceLine" id="cb2039-61" data-line-number="61">        <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2039-62" data-line-number="62">            omega =<span class="st"> </span><span class="kw">optimizing</span>(optimize, fc.layers[[l]]<span class="op">$</span>omega, </a>
<a class="sourceLine" id="cb2039-63" data-line-number="63">                               delta.params[[l]]<span class="op">$</span>omega, eta, t)</a>
<a class="sourceLine" id="cb2039-64" data-line-number="64">            layer<span class="op">$</span>fc.layers[[l]]<span class="op">$</span>omega =<span class="st"> </span>omega</a>
<a class="sourceLine" id="cb2039-65" data-line-number="65">            </a>
<a class="sourceLine" id="cb2039-66" data-line-number="66">            <span class="cf">if</span> (fc.layers[[l]]<span class="op">$</span>batchnorm <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) { </a>
<a class="sourceLine" id="cb2039-67" data-line-number="67">              batch.gamma =<span class="st"> </span><span class="kw">optimizing</span>(optimize, fc.layers[[l]]<span class="op">$</span>batch.gamma, </a>
<a class="sourceLine" id="cb2039-68" data-line-number="68">                               delta.params[[l]]<span class="op">$</span>gamma, eta, t)</a>
<a class="sourceLine" id="cb2039-69" data-line-number="69">              batch.beta =<span class="st"> </span><span class="kw">optimizing</span>(optimize, fc.layers[[l]]<span class="op">$</span>batch.beta, </a>
<a class="sourceLine" id="cb2039-70" data-line-number="70">                               delta.params[[l]]<span class="op">$</span>beta, eta, t)</a>
<a class="sourceLine" id="cb2039-71" data-line-number="71">              layer<span class="op">$</span>fc.layers[[l]]<span class="op">$</span>batch.gamma =<span class="st"> </span>batch.gamma</a>
<a class="sourceLine" id="cb2039-72" data-line-number="72">              layer<span class="op">$</span>fc.layers[[l]]<span class="op">$</span>batch.beta =<span class="st"> </span>batch.beta</a>
<a class="sourceLine" id="cb2039-73" data-line-number="73">            }</a>
<a class="sourceLine" id="cb2039-74" data-line-number="74">        }</a>
<a class="sourceLine" id="cb2039-75" data-line-number="75">     } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2039-76" data-line-number="76">     <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;convolv&quot;</span>) {</a>
<a class="sourceLine" id="cb2039-77" data-line-number="77">        delta.dw        =<span class="st"> </span>delta.dws[[L]]</a>
<a class="sourceLine" id="cb2039-78" data-line-number="78">        delta.pw        =<span class="st"> </span>delta.pws[[L]]</a>
<a class="sourceLine" id="cb2039-79" data-line-number="79">        delta.dw.bias   =<span class="st"> </span>delta.dw.biases[[L]]</a>
<a class="sourceLine" id="cb2039-80" data-line-number="80">        delta.pw.bias   =<span class="st"> </span>delta.pw.biases[[L]]</a>
<a class="sourceLine" id="cb2039-81" data-line-number="81"></a>
<a class="sourceLine" id="cb2039-82" data-line-number="82">        layer<span class="op">$</span>dw.kernel =<span class="st"> </span><span class="kw">optimizing</span>(optimize, layer<span class="op">$</span>dw.kernel, </a>
<a class="sourceLine" id="cb2039-83" data-line-number="83">                                     delta.dw, eta, t)</a>
<a class="sourceLine" id="cb2039-84" data-line-number="84">        layer<span class="op">$</span>pw.kernel =<span class="st"> </span><span class="kw">optimizing</span>(optimize, layer<span class="op">$</span>pw.kernel, </a>
<a class="sourceLine" id="cb2039-85" data-line-number="85">                                     delta.pw, eta, t)</a>
<a class="sourceLine" id="cb2039-86" data-line-number="86">        layer<span class="op">$</span>dw.bias   =<span class="st"> </span><span class="kw">optimizing</span>(optimize, layer<span class="op">$</span>dw.bias, </a>
<a class="sourceLine" id="cb2039-87" data-line-number="87">                                     delta.dw.bias, eta, t)</a>
<a class="sourceLine" id="cb2039-88" data-line-number="88">        layer<span class="op">$</span>pw.bias   =<span class="st"> </span><span class="kw">optimizing</span>(optimize, layer<span class="op">$</span>pw.bias, </a>
<a class="sourceLine" id="cb2039-89" data-line-number="89">                                     delta.pw.bias, eta, t)</a>
<a class="sourceLine" id="cb2039-90" data-line-number="90"></a>
<a class="sourceLine" id="cb2039-91" data-line-number="91">        <span class="cf">if</span> (layers[[L]]<span class="op">$</span>batchnorm<span class="op">$</span>normalize <span class="op">!=</span><span class="st"> </span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb2039-92" data-line-number="92">          batchnorm          =<span class="st"> </span>layer<span class="op">$</span>batchnorm</a>
<a class="sourceLine" id="cb2039-93" data-line-number="93">          batchnorm<span class="op">$</span>gamma =<span class="st"> </span><span class="kw">optimizing</span>(optimize, batchnorm<span class="op">$</span>gamma, </a>
<a class="sourceLine" id="cb2039-94" data-line-number="94">                                       delta.normparams[[L]]<span class="op">$</span>gamma, eta, t)</a>
<a class="sourceLine" id="cb2039-95" data-line-number="95">          batchnorm<span class="op">$</span>beta  =<span class="st"> </span><span class="kw">optimizing</span>(optimize, batchnorm<span class="op">$</span>beta, </a>
<a class="sourceLine" id="cb2039-96" data-line-number="96">                                       delta.normparams[[L]]<span class="op">$</span>beta, eta, t)</a>
<a class="sourceLine" id="cb2039-97" data-line-number="97">          layer<span class="op">$</span>batchnorm =<span class="st"> </span>batchnorm</a>
<a class="sourceLine" id="cb2039-98" data-line-number="98">        }</a>
<a class="sourceLine" id="cb2039-99" data-line-number="99">     } </a>
<a class="sourceLine" id="cb2039-100" data-line-number="100">     layers[[L]] =<span class="st"> </span>layer</a>
<a class="sourceLine" id="cb2039-101" data-line-number="101">  } </a>
<a class="sourceLine" id="cb2039-102" data-line-number="102">  layers</a>
<a class="sourceLine" id="cb2039-103" data-line-number="103">}</a></code></pre></div>

</div>
<div id="normalization" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.11</span> Normalization<a href="deeplearning1.html#normalization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the <strong>MLP</strong> section, we introduced the concept of <strong>Batch normalization</strong>, accompanied by the math behind it and its implementation. In this section, we continue to show <strong>Batch normalization</strong> for <strong>CNN</strong>, with a brief introduction to the concept of <strong>Layer normalization</strong>. While <strong>Layer normalization</strong> may not be as common in usage for <strong>CNN</strong>, it helps show that the idea is identical. Furthermore, the implementation is the same. In <strong>Batch normalization</strong>, we group the operation as a Depthwise operation. <strong>Layer normalization</strong> groups the operation as a Sample-wise operation.</p>

<div class="sourceCode" id="cb2040"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2040-1" data-line-number="1">normalize.forward &lt;-<span class="st"> </span><span class="cf">function</span>(H, layer, train, <span class="dt">eps=</span><span class="fl">1e-10</span>, <span class="dt">momentum =</span> <span class="fl">0.90</span>) {</a>
<a class="sourceLine" id="cb2040-2" data-line-number="2">    ntype             =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>normalize</a>
<a class="sourceLine" id="cb2040-3" data-line-number="3">    gamma             =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>gamma<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2040-4" data-line-number="4">    beta              =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>beta<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2040-5" data-line-number="5">    moving.mu         =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>moving.mu</a>
<a class="sourceLine" id="cb2040-6" data-line-number="6">    moving.variance   =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>moving.var </a>
<a class="sourceLine" id="cb2040-7" data-line-number="7">    <span class="cf">if</span> (ntype <span class="op">==</span><span class="st"> &quot;layer&quot;</span>) { </a>
<a class="sourceLine" id="cb2040-8" data-line-number="8">        <span class="co"># Layer normalization ( Height, Width, Depth, Sample)</span></a>
<a class="sourceLine" id="cb2040-9" data-line-number="9">        <span class="co"># normalize across Samples per Layer</span></a>
<a class="sourceLine" id="cb2040-10" data-line-number="10">        mu     =<span class="st"> </span><span class="kw">apply</span>(H, <span class="dv">4</span>, mean)</a>
<a class="sourceLine" id="cb2040-11" data-line-number="11">        H.mu   =<span class="st"> </span><span class="kw">sweep</span>(H, <span class="dv">4</span>, mu, <span class="st">&#39;-&#39;</span>)</a>
<a class="sourceLine" id="cb2040-12" data-line-number="12">        var    =<span class="st"> </span><span class="kw">apply</span>(H.mu<span class="op">^</span><span class="dv">2</span>, <span class="dv">4</span>, mean)</a>
<a class="sourceLine" id="cb2040-13" data-line-number="13">        istd   =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(var <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb2040-14" data-line-number="14">        H.norm =<span class="st"> </span><span class="kw">sweep</span>(H.mu, <span class="dv">4</span>, istd, <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2040-15" data-line-number="15">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2040-16" data-line-number="16">    <span class="cf">if</span> (ntype <span class="op">==</span><span class="st"> &quot;batch&quot;</span>) {</a>
<a class="sourceLine" id="cb2040-17" data-line-number="17">        <span class="co"># Batch normalization ( Height, Width, Depth, Sample)</span></a>
<a class="sourceLine" id="cb2040-18" data-line-number="18">        <span class="co"># normalize across Depth per Layer</span></a>
<a class="sourceLine" id="cb2040-19" data-line-number="19">        <span class="cf">if</span> (train <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb2040-20" data-line-number="20">            mu     =<span class="st"> </span><span class="kw">apply</span>(H, <span class="dv">3</span>, mean)</a>
<a class="sourceLine" id="cb2040-21" data-line-number="21">            H.mu   =<span class="st"> </span><span class="kw">sweep</span>(H, <span class="dv">3</span>, mu, <span class="st">&#39;-&#39;</span>)</a>
<a class="sourceLine" id="cb2040-22" data-line-number="22">            var    =<span class="st"> </span><span class="kw">apply</span>(H.mu<span class="op">^</span><span class="dv">2</span>, <span class="dv">3</span>, mean)</a>
<a class="sourceLine" id="cb2040-23" data-line-number="23">            istd   =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(var <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb2040-24" data-line-number="24">            H.norm =<span class="st"> </span><span class="kw">sweep</span>(H.mu, <span class="dv">3</span>, istd, <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2040-25" data-line-number="25">            <span class="co"># We can use moving average &amp; variance for this normalization</span></a>
<a class="sourceLine" id="cb2040-26" data-line-number="26">            <span class="co"># because normalization is across the entire mini-batch.</span></a>
<a class="sourceLine" id="cb2040-27" data-line-number="27">            moving.mu       =<span class="st"> </span>momentum <span class="op">*</span><span class="st"> </span>moving.mu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>momentum) <span class="op">*</span><span class="st"> </span>mu</a>
<a class="sourceLine" id="cb2040-28" data-line-number="28">            moving.variance =<span class="st"> </span>momentum <span class="op">*</span><span class="st"> </span>moving.variance <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>momentum)<span class="op">*</span>var</a>
<a class="sourceLine" id="cb2040-29" data-line-number="29">            layer<span class="op">$</span>batchnorm<span class="op">$</span>moving.mu   =<span class="st"> </span>moving.mu</a>
<a class="sourceLine" id="cb2040-30" data-line-number="30">            layer<span class="op">$</span>batchnorm<span class="op">$</span>moving.var  =<span class="st"> </span>moving.variance</a>
<a class="sourceLine" id="cb2040-31" data-line-number="31">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2040-32" data-line-number="32">            H.mu   =<span class="st"> </span><span class="kw">sweep</span>( H, <span class="dv">3</span>, moving.mu, <span class="st">&#39;-&#39;</span>)</a>
<a class="sourceLine" id="cb2040-33" data-line-number="33">            istd   =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(moving.variance <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb2040-34" data-line-number="34">            H.norm =<span class="st"> </span><span class="kw">sweep</span>(H.mu, <span class="dv">3</span>, istd, <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2040-35" data-line-number="35">        }</a>
<a class="sourceLine" id="cb2040-36" data-line-number="36">    }</a>
<a class="sourceLine" id="cb2040-37" data-line-number="37">    layer<span class="op">$</span>batchnorm<span class="op">$</span>H.norm  =<span class="st"> </span>H.norm</a>
<a class="sourceLine" id="cb2040-38" data-line-number="38">    layer<span class="op">$</span>batchnorm<span class="op">$</span>H.mu    =<span class="st"> </span>H.mu</a>
<a class="sourceLine" id="cb2040-39" data-line-number="39">    layer<span class="op">$</span>batchnorm<span class="op">$</span>istd    =<span class="st"> </span>istd</a>
<a class="sourceLine" id="cb2040-40" data-line-number="40">    H.hat =<span class="st"> </span>H.norm <span class="op">*</span><span class="st"> </span>gamma <span class="op">+</span><span class="st"> </span>beta</a>
<a class="sourceLine" id="cb2040-41" data-line-number="41">    <span class="kw">list</span>(<span class="st">&quot;feature.map&quot;</span> =<span class="st"> </span>H.hat, <span class="st">&quot;layer&quot;</span> =<span class="st"> </span>layer)</a>
<a class="sourceLine" id="cb2040-42" data-line-number="42">}</a></code></pre></div>
<div class="sourceCode" id="cb2041"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2041-1" data-line-number="1">normalize.backward &lt;-<span class="st"> </span><span class="cf">function</span>(Dout, layer) {</a>
<a class="sourceLine" id="cb2041-2" data-line-number="2">   ntype           =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>normalize</a>
<a class="sourceLine" id="cb2041-3" data-line-number="3">   H.norm          =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>H.norm</a>
<a class="sourceLine" id="cb2041-4" data-line-number="4">   H.mu            =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>H.mu </a>
<a class="sourceLine" id="cb2041-5" data-line-number="5">   istd            =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>istd</a>
<a class="sourceLine" id="cb2041-6" data-line-number="6">   gamma           =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>gamma<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2041-7" data-line-number="7">    </a>
<a class="sourceLine" id="cb2041-8" data-line-number="8">   s =<span class="st"> </span><span class="kw">ifelse</span>(ntype <span class="op">==</span><span class="st"> &quot;batch&quot;</span>, <span class="dv">3</span>, <span class="dv">4</span>) <span class="co"># batchnorm (3) or layernorm (4)</span></a>
<a class="sourceLine" id="cb2041-9" data-line-number="9">   m               =<span class="st"> </span><span class="kw">apply</span>(H.norm, s, length)</a>
<a class="sourceLine" id="cb2041-10" data-line-number="10">   delta.gamma     =<span class="st"> </span><span class="kw">apply</span>(Dout <span class="op">*</span><span class="st"> </span>H.norm, s, sum)  </a>
<a class="sourceLine" id="cb2041-11" data-line-number="11">   delta.beta      =<span class="st"> </span><span class="kw">apply</span>(Dout, s, sum) </a>
<a class="sourceLine" id="cb2041-12" data-line-number="12">   delta.H.norm    =<span class="st"> </span>Dout <span class="op">*</span><span class="st"> </span>gamma</a>
<a class="sourceLine" id="cb2041-13" data-line-number="13">   delta.std      =<span class="st"> </span><span class="kw">apply</span>(delta.H.norm <span class="op">*</span><span class="st"> </span>H.mu, s, sum) </a>
<a class="sourceLine" id="cb2041-14" data-line-number="14">   delta.var      =<span class="st"> </span>delta.std <span class="op">*</span><span class="st"> </span><span class="fl">-0.5</span> <span class="op">*</span><span class="st"> </span>istd<span class="op">^</span><span class="dv">3</span> </a>
<a class="sourceLine" id="cb2041-15" data-line-number="15">   delta.Hmu1     =<span class="st"> </span><span class="kw">apply</span>( delta.H.norm <span class="op">*</span><span class="st"> </span><span class="op">-</span>istd, s, sum)</a>
<a class="sourceLine" id="cb2041-16" data-line-number="16">   delta.Hmu2     =<span class="st"> </span>delta.var <span class="op">*</span><span class="st"> </span><span class="kw">apply</span>( <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>H.mu,s, mean)</a>
<a class="sourceLine" id="cb2041-17" data-line-number="17">   delta.mu       =<span class="st"> </span>delta.Hmu1 <span class="op">+</span><span class="st"> </span>delta.Hmu2</a>
<a class="sourceLine" id="cb2041-18" data-line-number="18">   delta.out      =<span class="st"> </span><span class="kw">sweep</span>(delta.H.norm <span class="op">*</span><span class="st"> </span>istd <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb2041-19" data-line-number="19"><span class="st">                    </span>delta.var <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>H.mu) <span class="op">/</span><span class="st"> </span>m, s, (delta.mu <span class="op">/</span><span class="st"> </span>m), <span class="st">&#39;+&#39;</span>)</a>
<a class="sourceLine" id="cb2041-20" data-line-number="20">  params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;gamma&quot;</span> =<span class="st"> </span>delta.gamma, <span class="st">&quot;beta&quot;</span> =<span class="st"> </span>delta.beta)</a>
<a class="sourceLine" id="cb2041-21" data-line-number="21">  <span class="kw">list</span>(<span class="st">&quot;gradient.loss&quot;</span>    =<span class="st"> </span>delta.out, <span class="st">&quot;params&quot;</span> =<span class="st"> </span>params)</a>
<a class="sourceLine" id="cb2041-22" data-line-number="22">}</a></code></pre></div>

</div>
<div id="step-decay" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.12</span> Step Decay<a href="deeplearning1.html#step-decay" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There is one other pointer to consider in <strong>CNN</strong>, which has to do with adjusting the learning rate during training. While other approaches may use different exponential formulations, here, we recall from <strong>MLP</strong> the use of the <strong>step decay</strong> approach using a typical formulation below:</p>
<p><span class="math display">\[\begin{align}
\eta^{(t+1)} = \eta^{(initial)} \times DF^{floor\left(\frac{t}{step size}\right)}
\end{align}\]</span></p>
<p>where <strong>t</strong> is the epoch and <span class="math inline">\(\mathbf{\eta}\)</span> is the learning rate. The equivalent implementation follows:</p>

<div class="sourceCode" id="cb2042"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2042-1" data-line-number="1">step.decay &lt;-<span class="st"> </span><span class="cf">function</span>(eta, epoch, <span class="dt">decay.factor=</span><span class="fl">0.55</span>, <span class="dt">step.size=</span><span class="dv">5</span>) { </a>
<a class="sourceLine" id="cb2042-2" data-line-number="2">    <span class="co"># default decay.factor=0.55 and step.size=5 for cifar-10 dataset</span></a>
<a class="sourceLine" id="cb2042-3" data-line-number="3">    eta <span class="op">*</span><span class="st"> </span>(decay.factor <span class="op">^</span><span class="st"> </span><span class="kw">floor</span>(epoch<span class="op">/</span>step.size)) </a>
<a class="sourceLine" id="cb2042-4" data-line-number="4">}</a></code></pre></div>

<p>To illustrate the use, below is an example of scheduled decay we use for a cifar-10 dataset of 50,000 images for training. The idea is to try to get the most (fastest) learning during the first five epochs using a large learning rate, then decay at a slightly lower learning rate between 5 and 10 epochs before we conveniently settle below <span class="math inline">\(\text{1e-3}\)</span> range past 15 epochs.</p>

<div class="sourceCode" id="cb2043"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2043-1" data-line-number="1"><span class="kw">step.decay</span>(<span class="fl">0.05</span>, <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">20</span>), <span class="dt">decay.factor=</span><span class="fl">0.80</span>, <span class="dt">step.size=</span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>##  [1] 0.04000000000 0.03200000000 0.02560000000
##  [4] 0.02048000000 0.01638400000 0.01310720000
##  [7] 0.01048576000 0.00838860800 0.00671088640
## [10] 0.00536870912 0.00429496730 0.00343597384
## [13] 0.00274877907 0.00219902326 0.00175921860
## [16] 0.00140737488 0.00112589991 0.00090071993
## [19] 0.00072057594 0.00057646075</code></pre>

<p>Before we finally discuss the implementation of the <strong>CNN</strong> function, let us first cover <strong>GEMM</strong> and <strong>Depthwise Separable Convolution</strong>.</p>
</div>
<div id="gemm-matrix-multiplication" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.13</span> GEMM (Matrix Multiplication) <a href="deeplearning1.html#gemm-matrix-multiplication" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Convolution</strong> is computationally expensive if we use the common <strong>dot-product</strong> operation against the raw image input. Doing so is otherwise known as <strong>Direct Convolution</strong>. Alternatively, we can use what is called <strong>GEMM-based Convolution</strong>. This convolution involves three consecutive techniques to help boost performance. The first technique is to perform <strong>image-to-column (im2col)</strong> conversion, which stacks all the receptive fields in a matrix. The second technique <strong>GEneral Matrix to Matrix Multiplication (GEMM)</strong> operation performs the actual <strong>dot-product</strong>. The third technique is a <strong>column-to-image (col2im)</strong> conversion which transforms the resultant <strong>Toeplitz Matrix</strong> back to its convolution form. </p>
<p>We use Figure <a href="deeplearning1.html#fig:convolv">12.41</a> to illustrate a feed-forward convolution using the three methods above. The main idea here is to stack up all the receptive fields into a matrix form and perform the same conversion for the kernels such that the two newly formed matrices can convolve to form the feature map. Note that in the figure, both ker2col and im2col are arranged horizontally to fit our text, labeled as <span class="math inline">\(2 \times 28\)</span> and <span class="math inline">\(9 \times 28\)</span>, respectively. However, we have it vertically in such an implementation, namely <span class="math inline">\(28 \times 2\)</span> and <span class="math inline">\(28 \times 9\)</span>. We then convolve so that <span class="math inline">\(t(28 \times 2) * (28 \times 9) = (2 \times 9)\)</span>. Also, note that we assume a tensor of (HxWxCxI) dimension. On that note, we intend to reduce the dimension into a 2-dimension matrix.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:convolv"></span>
<img src="convolv.png" alt="CNN (Main Convolution)" width="100%" />
<p class="caption">
Figure 12.41: CNN (Main Convolution)
</p>
</div>
<p>In terms of backpropagation, we use Figure <a href="deeplearning1.html#fig:fullconvolv">12.42</a> to illustrate the use of a similar <strong>GEMM-based</strong> method to perform full convolution, computing the gradient with respect to the filters to arrive at our delta gradients for the input. Similarly, the arrangement is plotted horizontally, where our filters and input tensors are converted into 2-dimensional matrices for convolution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fullconvolv"></span>
<img src="fullconvolv.png" alt="CNN (Gradient with respect to Filter)" width="100%" />
<p class="caption">
Figure 12.42: CNN (Gradient with respect to Filter)
</p>
</div>
<p>Similarly, Figure <a href="deeplearning1.html#fig:bpconvolv">12.43</a> illustrates the use of <strong>GEMM-based</strong> convolution to compute the gradient with respect to the input to arrive at the delta gradients for our filters.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bpconvolv"></span>
<img src="bpconvolv.png" alt="CNN (Gradient with respect to Input)" width="100%" />
<p class="caption">
Figure 12.43: CNN (Gradient with respect to Input)
</p>
</div>
<p>Finally, we also can use the <strong>GEMM-based</strong> method to perform the same operation for our max pool. See Figure <a href="deeplearning1.html#fig:bpmaxpool">12.44</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bpmaxpool"></span>
<img src="bpmaxpool.png" alt="CNN (Gradient with respect to MaxPool)" width="90%" />
<p class="caption">
Figure 12.44: CNN (Gradient with respect to MaxPool)
</p>
</div>
<p>While the use of the <strong>GEMM-based</strong> method benefits from slow convolution computation, one alternative method for machines or gadgets with low resources is called <strong>Depthwise Separable Convolution</strong> complemented by <strong>Pointwise Convolution</strong>. The next section discusses the idea in more detail.</p>
</div>
<div id="depthwise-separable-convolution-dsc" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.14</span> Depthwise Separable Convolution (DSC)  <a href="deeplearning1.html#depthwise-separable-convolution-dsc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our implementation of <strong>CNN</strong> uses <strong>Depthwise Separable Convolution (DSC)</strong> along with <strong>Pointwise Convolution (PC)</strong> and an intermediate <strong>GEMM</strong> implementation within the loop. </p>
<p>The idea is to <strong>separate</strong> the convolution operation for individual channels (depthwise) as the first step.
Each channel from the Input is separately convolved with each channel from the filter such that we have <span class="math inline">\((5\times5)_{(\text{input})} * (3\times3)_{(\text{kernel})} = (3\times3)_{(\text{feature map})}\)</span> for each Image input. We then conjoin to form a <span class="math inline">\((3\times 3\times 1)\)</span> feature map. This is followed by performing <strong>pointwise</strong> convolution with a set of N <span class="math inline">\((1\times C)\)</span> filters. See Figure <a href="deeplearning1.html#fig:depthwise">12.45</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:depthwise"></span>
<img src="depthwise.png" alt="CNN (Depthwise)" width="90%" />
<p class="caption">
Figure 12.45: CNN (Depthwise)
</p>
</div>
<p>Below are the implementations of convolving matrices starting with the <strong>convolve.image(.)</strong> function, which handles the main convolution using <strong>DSC</strong> and <strong>PC</strong>. Our <strong>DSC</strong> implementation performs the <strong>ker2col</strong> conversion first as a one-time step, then a set of <strong>im2col</strong> conversions after accumulating the receptive fields row-wise. We then use the converted <strong>im2col</strong> matrix for every row to perform <strong>GEMM</strong> convolution with <strong>ker2col</strong>. This extra step assumes handling a decent mini-batch size (labeled as I in our text and S in our implementation). The convolution result is conjoined to form an <span class="math inline">\((O\times O\times C \times S)\)</span> matrix that is then convolved with a <span class="math inline">\(1\times C\)</span> of N filters which are simply represented as <span class="math inline">\(C\times N\)</span> in our implementation. Looping through <strong>S</strong>, we end up with the expected <span class="math inline">\((O\times O\times N \times S)\)</span> feature map.</p>

<div class="sourceCode" id="cb2045"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2045-1" data-line-number="1">convolve.image &lt;-<span class="st"> </span><span class="cf">function</span>(image, dw.kernel, pw.kernel, </a>
<a class="sourceLine" id="cb2045-2" data-line-number="2">                           dw.bias, pw.bias, bias, dil_rate, O, h, w, r, c) {</a>
<a class="sourceLine" id="cb2045-3" data-line-number="3">  dil.filters =<span class="st"> </span><span class="kw">dilate.filters</span>(dw.kernel, dil_rate)</a>
<a class="sourceLine" id="cb2045-4" data-line-number="4">  len =<span class="st"> </span><span class="kw">length</span>(dil.filters)</a>
<a class="sourceLine" id="cb2045-5" data-line-number="5">  di   =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb2045-6" data-line-number="6">  di.k =<span class="st"> </span><span class="kw">dim</span>(dw.kernel)</a>
<a class="sourceLine" id="cb2045-7" data-line-number="7">  len =<span class="st"> </span><span class="kw">dim</span>(pw.kernel)[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb2045-8" data-line-number="8">  ker2col =<span class="st"> </span><span class="kw">array</span>(dw.kernel, <span class="kw">c</span>(di.k[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di.k[<span class="dv">2</span>], di.k[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2045-9" data-line-number="9">  <span class="co">#ker2col = rbind(dw.bias, ker2col) # add bias</span></a>
<a class="sourceLine" id="cb2045-10" data-line-number="10">  n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2045-11" data-line-number="11">  I =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2045-12" data-line-number="12">  dw.fmap =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(O, O, di[<span class="dv">3</span>], di[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2045-13" data-line-number="13">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2045-14" data-line-number="14">    n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2045-15" data-line-number="15">    I =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2045-16" data-line-number="16">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2045-17" data-line-number="17">        n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2045-18" data-line-number="18">        hs =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2045-19" data-line-number="19">        ws =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2045-20" data-line-number="20">        I[[n]] =<span class="st"> </span><span class="kw">array</span>(image[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,], <span class="kw">c</span>(r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">3</span>], di[<span class="dv">4</span>])) </a>
<a class="sourceLine" id="cb2045-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb2045-22" data-line-number="22">    D =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(I), <span class="kw">c</span>( r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">3</span>], di[<span class="dv">4</span>] <span class="op">*</span><span class="st"> </span>n))</a>
<a class="sourceLine" id="cb2045-23" data-line-number="23">    <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">3</span>]) { <span class="co"># Depthwise Separable Convolution (KxKxM filter)</span></a>
<a class="sourceLine" id="cb2045-24" data-line-number="24">      im2col =<span class="st"> </span>D[,d,]</a>
<a class="sourceLine" id="cb2045-25" data-line-number="25">      <span class="co">#im2col = rbind(rep(1,  di[4] * n), im2col) # Add bias constant</span></a>
<a class="sourceLine" id="cb2045-26" data-line-number="26">      conv =<span class="st">  </span><span class="kw">t</span>(ker2col[,d]) <span class="op">%*%</span><span class="st"> </span>im2col</a>
<a class="sourceLine" id="cb2045-27" data-line-number="27">      dw.fmap[i,,d,] =<span class="st"> </span><span class="kw">t</span>(<span class="kw">array</span>(conv, <span class="kw">c</span>(di[<span class="dv">4</span>], n)))</a>
<a class="sourceLine" id="cb2045-28" data-line-number="28">    }</a>
<a class="sourceLine" id="cb2045-29" data-line-number="29">  }</a>
<a class="sourceLine" id="cb2045-30" data-line-number="30">  dw.fmap =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(dw.fmap), <span class="kw">c</span>(O, O, di[<span class="dv">3</span>], di[<span class="dv">4</span>])) <span class="co"># DfxDfxMxS</span></a>
<a class="sourceLine" id="cb2045-31" data-line-number="31">  pw.fmap =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2045-32" data-line-number="32">  <span class="cf">if</span> (bias<span class="op">==</span><span class="ot">TRUE</span>) { pw.kernel =<span class="st"> </span><span class="kw">rbind</span>(pw.bias, pw.kernel) } <span class="co"># add bias</span></a>
<a class="sourceLine" id="cb2045-33" data-line-number="33">  sz =<span class="st"> </span>O <span class="op">*</span><span class="st"> </span>O</a>
<a class="sourceLine" id="cb2045-34" data-line-number="34">  <span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">4</span>]) { <span class="co"># Pointwise Convolution (MxN filter)</span></a>
<a class="sourceLine" id="cb2045-35" data-line-number="35">      pw.fmap[[s]] =<span class="st"> </span><span class="kw">array</span>( dw.fmap[,,,s], <span class="kw">c</span>(sz, di[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2045-36" data-line-number="36">      <span class="cf">if</span> (bias<span class="op">==</span><span class="ot">TRUE</span>) { </a>
<a class="sourceLine" id="cb2045-37" data-line-number="37">          pw.fmap[[s]] =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>, sz), pw.fmap[[s]]) <span class="co"># add bias constant</span></a>
<a class="sourceLine" id="cb2045-38" data-line-number="38">      }</a>
<a class="sourceLine" id="cb2045-39" data-line-number="39">      pw.fmap[[s]] =<span class="st"> </span>pw.fmap[[s]] <span class="op">%*%</span><span class="st"> </span>pw.kernel </a>
<a class="sourceLine" id="cb2045-40" data-line-number="40">  }</a>
<a class="sourceLine" id="cb2045-41" data-line-number="41">  pw.fmap =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(pw.fmap), <span class="kw">c</span>(O, O, len, di[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2045-42" data-line-number="42">  <span class="kw">remove</span>(I, ker2col, D)  </a>
<a class="sourceLine" id="cb2045-43" data-line-number="43">  <span class="kw">list</span>(<span class="st">&quot;dw.fmap&quot;</span> =<span class="st"> </span>dw.fmap, <span class="st">&quot;pw.fmap&quot;</span> =<span class="st"> </span>pw.fmap)   </a>
<a class="sourceLine" id="cb2045-44" data-line-number="44">}</a></code></pre></div>

<p>A note to emphasize in the implementation above is the addition of <strong>bias</strong> only to the <strong>Pointwise</strong> convolution. We leave readers to investigate the addition of <strong>bias</strong> to the <strong>Depthwise separable</strong> convolution.</p>
<p>Next, we account for solving the gradient with respect to the filter to generate our Input gradients. Our implementation performs a full convolution by rotating our kernel to 180 degrees. See <strong>back.propagation(.)</strong>.</p>

<div class="sourceCode" id="cb2046"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2046-1" data-line-number="1">gradient.I.wrt.K &lt;-<span class="st"> </span><span class="cf">function</span>(image, dw.kernel, O, h, w, r, c) {</a>
<a class="sourceLine" id="cb2046-2" data-line-number="2">   di =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb2046-3" data-line-number="3">   fmap =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(O, O, di[<span class="dv">3</span>], di[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2046-4" data-line-number="4">   ker2col   =<span class="st"> </span><span class="kw">array</span>(dw.kernel, <span class="kw">c</span>(r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2046-5" data-line-number="5">   <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2046-6" data-line-number="6">     n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2046-7" data-line-number="7">     I =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2046-8" data-line-number="8">     <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2046-9" data-line-number="9">        n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2046-10" data-line-number="10">        hs =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2046-11" data-line-number="11">        ws =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2046-12" data-line-number="12">        I[[n]] =<span class="st"> </span><span class="kw">array</span>(image[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,], <span class="kw">c</span>(r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">3</span>], di[<span class="dv">4</span>])) </a>
<a class="sourceLine" id="cb2046-13" data-line-number="13">     }</a>
<a class="sourceLine" id="cb2046-14" data-line-number="14">     D =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(I), <span class="kw">c</span>( r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">3</span>], di[<span class="dv">4</span>] <span class="op">*</span><span class="st"> </span>n))</a>
<a class="sourceLine" id="cb2046-15" data-line-number="15">     <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">3</span>]) { <span class="co"># Depthwise Separable Backprop. (KxKxM filter)</span></a>
<a class="sourceLine" id="cb2046-16" data-line-number="16">       conv =<span class="st">  </span><span class="kw">t</span>(ker2col[,d]) <span class="op">%*%</span><span class="st"> </span>D[,d,]</a>
<a class="sourceLine" id="cb2046-17" data-line-number="17">       fmap[i,,d,] =<span class="st"> </span><span class="kw">t</span>(<span class="kw">array</span>(conv, <span class="kw">c</span>(di[<span class="dv">4</span>], n)))</a>
<a class="sourceLine" id="cb2046-18" data-line-number="18">     }</a>
<a class="sourceLine" id="cb2046-19" data-line-number="19">   }</a>
<a class="sourceLine" id="cb2046-20" data-line-number="20">   <span class="kw">remove</span>(I, ker2col, conv)  </a>
<a class="sourceLine" id="cb2046-21" data-line-number="21">   fmap</a>
<a class="sourceLine" id="cb2046-22" data-line-number="22">}</a></code></pre></div>

<p>Next, we also account for solving the gradient with respect to input (or feature map) to generate our filter gradients. Our implementation performs a convolution of the gradient derived from a prior deeper layer (l) with the input of the current layer (l-1). See <strong>back.propagation(.)</strong>.</p>

<div class="sourceCode" id="cb2047"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2047-1" data-line-number="1">gradient.K.wrt.I &lt;-<span class="st"> </span><span class="cf">function</span>(image, Dout, cur.fmaps, pw.kernel, </a>
<a class="sourceLine" id="cb2047-2" data-line-number="2">                             dil_rate, O, h, w) {</a>
<a class="sourceLine" id="cb2047-3" data-line-number="3">    pw.Dout  =<span class="st"> </span>Dout</a>
<a class="sourceLine" id="cb2047-4" data-line-number="4">    dw.fmap  =<span class="st"> </span>cur.fmaps<span class="op">$</span>dw.fmap</a>
<a class="sourceLine" id="cb2047-5" data-line-number="5">    pw.fmap  =<span class="st"> </span>cur.fmaps<span class="op">$</span>pw.fmap</a>
<a class="sourceLine" id="cb2047-6" data-line-number="6">    di.dw    =<span class="st"> </span><span class="kw">dim</span>(dw.fmap)  </a>
<a class="sourceLine" id="cb2047-7" data-line-number="7">    di.pw    =<span class="st"> </span><span class="kw">dim</span>(pw.fmap) </a>
<a class="sourceLine" id="cb2047-8" data-line-number="8">    delta.pw =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2047-9" data-line-number="9">    dw.Dout  =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2047-10" data-line-number="10">    <span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di.dw[<span class="dv">4</span>]) {</a>
<a class="sourceLine" id="cb2047-11" data-line-number="11">       im2col   =<span class="st"> </span><span class="kw">array</span>( dw.fmap[,,,s], <span class="kw">c</span>(di.dw[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di.dw[<span class="dv">2</span>], di.dw[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2047-12" data-line-number="12">       Dout2col =<span class="st"> </span><span class="kw">array</span>( pw.Dout[,,,s], <span class="kw">c</span>(di.pw[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di.pw[<span class="dv">2</span>], di.pw[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2047-13" data-line-number="13">       delta.pw =<span class="st"> </span>delta.pw <span class="op">+</span><span class="st"> </span><span class="kw">t</span>(im2col) <span class="op">%*%</span><span class="st"> </span>Dout2col</a>
<a class="sourceLine" id="cb2047-14" data-line-number="14">       dw.Dout[[s]] =<span class="st">  </span>(Dout2col) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(pw.kernel)</a>
<a class="sourceLine" id="cb2047-15" data-line-number="15">    }</a>
<a class="sourceLine" id="cb2047-16" data-line-number="16">    <span class="co">#delta.pw = delta.pw / di.pw[4] # take average gradient of minibatch</span></a>
<a class="sourceLine" id="cb2047-17" data-line-number="17">    Dout2col =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(dw.Dout), <span class="kw">c</span>(di.pw[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di.pw[<span class="dv">2</span>], </a>
<a class="sourceLine" id="cb2047-18" data-line-number="18">                                        di.dw[<span class="dv">3</span>], di.dw[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2047-19" data-line-number="19">    dw.Dout =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(dw.Dout), <span class="kw">c</span>(di.pw[<span class="dv">1</span>], di.pw[<span class="dv">2</span>], </a>
<a class="sourceLine" id="cb2047-20" data-line-number="20">                                       di.dw[<span class="dv">3</span>], di.dw[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2047-21" data-line-number="21">    K =<span class="st"> </span><span class="kw">dilate</span>(dw.Dout, dil_rate) <span class="co"># Becomes Filter for gradient.I.wrt.K</span></a>
<a class="sourceLine" id="cb2047-22" data-line-number="22">    r =<span class="st"> </span><span class="kw">nrow</span>(K)</a>
<a class="sourceLine" id="cb2047-23" data-line-number="23">    c =<span class="st"> </span><span class="kw">ncol</span>(K)</a>
<a class="sourceLine" id="cb2047-24" data-line-number="24">    di.k =<span class="st"> </span><span class="kw">dim</span>(K)</a>
<a class="sourceLine" id="cb2047-25" data-line-number="25">    di =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb2047-26" data-line-number="26">    n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2047-27" data-line-number="27">    delta.dw =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(O, O, di[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2047-28" data-line-number="28">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2047-29" data-line-number="29">      <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2047-30" data-line-number="30">         n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2047-31" data-line-number="31">         hs =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2047-32" data-line-number="32">         ws =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2047-33" data-line-number="33">         <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">3</span>]) {</a>
<a class="sourceLine" id="cb2047-34" data-line-number="34">           Dout2col.sub     =<span class="st"> </span><span class="kw">array</span>(K[,,d,] , <span class="kw">c</span>(r <span class="op">*</span><span class="st"> </span>c,  di[<span class="dv">4</span>] )  )</a>
<a class="sourceLine" id="cb2047-35" data-line-number="35">           im2col.sub       =<span class="st"> </span><span class="kw">array</span>(image[hs<span class="op">:</span>he, ws<span class="op">:</span>we,d,], <span class="kw">c</span>(r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">4</span>])) </a>
<a class="sourceLine" id="cb2047-36" data-line-number="36"></a>
<a class="sourceLine" id="cb2047-37" data-line-number="37">           delta.dw[i,j,d] =<span class="st"> </span><span class="kw">sum</span>(Dout2col.sub <span class="op">*</span><span class="st"> </span>im2col.sub)</a>
<a class="sourceLine" id="cb2047-38" data-line-number="38">         } </a>
<a class="sourceLine" id="cb2047-39" data-line-number="39">      }</a>
<a class="sourceLine" id="cb2047-40" data-line-number="40">    }</a>
<a class="sourceLine" id="cb2047-41" data-line-number="41">    <span class="co">#delta.dw = delta.dw / di[4] # take average gradient of minibatch</span></a>
<a class="sourceLine" id="cb2047-42" data-line-number="42">    <span class="kw">remove</span>(Dout2col, im2col.sub, Dout2col.sub) </a>
<a class="sourceLine" id="cb2047-43" data-line-number="43">    <span class="kw">list</span>(<span class="st">&quot;dw.Dout&quot;</span> =<span class="st"> </span>dw.Dout, <span class="st">&quot;pw.Dout&quot;</span> =<span class="st"> </span>pw.Dout,</a>
<a class="sourceLine" id="cb2047-44" data-line-number="44">         <span class="st">&quot;delta.dw&quot;</span> =<span class="st"> </span>delta.dw, <span class="st">&quot;delta.pw&quot;</span> =<span class="st"> </span>delta.pw)</a>
<a class="sourceLine" id="cb2047-45" data-line-number="45">}</a></code></pre></div>

<p>A note to emphasize in our implementation is the summation of gradients accumulated from all minibatch samples. We leave readers to investigate the effect of averaging the gradients instead, granting that the average does not swing the value too large or too small enough to offset the trajectory.</p>
<p>Lastly, we derive the gradient with respect to a maximum pool. Here, we rely on a cache containing the location of the maximum values.</p>

<div class="sourceCode" id="cb2048"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2048-1" data-line-number="1">gradient.I.wrt.P &lt;-<span class="st"> </span><span class="cf">function</span>(image, Dout, K, O, pool, h, w, pool.cache) {</a>
<a class="sourceLine" id="cb2048-2" data-line-number="2">      r         =<span class="st"> </span><span class="kw">nrow</span>(K)</a>
<a class="sourceLine" id="cb2048-3" data-line-number="3">      c         =<span class="st"> </span><span class="kw">ncol</span>(K)</a>
<a class="sourceLine" id="cb2048-4" data-line-number="4">      img.copy =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">dim</span>(image))</a>
<a class="sourceLine" id="cb2048-5" data-line-number="5">      di  =<span class="st"> </span><span class="kw">dim</span>(Dout)</a>
<a class="sourceLine" id="cb2048-6" data-line-number="6">      gr2col =<span class="st"> </span><span class="kw">array</span>(Dout, <span class="kw">c</span>(<span class="dv">1</span>, di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2048-7" data-line-number="7">      len =<span class="st"> </span><span class="kw">length</span>(gr2col)</a>
<a class="sourceLine" id="cb2048-8" data-line-number="8">      <span class="cf">if</span> (pool <span class="op">==</span><span class="st"> &quot;maxpool&quot;</span>) {</a>
<a class="sourceLine" id="cb2048-9" data-line-number="9">          one.hot =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(len,r <span class="op">*</span><span class="st"> </span>c))</a>
<a class="sourceLine" id="cb2048-10" data-line-number="10">          <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {  one.hot[i, pool.cache[i]] =<span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb2048-11" data-line-number="11">      } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2048-12" data-line-number="12">        all.one =<span class="st"> </span><span class="kw">array</span>(<span class="dv">1</span>, <span class="kw">c</span>(len, r<span class="op">*</span>c))</a>
<a class="sourceLine" id="cb2048-13" data-line-number="13">      }</a>
<a class="sourceLine" id="cb2048-14" data-line-number="14">      n    =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2048-15" data-line-number="15">      skip =<span class="st"> </span>di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb2048-16" data-line-number="16">      k    =<span class="st"> </span><span class="kw">prod</span>(di)</a>
<a class="sourceLine" id="cb2048-17" data-line-number="17">      <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2048-18" data-line-number="18">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2048-19" data-line-number="19">        n     =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2048-20" data-line-number="20">        hs    =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2048-21" data-line-number="21">        ws    =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2048-22" data-line-number="22">        b     =<span class="st"> </span>(n<span class="dv">-1</span>) <span class="op">*</span><span class="st"> </span>skip <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2048-23" data-line-number="23">        e     =<span class="st"> </span>n <span class="op">*</span><span class="st"> </span>skip</a>
<a class="sourceLine" id="cb2048-24" data-line-number="24">        a.idx =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span>b, <span class="dt">to=</span>e)</a>
<a class="sourceLine" id="cb2048-25" data-line-number="25">        b.idx =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span>n,  <span class="dt">to=</span>k, <span class="dt">by=</span>di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb2048-26" data-line-number="26">        <span class="cf">if</span> (pool <span class="op">==</span><span class="st"> &quot;maxpool&quot;</span>) {</a>
<a class="sourceLine" id="cb2048-27" data-line-number="27">            z =<span class="st"> </span><span class="kw">sweep</span>(<span class="kw">t</span>(one.hot[a.idx,]), <span class="dv">2</span>, gr2col[,b.idx], <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2048-28" data-line-number="28">        } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb2048-29" data-line-number="29">        <span class="cf">if</span> (pool <span class="op">==</span><span class="st"> &quot;avgpool&quot;</span>) {</a>
<a class="sourceLine" id="cb2048-30" data-line-number="30">            z =<span class="st"> </span><span class="kw">sweep</span>(<span class="kw">t</span>(all.one[a.idx,]), <span class="dv">2</span>, gr2col[,b.idx], <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2048-31" data-line-number="31">        }</a>
<a class="sourceLine" id="cb2048-32" data-line-number="32">        dimen.pool =<span class="st"> </span><span class="kw">array</span>(z, <span class="kw">c</span>(r,c, di[<span class="dv">3</span>], di[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2048-33" data-line-number="33">        img.copy[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,] =<span class="st"> </span>img.copy[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,] <span class="op">+</span><span class="st"> </span>dimen.pool</a>
<a class="sourceLine" id="cb2048-34" data-line-number="34">      }</a>
<a class="sourceLine" id="cb2048-35" data-line-number="35">      }</a>
<a class="sourceLine" id="cb2048-36" data-line-number="36">      img.copy</a>
<a class="sourceLine" id="cb2048-37" data-line-number="37">}</a></code></pre></div>

<p>Other types of <strong>Convolution</strong> cater to improving the computation of <strong>Convolution</strong>, of which two of them are necessary for investigation, namely <strong>FFT (Fast Fourier Transform)</strong> Convolution vs. <strong>WinoGrad</strong> Convolution. However, we leave readers to investigate these algorithms further, along with how the computations are distributed across a set of <strong>GPU</strong> processors in a parallel fashion; for example, having each kernel affined to its dedicated <strong>GPU</strong> unit and performing <strong>GEMM</strong>.</p>
</div>
<div id="cnn-implementation" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.15</span> CNN Implementation<a href="deeplearning1.html#cnn-implementation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Below is a rudimentary implementation of our <strong>CNN</strong> with the forward feed for CNN and forward feed for <strong>MLP</strong> (taking part to support <strong>fully-connected</strong> layers).</p>

<div class="sourceCode" id="cb2049"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2049-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb2049-2" data-line-number="2">accuracy &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) {</a>
<a class="sourceLine" id="cb2049-3" data-line-number="3">    p1 =<span class="st"> </span><span class="kw">apply</span>(t, <span class="dv">1</span>, which.max)</a>
<a class="sourceLine" id="cb2049-4" data-line-number="4">    p2 =<span class="st"> </span><span class="kw">apply</span>(o, <span class="dv">1</span>, which.max)</a>
<a class="sourceLine" id="cb2049-5" data-line-number="5">    p =<span class="st"> </span>p1 <span class="op">==</span><span class="st"> </span>p2</a>
<a class="sourceLine" id="cb2049-6" data-line-number="6">    <span class="kw">sum</span>(p)<span class="op">/</span><span class="kw">length</span>(p)</a>
<a class="sourceLine" id="cb2049-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb2049-8" data-line-number="8">get.batch &lt;-<span class="st"> </span><span class="cf">function</span>(sample.data, labels, k, t) {  </a>
<a class="sourceLine" id="cb2049-9" data-line-number="9">   <span class="kw">set.seed</span>(t)</a>
<a class="sourceLine" id="cb2049-10" data-line-number="10">   batch.indices =<span class="st"> </span><span class="kw">createFolds</span>(sample.data, <span class="dt">k =</span> k, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2049-11" data-line-number="11">   batches =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2049-12" data-line-number="12">   <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {</a>
<a class="sourceLine" id="cb2049-13" data-line-number="13">       indices =<span class="st"> </span>batch.indices[[i]]</a>
<a class="sourceLine" id="cb2049-14" data-line-number="14">       X =<span class="st"> </span><span class="kw">extract.image</span>(images, indices)</a>
<a class="sourceLine" id="cb2049-15" data-line-number="15">       Y =<span class="st"> </span><span class="kw">extract.label.onehot</span>(images, labels, indices) </a>
<a class="sourceLine" id="cb2049-16" data-line-number="16">       <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>k) {</a>
<a class="sourceLine" id="cb2049-17" data-line-number="17">         batches[[i]] =<span class="st"> </span><span class="kw">list</span>( <span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y )</a>
<a class="sourceLine" id="cb2049-18" data-line-number="18">       } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2049-19" data-line-number="19">         validation =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y)</a>
<a class="sourceLine" id="cb2049-20" data-line-number="20">       }</a>
<a class="sourceLine" id="cb2049-21" data-line-number="21">   }</a>
<a class="sourceLine" id="cb2049-22" data-line-number="22">   <span class="kw">list</span>(<span class="st">&quot;train&quot;</span> =<span class="st"> </span>batches, <span class="st">&quot;validation&quot;</span> =<span class="st"> </span>validation)</a>
<a class="sourceLine" id="cb2049-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb2049-24" data-line-number="24">flush.str &lt;-<span class="st"> </span><span class="cf">function</span>(...) {str =<span class="st"> </span><span class="kw">sprintf</span>(...); <span class="kw">print</span>(str); <span class="kw">flush.console</span>()}</a>
<a class="sourceLine" id="cb2049-25" data-line-number="25">transfer.learning &lt;-<span class="st"> </span><span class="cf">function</span>(model.file) {</a>
<a class="sourceLine" id="cb2049-26" data-line-number="26">     my.cnn.model =<span class="st"> </span><span class="kw">readRDS</span>(model.file)</a>
<a class="sourceLine" id="cb2049-27" data-line-number="27">     my.cnn.model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2049-28" data-line-number="28">}</a></code></pre></div>
<div class="sourceCode" id="cb2050"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2050-1" data-line-number="1">my.CNN &lt;-<span class="st"> </span><span class="cf">function</span>(target.images, labels, <span class="dt">layers=</span><span class="ot">NULL</span>, optimize, </a>
<a class="sourceLine" id="cb2050-2" data-line-number="2">                   <span class="dt">transfer=</span><span class="ot">NULL</span>, <span class="dt">minibatch=</span><span class="dv">40</span>,  <span class="dt">epoch=</span><span class="dv">100</span>, <span class="dt">eta =</span> <span class="fl">0.01</span>) {</a>
<a class="sourceLine" id="cb2050-3" data-line-number="3">  <span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">16</span>)  <span class="co"># 16 digits precision </span></a>
<a class="sourceLine" id="cb2050-4" data-line-number="4">  eta            =<span class="st"> </span><span class="kw">c</span>(eta)</a>
<a class="sourceLine" id="cb2050-5" data-line-number="5">  total.cost     =<span class="st"> </span>epoch.cost     =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2050-6" data-line-number="6">  total.accuracy =<span class="st"> </span>epoch.accuracy =<span class="st"> </span>fc.params =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2050-7" data-line-number="7">  total.validate =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2050-8" data-line-number="8">    </a>
<a class="sourceLine" id="cb2050-9" data-line-number="9">  <span class="co"># Target images</span></a>
<a class="sourceLine" id="cb2050-10" data-line-number="10">  population     =<span class="st"> </span>target.images<span class="op">$</span>rgb</a>
<a class="sourceLine" id="cb2050-11" data-line-number="11">  population.len =<span class="st"> </span><span class="kw">length</span>(population)</a>
<a class="sourceLine" id="cb2050-12" data-line-number="12">  sample.set     =<span class="st"> </span><span class="dv">250</span> <span class="co"># population.len</span></a>
<a class="sourceLine" id="cb2050-13" data-line-number="13">  shuffled.data  =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dt">n=</span>population.len, <span class="dt">size=</span>population.len, </a>
<a class="sourceLine" id="cb2050-14" data-line-number="14">                              <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2050-15" data-line-number="15">  sample.size    =<span class="st"> </span><span class="kw">ifelse</span>(sample.set <span class="op">&lt;</span><span class="st"> </span>population.len, sample.set, </a>
<a class="sourceLine" id="cb2050-16" data-line-number="16">                          population.len)</a>
<a class="sourceLine" id="cb2050-17" data-line-number="17">  sample.data    =<span class="st"> </span><span class="kw">sample</span>(shuffled.data, sample.size)</a>
<a class="sourceLine" id="cb2050-18" data-line-number="18">  k              =<span class="st"> </span><span class="kw">ceiling</span>(sample.size <span class="op">/</span><span class="st"> </span>minibatch)</a>
<a class="sourceLine" id="cb2050-19" data-line-number="19">  <span class="kw">flush.str</span>( <span class="st">&quot;No of batches: %d&quot;</span>, k )</a>
<a class="sourceLine" id="cb2050-20" data-line-number="20">  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(transfer)) {</a>
<a class="sourceLine" id="cb2050-21" data-line-number="21">      layers =<span class="st"> </span><span class="kw">transfer.learning</span>(transfer)</a>
<a class="sourceLine" id="cb2050-22" data-line-number="22">  } </a>
<a class="sourceLine" id="cb2050-23" data-line-number="23">  <span class="cf">if</span> (<span class="kw">is.null</span>(layers)) {</a>
<a class="sourceLine" id="cb2050-24" data-line-number="24">      <span class="kw">stop</span>(<span class="st">&quot;No network layers defined.&quot;</span>)</a>
<a class="sourceLine" id="cb2050-25" data-line-number="25">  }</a>
<a class="sourceLine" id="cb2050-26" data-line-number="26">  my.time =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2050-27" data-line-number="27">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {  </a>
<a class="sourceLine" id="cb2050-28" data-line-number="28">    batch.cost =<span class="st"> </span>batch.accuracy =<span class="st"> </span><span class="ot">NULL</span>     </a>
<a class="sourceLine" id="cb2050-29" data-line-number="29">    batch.time =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2050-30" data-line-number="30">    step.eta   =<span class="st"> </span><span class="kw">step.decay</span>(eta, t, <span class="dt">decay.factor=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2050-31" data-line-number="31">    n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2050-32" data-line-number="32">    batches =<span class="st"> </span><span class="kw">get.batch</span>(sample.data, labels, k, t)</a>
<a class="sourceLine" id="cb2050-33" data-line-number="33">    <span class="cf">for</span> (batch <span class="cf">in</span> batches<span class="op">$</span>train) {</a>
<a class="sourceLine" id="cb2050-34" data-line-number="34">        n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>        </a>
<a class="sourceLine" id="cb2050-35" data-line-number="35">        X =<span class="st"> </span>batch<span class="op">$</span>X</a>
<a class="sourceLine" id="cb2050-36" data-line-number="36">        Y =<span class="st"> </span>batch<span class="op">$</span>Y</a>
<a class="sourceLine" id="cb2050-37" data-line-number="37">        model          =<span class="st"> </span><span class="kw">forward.pass.cnn</span>(X, layers)</a>
<a class="sourceLine" id="cb2050-38" data-line-number="38">        backprop       =<span class="st"> </span><span class="kw">back.propagation.cnn</span>(X, Y, model)</a>
<a class="sourceLine" id="cb2050-39" data-line-number="39">        layers         =<span class="st"> </span><span class="kw">optimizer</span>(backprop, model<span class="op">$</span>layers, optimize, </a>
<a class="sourceLine" id="cb2050-40" data-line-number="40">                                   (t <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>k <span class="op">+</span><span class="st"> </span>n, step.eta)</a>
<a class="sourceLine" id="cb2050-41" data-line-number="41">        len            =<span class="st"> </span><span class="kw">length</span>(model<span class="op">$</span>fc.model<span class="op">$</span>layers)</a>
<a class="sourceLine" id="cb2050-42" data-line-number="42">        softmax.prob   =<span class="st"> </span>model<span class="op">$</span>fc.model<span class="op">$</span>layers[[len]]<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2050-43" data-line-number="43">        loss           =<span class="st"> </span><span class="kw">softmax.loss</span>(Y, softmax.prob)</a>
<a class="sourceLine" id="cb2050-44" data-line-number="44">        accurate       =<span class="st"> </span><span class="kw">accuracy</span>(Y, softmax.prob)</a>
<a class="sourceLine" id="cb2050-45" data-line-number="45">        batch.cost     =<span class="st"> </span><span class="kw">c</span>(batch.cost, <span class="kw">mean</span>(loss))</a>
<a class="sourceLine" id="cb2050-46" data-line-number="46">        batch.accuracy =<span class="st"> </span><span class="kw">c</span>(batch.accuracy, accurate)</a>
<a class="sourceLine" id="cb2050-47" data-line-number="47">        </a>
<a class="sourceLine" id="cb2050-48" data-line-number="48">        <span class="cf">if</span> (n <span class="op">%%</span><span class="st"> </span><span class="dv">50</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb2050-49" data-line-number="49">          new.time =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2050-50" data-line-number="50">          lag.time =<span class="st"> </span><span class="kw">difftime</span>(new.time, my.time, <span class="dt">units=</span><span class="st">&quot;secs&quot;</span>) </a>
<a class="sourceLine" id="cb2050-51" data-line-number="51">          my.time  =<span class="st"> </span>new.time</a>
<a class="sourceLine" id="cb2050-52" data-line-number="52">          stime =<span class="st"> </span><span class="kw">format</span>(<span class="kw">Sys.Date</span>(), <span class="st">&quot;%c&quot;</span>)</a>
<a class="sourceLine" id="cb2050-53" data-line-number="53">          <span class="kw">flush.str</span>( </a>
<a class="sourceLine" id="cb2050-54" data-line-number="54">          <span class="st">&quot;batch %d - loss: %2.3f t: %d, accuracy %2.3f lagtime (sec): %5.3f&quot;</span>,</a>
<a class="sourceLine" id="cb2050-55" data-line-number="55">               n, <span class="kw">mean</span>(loss),  (t <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>k <span class="op">+</span><span class="st"> </span>n, accurate, lag.time)</a>
<a class="sourceLine" id="cb2050-56" data-line-number="56">        }</a>
<a class="sourceLine" id="cb2050-57" data-line-number="57">        <span class="cf">if</span> (<span class="kw">is.na</span>(<span class="kw">mean</span>(loss)) <span class="op">||</span><span class="st"> </span><span class="kw">mean</span>(loss) <span class="op">&gt;</span><span class="st"> </span><span class="dv">40</span>) { </a>
<a class="sourceLine" id="cb2050-58" data-line-number="58">            <span class="kw">flush.str</span>( </a>
<a class="sourceLine" id="cb2050-59" data-line-number="59">              <span class="st">&quot;loss NaN/increasing at %d epoch.&quot;</span>,  (t <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>k <span class="op">+</span><span class="st"> </span>n) </a>
<a class="sourceLine" id="cb2050-60" data-line-number="60">            <span class="kw">stop</span>(<span class="st">&quot;&quot;</span>) </a>
<a class="sourceLine" id="cb2050-61" data-line-number="61">        }</a>
<a class="sourceLine" id="cb2050-62" data-line-number="62">     }</a>
<a class="sourceLine" id="cb2050-63" data-line-number="63">      </a>
<a class="sourceLine" id="cb2050-64" data-line-number="64">     <span class="co">## Validate</span></a>
<a class="sourceLine" id="cb2050-65" data-line-number="65">     batches   =<span class="st"> </span>batches<span class="op">$</span>validation</a>
<a class="sourceLine" id="cb2050-66" data-line-number="66">     X =<span class="st"> </span>batch<span class="op">$</span>X</a>
<a class="sourceLine" id="cb2050-67" data-line-number="67">     Y =<span class="st"> </span>batch<span class="op">$</span>Y</a>
<a class="sourceLine" id="cb2050-68" data-line-number="68">     valid.model    =<span class="st"> </span><span class="kw">forward.pass.cnn</span>(X, layers, <span class="dt">train=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2050-69" data-line-number="69">     len            =<span class="st"> </span><span class="kw">length</span>(valid.model<span class="op">$</span>fc.model<span class="op">$</span>layers)</a>
<a class="sourceLine" id="cb2050-70" data-line-number="70">     softmax.prob   =<span class="st"> </span>valid.model<span class="op">$</span>fc.model<span class="op">$</span>layers[[len]]<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2050-71" data-line-number="71">     valid.accurate =<span class="st"> </span><span class="kw">accuracy</span>(Y, softmax.prob)</a>
<a class="sourceLine" id="cb2050-72" data-line-number="72">      </a>
<a class="sourceLine" id="cb2050-73" data-line-number="73">     new.time   =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2050-74" data-line-number="74">     lag.time =<span class="st"> </span><span class="kw">difftime</span>(new.time, batch.time, <span class="dt">units=</span><span class="st">&quot;secs&quot;</span>)</a>
<a class="sourceLine" id="cb2050-75" data-line-number="75">     epoch.cost      =<span class="st"> </span><span class="kw">c</span>(epoch.cost, <span class="kw">mean</span>(batch.cost))</a>
<a class="sourceLine" id="cb2050-76" data-line-number="76">     epoch.accuracy  =<span class="st"> </span><span class="kw">c</span>(epoch.accuracy, <span class="kw">mean</span>(batch.accuracy))</a>
<a class="sourceLine" id="cb2050-77" data-line-number="77">     total.cost      =<span class="st"> </span><span class="kw">c</span>(total.cost, batch.cost)</a>
<a class="sourceLine" id="cb2050-78" data-line-number="78">     total.accuracy  =<span class="st"> </span><span class="kw">c</span>(total.accuracy, batch.accuracy)</a>
<a class="sourceLine" id="cb2050-79" data-line-number="79">     total.validate  =<span class="st"> </span><span class="kw">c</span>(total.validate, valid.accurate)</a>
<a class="sourceLine" id="cb2050-80" data-line-number="80">     <span class="kw">flush.str</span>( </a>
<a class="sourceLine" id="cb2050-81" data-line-number="81">     <span class="st">&quot;epoch %d: loss %2.3f accuracy %2.3f val %2.3f lag time (sec): %5.3f&quot;</span>,</a>
<a class="sourceLine" id="cb2050-82" data-line-number="82">     t, <span class="kw">mean</span>(batch.cost),  <span class="kw">mean</span>(batch.accuracy), valid.accurate, lag.time)</a>
<a class="sourceLine" id="cb2050-83" data-line-number="83">     <span class="cf">if</span> (t <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) { <span class="kw">gc</span>() } <span class="co"># garbage collection</span></a>
<a class="sourceLine" id="cb2050-84" data-line-number="84">     <span class="cf">if</span> (valid.accurate <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.90</span>) {</a>
<a class="sourceLine" id="cb2050-85" data-line-number="85">       <span class="kw">flush.str</span>( </a>
<a class="sourceLine" id="cb2050-86" data-line-number="86">       <span class="st">&quot;We have reached a validation accuracy of %2.3f. This is good enough&quot;</span>,</a>
<a class="sourceLine" id="cb2050-87" data-line-number="87">       valid.accurate)</a>
<a class="sourceLine" id="cb2050-88" data-line-number="88">       <span class="cf">break</span></a>
<a class="sourceLine" id="cb2050-89" data-line-number="89">     }</a>
<a class="sourceLine" id="cb2050-90" data-line-number="90">  } </a>
<a class="sourceLine" id="cb2050-91" data-line-number="91">  <span class="kw">list</span>(<span class="st">&quot;model&quot;</span> =<span class="st"> </span>model, <span class="st">&quot;layers&quot;</span> =<span class="st"> </span>layers, <span class="st">&quot;eta&quot;</span> =<span class="st"> </span>eta, </a>
<a class="sourceLine" id="cb2050-92" data-line-number="92">       <span class="st">&quot;cost&quot;</span> =<span class="st"> </span>epoch.cost, <span class="st">&quot;accuracy&quot;</span> =<span class="st"> </span>epoch.accuracy, </a>
<a class="sourceLine" id="cb2050-93" data-line-number="93">       <span class="st">&quot;total.cost&quot;</span> =<span class="st"> </span>total.cost, <span class="st">&quot;total.accuracy&quot;</span> =<span class="st"> </span>total.accuracy,</a>
<a class="sourceLine" id="cb2050-94" data-line-number="94">       <span class="st">&quot;total.valid&quot;</span> =<span class="st"> </span>total.validate)</a>
<a class="sourceLine" id="cb2050-95" data-line-number="95"></a>
<a class="sourceLine" id="cb2050-96" data-line-number="96">}</a></code></pre></div>

<p>In the following two sections, we discuss the use of <strong>GEMM</strong> in our implementation and a more efficient convolution algorithm, namely <strong>Depthwise Separable Convolution</strong> and <strong>Pointwise Convolution</strong>.</p>
</div>
<div id="cnn-application" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.16</span> CNN Application<a href="deeplearning1.html#cnn-application" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, let us perform image classification using <strong>CIFAR-10</strong> as our dataset, containing 60000 <span class="math inline">\(32 \times 32\)</span> tiny images <span class="citation">(Krizhevsky A. <a href="bibliography.html#ref-ref1350a">2009</a>)</span>. Here, we use the binary version of the dataset. At the time of writing, the currently available dataset is broken down into six files: data_batch{1,2,3,4,5}.bin and a test_batch.bin. However, because the dataset is a collection of images instead of one single <strong>JPEG</strong> image, we have to read differently such that instead of using <strong>readJPEG(.)</strong>, we use <strong>readBin(.)</strong> to read raw data in binary format.</p>
<p>Below is a modified implementation of reading and parsing the <strong>CIFAR-10</strong> dataset (motivated by an R script written by an anonymous <strong>matt</strong> from Stackoverflow (questions/32113942)):</p>

<div class="sourceCode" id="cb2051"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2051-1" data-line-number="1">getNext &lt;-<span class="st"> </span><span class="cf">function</span>(fp, <span class="dt">typ =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb2051-2" data-line-number="2">    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(typ) <span class="op">&amp;&amp;</span><span class="st"> </span>typ <span class="op">==</span><span class="st"> &quot;label&quot;</span>) {</a>
<a class="sourceLine" id="cb2051-3" data-line-number="3">        <span class="kw">readBin</span>(fp, <span class="kw">integer</span>(), <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">n=</span><span class="dv">1</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb2051-4" data-line-number="4">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2051-5" data-line-number="5">        <span class="kw">as.integer</span>(<span class="kw">readBin</span>(fp, <span class="kw">raw</span>(), <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">n=</span><span class="dv">1024</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>))      </a>
<a class="sourceLine" id="cb2051-6" data-line-number="6">    }</a>
<a class="sourceLine" id="cb2051-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb2051-8" data-line-number="8">readBatch &lt;-<span class="st"> </span><span class="cf">function</span>(fn, images, labels) {</a>
<a class="sourceLine" id="cb2051-9" data-line-number="9">    fp         =<span class="st"> </span><span class="kw">file</span>(fn, <span class="st">&quot;rb&quot;</span>)</a>
<a class="sourceLine" id="cb2051-10" data-line-number="10">    i =<span class="st"> </span><span class="kw">length</span>(images<span class="op">$</span>lab)</a>
<a class="sourceLine" id="cb2051-11" data-line-number="11">    <span class="cf">for</span> (g <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>) {</a>
<a class="sourceLine" id="cb2051-12" data-line-number="12">        i =<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2051-13" data-line-number="13">        images<span class="op">$</span>lab[i] =<span class="st"> </span>labels[ <span class="kw">getNext</span>(fp, <span class="dt">ty =</span> <span class="st">&quot;label&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>, ]</a>
<a class="sourceLine" id="cb2051-14" data-line-number="14">        images<span class="op">$</span>rgb[[i]] =<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">r =</span> <span class="kw">getNext</span>(fp), <span class="dt">g =</span> <span class="kw">getNext</span>(fp), </a>
<a class="sourceLine" id="cb2051-15" data-line-number="15">                                      <span class="dt">b =</span> <span class="kw">getNext</span>(fp))</a>
<a class="sourceLine" id="cb2051-16" data-line-number="16">    }</a>
<a class="sourceLine" id="cb2051-17" data-line-number="17">    <span class="kw">close</span>(fp)</a>
<a class="sourceLine" id="cb2051-18" data-line-number="18">    <span class="kw">list</span>(<span class="st">&quot;label&quot;</span> =<span class="st"> </span>images<span class="op">$</span>lab, <span class="st">&quot;rgb&quot;</span> =<span class="st"> </span>images<span class="op">$</span>rgb)</a>
<a class="sourceLine" id="cb2051-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb2051-20" data-line-number="20">extract.label &lt;-<span class="st"> </span><span class="cf">function</span>(images, labels, index) {</a>
<a class="sourceLine" id="cb2051-21" data-line-number="21">    labels[ images<span class="op">$</span>lab[[index]],]</a>
<a class="sourceLine" id="cb2051-22" data-line-number="22">}</a>
<a class="sourceLine" id="cb2051-23" data-line-number="23">extract.label.onehot &lt;-<span class="st"> </span><span class="cf">function</span>(images, labels, index) {</a>
<a class="sourceLine" id="cb2051-24" data-line-number="24">    len =<span class="st"> </span><span class="kw">length</span>(index)</a>
<a class="sourceLine" id="cb2051-25" data-line-number="25">    list.label =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(len,<span class="dv">10</span>))</a>
<a class="sourceLine" id="cb2051-26" data-line-number="26">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2051-27" data-line-number="27">      n =<span class="st"> </span><span class="kw">nrow</span>(labels)</a>
<a class="sourceLine" id="cb2051-28" data-line-number="28">      label =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb2051-29" data-line-number="29">      label[images<span class="op">$</span>lab[[index[i]]]] =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2051-30" data-line-number="30">      list.label[i,] =<span class="st"> </span>label</a>
<a class="sourceLine" id="cb2051-31" data-line-number="31">    }</a>
<a class="sourceLine" id="cb2051-32" data-line-number="32">    list.label</a>
<a class="sourceLine" id="cb2051-33" data-line-number="33">}</a></code></pre></div>
<div class="sourceCode" id="cb2052"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2052-1" data-line-number="1">extract.image &lt;-<span class="st"> </span><span class="cf">function</span>(images, index) {</a>
<a class="sourceLine" id="cb2052-2" data-line-number="2">  len =<span class="st"> </span><span class="kw">length</span>(index)</a>
<a class="sourceLine" id="cb2052-3" data-line-number="3">  list.img =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(<span class="dv">32</span>,<span class="dv">32</span>,<span class="dv">3</span>, len))</a>
<a class="sourceLine" id="cb2052-4" data-line-number="4">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2052-5" data-line-number="5">   img   =<span class="st"> </span>images<span class="op">$</span>rgb[[index[i]]]</a>
<a class="sourceLine" id="cb2052-6" data-line-number="6">   r     =<span class="st"> </span><span class="kw">matrix</span>(img<span class="op">$</span>r, <span class="dt">ncol=</span><span class="dv">32</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2052-7" data-line-number="7">   g     =<span class="st"> </span><span class="kw">matrix</span>(img<span class="op">$</span>g, <span class="dt">ncol=</span><span class="dv">32</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2052-8" data-line-number="8">   b     =<span class="st"> </span><span class="kw">matrix</span>(img<span class="op">$</span>b, <span class="dt">ncol=</span><span class="dv">32</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2052-9" data-line-number="9">   norm  =<span class="st"> </span><span class="dv">255</span></a>
<a class="sourceLine" id="cb2052-10" data-line-number="10">   img   =<span class="st"> </span><span class="kw">array</span>(<span class="kw">cbind</span>(r,g,b) <span class="op">/</span><span class="st"> </span>norm, <span class="kw">c</span>(<span class="dv">32</span>,<span class="dv">32</span>,<span class="dv">3</span>))  <span class="co"># normalize</span></a>
<a class="sourceLine" id="cb2052-11" data-line-number="11">   list.img[,,,i] =<span class="st"> </span>img</a>
<a class="sourceLine" id="cb2052-12" data-line-number="12">  }</a>
<a class="sourceLine" id="cb2052-13" data-line-number="13">  list.img</a>
<a class="sourceLine" id="cb2052-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb2052-15" data-line-number="15">search.image &lt;-<span class="st"> </span><span class="cf">function</span>(images, labels, <span class="dt">label =</span> <span class="st">&#39;ship&#39;</span>) {</a>
<a class="sourceLine" id="cb2052-16" data-line-number="16">  idx =<span class="st"> </span><span class="kw">which</span>(labels <span class="op">==</span><span class="st"> </span>label)</a>
<a class="sourceLine" id="cb2052-17" data-line-number="17">  len =<span class="st"> </span><span class="kw">length</span>(images<span class="op">$</span>rgb)</a>
<a class="sourceLine" id="cb2052-18" data-line-number="18">  what =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2052-19" data-line-number="19">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2052-20" data-line-number="20">    <span class="cf">if</span> (images<span class="op">$</span>lab[[i]] <span class="op">==</span><span class="st"> </span>idx) {</a>
<a class="sourceLine" id="cb2052-21" data-line-number="21">        what =<span class="st"> </span><span class="kw">c</span>(what, i)</a>
<a class="sourceLine" id="cb2052-22" data-line-number="22">    }</a>
<a class="sourceLine" id="cb2052-23" data-line-number="23">  }</a>
<a class="sourceLine" id="cb2052-24" data-line-number="24">  what</a>
<a class="sourceLine" id="cb2052-25" data-line-number="25">}</a>
<a class="sourceLine" id="cb2052-26" data-line-number="26">draw.image &lt;-<span class="st"> </span><span class="cf">function</span>(image, <span class="dt">main=</span><span class="st">&quot;Apple, Oranges, and Banana&quot;</span>) { </a>
<a class="sourceLine" id="cb2052-27" data-line-number="27">  sz =<span class="st"> </span><span class="kw">nrow</span>(image)</a>
<a class="sourceLine" id="cb2052-28" data-line-number="28">  <span class="kw">par</span>(<span class="dt">pty=</span><span class="st">&quot;s&quot;</span>) </a>
<a class="sourceLine" id="cb2052-29" data-line-number="29">  <span class="kw">plot</span>(<span class="dv">1</span>, <span class="dt">type=</span><span class="st">&quot;n&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">1</span>, sz), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">1</span>, sz), </a>
<a class="sourceLine" id="cb2052-30" data-line-number="30">     <span class="dt">xlab=</span><span class="st">&quot;Image Width&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Image Height&quot;</span>, <span class="dt">main=</span>main)</a>
<a class="sourceLine" id="cb2052-31" data-line-number="31">  <span class="kw">rasterImage</span>(image,<span class="dt">xleft=</span><span class="dv">1</span>, <span class="dt">ybottom=</span>sz, <span class="dt">xright=</span>sz, <span class="dt">ytop=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2052-32" data-line-number="32">}</a></code></pre></div>

<p>We then use our <strong>draw.image(.)</strong> function like so (for example, reading the 20th image in the dataset):</p>

<div class="sourceCode" id="cb2053"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2053-1" data-line-number="1">images =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;label&quot;</span> =<span class="st"> </span><span class="kw">list</span>(), <span class="st">&quot;rgb&quot;</span> =<span class="st"> </span><span class="kw">list</span>())</a>
<a class="sourceLine" id="cb2053-2" data-line-number="2">labels =<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;cifar-10-batches-bin/batches.meta.txt&quot;</span>)</a>
<a class="sourceLine" id="cb2053-3" data-line-number="3"><span class="cf">for</span> (f <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb2053-4" data-line-number="4">    fn       =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;cifar-10-batches-bin/data_batch_&quot;</span>, f, <span class="st">&quot;.bin&quot;</span>)</a>
<a class="sourceLine" id="cb2053-5" data-line-number="5">    images =<span class="st"> </span><span class="kw">readBatch</span>(fn, images, labels ) </a>
<a class="sourceLine" id="cb2053-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb2053-7" data-line-number="7">this.image =<span class="st"> </span><span class="kw">extract.image</span>(images,<span class="dv">20</span>)</a>
<a class="sourceLine" id="cb2053-8" data-line-number="8">this.label =<span class="st"> </span><span class="kw">extract.label</span>(images,labels,<span class="dv">20</span>)</a>
<a class="sourceLine" id="cb2053-9" data-line-number="9"><span class="kw">draw.image</span>(<span class="kw">array</span>(this.image, <span class="kw">c</span>(<span class="dv">32</span>,<span class="dv">32</span>,<span class="dv">3</span>)) , <span class="dt">main=</span>this.label)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cifar1020"></span>
<img src="cifarfrog.png" alt="CIFAR-10 (20th Image)" width="70%" />
<p class="caption">
Figure 12.46: CIFAR-10 (20th Image)
</p>
</div>

<p>The image has the following dimension:</p>

<div class="sourceCode" id="cb2054"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2054-1" data-line-number="1">image =<span class="st"> </span><span class="kw">extract.image</span>(images,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2054-2" data-line-number="2"><span class="kw">dim</span>(image)</a></code></pre></div>
<pre><code>## [1] 32 32  3  1</code></pre>

<p>The corresponding label is translated into a one-hot vector with the 7th index matching the label for a <strong>frog</strong>:</p>

<div class="sourceCode" id="cb2056"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2056-1" data-line-number="1">label =<span class="st"> </span><span class="kw">extract.label.onehot</span>(images, labels, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2056-2" data-line-number="2">label</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0    0    0    0    0    1    0    0     0</code></pre>
<div class="sourceCode" id="cb2058"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2058-1" data-line-number="1"><span class="kw">t</span>(labels)</a></code></pre></div>
<pre><code>##    [,1]       [,2]         [,3]   [,4]  [,5]   [,6] 
## V1 &quot;airplane&quot; &quot;automobile&quot; &quot;bird&quot; &quot;cat&quot; &quot;deer&quot; &quot;dog&quot;
##    [,7]   [,8]    [,9]   [,10]  
## V1 &quot;frog&quot; &quot;horse&quot; &quot;ship&quot; &quot;truck&quot;</code></pre>

<p>With that, let us now design our <strong>CNN layers</strong> like so:</p>

<div class="sourceCode" id="cb2060"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2060-1" data-line-number="1"><span class="kw">library</span>(dequer)</a>
<a class="sourceLine" id="cb2060-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb2060-3" data-line-number="3">size =<span class="st"> </span><span class="dv">32</span></a>
<a class="sourceLine" id="cb2060-4" data-line-number="4">depth =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2060-5" data-line-number="5">minibatch =<span class="st"> </span><span class="dv">32</span></a>
<a class="sourceLine" id="cb2060-6" data-line-number="6">X =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>, size <span class="op">*</span><span class="st"> </span>size <span class="op">*</span><span class="st"> </span>depth <span class="op">*</span><span class="st"> </span>minibatch), </a>
<a class="sourceLine" id="cb2060-7" data-line-number="7">          <span class="kw">c</span>(size, size, depth, minibatch))</a>
<a class="sourceLine" id="cb2060-8" data-line-number="8"><span class="kw">dim</span>(X)</a></code></pre></div>
<pre><code>## [1] 32 32  3 32</code></pre>
<div class="sourceCode" id="cb2062"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2062-1" data-line-number="1">cnn.layers =<span class="st"> </span><span class="kw">deep.cnn.layers</span>( X, </a>
<a class="sourceLine" id="cb2062-2" data-line-number="2"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">32</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb2062-3" data-line-number="3">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>),     </a>
<a class="sourceLine" id="cb2062-4" data-line-number="4"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;pooling&quot;</span>, <span class="dt">size=</span><span class="dv">2</span>, <span class="dt">stride=</span><span class="dv">2</span>, <span class="dt">ptype=</span><span class="st">&quot;maxpool&quot;</span>),</a>
<a class="sourceLine" id="cb2062-5" data-line-number="5"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">64</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2062-6" data-line-number="6">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>),</a>
<a class="sourceLine" id="cb2062-7" data-line-number="7"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;pooling&quot;</span>, <span class="dt">size=</span><span class="dv">2</span>, <span class="dt">stride=</span><span class="dv">2</span>, <span class="dt">ptype=</span><span class="st">&quot;maxpool&quot;</span>),</a>
<a class="sourceLine" id="cb2062-8" data-line-number="8"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;dense&quot;</span>,  <span class="kw">list</span>(  <span class="dt">size=</span><span class="dv">256</span>, <span class="dt">drop=</span><span class="fl">0.05</span> ),<span class="kw">list</span>(  <span class="dt">size=</span><span class="dv">10</span> ))</a>
<a class="sourceLine" id="cb2062-9" data-line-number="9">)</a></code></pre></div>

<p>We then invoke our <strong>my.CNN(.)</strong> function, feeding a minibatch=32 and epoch=10. See plot in Figure <a href="deeplearning1.html#fig:cnnplot">12.47</a>.</p>

<div class="sourceCode" id="cb2063"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2063-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb2063-2" data-line-number="2">cnn.model =<span class="st"> </span><span class="kw">my.CNN</span>(images, labels, <span class="dt">layers =</span> cnn.layers, <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>,</a>
<a class="sourceLine" id="cb2063-3" data-line-number="3">                               <span class="dt">minibatch=</span>minibatch, <span class="dt">epoch=</span><span class="dv">10</span>, <span class="dt">eta=</span><span class="fl">0.001</span>)</a></code></pre></div>
<pre><code>## [1] &quot;No of batches: 8&quot;
## [1] &quot;epoch 1: loss 6.246 accuracy 0.146 val 0.161 lag time (sec): 13.299&quot;
## [1] &quot;epoch 2: loss 3.078 accuracy 0.273 val 0.194 lag time (sec): 12.607&quot;
## [1] &quot;epoch 3: loss 1.599 accuracy 0.508 val 0.000 lag time (sec): 11.835&quot;
## [1] &quot;epoch 4: loss 1.251 accuracy 0.580 val 0.258 lag time (sec): 12.118&quot;
## [1] &quot;epoch 5: loss 0.982 accuracy 0.689 val 0.000 lag time (sec): 11.839&quot;
## [1] &quot;epoch 6: loss 0.842 accuracy 0.736 val 0.125 lag time (sec): 12.420&quot;
## [1] &quot;epoch 7: loss 0.602 accuracy 0.845 val 0.065 lag time (sec): 11.957&quot;
## [1] &quot;epoch 8: loss 0.408 accuracy 0.916 val 0.094 lag time (sec): 11.886&quot;
## [1] &quot;epoch 9: loss 0.307 accuracy 0.959 val 0.194 lag time (sec): 11.908&quot;
## [1] &quot;epoch 10: loss 0.262 accuracy 0.955 val 0.219 lag time (sec): 12.232&quot;</code></pre>
<div class="sourceCode" id="cb2065"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2065-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(cnn.model<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb2065-2" data-line-number="2">y =<span class="st"> </span>cnn.model<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb2065-3" data-line-number="3">y1 =<span class="st"> </span>(y <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))<span class="op">/</span>(<span class="kw">max</span>(y) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))</a>
<a class="sourceLine" id="cb2065-4" data-line-number="4">y2 =<span class="st"> </span>cnn.model<span class="op">$</span>accuracy</a>
<a class="sourceLine" id="cb2065-5" data-line-number="5"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,y1),   </a>
<a class="sourceLine" id="cb2065-6" data-line-number="6">      <span class="dt">xlab=</span><span class="st">&quot;Epoch&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Cross-Entropy Loss / Accuracy&quot;</span>,   </a>
<a class="sourceLine" id="cb2065-7" data-line-number="7">      <span class="dt">main=</span><span class="st">&quot;CNN (250, 32 - 32dm, 64dm, fc256)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2065-8" data-line-number="8"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb2065-9" data-line-number="9"><span class="kw">lines</span>(x, y1, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2065-10" data-line-number="10"><span class="kw">lines</span>(x, y2, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cnnplot"></span>
<img src="DS_files/figure-html/cnnplot-1.png" alt="CNN PLOT" width="70%" />
<p class="caption">
Figure 12.47: CNN PLOT
</p>
</div>

<p>Notice in the figure that we trained only 250 images. It is worth noting that our <strong>CNN</strong> implementation indeed demonstrates the ability to train. We can shoot to 96.8% (even up to 100%) <strong>top-1</strong> train accuracy. However, if we perform prediction, we may see our <strong>top-1</strong> test accuracy as low as 20%. That is because there is insufficient data to train; thus, we are overfitting the train data.</p>
<p>Over time, we performed several iterations to achieve a better train and test accuracy using the entire 50,000 images. Our model was trained with a <strong>top-1</strong> train accuracy of 94.20% after epoch 22 for 50000 cifar-10 images, which took 19.8 hours with an average lag time of 3750 seconds per epoch using only four CPUs (from quad-core) with no GPU support. Here, we show how we can use our saved model:</p>

<div class="sourceCode" id="cb2066"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2066-1" data-line-number="1">transfer.cnn.model =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;transfer_model.rds&quot;</span>) </a></code></pre></div>

<p>The model uses the following neural network configuration:</p>

<div class="sourceCode" id="cb2067"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2067-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb2067-2" data-line-number="2">size =<span class="st"> </span><span class="dv">32</span></a>
<a class="sourceLine" id="cb2067-3" data-line-number="3">depth =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2067-4" data-line-number="4">minibatch =<span class="st"> </span><span class="dv">32</span></a>
<a class="sourceLine" id="cb2067-5" data-line-number="5">X =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>, size <span class="op">*</span><span class="st"> </span>size <span class="op">*</span><span class="st"> </span>depth <span class="op">*</span><span class="st"> </span>minibatch), </a>
<a class="sourceLine" id="cb2067-6" data-line-number="6">          <span class="kw">c</span>(size, size, depth, minibatch))</a>
<a class="sourceLine" id="cb2067-7" data-line-number="7"><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb2067-8" data-line-number="8">cnn.layers =<span class="st"> </span><span class="kw">deep.cnn.layers</span>( X, </a>
<a class="sourceLine" id="cb2067-9" data-line-number="9"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">32</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2067-10" data-line-number="10">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>),</a>
<a class="sourceLine" id="cb2067-11" data-line-number="11"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">32</span>, <span class="dt">stride=</span><span class="dv">2</span>,  <span class="dt">padding=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2067-12" data-line-number="12">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>),</a>
<a class="sourceLine" id="cb2067-13" data-line-number="13"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">92</span>, <span class="dt">stride=</span><span class="dv">1</span>,  <span class="dt">padding=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2067-14" data-line-number="14">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>), </a>
<a class="sourceLine" id="cb2067-15" data-line-number="15"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">92</span>, <span class="dt">stride=</span><span class="dv">2</span>,  <span class="dt">padding=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2067-16" data-line-number="16">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>),</a>
<a class="sourceLine" id="cb2067-17" data-line-number="17"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;dense&quot;</span>, <span class="kw">list</span>(  <span class="dt">size=</span><span class="dv">1024</span>, <span class="dt">drop=</span><span class="fl">0.95</span>),<span class="kw">list</span>(  <span class="dt">size=</span><span class="dv">10</span> ))</a>
<a class="sourceLine" id="cb2067-18" data-line-number="18">)</a></code></pre></div>

<p>The <strong>CNN</strong> architecture shows only four convolution layers and one dense layer with dropout. Instead of pooling, we adjust the stride to be 2 in the first and fourth convolution layers to reduce dimensionality.</p>
<p>With that, the model gives us a plot of the cost and accuracy in Figure <a href="deeplearning1.html#fig:transferred">12.48</a>. Note that the line, in increasing direction (and in navy blue color), represents accuracy.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:transferred"></span>
<img src="DS_files/figure-html/transferred-1.png" alt="Transferred Model (CNN)" width="80%" />
<p class="caption">
Figure 12.48: Transferred Model (CNN)
</p>
</div>

<p>To generate prediction, we use the <strong>my.predict.NN(.)</strong> function below:</p>

<div class="sourceCode" id="cb2068"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2068-1" data-line-number="1">get.test.batch &lt;-<span class="st"> </span><span class="cf">function</span>(sample.data, labels, k, t) {  </a>
<a class="sourceLine" id="cb2068-2" data-line-number="2">   <span class="kw">set.seed</span>(<span class="dv">2019</span>)</a>
<a class="sourceLine" id="cb2068-3" data-line-number="3">   batch.indices =<span class="st"> </span><span class="kw">createFolds</span>(sample.data, <span class="dt">k =</span> k, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2068-4" data-line-number="4">   batches =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2068-5" data-line-number="5">   <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {</a>
<a class="sourceLine" id="cb2068-6" data-line-number="6">       indices =<span class="st"> </span>batch.indices[[i]]</a>
<a class="sourceLine" id="cb2068-7" data-line-number="7">       X =<span class="st"> </span><span class="kw">extract.image</span>(images, indices)</a>
<a class="sourceLine" id="cb2068-8" data-line-number="8">       Y =<span class="st"> </span><span class="kw">extract.label.onehot</span>(images, labels, indices) </a>
<a class="sourceLine" id="cb2068-9" data-line-number="9">         batches[[i]] =<span class="st"> </span><span class="kw">list</span>( <span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y )</a>
<a class="sourceLine" id="cb2068-10" data-line-number="10">   }</a>
<a class="sourceLine" id="cb2068-11" data-line-number="11">   batches</a>
<a class="sourceLine" id="cb2068-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb2068-13" data-line-number="13">my.predict.CNN &lt;-<span class="st"> </span><span class="cf">function</span>(test.images, labels,  model, <span class="dt">minibatch=</span><span class="dv">32</span>,</a>
<a class="sourceLine" id="cb2068-14" data-line-number="14">                           <span class="dt">first_batch_only=</span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb2068-15" data-line-number="15">  <span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">16</span>)  <span class="co"># 16 digits precision for our example</span></a>
<a class="sourceLine" id="cb2068-16" data-line-number="16">  layers        =<span class="st"> </span>model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2068-17" data-line-number="17">  img.len       =<span class="st"> </span><span class="kw">length</span>(images<span class="op">$</span>rgb)</a>
<a class="sourceLine" id="cb2068-18" data-line-number="18">    </a>
<a class="sourceLine" id="cb2068-19" data-line-number="19">  <span class="co"># Test images</span></a>
<a class="sourceLine" id="cb2068-20" data-line-number="20">  population     =<span class="st"> </span>test.images<span class="op">$</span>rgb</a>
<a class="sourceLine" id="cb2068-21" data-line-number="21">  population.len =<span class="st"> </span><span class="kw">length</span>(population)</a>
<a class="sourceLine" id="cb2068-22" data-line-number="22">  sample.set     =<span class="st"> </span>population.len</a>
<a class="sourceLine" id="cb2068-23" data-line-number="23">  shuffled.data  =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dt">n=</span>population.len, <span class="dt">size=</span>population.len, </a>
<a class="sourceLine" id="cb2068-24" data-line-number="24">                              <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2068-25" data-line-number="25">  sample.size    =<span class="st"> </span><span class="kw">ifelse</span>(sample.set <span class="op">&lt;</span><span class="st"> </span>population.len, sample.set, </a>
<a class="sourceLine" id="cb2068-26" data-line-number="26">                          population.len)</a>
<a class="sourceLine" id="cb2068-27" data-line-number="27">  sample.data    =<span class="st"> </span><span class="kw">sample</span>(shuffled.data, sample.size)</a>
<a class="sourceLine" id="cb2068-28" data-line-number="28">  k              =<span class="st"> </span><span class="kw">ceiling</span>(sample.size <span class="op">/</span><span class="st"> </span>minibatch)    </a>
<a class="sourceLine" id="cb2068-29" data-line-number="29">  my.time =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2068-30" data-line-number="30">  <span class="kw">flush.str</span>( <span class="st">&quot;No of batches: %d&quot;</span>, k )</a>
<a class="sourceLine" id="cb2068-31" data-line-number="31">  n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2068-32" data-line-number="32">  total.accuracy =<span class="st"> </span><span class="dv">0</span>; total.loss =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2068-33" data-line-number="33">  <span class="cf">for</span> (batch <span class="cf">in</span> <span class="kw">get.test.batch</span>(sample.data, labels, k, t)) {</a>
<a class="sourceLine" id="cb2068-34" data-line-number="34">    n             =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>        </a>
<a class="sourceLine" id="cb2068-35" data-line-number="35">    X             =<span class="st"> </span>batch<span class="op">$</span>X</a>
<a class="sourceLine" id="cb2068-36" data-line-number="36">    Y             =<span class="st"> </span>batch<span class="op">$</span>Y   </a>
<a class="sourceLine" id="cb2068-37" data-line-number="37">    test.model    =<span class="st"> </span><span class="kw">forward.pass.cnn</span>(X, layers, <span class="dt">train=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2068-38" data-line-number="38">    len           =<span class="st"> </span><span class="kw">length</span>(test.model<span class="op">$</span>fc.model<span class="op">$</span>layers)</a>
<a class="sourceLine" id="cb2068-39" data-line-number="39">    softmax.prob  =<span class="st"> </span>test.model<span class="op">$</span>fc.model<span class="op">$</span>layers[[len]]<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2068-40" data-line-number="40"></a>
<a class="sourceLine" id="cb2068-41" data-line-number="41">    loss          =<span class="st"> </span><span class="kw">softmax.loss</span>(Y, softmax.prob)</a>
<a class="sourceLine" id="cb2068-42" data-line-number="42">    test.accurate =<span class="st"> </span><span class="kw">accuracy</span>(Y, softmax.prob)</a>
<a class="sourceLine" id="cb2068-43" data-line-number="43">    total.loss     =<span class="st"> </span><span class="kw">c</span>(total.loss, <span class="kw">mean</span>(loss))</a>
<a class="sourceLine" id="cb2068-44" data-line-number="44">    total.accuracy =<span class="st"> </span><span class="kw">c</span>(total.accuracy, test.accurate)</a>
<a class="sourceLine" id="cb2068-45" data-line-number="45">      </a>
<a class="sourceLine" id="cb2068-46" data-line-number="46">    <span class="cf">if</span> (n <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb2068-47" data-line-number="47">       new.time =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2068-48" data-line-number="48">       lag.time =<span class="st"> </span><span class="kw">difftime</span>(new.time, my.time, <span class="dt">units=</span><span class="st">&quot;secs&quot;</span>) </a>
<a class="sourceLine" id="cb2068-49" data-line-number="49">       my.time  =<span class="st"> </span>new.time</a>
<a class="sourceLine" id="cb2068-50" data-line-number="50">       stime =<span class="st"> </span><span class="kw">format</span>(<span class="kw">Sys.Date</span>(), <span class="st">&quot;%c&quot;</span>)</a>
<a class="sourceLine" id="cb2068-51" data-line-number="51">       <span class="kw">flush.str</span>( </a>
<a class="sourceLine" id="cb2068-52" data-line-number="52">      <span class="st">&quot;batch %d - loss: %2.3f t: %d, accuracy %2.3f lag time (sec): %5.3f&quot;</span>,</a>
<a class="sourceLine" id="cb2068-53" data-line-number="53">                  n, <span class="kw">mean</span>(loss),   n, test.accurate, lag.time)</a>
<a class="sourceLine" id="cb2068-54" data-line-number="54">       <span class="cf">if</span> (first_batch_only <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) { <span class="cf">break</span> }</a>
<a class="sourceLine" id="cb2068-55" data-line-number="55">    }</a>
<a class="sourceLine" id="cb2068-56" data-line-number="56"></a>
<a class="sourceLine" id="cb2068-57" data-line-number="57">  }</a>
<a class="sourceLine" id="cb2068-58" data-line-number="58">  <span class="kw">list</span>(<span class="st">&quot;accuracy&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(total.accuracy), <span class="st">&quot;loss&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(total.loss),</a>
<a class="sourceLine" id="cb2068-59" data-line-number="59">       <span class="st">&quot;prediction&quot;</span> =<span class="st"> </span>softmax.prob, <span class="st">&quot;target&quot;</span> =<span class="st"> </span>Y)</a>
<a class="sourceLine" id="cb2068-60" data-line-number="60">}</a></code></pre></div>

<p>Along with that, we use the test images from the <strong>cifar-10</strong> dataset.</p>

<div class="sourceCode" id="cb2069"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2069-1" data-line-number="1">test.images =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;label&quot;</span> =<span class="st"> </span><span class="kw">list</span>(), <span class="st">&quot;rgb&quot;</span> =<span class="st"> </span><span class="kw">list</span>())</a>
<a class="sourceLine" id="cb2069-2" data-line-number="2">fn       =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;cifar-10-batches-bin/test_batch.bin&quot;</span>)</a>
<a class="sourceLine" id="cb2069-3" data-line-number="3">test.images =<span class="st"> </span><span class="kw">readBatch</span>(fn, test.images, labels ) </a></code></pre></div>

<p>Now, we run prediction and get the result - let us show the result of the first batch only.</p>

<div class="sourceCode" id="cb2070"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2070-1" data-line-number="1">result =<span class="st"> </span><span class="kw">my.predict.CNN</span>(test.images, labels, transfer.cnn.model, </a>
<a class="sourceLine" id="cb2070-2" data-line-number="2">                        <span class="dt">first_batch_only =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] &quot;No of batches: 313&quot;
## [1] &quot;batch 10 - loss: 0.170 t: 10, accuracy 0.969 lag time (sec): 6.761&quot;</code></pre>

<p>That gives us a <strong>top-1</strong> test accuracy of 87.78% and a loss of 0.122853209833548.</p>
<p>Below, we see the actual predictions compared to the target:</p>

<div class="sourceCode" id="cb2072"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2072-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">digits=</span><span class="dv">8</span>)</a>
<a class="sourceLine" id="cb2072-2" data-line-number="2"><span class="kw">round</span>(result<span class="op">$</span>prediction[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,],<span class="dv">2</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0 0.00 0.00    0 0.00    0    1    0 0.00  0.00
## [2,]    0 0.00 0.00    0 0.00    0    0    1 0.00  0.00
## [3,]    0 0.85 0.05    0 0.01    0    0    0 0.00  0.09
## [4,]    0 0.00 0.99    0 0.00    0    0    0 0.00  0.00
## [5,]    0 0.02 0.00    0 0.00    0    0    0 0.98  0.00</code></pre>
<div class="sourceCode" id="cb2074"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2074-1" data-line-number="1">result<span class="op">$</span>target[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0    0    0    0    0    1    0    0     0
## [2,]    0    0    0    0    0    0    0    1    0     0
## [3,]    0    1    0    0    0    0    0    0    0     0
## [4,]    0    0    1    0    0    0    0    0    0     0
## [5,]    0    0    0    0    0    0    0    0    1     0</code></pre>

<p>Heuristically, we have observed that our implementation reaches a <strong>local minima</strong> somewhere at epoch 22 with about 96% train accuracy using a learning rate of 0.0001. If we decay the learning rate by 0.10 to 0.00001 at epoch 20, we get a little bit more increase in accuracy. For example, below, we can use a step decay like so:</p>

<div class="sourceCode" id="cb2076"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2076-1" data-line-number="1"><span class="kw">step.decay</span>(<span class="fl">0.0001</span>, <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">30</span>), <span class="dt">decay.factor=</span><span class="fl">0.10</span>, <span class="dt">step.size=</span><span class="dv">20</span>)</a></code></pre></div>
<pre><code>##  [1] 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04
##  [9] 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04
## [17] 1e-04 1e-04 1e-04 1e-05 1e-05 1e-05 1e-05 1e-05
## [25] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05</code></pre>

<p>Alternatively, as we have already trained our model, we can instead use the saved <strong>transfer.cnn.model</strong> to be re-trained via <strong>transfer learning</strong> and manually adjust the learning rate to 0.00001.</p>

<div class="sourceCode" id="cb2078"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2078-1" data-line-number="1"><span class="kw">system.time</span>((<span class="dt">retrained_model =</span> <span class="kw">my.CNN</span>(images, labels, <span class="dt">layers =</span> cnn.layers,</a>
<a class="sourceLine" id="cb2078-2" data-line-number="2">              <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>,  <span class="dt">minibatch=</span>minibatch, <span class="dt">epoch=</span><span class="dv">10</span>, <span class="dt">eta=</span><span class="fl">0.00001</span>, </a>
<a class="sourceLine" id="cb2078-3" data-line-number="3">              <span class="dt">transfer=</span><span class="st">&quot;transfer_model.rds&quot;</span>)))</a></code></pre></div>

<p>After a few more epochs during the training, we again try to execute our prediction. Note that because training takes much longer, we are now using the saved model instead, namely <strong>retrained.cnn.model</strong>, as shown:</p>

<div class="sourceCode" id="cb2079"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2079-1" data-line-number="1">retrained.cnn.model =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;retrained_model.rds&quot;</span>) </a>
<a class="sourceLine" id="cb2079-2" data-line-number="2">result =<span class="st"> </span><span class="kw">my.predict.CNN</span>(test.images, labels, retrained.cnn.model, </a>
<a class="sourceLine" id="cb2079-3" data-line-number="3">                        <span class="dt">first_batch_only=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] &quot;No of batches: 313&quot;
## [1] &quot;batch 10 - loss: 0.037 t: 10, accuracy 1.000 lag time (sec): 5.961&quot;</code></pre>

<p>That gives us a <strong>top-1</strong> test accuracy of 89.49% and a loss of 0.0718042660598904.</p>
<p>Below, we see the actual predictions compared to the target:</p>

<div class="sourceCode" id="cb2081"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2081-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">digits=</span><span class="dv">8</span>)</a>
<a class="sourceLine" id="cb2081-2" data-line-number="2"><span class="kw">round</span>(result<span class="op">$</span>prediction[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,],<span class="dv">2</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,] 0.00    0 0.00 0.00    0 0.00 0.00 0.00    0     1
## [2,] 0.00    0 0.99 0.00    0 0.00 0.01 0.00    0     0
## [3,] 0.00    0 0.00 0.98    0 0.02 0.00 0.00    0     0
## [4,] 0.99    0 0.00 0.00    0 0.00 0.00 0.00    0     0
## [5,] 0.00    0 0.00 0.00    0 0.96 0.00 0.04    0     0</code></pre>
<div class="sourceCode" id="cb2083"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2083-1" data-line-number="1">result<span class="op">$</span>target[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0    0    0    0    0    0    0    0     1
## [2,]    0    0    1    0    0    0    0    0    0     0
## [3,]    0    0    0    1    0    0    0    0    0     0
## [4,]    1    0    0    0    0    0    0    0    0     0
## [5,]    0    0    0    0    0    1    0    0    0     0</code></pre>

</div>
<div id="summary-7" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.17</span> Summary<a href="deeplearning1.html#summary-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Convolutional Neural Network</strong> is a topic that is wide and deep. There are more than a handful of knobs to cover. The interplay of these knobs is essential to get where we want to be in train and test accuracy. Each knob deserves a section for illustration and discussion. However, one section alone does not give justice to each. Our previous discussions perhaps only scratch the surface.</p>
<p>Nevertheless, we leave readers to continue investigating novel tricks and techniques to improve <strong>CNN</strong> and to read about <strong>Data Augmentation</strong> and <strong>Adversarial Sampling</strong>. The former - <strong>Data Augmentation</strong> - allows us to use newly added datasets for further training based on existing datasets that are synthetically altered (e.g., image transformation). Given an insufficient dataset, the technique is helpful if we need to improve test accuracy. The latter - <strong>Adversarial Sampling</strong> - performs a similar technique as <strong>Augmentation</strong>; however, we need to be wary about intentionally tampered datasets that could mislead training.</p>
<p>It is also worth mentioning that our <strong>CNN</strong> implementation is tailored based on a machine with minimal computing capability. However, advanced frameworks can perform <strong>distributed</strong> (parallel) processing across powerful machines with <strong>GPU</strong> support, achieving speeds down to seconds instead of hours. We leave readers to explore this next.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="machinelearning3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="deeplearning2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
