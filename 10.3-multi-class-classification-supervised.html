<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10.3 Multi-class Classification (Supervised)  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="10.3 Multi-class Classification (Supervised)  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10.3 Multi-class Classification (Supervised)  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="10.2-binary-classification-supervised.html"/>
<link rel="next" href="11-machinelearning3.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to System of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multi-class-classification-supervised" class="section level2 hasAnchor">
<h2><span class="header-section-number">10.3</span> Multi-class Classification (Supervised) <a href="10.3-multi-class-classification-supervised.html#multi-class-classification-supervised" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Most of our discussions are around <strong>binary</strong> classifications in previous sections. Our goal now is to demonstrate <strong>multivariate</strong> classification. Here, we revisit <strong>Decision Trees</strong> and review <strong>goodness of split</strong> based on the purity (or impurity) of nodes. Our goal is to group observations of variables into <strong>homogeneous</strong> instances (or nodes) which have low degrees of impurity.</p>
<p>A starting point is to discuss <strong>Naive Bayes</strong> as a simple classifier.</p>
<div id="bayesian-classification" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.1</span> Bayesian Classification <a href="10.3-multi-class-classification-supervised.html#bayesian-classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us start the discussion of <strong>multi-classification</strong> with <strong>Naive Bayes</strong> as a simple classifier. Recall the Bayes formula derived from Chapter <strong>7</strong> (<strong>Bayesian Computation I</strong>):</p>
<p><span class="math display" id="eq:equate1120130">\[\begin{align}
P(\text{Class|Features}) =
\frac{P(\text{Features|Class}) P(\text{Class}) }{P(\text{Features}) }
\ \ \rightarrow \ \ 
P(Y|X) =  \frac{P(X|Y)P(Y)}{P(X)} \tag{10.146} 
\end{align}\]</span></p>
<p>We can then expand the formula to accommodate a case in which our posterior distribution is based on the condition of multiple distributions.</p>
<p><span class="math display" id="eq:equate1120131">\[\begin{align}
P(y_k|x_{1}, x_{2}, x_{3},...,x_{n}) = 
\frac{\left[\prod_{i=1}^{n}P(x_{i}|y_k)\right]\cdot P(y_k)}
{\prod_{i=1}^{n}P(x_{i})} \tag{10.147} 
\end{align}\]</span></p>
<p>Expanding the <strong>likelihood</strong> and <strong>prior</strong>, we get the following:</p>
<p><span class="math display" id="eq:equate1120132">\[\begin{align}
\underbrace{P(x_i = j|y_i = k) = \frac{\sum_i^N\mathbf{I}(x_i=j, y_i = k)}{\sum_i^N\mathbf{I}(y_i = k) }}_{\text{likelihood}}
\ \ \ \ \ \ \ \ \ \ \ \ \
\underbrace{P(y_i = k) = \frac{\sum_i^N\mathbf{I}(y_i = k)}{N}}_{\text{prior}} \tag{10.148} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(X\)</span> represents a set of <strong>features</strong> that are continuous or discrete</li>
<li><strong>y</strong> being the class or label</li>
<li><strong>k</strong> is a specific value of a class in Y.</li>
<li><strong>j</strong> is a specific value of a feature in X.</li>
</ul>
<p>For a toy example, let us use the common <strong>frequency (probabilities) table</strong> to illustrate <strong>naive bayesian classification</strong>. Suppose we are looking to form a new basketball team to represent a national basketball competition. To form the team, we need quality players with specific requirements. - the idea is to classify whether a candidate is a <strong>Class A</strong> caliber player, a <strong>Class B</strong> caliber player, and so on, based on the following <strong>features</strong>:</p>
<ul>
<li>SPG - Scores Per Game<br />
</li>
<li>RPG - Rebounds Per Game<br />
</li>
<li>APG - Assists Per Game<br />
</li>
<li>FTPG - Free Throws per Game</li>
</ul>
<p>A qualified candidate is with high SPG, high RPG, or high APG if one gets double-digit statistics, e.g., 20 points per game and ten assists per game. Table <a href="10.3-multi-class-classification-supervised.html#tab:nbaclass">10.2</a> shows 200 <strong>observed</strong> candidates with the qualifications we seek:</p>
<table>
<caption><span id="tab:nbaclass">Table 10.2: </span>National Basketball Competition</caption>
<thead>
<tr class="header">
<th align="right">Player (y)</th>
<th align="right">FTPG (<span class="math inline">\(x_{1}\)</span>)</th>
<th align="right">APG (<span class="math inline">\(x_{2}\)</span>)</th>
<th align="right">RPG (<span class="math inline">\(x_{3}\)</span>)</th>
<th align="right">SPG (<span class="math inline">\(x_{4}\)</span>)</th>
<th align="right">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Class A</td>
<td align="right">5 (0.50)</td>
<td align="right">9 (0.90)</td>
<td align="right">7 (0.70)</td>
<td align="right">10 (1.00)</td>
<td align="right"><span class="math inline">\(\mathbf{10}\)</span> (0.05)</td>
</tr>
<tr class="even">
<td align="right">Class B</td>
<td align="right">10 (0.20)</td>
<td align="right">40 (0.80)</td>
<td align="right">25 (0.50)</td>
<td align="right">25 (0.50)</td>
<td align="right"><span class="math inline">\(\mathbf{50}\)</span> (0.25)</td>
</tr>
<tr class="odd">
<td align="right">Class C</td>
<td align="right">28 (0.20)</td>
<td align="right">70 (0.50)</td>
<td align="right">84 (0.60)</td>
<td align="right">14 (0.01)</td>
<td align="right"><span class="math inline">\(\mathbf{140}\)</span> (0.70)</td>
</tr>
<tr class="even">
<td align="right">Total</td>
<td align="right">43 (0.22)</td>
<td align="right">119 (0.60)</td>
<td align="right">116 (0.58)</td>
<td align="right">49 (0.24)</td>
<td align="right"><span class="math inline">\(\mathbf{200}\)</span> (1.00)</td>
</tr>
</tbody>
</table>
<p>Of the overall 200 candidates in Table <a href="10.3-multi-class-classification-supervised.html#tab:nbaclass">10.2</a>, we see only 5% are <strong>Class A</strong> caliber players. Of the 10 <strong>Class A</strong> caliber players, only 5 achieve high <strong>FTPG</strong>.</p>
<p>With reference to Table <a href="10.3-multi-class-classification-supervised.html#tab:nbaclass">10.2</a>, we can formulate the probabilities. It can be shown that the <strong>probability</strong> of getting players with high free throws per game (<strong>FTPG</strong>) is 22% and that the probability of players with high scores per game (<strong>SPG</strong>) is 24%. They are expressed accordingly - they form the marginal probability, namely <span class="math inline">\(P(X) \equiv P(Features)\)</span>:</p>
<p><span class="math display">\[
P(x_1 = FTPG) = \frac{43}{200} = 22\%\ \ \ \ \ \ \ \ \ \ \ \ \
P(x_4 = SPG) = \frac{49}{200} = 24\%
\]</span></p>
<p>We can also show that the <strong>likelihood</strong> of a high <strong>FTPG</strong> given <strong>Class A</strong> caliber player is 50% and that the <strong>likelihood</strong> of a high <strong>SP</strong> given <strong>Class A</strong> caliber player is 100%. They are also expressed accordingly - and they form the <strong>likelihood</strong>, namely <span class="math inline">\(P(x|y) \equiv P(Features|\text{Class})\)</span>:</p>
<p><span class="math display">\[\begin{align*}
P(x_1 = FTPG | y = \text{Class A}) = \frac{5}{10} = 50\% \\
P(x_4 = SPG | y = \text{Class A}) = \frac{10}{10} = 100\%
\end{align*}\]</span></p>
<p>Now the probability of getting a <strong>Class A</strong> player given all the <strong>features</strong> is expressed as such:</p>
<p><span class="math display" id="eq:equate1120133">\[\begin{align}
P(\text{Class A}&amp;|FTPG, APG, RPG, SPG)  \nonumber \\ 
&amp;=\frac{P(FTPG|A)\ P(APG|A)\ P(RPG|A) \ P(SPG|A)\ P(A)}
{P(FTPG)\ P(APG)\ P(RPG)\ P(SPG)}  \tag{10.149} \\
&amp;= \frac{0.50\times 0.90\times 0.70\times 1.00\times 0.05}{0.22\times 0.60\times 0.58\times 0.24} \nonumber \\
&amp;= 0.8571708 \nonumber
\end{align}\]</span></p>
<p>If we are looking for <strong>Class B</strong> players, we will be estimating it this way:</p>
<p><span class="math display" id="eq:equate1120134">\[\begin{align}
P(\text{Class B}&amp;|FTPG, APG, RPG, SPG)  \nonumber \\ 
&amp;=\frac{P(FTPG|B)\ P(APG|B)\ P(RPG|B)\ P(SPG|B)\ P(B)}
{P(FTPG)\ P(APG)\ P(RPG)\ P(SPG)}  \tag{10.150} \\
&amp;= \frac{0.20 \times 0.80\times 0.50\times 0.50\times 0.25}{0.22\times 0.60\times 0.58\times 0.24} \nonumber \\
&amp;= 0.5442355 \nonumber
\end{align}\]</span></p>
<p>If we are looking for <strong>Class C</strong> players, we will be estimating it this way:</p>
<p><span class="math display" id="eq:equate1120135">\[\begin{align}
P(\text{Class C}&amp;|FTPG, APG, RPG, SPG)   \nonumber \\ 
&amp;=\frac{P(FTPG|C)\ P(APG|C)\ P(RPG|C)\ P(SPG|C)\ P(C)}
{P(FTPG)\ P(APG)\ P(RPG)\ P(SPG)}  \tag{10.151} \\
&amp;= \frac{0.20\times 0.50\times 0.60\times 0.01\times 0.70}{0.22\times 0.60\times 0.58\times 0.24} \nonumber \\
&amp;= 0.02285789 \nonumber
\end{align}\]</span></p>
<p>Therefore, it can be shown that with the 200 available candidates, we have about 85.72% probability of getting a <strong>Class A</strong> caliber player given the presented statistics, about 54.42% probability of getting a <strong>Class B</strong> caliber player, and about 2.29% probability of getting a <strong>Class C</strong> caliber player.</p>
<p>Now, assume a training set claims to have the following probabilities for high <strong>FTPG</strong>, high <strong>APG</strong>, and high <strong>RPG</strong>, but zero probability for having low in <strong>SPG</strong>.</p>

<p><span class="math display">\[
\begin{array}{rl}
P(x_1 = FTPG | \text{A}) = 0.50\\
P(x_1 = APG | \text{A}) =  0.90\\
P(x_1 = RPG | \text{A}) =  0.70\\
P(x_1 = SPG | \text{A}) =  0.00\\
\end{array}\ \ 
\begin{array}{rl}
P(x_1 = FTPG | \text{B}) = 0.20\\
P(x_1 = APG | \text{B}) =  0.80\\
P(x_1 = RPG | \text{B}) =  0.50\\
P(x_1 = SPG | \text{B}) =  0.00\\
\end{array}\ \ 
\begin{array}{rl}
P(x_1 = FTPG | \text{C}) = 0.20\\
P(x_1 = APG | \text{C}) =  0.50\\
P(x_1 = RPG | \text{C}) =  0.60\\
P(x_1 = SPG | \text{C}) =  0.00\\
\end{array}
\]</span>
</p>
<p>If even one of the high statistics per game has zero frequency, then the probability of predicting such a class is always zero. It is a disadvantage for <strong>Bayes Classification</strong>. Here, we use <strong>Laplacian Smoothing</strong> as a solution to correct such a disadvantage. </p>
<p>Basically, for <strong>Naive Bayes</strong> which has the form <span class="math inline">\(P(y_i = k|x_i) \propto P(x_i|y_i = k)P(y_i = k)\)</span>, we add <span class="math inline">\(\lambda\)</span> like so:</p>
<p><span class="math display" id="eq:equate1120136">\[\begin{align}
\underbrace{P(x_i = j|y_i = k) = \frac{\sum_i^N\mathbf{I}(x_i=j, y_i = k)  + \lambda}{\sum_i^N\mathbf{I}(y_i = k) + J\lambda}}_{\text{likelihood}}
\ \ \ \ \
\underbrace{P(y_i = k) = \frac{\sum_i^N\mathbf{I}(y_i = k) + \lambda}{N + K\lambda}}_{\text{prior}} \tag{10.152} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>J</strong> is the number of categorical values for <span class="math inline">\(x_i\)</span>. Here, we only have two (low or high).</li>
<li><strong>K</strong> is the number of categorical values for <span class="math inline">\(y_i\)</span>. Here, we have three (A, B, C).</li>
</ul>
<p>So that if <span class="math inline">\(\lambda = 1\)</span> (arbitrarily choose a small number)</p>
<p><span class="math display">\[
\begin{array}{lrr}
P(x_1 = FTPG | \text{A}) &amp;= \frac{5 + 1}{10 + 2\times1} = 0.50\\
P(x_1 = APG | \text{A}) &amp;= \frac{9 + 1}{10 + 2\times1}  = 0.83\\
P(x_1 = RPG | \text{A}) &amp;= \frac{7 + 1}{10 + 2\times1}  = 0.67\\
P(x_1 = SPG | \text{A}) &amp;= \frac{0 + 1}{10 + 2\times1}  = 0.08\\
\end{array}\ \ \ \ \ \ 
\begin{array}{lrr}
P(y_k = \text{A}) = \frac{10 + 1}{200 + 3\times1} = 0.0541872\\
P(y_k = \text{B}) = \frac{50 + 1}{200 + 3\times1}  = 0.2512315\\
P(y_k = \text{C}) = \frac{140 + 1}{200 + 3\times1}  = 0.6945813\\
\end{array}
\]</span>
Notice now that we have a small probability assigned to <span class="math inline">\(P(x_1 = SPG | \text{A})\)</span>. We leave readers to compute the other probabilities.</p>
</div>
<div id="classification-trees" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.2</span> Classification Trees <a href="10.3-multi-class-classification-supervised.html#classification-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, it helps to refresh our understanding of <strong>Decision Trees</strong>. Recall in <strong>Regression trees</strong> the use of <strong>SSE</strong> as basis for our <strong>loss function</strong> to determine <strong>goodness of fit</strong>. We now extend the concept by using other <strong>splitting criteria</strong> based from <strong>probability theory</strong> and <strong>information theory</strong>. In the context of <strong>Classification Trees</strong> when dealing with <strong>categorical</strong> target variables, we consider basic metrics such <strong>Entropy</strong>, <strong>Gini Index</strong>, <strong>Gini Impurity</strong>, and <strong>Information Gain</strong> for our <strong>loss (and gain) function</strong>. In the same context, we use <strong>classification error rate</strong> to validate performance.     </p>
<p><strong>CHAID, ID3, C4.5, C5.0</strong></p>
<p>Apart from <strong>CART</strong> developed in 1984, other decision tree algorithms are available for review. Three such algorithms were introduced by <strong>Ross Quinlan</strong> <span class="citation">(<a href="bibliography.html#ref-ref559jr">1986</a>, <a href="bibliography.html#ref-ref563jr">1996</a>)</span>. The algorithms differ in addressing the following core criteria and strategy, among other considerations: <strong>splitting criterion</strong>, <strong>stopping criterion</strong>, and <strong>pruning strategy</strong>.</p>
<p>Note that <strong>splitting criterion</strong> is based on measures of node impurity. Also, note in the <strong>Decision Tree</strong> section that we put a discussion of <strong>CART</strong> in the context of <strong>Regression</strong> and <strong>bifurcation</strong> (binary split); however, in the context of <strong>Classification</strong>, the implementation of <strong>Cart</strong> supports multiway split along with the use of <strong>Gini Index (also called Gini Impurity)</strong> to determine the split.</p>
<p>Here, instead of detailing classic classification tree algorithms, let us briefly introduce them <span class="citation">(Hssina B. et al. <a href="bibliography.html#ref-ref580h">2014</a>)</span>:</p>
<ul>
<li><p><strong>CHAID</strong> is called <strong>Chi-square automatic interaction detector</strong> and was introduced by Gordon Kass in 1980. The algorithm uses <strong>Chi-square</strong> as <strong>splitting criterion</strong>. </p></li>
<li><p><strong>ID3</strong> is called <strong>Iterative Dichotomiser 3</strong> introduced by R. Quinlan in 1983-86. The algorithm uses <strong>Information gain</strong> as the <strong>splitting criterion</strong> and supports <strong>multiway</strong> split. </p></li>
<li><p><strong>C4.5</strong> is a successor of <strong>ID3</strong> introduced by R. Quinlan in 1993. The algorithm uses <strong>Gain Ratio</strong> as the <strong>splitting criterion</strong>. It enforces a rule-based algorithm that supports continuous and categorical attributes, missing data, post-pruning, and <strong>multiway</strong> split.  </p></li>
<li><p><strong>C5.0</strong> is a successor of <strong>C4.5</strong> introduced by R. Quinlan R. in 1994. The algorithm uses <strong>Information Gain</strong> as the <strong>splitting criterion</strong>. It comes with improved speed and memory efficiency, among other enhancements. It supports discrete, continuous, date and time, timestamp, and categorical attributes.  </p></li>
</ul>
<p>In our discussions ahead, we illustrate the use of <strong>Gini Index</strong> and <strong>Information Gain</strong> as we build our classification tree.</p>
<p>Recall under the <strong>Regression Trees</strong> Subsection under <strong>Regression</strong> Section the use of <strong>CART</strong> to perform tree regression. Here, we use the same method with the intent to perform tree classification. A simple tree classification using <strong>CART</strong> is shown in Figure <a href="10.3-multi-class-classification-supervised.html#fig:classificationtree1">10.30</a> using an R dataset called <strong>iris</strong>.</p>
<div class="sourceCode" id="cb1485"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1485-1" data-line-number="1">pacman<span class="op">::</span><span class="kw">p_load</span>(rpart,rpart.plot)</a>
<a class="sourceLine" id="cb1485-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1485-3" data-line-number="3"><span class="kw">data</span>(iris)</a>
<a class="sourceLine" id="cb1485-4" data-line-number="4">rpart.control =<span class="st"> </span><span class="kw">rpart.control</span>(<span class="dt">minsplit=</span><span class="dv">2</span>, <span class="dt">maxdepth=</span><span class="dv">15</span>, <span class="dt">minbucket =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1485-5" data-line-number="5">tree.model =<span class="st"> </span><span class="kw">rpart</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> iris, <span class="dt">control =</span> rpart.control)</a>
<a class="sourceLine" id="cb1485-6" data-line-number="6"><span class="kw">rpart.plot</span>(tree.model)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:classificationtree1"></span>
<img src="DS_files/figure-html/classificationtree1-1.png" alt="Classification Tree" width="70%" />
<p class="caption">
Figure 10.30: Classification Tree
</p>
</div>
<p>To illustrate tree classification, we start by building a tree and splitting tree nodes.</p>
<p><strong>First</strong>, let us view the structure of our dataset, namely <strong>iris</strong>. Here, we have three independent variables, namely <strong>Sepal.Length</strong>, <strong>Sepal.Width</strong>, <strong>Petal.Length</strong>, and <strong>Petal.Width</strong>. Our target (or dependent) variable is <strong>Species</strong>. See below:</p>

<div class="sourceCode" id="cb1486"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1486-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb1486-2" data-line-number="2"><span class="kw">str</span>(iris, <span class="dt">width=</span><span class="dv">56</span>, <span class="dt">strict.width=</span><span class="st">&quot;wrap&quot;</span>)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  5 variables:
## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9
##    ...
## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9
##    3.1 ...
## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4
##    1.5 ...
## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2
##    0.1 ...
## $ Species : Factor w/ 3 levels
##    &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>

<p><strong>Second</strong>, we also use Figure <a href="10.1-regression.html#fig:regressiontree">10.2</a> similar to <strong>Regression Trees</strong> under <strong>Regression</strong> Section. Note that our target variable is <strong>categorical</strong> in this case. Also, to illustrate, let us choose one of the features, namely <strong>Sepal.Length</strong> and arbitrarily split the observations perhaps somewhere in the middle (say at index 70).</p>

<div class="sourceCode" id="cb1488"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1488-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1488-2" data-line-number="2">datacars =<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb1488-3" data-line-number="3">sepal.length =<span class="st"> </span>datacars[[<span class="st">&quot;Sepal.Length&quot;</span>]]</a>
<a class="sourceLine" id="cb1488-4" data-line-number="4">(<span class="dt">sorted.input =</span> <span class="kw">sort</span>(sepal.length, <span class="dt">index.return =</span> <span class="ot">TRUE</span>))<span class="op">$</span>x</a></code></pre></div>
<pre><code>##   [1] 4.3 4.4 4.4 4.4 4.5 4.6 4.6 4.6 4.6 4.7 4.7 4.8 4.8 4.8 4.8 4.8
##  [17] 4.9 4.9 4.9 4.9 4.9 4.9 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0
##  [33] 5.1 5.1 5.1 5.1 5.1 5.1 5.1 5.1 5.1 5.2 5.2 5.2 5.2 5.3 5.4 5.4
##  [49] 5.4 5.4 5.4 5.4 5.5 5.5 5.5 5.5 5.5 5.5 5.5 5.6 5.6 5.6 5.6 5.6
##  [65] 5.6 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.8 5.8 5.8 5.8 5.8 5.8 5.8
##  [81] 5.9 5.9 5.9 6.0 6.0 6.0 6.0 6.0 6.0 6.1 6.1 6.1 6.1 6.1 6.1 6.2
##  [97] 6.2 6.2 6.2 6.3 6.3 6.3 6.3 6.3 6.3 6.3 6.3 6.3 6.4 6.4 6.4 6.4
## [113] 6.4 6.4 6.4 6.5 6.5 6.5 6.5 6.5 6.6 6.6 6.7 6.7 6.7 6.7 6.7 6.7
## [129] 6.7 6.7 6.8 6.8 6.8 6.9 6.9 6.9 6.9 7.0 7.1 7.2 7.2 7.2 7.3 7.4
## [145] 7.6 7.7 7.7 7.7 7.7 7.9</code></pre>
<div class="sourceCode" id="cb1490"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1490-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1490-2" data-line-number="2">split.input &lt;-<span class="st"> </span><span class="cf">function</span>(sorted.input, idx, <span class="dt">is.factor =</span> <span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb1490-3" data-line-number="3">  n =<span class="st"> </span><span class="kw">length</span>(sorted.input<span class="op">$</span>x)</a>
<a class="sourceLine" id="cb1490-4" data-line-number="4">  left.indices  =<span class="st">  </span>sorted.input<span class="op">$</span>ix[<span class="dv">1</span><span class="op">:</span>idx]</a>
<a class="sourceLine" id="cb1490-5" data-line-number="5">  right.indices =<span class="st">  </span>sorted.input<span class="op">$</span>ix[(idx<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n]</a>
<a class="sourceLine" id="cb1490-6" data-line-number="6">  <span class="kw">list</span>(<span class="st">&quot;left&quot;</span> =<span class="st"> </span>left.indices, <span class="st">&quot;right&quot;</span> =<span class="st"> </span>right.indices )</a>
<a class="sourceLine" id="cb1490-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb1490-8" data-line-number="8">split.index =<span class="st"> </span><span class="dv">70</span></a>
<a class="sourceLine" id="cb1490-9" data-line-number="9">(<span class="dt">split =</span> <span class="kw">split.input</span>(sorted.input, split.index ))</a></code></pre></div>
<pre><code>## $left
##  [1]  14   9  39  43  42   4   7  23  48   3  30  12  13  25  31  46
## [17]   2  10  35  38  58 107   5   8  26  27  36  41  44  50  61  94
## [33]   1  18  20  22  24  40  45  47  99  28  29  33  60  49   6  11
## [49]  17  21  32  85  34  37  54  81  82  90  91  65  67  70  89  95
## [65] 122  16  19  56  80  96
## 
## $right
##  [1]  97 100 114  15  68  83  93 102 115 143  62  71 150  63  79  84
## [17]  86 120 139  64  72  74  92 128 135  69  98 127 149  57  73  88
## [33] 101 104 124 134 137 147  52  75 112 116 129 133 138  55 105 111
## [49] 117 148  59  76  66  78  87 109 125 141 145 146  77 113 144  53
## [65] 121 140 142  51 103 110 126 130 108 131 106 118 119 123 136 132</code></pre>

<p>The left split has a total of 70 observations, and the right split has a total of 80 observations.</p>
<p><strong>Third</strong>, we determine the number of classes in our target variable. Below, we have three classes in the <strong>Species</strong> target variable, namely <strong>Setosa</strong>, <strong>Versicolor</strong>, and <strong>Virginica</strong>.</p>
<div class="sourceCode" id="cb1492"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1492-1" data-line-number="1"><span class="kw">levels</span>(iris<span class="op">$</span>Species) </a></code></pre></div>
<pre><code>## [1] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;</code></pre>
<p><strong>Fourth</strong>, we then determine the distribution of the classes between the two splits.</p>

<div class="sourceCode" id="cb1494"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1494-1" data-line-number="1"><span class="kw">library</span>(plyr)</a>
<a class="sourceLine" id="cb1494-2" data-line-number="2">left =<span class="st"> </span>iris[split<span class="op">$</span>left,]</a>
<a class="sourceLine" id="cb1494-3" data-line-number="3">right =<span class="st"> </span>iris[split<span class="op">$</span>right,]</a>
<a class="sourceLine" id="cb1494-4" data-line-number="4">iris.dist =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">count</span>(left, <span class="st">&#39;Species&#39;</span>), <span class="kw">count</span>(right, <span class="st">&#39;Species&#39;</span>),</a>
<a class="sourceLine" id="cb1494-5" data-line-number="5">            <span class="kw">count</span>(iris, <span class="st">&#39;Species&#39;</span>))[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>)]</a>
<a class="sourceLine" id="cb1494-6" data-line-number="6"><span class="kw">colnames</span>(iris.dist) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Class&quot;</span>, <span class="st">&quot;Left Count&quot;</span>, <span class="st">&quot;Right Count&quot;</span>, <span class="st">&quot;Total Count&quot;</span>)</a>
<a class="sourceLine" id="cb1494-7" data-line-number="7">iris.dist</a></code></pre></div>
<pre><code>##        Class Left Count Right Count Total Count
## 1     setosa         49           1          50
## 2 versicolor         19          31          50
## 3  virginica          2          48          50</code></pre>

<p><strong>Fifth</strong>, to know if the split is good, we need to use specific metrics. For Regression, we use <strong>SSE</strong>. For classification, let us illustrate the use of <strong>Gini Index (GI)</strong> and <strong>Gini Information Gain (IG)</strong>.</p>
<p><span class="math display" id="eq:equate1120138" id="eq:equate1120137">\[\begin{align}
GI_{\text{(gini index)}} &amp;=\sum_{k=1}^K P_k \times ( 1 - P_k) &amp;\ \ \ \ \ \text{where K = number of classes}   \tag{10.153} \\
&amp;=1 - \sum_{k=1}^K\left[P(Y=y_k)\right]^2   \tag{10.154} 
\end{align}\]</span></p>
<p>For a perfect split, we have the following <strong>Gini Index</strong>:</p>
<p><span class="math display">\[
\begin{array}{ll}
GI_{(perfect)} &amp;= 1 - \left[ P(Y=\text{setosa})^2 + P(Y=\text{versicola})^2 + P(Y=\text{virginica})^2\right] \\
&amp;= 1 - \left[ \left(\frac{50}{150}\right)^2 + \left(\frac{50}{150}\right)^2 + \left(\frac{50}{150}\right)^2\right]\\
&amp;= 0.6666667
\end{array}
\]</span></p>
<p>For each split in our example above, we have the following <strong>Gini Index</strong>:</p>
<p><span class="math display" id="eq:equate1120139">\[\begin{align}
GI_{(left)} &amp;= 1 - [ P(Y=\text{setosa}|X_{(left)})^2 + \nonumber\\
&amp;\ \ \ \ P(Y=\text{versicola}|X_{(left)})^2 + P(Y=\text{virginica}|X_{(left)})^2 ]  \tag{10.155} \\
&amp;= 1 - \left[ \left(\frac{49}{70}\right)^2 + \left(\frac{19}{70}\right)^2 + \left(\frac{2}{70}\right)^2\right] \nonumber \\
&amp;= 0.4355102 \nonumber
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1120140">\[\begin{align}
GI_{(right)} &amp;= 1 - [ P(Y=\text{setosa}|X_{(right)})^2 + P(Y=\text{versicola}|X_{(right)})^2 + \nonumber \\
&amp;\ \ \ \ P(Y=\text{virginica}|X_{(right)})^2 ]  \tag{10.156} \\
&amp;= 1 - \left[ \left(\frac{1}{80}\right)^2 + \left(\frac{31}{80}\right)^2 + \left(\frac{48}{80}\right)^2\right] \nonumber\\
&amp;= 0.4896875 \nonumber
\end{align}\]</span></p>
<p>To now get the <strong>goodness of split</strong>, we use the following formula: </p>
<p><span class="math display" id="eq:equate1120141">\[\begin{align}
IG_{(gini)} = GI_{(perfect)} - \left[ P(left) \times GI_{(left)} + P(right) \times GI_{(right)}  \right] \tag{10.157} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(P(left)\)</span> is the <strong>weight of impurity of the left split</strong> and <span class="math inline">\(P(right)\)</span> is the <strong>weight of impurity of the right split</strong>.</p>
<p><span class="math display">\[
\begin{array}{lll}
P(left) = \frac{49 + 19 + 2}{150} = \frac{70}{150} &amp;\ \ \ \ \ &amp;P(left) = \frac{1 + 31 + 48}{150} = \frac{80}{150}
\end{array} 
\]</span></p>
<p>Therefore, our <strong>Gini Gain</strong> which calculates the amount of <strong>impurity</strong> removed is:</p>
<p><span class="math display">\[
\begin{array}{ll}
IG_{(gini)} &amp;= 0.6666667 - \left[ \frac{70}{150}\times(0.4355102) + \frac{80}{150} \times (0.4896875) \right]\\
&amp;= 0.2022619
\end{array}
\]</span></p>
<p>Below, we have our example implementation of <strong>Gini Gain</strong>:</p>

<div class="sourceCode" id="cb1496"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1496-1" data-line-number="1">gini =<span class="st"> </span>gini.index &lt;-<span class="st"> </span><span class="cf">function</span>(y, lev) {</a>
<a class="sourceLine" id="cb1496-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1496-3" data-line-number="3">  gi =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1496-4" data-line-number="4">  <span class="cf">for</span> (k <span class="cf">in</span> lev) {</a>
<a class="sourceLine" id="cb1496-5" data-line-number="5">    cl.n   =<span class="st"> </span><span class="kw">sum</span>(y <span class="op">==</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1496-6" data-line-number="6">    gi =<span class="st"> </span>gi <span class="op">+</span><span class="st"> </span>(cl.n <span class="op">/</span><span class="st"> </span>n)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1496-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb1496-8" data-line-number="8">  <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>gi</a>
<a class="sourceLine" id="cb1496-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb1496-10" data-line-number="10">gain =<span class="st"> </span>gini.gain &lt;-<span class="st"> </span><span class="cf">function</span>(parent, left, right, lev) {</a>
<a class="sourceLine" id="cb1496-11" data-line-number="11">  left.n =<span class="st"> </span><span class="kw">length</span>(left)</a>
<a class="sourceLine" id="cb1496-12" data-line-number="12">  right.n =<span class="st"> </span><span class="kw">length</span>(right)</a>
<a class="sourceLine" id="cb1496-13" data-line-number="13">  total.n =<span class="st"> </span>left.n <span class="op">+</span><span class="st"> </span>right.n</a>
<a class="sourceLine" id="cb1496-14" data-line-number="14">  p.left =<span class="st"> </span>left.n <span class="op">/</span><span class="st"> </span>total.n</a>
<a class="sourceLine" id="cb1496-15" data-line-number="15">  p.right =<span class="st"> </span>right.n <span class="op">/</span><span class="st"> </span>total.n</a>
<a class="sourceLine" id="cb1496-16" data-line-number="16">  <span class="kw">gini</span>(parent, lev) <span class="op">-</span><span class="st"> </span>(p.left <span class="op">*</span><span class="st"> </span><span class="kw">gini</span>(left, lev)  <span class="op">+</span><span class="st"> </span>p.right <span class="op">*</span><span class="st"> </span><span class="kw">gini</span>(right, lev))</a>
<a class="sourceLine" id="cb1496-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb1496-18" data-line-number="18">y =<span class="st"> </span>iris<span class="op">$</span>Species</a>
<a class="sourceLine" id="cb1496-19" data-line-number="19">lev =<span class="st"> </span><span class="kw">levels</span>(y)</a>
<a class="sourceLine" id="cb1496-20" data-line-number="20"><span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Gini Gain: &quot;</span>, <span class="kw">gain</span> (y, y[split<span class="op">$</span>left], y[split<span class="op">$</span>right], lev)))</a></code></pre></div>
<pre><code>## [1] &quot;Gini Gain: 0.202261904761905&quot;</code></pre>

<p><strong>Sixth</strong>, alternatively, we can also use <strong>Entropy (H)</strong> and <strong>Entropy Information Gain (IG)</strong> to measure the <strong>goodness of fit</strong>.</p>
<p><span class="math display" id="eq:equate1120142">\[\begin{align}
H_{\text{(entropy)}} = - \sum_{i=1}^K P_k \times\ \log_e(P_k) \tag{10.158} 
\end{align}\]</span></p>
<p>For a perfect split, we have the following <strong>Entropy</strong>:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{rl}
H{\text{(perfect)}} &amp;= - \left[
\begin{array}{ll}
P(Y = setosa) \times\ \log_e P(Y = setosa) +\\
P(Y = versicola) \times\ \log_e P(Y = versicola) +\\
P(Y = virginica) \times\ \log_e P(Y = virginica)
\end{array}
\right]\\
\\
&amp;= - \left[
\frac{50}{150} \log_e \frac{50}{150} +
\frac{50}{150} \log_e \frac{50}{150}+
\frac{50}{150} \log_e \frac{50}{150}
\right]\\
&amp;= - \left[-1.098612\right] = 1.098612
\end{array}
\end{align*}\]</span></p>
<p>For each split in our example above, we have the following <strong>Entropy</strong>:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{rl}
H{\text{(left)}} &amp;= - \left[
\begin{array}{ll}
P(Y = setosa|X_{(left)}) \times\ \log_e P(Y = setosa|X_{(left)}) +\\
P(Y = versicola|X_{(left)}) \times\ \log_e P(Y = versicola|X_{(left)}) +\\
P(Y = virginica|X_{(left)}) \times\ \log_e P(Y = virginica|X_{(left)})
\end{array}
\right]\\
\\
&amp;= - \left[
\frac{49}{70} \log_e \frac{49}{70} +
\frac{19}{70} \log_e \frac{19}{70}+
\frac{2}{70} \log_e \frac{2}{70}
\right]\\
&amp;= -\left[-0.705212\right] = 0.705212
\\
\\
H{\text{(right)}} &amp;= - \left[
\begin{array}{ll}
P(Y = setosa|X_{(right)}) \times\ \log_e P(Y = setosa|X_{(right)}) +\\
P(Y = versicola|X_{(right)}) \times\ \log_e P(Y = versicola|X_{(right)}) +\\
P(Y = virginica|X_{(right)}) \times\ \log_e P(Y = virginica|X_{(right)})
\end{array}
\right]\\
\\
&amp;= - \left[
\frac{1}{80} \log_e \frac{1}{80} +
\frac{31}{80} \log_e \frac{31}{80}+
\frac{48}{80} \log_e \frac{48}{80}
\right]\\
&amp;= -\left[-0.728636\right] = 0.728636
\end{array}
\end{align*}\]</span></p>
<p>To now get the <strong>goodness of split</strong>, we use the following formula:</p>
<p><span class="math display" id="eq:equate1120143">\[\begin{align}
IG_{(entropy)} &amp;= H_{(perfect)} - \left[ P(left) \times H_{(left)} + P(right) \times H_{(right)}  \right] \tag{10.159} \\
&amp;= 1.098612 - \left[ \frac{70}{150} (0.705212) + \frac{80}{150} (0.728636) \right] \nonumber \\
&amp;= 0.3809072 \nonumber
\end{align}\]</span></p>
<p>Below, we have our example implementation of <strong>Entropy Gain</strong>:</p>

<div class="sourceCode" id="cb1498"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1498-1" data-line-number="1">entropy &lt;-<span class="st"> </span><span class="cf">function</span>(y, lev) {</a>
<a class="sourceLine" id="cb1498-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1498-3" data-line-number="3">  e =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1498-4" data-line-number="4">  <span class="cf">for</span> (k <span class="cf">in</span> lev) {</a>
<a class="sourceLine" id="cb1498-5" data-line-number="5">    cl.n   =<span class="st"> </span><span class="kw">sum</span>(y <span class="op">==</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1498-6" data-line-number="6">    p =<span class="st"> </span>cl.n <span class="op">/</span><span class="st"> </span>n</a>
<a class="sourceLine" id="cb1498-7" data-line-number="7">    e =<span class="st"> </span>e <span class="op">+</span><span class="st"> </span>p <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(p, <span class="kw">exp</span>(<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1498-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb1498-9" data-line-number="9">  <span class="op">-</span>e</a>
<a class="sourceLine" id="cb1498-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb1498-11" data-line-number="11">entropy.gain &lt;-<span class="st"> </span><span class="cf">function</span>(parent, left, right, lev) {</a>
<a class="sourceLine" id="cb1498-12" data-line-number="12">  left.n =<span class="st"> </span><span class="kw">length</span>(left)</a>
<a class="sourceLine" id="cb1498-13" data-line-number="13">  right.n =<span class="st"> </span><span class="kw">length</span>(right)</a>
<a class="sourceLine" id="cb1498-14" data-line-number="14">  total.n =<span class="st"> </span>left.n <span class="op">+</span><span class="st"> </span>right.n</a>
<a class="sourceLine" id="cb1498-15" data-line-number="15">  p.left =<span class="st"> </span>left.n <span class="op">/</span><span class="st"> </span>total.n</a>
<a class="sourceLine" id="cb1498-16" data-line-number="16">  p.right =<span class="st"> </span>right.n <span class="op">/</span><span class="st"> </span>total.n</a>
<a class="sourceLine" id="cb1498-17" data-line-number="17">  <span class="kw">entropy</span>(parent, lev) <span class="op">-</span><span class="st"> </span>(p.left <span class="op">*</span><span class="st"> </span><span class="kw">entropy</span>(left, lev)  <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1498-18" data-line-number="18"><span class="st">                            </span>p.right <span class="op">*</span><span class="st"> </span><span class="kw">entropy</span>(right, lev))</a>
<a class="sourceLine" id="cb1498-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb1498-20" data-line-number="20">y =<span class="st"> </span>iris<span class="op">$</span>Species</a>
<a class="sourceLine" id="cb1498-21" data-line-number="21">lev =<span class="st"> </span><span class="kw">levels</span>(y)</a>
<a class="sourceLine" id="cb1498-22" data-line-number="22"><span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Entropy Gain: &quot;</span>, </a>
<a class="sourceLine" id="cb1498-23" data-line-number="23">             <span class="kw">entropy.gain</span> (y, y[split<span class="op">$</span>left], y[split<span class="op">$</span>right], lev)))</a></code></pre></div>
<pre><code>## [1] &quot;Entropy Gain: 0.38090751345448&quot;</code></pre>

<p><strong>Seventh</strong>, let us now revisit our implementation of <strong>Regression Trees</strong> and modify a few of the functions we used to build a <strong>Regression Tree</strong>. For intuition of the few functions, reference our detailed discussion in the <strong>Regression Trees</strong> Section.</p>
<p>The first function to modify is the <strong>split.loss(.)</strong> function. The modification replaces the loss function that uses the <strong>SSE</strong> measure with the <strong>Gini Index</strong> and <strong>Gini Gain</strong> measure. Also, the minimum bucket applies not only to the tree node but also to the individual classes so that if only one class has observations greater than <strong>minbucket</strong>, then no further split is needed. The measure of <strong>improvement</strong> is measured using the <strong>Gini Gain</strong> of parents and children like so:   </p>
<p><span class="math display" id="eq:equate1120144">\[\begin{align}
improve = \frac{IG_{(splits)}}{IG_{(parent)}}   \tag{10.160} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb1500"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1500-1" data-line-number="1">class.probs &lt;-<span class="st"> </span><span class="cf">function</span>(y) {</a>
<a class="sourceLine" id="cb1500-2" data-line-number="2">  K       =<span class="st"> </span><span class="kw">length</span>(levs)</a>
<a class="sourceLine" id="cb1500-3" data-line-number="3">  total   =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1500-4" data-line-number="4">  probs   =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, K); i =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1500-5" data-line-number="5">  <span class="cf">for</span> (lev <span class="cf">in</span> levs) { i =<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>; probs[i] =<span class="st"> </span><span class="kw">sum</span>(y <span class="op">==</span><span class="st"> </span>lev) <span class="op">/</span><span class="st"> </span>total } </a>
<a class="sourceLine" id="cb1500-6" data-line-number="6">  probs</a>
<a class="sourceLine" id="cb1500-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb1500-8" data-line-number="8">split.loss &lt;-<span class="st"> </span><span class="cf">function</span>(loss, sort.input, output, left, right, </a>
<a class="sourceLine" id="cb1500-9" data-line-number="9">                       avg, minbucket ) {</a>
<a class="sourceLine" id="cb1500-10" data-line-number="10">  cs          =<span class="st"> </span>loss</a>
<a class="sourceLine" id="cb1500-11" data-line-number="11">  n           =<span class="st"> </span><span class="kw">length</span>(output)</a>
<a class="sourceLine" id="cb1500-12" data-line-number="12">  probs       =<span class="st"> </span><span class="kw">class.probs</span>(output)</a>
<a class="sourceLine" id="cb1500-13" data-line-number="13">  parent.gain =<span class="st"> </span><span class="kw">gini</span>(output, levs)</a>
<a class="sourceLine" id="cb1500-14" data-line-number="14">  <span class="cf">if</span> (<span class="kw">length</span>(left) <span class="op">&gt;=</span><span class="st"> </span>minbucket <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">length</span>(right) <span class="op">&gt;=</span><span class="st"> </span>minbucket ) {</a>
<a class="sourceLine" id="cb1500-15" data-line-number="15">    <span class="co"># no split if only one class has &gt; minbucket</span></a>
<a class="sourceLine" id="cb1500-16" data-line-number="16">    <span class="cf">if</span> (<span class="kw">sum</span>( probs <span class="op">*</span><span class="st"> </span>n <span class="op">&gt;</span><span class="st"> </span>minbucket ) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1500-17" data-line-number="17">      left.idx    =<span class="st"> </span>sort.input<span class="op">$</span>ix[left]</a>
<a class="sourceLine" id="cb1500-18" data-line-number="18">      right.idx   =<span class="st"> </span>sort.input<span class="op">$</span>ix[<span class="op">-</span>left]</a>
<a class="sourceLine" id="cb1500-19" data-line-number="19">      o1 =<span class="st"> </span>output[left.idx]; o2 =<span class="st"> </span>output[right.idx]</a>
<a class="sourceLine" id="cb1500-20" data-line-number="20">      child.probs =<span class="st"> </span><span class="kw">round</span>(probs[<span class="kw">which.max</span>(probs)]<span class="op">*</span><span class="dv">100</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1500-21" data-line-number="21">      child.class =<span class="st"> </span>levs[<span class="kw">which.max</span>(probs)]</a>
<a class="sourceLine" id="cb1500-22" data-line-number="22">      child.gain  =<span class="st"> </span><span class="kw">gain</span>(output, o1, o2, levs)</a>
<a class="sourceLine" id="cb1500-23" data-line-number="23">      child.improve    =<span class="st">  </span>child.gain <span class="op">/</span><span class="st"> </span>parent.gain</a>
<a class="sourceLine" id="cb1500-24" data-line-number="24">      o.len =<span class="st"> </span>n; l.len =<span class="st"> </span><span class="kw">length</span>(o1); r.len =<span class="st"> </span><span class="kw">length</span>(o2)</a>
<a class="sourceLine" id="cb1500-25" data-line-number="25">      cs<span class="op">$</span>split   =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>split, avg)</a>
<a class="sourceLine" id="cb1500-26" data-line-number="26">      cs<span class="op">$</span>Pr      =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>Pr, child.probs)</a>
<a class="sourceLine" id="cb1500-27" data-line-number="27">      cs<span class="op">$</span>class   =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>class, child.class)</a>
<a class="sourceLine" id="cb1500-28" data-line-number="28">      cs<span class="op">$</span>gain    =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>gain, <span class="kw">round</span>(child.gain,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb1500-29" data-line-number="29">      cs<span class="op">$</span>improve =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>improve, <span class="kw">round</span>(child.improve<span class="op">*</span><span class="dv">100</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb1500-30" data-line-number="30">      cs<span class="op">$</span>obs     =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>obs, o.len);  </a>
<a class="sourceLine" id="cb1500-31" data-line-number="31">      cs<span class="op">$</span>l.son   =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>l.son, l.len); cs<span class="op">$</span>r.son   =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>r.son, r.len)</a>
<a class="sourceLine" id="cb1500-32" data-line-number="32">      cs<span class="op">$</span>left.indices[[<span class="kw">as.character</span>(avg)]] =<span class="st"> </span>left.idx</a>
<a class="sourceLine" id="cb1500-33" data-line-number="33">      cs<span class="op">$</span>right.indices[[<span class="kw">as.character</span>(avg)]] =<span class="st"> </span>right.idx</a>
<a class="sourceLine" id="cb1500-34" data-line-number="34">    }</a>
<a class="sourceLine" id="cb1500-35" data-line-number="35">  }</a>
<a class="sourceLine" id="cb1500-36" data-line-number="36">  cs</a>
<a class="sourceLine" id="cb1500-37" data-line-number="37">}</a></code></pre></div>

<p>The second function to modify is our <strong>optimizer</strong> function which references the <strong>minimum.loss(.)</strong> function in <strong>Regression</strong>. Here in <strong>Classification</strong>, we replace the optimizer with <strong>maximum.gain(.)</strong> because our measure is now to maximize the gain.</p>

<div class="sourceCode" id="cb1501"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1501-1" data-line-number="1">maximum.gain &lt;-<span class="st"> </span><span class="cf">function</span>(feature, loss, output, data, factor) {</a>
<a class="sourceLine" id="cb1501-2" data-line-number="2">    cs =<span class="st"> </span>loss</a>
<a class="sourceLine" id="cb1501-3" data-line-number="3">    indices      =<span class="st"> </span>data<span class="op">$</span>indices</a>
<a class="sourceLine" id="cb1501-4" data-line-number="4">    data         =<span class="st"> </span>data<span class="op">$</span>dataset[indices,]</a>
<a class="sourceLine" id="cb1501-5" data-line-number="5">    parent.gain =<span class="st"> </span><span class="kw">gini</span>(output, levs)</a>
<a class="sourceLine" id="cb1501-6" data-line-number="6">    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(cs<span class="op">$</span>gain) <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">length</span>(output) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1501-7" data-line-number="7">        max.idx  =<span class="st"> </span><span class="kw">which.max</span>(cs<span class="op">$</span>gain)  </a>
<a class="sourceLine" id="cb1501-8" data-line-number="8">        avg      =<span class="st"> </span><span class="kw">as.character</span>(cs<span class="op">$</span>split[max.idx])</a>
<a class="sourceLine" id="cb1501-9" data-line-number="9">        perc     =<span class="st"> </span><span class="kw">round</span>(cs<span class="op">$</span>obs[max.idx] <span class="op">/</span><span class="st"> </span>sample <span class="op">*</span><span class="st"> </span><span class="dv">100</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1501-10" data-line-number="10">        lson     =<span class="st"> </span>cs<span class="op">$</span>l.son[max.idx]</a>
<a class="sourceLine" id="cb1501-11" data-line-number="11">        rson     =<span class="st"> </span>cs<span class="op">$</span>r.son[max.idx]</a>
<a class="sourceLine" id="cb1501-12" data-line-number="12">        lindices =<span class="st"> </span>cs<span class="op">$</span>left.indices[[avg]];  lindices  =<span class="st"> </span>indices[lindices]</a>
<a class="sourceLine" id="cb1501-13" data-line-number="13">        rindices =<span class="st"> </span>cs<span class="op">$</span>right.indices[[avg]]; rindices  =<span class="st"> </span>indices[rindices]</a>
<a class="sourceLine" id="cb1501-14" data-line-number="14">        node =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;feature&quot;</span> =<span class="st"> </span>feature,       <span class="st">&quot;split&quot;</span>  =<span class="st"> </span>avg,  </a>
<a class="sourceLine" id="cb1501-15" data-line-number="15">         <span class="st">&quot;obs&quot;</span>           =<span class="st"> </span>cs<span class="op">$</span>obs[max.idx],     <span class="st">&quot;left&quot;</span>  =<span class="st"> </span>lson, </a>
<a class="sourceLine" id="cb1501-16" data-line-number="16">         <span class="st">&quot;right&quot;</span>         =<span class="st"> </span>rson,                  <span class="st">&quot;Pr&quot;</span>  =<span class="st"> </span>cs<span class="op">$</span>Pr[max.idx],       </a>
<a class="sourceLine" id="cb1501-17" data-line-number="17">         <span class="st">&quot;class&quot;</span>         =<span class="st"> </span>cs<span class="op">$</span>class[max.idx],   <span class="st">&quot;gain&quot;</span>  =<span class="st"> </span>cs<span class="op">$</span>gain[max.idx],           </a>
<a class="sourceLine" id="cb1501-18" data-line-number="18">         <span class="st">&quot;improve&quot;</span>       =<span class="st"> </span>cs<span class="op">$</span>improve[max.idx], <span class="st">&quot;perc&quot;</span>          =<span class="st"> </span>perc,</a>
<a class="sourceLine" id="cb1501-19" data-line-number="19">         <span class="st">&quot;left.indices&quot;</span>  =<span class="st"> </span>lindices,            <span class="st">&quot;right.indices&quot;</span> =<span class="st"> </span>rindices,</a>
<a class="sourceLine" id="cb1501-20" data-line-number="20">         <span class="st">&quot;indices&quot;</span>       =<span class="st"> </span>indices,             </a>
<a class="sourceLine" id="cb1501-21" data-line-number="21">         <span class="st">&quot;response&quot;</span>      =<span class="st"> </span><span class="kw">as.character</span>(output),</a>
<a class="sourceLine" id="cb1501-22" data-line-number="22">         <span class="st">&quot;ntype&quot;</span>         =<span class="st"> &quot;node&quot;</span>)</a>
<a class="sourceLine" id="cb1501-23" data-line-number="23">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1501-24" data-line-number="24">        probs         =<span class="st"> </span><span class="kw">class.probs</span>(output)</a>
<a class="sourceLine" id="cb1501-25" data-line-number="25">        parent.probs  =<span class="st"> </span><span class="kw">round</span>(probs[<span class="kw">which.max</span>(probs)]<span class="op">*</span><span class="dv">100</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1501-26" data-line-number="26">        parent.class  =<span class="st"> </span>levs[<span class="kw">which.max</span>(probs)]</a>
<a class="sourceLine" id="cb1501-27" data-line-number="27">        node =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;feature&quot;</span>  =<span class="st">  </span>feature,   <span class="st">&quot;split&quot;</span> =<span class="st"> &quot;.&quot;</span>,</a>
<a class="sourceLine" id="cb1501-28" data-line-number="28">            <span class="st">&quot;obs&quot;</span>      =<span class="st"> </span><span class="kw">length</span>(output),  <span class="st">&quot;left&quot;</span> =<span class="st"> &quot;.&quot;</span>,</a>
<a class="sourceLine" id="cb1501-29" data-line-number="29">            <span class="st">&quot;right&quot;</span>    =<span class="st"> &quot;.&quot;</span>,               <span class="st">&quot;Pr&quot;</span> =<span class="st"> </span>parent.probs,</a>
<a class="sourceLine" id="cb1501-30" data-line-number="30">            <span class="st">&quot;class&quot;</span>    =<span class="st"> </span>parent.class,    <span class="st">&quot;gain&quot;</span> =<span class="st"> </span><span class="kw">round</span>(parent.gain,<span class="dv">4</span>),</a>
<a class="sourceLine" id="cb1501-31" data-line-number="31">            <span class="st">&quot;improve&quot;</span>  =<span class="st"> &quot;.&quot;</span>, </a>
<a class="sourceLine" id="cb1501-32" data-line-number="32">            <span class="st">&quot;perc&quot;</span>     =<span class="st"> </span><span class="kw">round</span>( <span class="kw">length</span>(output) <span class="op">/</span><span class="st"> </span>sample <span class="op">*</span><span class="st"> </span><span class="dv">100</span>, <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb1501-33" data-line-number="33">            <span class="st">&quot;indices&quot;</span>  =<span class="st"> </span>indices,    <span class="st">&quot;response&quot;</span>  =<span class="st"> </span><span class="kw">as.character</span>(output),</a>
<a class="sourceLine" id="cb1501-34" data-line-number="34">            <span class="st">&quot;ntype&quot;</span>   =<span class="st"> &quot;leaf&quot;</span>)</a>
<a class="sourceLine" id="cb1501-35" data-line-number="35">    }</a>
<a class="sourceLine" id="cb1501-36" data-line-number="36">    node</a>
<a class="sourceLine" id="cb1501-37" data-line-number="37">}</a>
<a class="sourceLine" id="cb1501-38" data-line-number="38">optimizer &lt;-<span class="st"> </span>maximum.gain</a></code></pre></div>

<p>The third function to modify is the <strong>split.continuous(.)</strong>, using the <strong>maximum.gain(.)</strong> function above.</p>

<div class="sourceCode" id="cb1502"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1502-1" data-line-number="1">split.continuous &lt;-<span class="st"> </span><span class="cf">function</span>(loss, feature, target, data, minbucket) {</a>
<a class="sourceLine" id="cb1502-2" data-line-number="2">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1502-3" data-line-number="3">    <span class="co">### Similar content as split.continuous (...) in Regression Tree</span></a>
<a class="sourceLine" id="cb1502-4" data-line-number="4">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1502-5" data-line-number="5">}</a></code></pre></div>

<p>The fourth function to modify is the <strong>split.categorical(.)</strong>, using the <strong>maximum.gain(.)</strong> function above. This requires the <strong>get.categories(.)</strong> function to derive the full categorical list.</p>

<div class="sourceCode" id="cb1503"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1503-1" data-line-number="1">split.categorical &lt;-<span class="st"> </span><span class="cf">function</span>(loss, feature, target, data, minbucket, </a>
<a class="sourceLine" id="cb1503-2" data-line-number="2">                              categories) {</a>
<a class="sourceLine" id="cb1503-3" data-line-number="3">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1503-4" data-line-number="4">    <span class="co">### Similar content as split.categorical(...) in Regression Tree</span></a>
<a class="sourceLine" id="cb1503-5" data-line-number="5">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1503-6" data-line-number="6">}</a></code></pre></div>

<p>The fifth function to modify is the <strong>split.goodness(.)</strong> function.</p>

<div class="sourceCode" id="cb1504"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1504-1" data-line-number="1">sp.model &lt;-<span class="st"> </span><span class="cf">function</span>() {</a>
<a class="sourceLine" id="cb1504-2" data-line-number="2">    cs =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1504-3" data-line-number="3">    cs<span class="op">$</span>split    =<span class="st"> </span>cs<span class="op">$</span>gain     =<span class="st"> </span>cs<span class="op">$</span>obs   =<span class="st"> </span>cs<span class="op">$</span>improve =<span class="st"> </span><span class="ot">NULL</span> </a>
<a class="sourceLine" id="cb1504-4" data-line-number="4">    cs<span class="op">$</span>Pr       =<span class="st"> </span>cs<span class="op">$</span>class    =<span class="st"> </span>cs<span class="op">$</span>l.son =<span class="st"> </span>cs<span class="op">$</span>r.son   =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1504-5" data-line-number="5">    cs<span class="op">$</span>left.indices =<span class="st"> </span><span class="kw">list</span>(); cs<span class="op">$</span>right.indices =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1504-6" data-line-number="6">    cs</a>
<a class="sourceLine" id="cb1504-7" data-line-number="7">}</a></code></pre></div>
<div class="sourceCode" id="cb1505"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1505-1" data-line-number="1">split.goodness &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, data, <span class="dt">minbucket =</span> <span class="dv">2</span>, </a>
<a class="sourceLine" id="cb1505-2" data-line-number="2">                           categories) {</a>
<a class="sourceLine" id="cb1505-3" data-line-number="3">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1505-4" data-line-number="4">    <span class="co">### Similar content as split.goodness(...) in Regression Tree</span></a>
<a class="sourceLine" id="cb1505-5" data-line-number="5">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1505-6" data-line-number="6">}</a></code></pre></div>

<p>To test the <strong>goodness of split</strong>, we apply the function <strong>split.goodness(.)</strong> like so:</p>

<div class="sourceCode" id="cb1506"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1506-1" data-line-number="1">features    =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1506-2" data-line-number="2">target      =<span class="st"> &quot;Species&quot;</span></a>
<a class="sourceLine" id="cb1506-3" data-line-number="3">levs        =<span class="st"> </span><span class="kw">levels</span>(iris[,target])</a>
<a class="sourceLine" id="cb1506-4" data-line-number="4">datacars    =<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb1506-5" data-line-number="5">sample      =<span class="st"> </span><span class="kw">nrow</span>(datacars)</a>
<a class="sourceLine" id="cb1506-6" data-line-number="6">categories  =<span class="st"> </span><span class="kw">get.categories</span>(target, datacars)</a>
<a class="sourceLine" id="cb1506-7" data-line-number="7">data      =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;indices&quot;</span> =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, sample), <span class="st">&quot;dataset&quot;</span> =<span class="st"> </span>datacars)</a>
<a class="sourceLine" id="cb1506-8" data-line-number="8">ft =<span class="st"> </span><span class="kw">split.goodness</span>(features, target, data, <span class="dt">minbucket=</span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb1506-9" data-line-number="9">                    categories)<span class="op">$</span>Petal.Length</a>
<a class="sourceLine" id="cb1506-10" data-line-number="10">ft<span class="op">$</span>left.indices =<span class="st"> </span>ft<span class="op">$</span>right.indices  =<span class="st"> </span>ft<span class="op">$</span>indices =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1506-11" data-line-number="11"><span class="kw">print</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(ft)), <span class="dt">right=</span><span class="ot">TRUE</span>, <span class="dt">quote=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##             feature  split obs left right Pr    class   gain improve
## [1,] &quot;Petal.Length&quot; &quot;2.45&quot; 150   50   100 33 &quot;setosa&quot; 0.3333      50
##      perc      response  ntype
## [1,]  100 Character,150 &quot;node&quot;</code></pre>

<p>Then finally, the last function to modify is the <strong>rank.importance(.)</strong> function to support the ranking of <strong>improvement</strong> based on <strong>Gini Gain</strong>.</p>

<div class="sourceCode" id="cb1508"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1508-1" data-line-number="1">rank.importance &lt;-<span class="st"> </span><span class="cf">function</span>(features, target,  data, minbucket, categories) {</a>
<a class="sourceLine" id="cb1508-2" data-line-number="2">    goodness =<span class="st"> </span><span class="kw">split.goodness</span>(features, target, data, minbucket, categories)</a>
<a class="sourceLine" id="cb1508-3" data-line-number="3">    ranks =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1508-4" data-line-number="4">    <span class="cf">for</span> (f <span class="cf">in</span> features) {</a>
<a class="sourceLine" id="cb1508-5" data-line-number="5">        r =<span class="st"> </span>goodness[[f]]</a>
<a class="sourceLine" id="cb1508-6" data-line-number="6">        r<span class="op">$</span>left.indices  =<span class="st">  </span>r<span class="op">$</span>right.indices =<span class="st"> </span>r<span class="op">$</span>response =<span class="st"> </span>r<span class="op">$</span>indices =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1508-7" data-line-number="7">        r   =<span class="st"> </span><span class="kw">data.frame</span>(r, <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1508-8" data-line-number="8">        ranks =<span class="st"> </span><span class="kw">rbind</span>(ranks, r)</a>
<a class="sourceLine" id="cb1508-9" data-line-number="9">    }</a>
<a class="sourceLine" id="cb1508-10" data-line-number="10">    ordered.idx =<span class="st"> </span><span class="kw">order</span>(ranks[,<span class="kw">c</span>(<span class="st">&quot;improve&quot;</span>)],</a>
<a class="sourceLine" id="cb1508-11" data-line-number="11">                stringr<span class="op">::</span><span class="kw">str_detect</span>(ranks[,<span class="kw">c</span>(<span class="st">&quot;split&quot;</span>)], <span class="st">&quot;[LR-]&quot;</span>),</a>
<a class="sourceLine" id="cb1508-12" data-line-number="12">                <span class="dt">decreasing=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb1508-13" data-line-number="13">    ranks =<span class="st"> </span><span class="kw">data.frame</span>(ranks[ordered.idx,], <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1508-14" data-line-number="14">    feature =<span class="st"> </span>ranks[<span class="dv">1</span>,<span class="kw">c</span>(<span class="st">&quot;feature&quot;</span>)] </a>
<a class="sourceLine" id="cb1508-15" data-line-number="15">    top     =<span class="st">  </span>goodness[[feature]]  </a>
<a class="sourceLine" id="cb1508-16" data-line-number="16">    <span class="kw">list</span>(<span class="st">&quot;ranks&quot;</span> =<span class="st"> </span>ranks, <span class="st">&quot;top&quot;</span> =<span class="st"> </span>top) </a>
<a class="sourceLine" id="cb1508-17" data-line-number="17">}</a></code></pre></div>

<p>Using the modified implementation, we can rank our classification such that we get the top feature to use for the optimal split:</p>

<div class="sourceCode" id="cb1509"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1509-1" data-line-number="1">r =<span class="st"> </span><span class="kw">rank.importance</span>(features, target, data, <span class="dt">minbucket=</span><span class="dv">2</span>, categories)</a>
<a class="sourceLine" id="cb1509-2" data-line-number="2">my.rank =<span class="st"> </span><span class="kw">as.data.frame</span>(r<span class="op">$</span>ranks)</a></code></pre></div>
<pre><code>##        feature split obs   L   R Pr  class  gain improve   % ntyp
## 3 Petal.Length  2.45 150  50 100 33 setosa 0.333   50.00 100 node
## 4  Petal.Width   0.8 150  50 100 33 setosa 0.333   50.00 100 node
## 1 Sepal.Length  5.45 150  52  98 33 setosa 0.228   34.16 100 node
## 2  Sepal.Width  3.35 150 113  37 33 setosa 0.127   19.04 100 node</code></pre>
<div class="sourceCode" id="cb1511"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1511-1" data-line-number="1">ft =<span class="st"> </span>r<span class="op">$</span>top</a>
<a class="sourceLine" id="cb1511-2" data-line-number="2">ft<span class="op">$</span>left.indices =<span class="st"> </span>ft<span class="op">$</span>right.indices =<span class="st"> </span>ft<span class="op">$</span>indices =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1511-3" data-line-number="3"><span class="kw">print</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(ft)), <span class="dt">right=</span><span class="ot">TRUE</span>, <span class="dt">quote=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##             feature  split obs left right Pr    class   gain improve
## [1,] &quot;Petal.Length&quot; &quot;2.45&quot; 150   50   100 33 &quot;setosa&quot; 0.3333      50
##      perc      response  ntype
## [1,]  100 Character,150 &quot;node&quot;</code></pre>

<p>To validate, let us print a summary of the result from our previous tree.model:</p>

<div class="sourceCode" id="cb1513"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1513-1" data-line-number="1">summ =<span class="st"> </span><span class="kw">capture.output</span>(<span class="kw">summary</span>(tree.model))</a></code></pre></div>
<div class="sourceCode" id="cb1514"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1514-1" data-line-number="1"><span class="kw">show.node</span>(summ,<span class="dv">1</span>, <span class="dt">lnum=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## Node number 1: 150 observations,    complexity param=0.5
##   predicted class=setosa      expected loss=0.6667  P(node) =1
##     class counts:    50    50    50
##    probabilities: 0.333 0.333 0.333 
##   left son=2 (50 obs) right son=3 (100 obs)
##   Primary splits:
##       Petal.Length &lt; 2.45 to the left,  improve=50.00, (0 missing)
##       Petal.Width  &lt; 0.8  to the left,  improve=50.00, (0 missing)
##       Sepal.Length &lt; 5.45 to the left,  improve=34.16, (0 missing)
##       Sepal.Width  &lt; 3.35 to the right, improve=19.04, (0 missing)
##   Surrogate splits:
##       Petal.Width  &lt; 0.8  to the left,  agree=1.000, adj=1.00, (0 split)
##       Sepal.Length &lt; 5.45 to the left,  agree=0.920, adj=0.76, (0 split)
##       Sepal.Width  &lt; 3.35 to the right, agree=0.833, adj=0.50, (0 split)</code></pre>

<p>Let us now implement a new base learner for our classification tree. Here, we use <strong>my.regression.tree(.)</strong> with no modification. We only rename the function as <strong>my.classification.tree(.)</strong>, but uses <strong>rank.importance(.)</strong> as modified above for classification.</p>

<div class="sourceCode" id="cb1516"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1516-1" data-line-number="1"><span class="kw">library</span>(dequer)</a>
<a class="sourceLine" id="cb1516-2" data-line-number="2">my.classification.tree &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, dataset,  <span class="dt">minbucket =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1516-3" data-line-number="3">                                   <span class="dt">maxdepth=</span><span class="dv">50</span>) {</a>
<a class="sourceLine" id="cb1516-4" data-line-number="4">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1516-5" data-line-number="5">    <span class="co">### Similar content as my.regression.tree (...) in Regression Tree</span></a>
<a class="sourceLine" id="cb1516-6" data-line-number="6">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1516-7" data-line-number="7">}</a></code></pre></div>
<div class="sourceCode" id="cb1517"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1517-1" data-line-number="1">features    =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1517-2" data-line-number="2">target      =<span class="st"> &quot;Species&quot;</span></a>
<a class="sourceLine" id="cb1517-3" data-line-number="3">levs        =<span class="st"> </span><span class="kw">levels</span>(iris[,target])</a>
<a class="sourceLine" id="cb1517-4" data-line-number="4">datacars    =<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb1517-5" data-line-number="5">my.model =<span class="st"> </span><span class="kw">h.learner</span>(features, target, datacars, <span class="dt">minbucket=</span><span class="dv">2</span>, <span class="dt">maxdepth=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb1517-6" data-line-number="6"><span class="co"># See Regression Tree Section for my.table.tree(.)</span></a>
<a class="sourceLine" id="cb1517-7" data-line-number="7">my.tree.model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.model<span class="op">$</span>model, <span class="dt">display_mode=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##   N P      feature split obs  L   R  Pr      class  gain improv perc
## 1 1 0 Petal.Length  2.45 150 50 100  33     setosa 0.333     50  100
## 2 2 1       &lt;leaf&gt;     .  50  .   . 100     setosa 0.000      .   33
## 3 3 1  Petal.Width  1.75 100 54  46  50 versicolor 0.390 77.939   67
## 4 4 3 Petal.Length  4.95  54 48   6  91 versicolor 0.082 49.031   36
## 5 5 3       &lt;leaf&gt;     .  46  .   .  98  virginica 0.042      .   31
## 6 6 4       &lt;leaf&gt;     .  48  .   .  98 versicolor 0.041      .   32
## 7 7 4       &lt;leaf&gt;     .   6  .   .  67  virginica 0.444      .    4</code></pre>

<p>In terms of prediction, one can construct the same prediction function, namely <strong>my.predict(.)</strong>, which we implemented for <strong>Regression Trees</strong>. A modified version of the function is implemented in a section covering <strong>AdaBoost</strong> for classification.</p>
</div>
<div id="ensemble-methods-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.3</span> Ensemble Methods <a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using <strong>Classification Trees</strong>, similar to <strong>Regression</strong>, we have ensemble methods available for classification, namely <strong>Random Forest</strong>, <strong>Adaboost</strong>, and <strong>Gradient Boost</strong>. For a review of the methods, recall the use of <strong>MSE</strong> for regression to evaluate individual trees of an ensemble. For classification, in our case, we can reference a <strong>Confusion Matrix</strong> to evaluate models based on <strong>specificity</strong>, <strong>sensitivity</strong>, <strong>accuracy</strong>, <strong>F1 score</strong>, and others.</p>
<p>To illustrate, let us continue to use the <strong>iris</strong> train set and test set.</p>

<div class="sourceCode" id="cb1519"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1519-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1519-2" data-line-number="2">features     =<span class="st"> </span><span class="kw">names</span>(iris)[<span class="kw">which</span>(<span class="op">!</span><span class="kw">names</span>(iris) <span class="op">%in%</span><span class="st"> &quot;Species&quot;</span>)]  </a>
<a class="sourceLine" id="cb1519-3" data-line-number="3">target       =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)</a>
<a class="sourceLine" id="cb1519-4" data-line-number="4">datacars     =<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb1519-5" data-line-number="5">fold.indices =<span class="st"> </span><span class="kw">createFolds</span>(datacars<span class="op">$</span>Species, <span class="dt">k =</span> <span class="dv">10</span>, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1519-6" data-line-number="6"><span class="co"># choose the first fold for our test group.</span></a>
<a class="sourceLine" id="cb1519-7" data-line-number="7">test =<span class="st"> </span>datacars[fold.indices<span class="op">$</span>Fold01,]</a>
<a class="sourceLine" id="cb1519-8" data-line-number="8"><span class="co"># choose the other folds for training group.</span></a>
<a class="sourceLine" id="cb1519-9" data-line-number="9">train =<span class="st"> </span>datacars[<span class="op">-</span>fold.indices<span class="op">$</span>Fold01,]</a>
<a class="sourceLine" id="cb1519-10" data-line-number="10"><span class="co"># test target</span></a>
<a class="sourceLine" id="cb1519-11" data-line-number="11">test.class =<span class="st"> </span>test<span class="op">$</span>Species</a>
<a class="sourceLine" id="cb1519-12" data-line-number="12">test<span class="op">$</span>Species =<span class="st"> </span><span class="ot">NULL</span></a></code></pre></div>

<p>Now, recall our discussion of ensemble methods under <strong>Ensemble Methods</strong> Subsection under <strong>Regression</strong> Section. The concept of <strong>bagging</strong> and <strong>boosting</strong> remains the same for <strong>classification</strong>. Learning a tree model by random sampling - so-called <strong>bootstrapping</strong> - and then testing using out of bag applies to <strong>classification</strong> also. Similarly, learning a tree model by <strong>boosting</strong> weak learners, e.g., <strong>AdaBoost</strong>, or using the gradient method to speed up the learning process, e.g., <strong>Gradient Boost</strong>, also applies to <strong>classification</strong>.</p>
<p>The next few sections review how ensemble methods use the train set and test set for classification. Note that details of the ensemble methods are already covered in <strong>regression</strong>; thus, the following sections complement the intuition with the use of 3rd-party R packages and emphasize the classification performance of the methods.</p>
</div>
<div id="random-forest-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.4</span> Random Forest <a href="10.3-multi-class-classification-supervised.html#random-forest-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We reference <strong>Random Forest</strong> Subsection under <strong>Regression</strong> Section for a detailed intuition of <strong>Bagging</strong> using <strong>Random Forest</strong> as our <strong>Ensemble</strong> algorithm. With modification to our implementation, we replace the <strong>regressor</strong> with our own implementation of <strong>Random Forest classifier</strong>. For example, we use both <strong>my.random.forest(.)</strong> and <strong>my.rf.tree(.)</strong> functions; however, content of <strong>build.level.tree(.)</strong> uses a classifier, namely <strong>my.classification.tree(.)</strong>.</p>
<p>In this section, we extend our discussion of <strong>Random Forest</strong> by covering the modelâs performance. Using our train set and test set, we learn the model like so:</p>
<div class="sourceCode" id="cb1520"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1520-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb1520-2" data-line-number="2">ntree =<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb1520-3" data-line-number="3">rf.model =<span class="st"> </span><span class="kw">randomForest</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">importance=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1520-4" data-line-number="4">                        <span class="dt">ntree=</span>ntree, <span class="dt">mtry=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1520-5" data-line-number="5">rf.model<span class="op">$</span>confusion</a></code></pre></div>
<pre><code>##            setosa versicolor virginica class.error
## setosa         43          0         0      0.0000
## versicolor      0         36         5      0.1220
## virginica       0          5        31      0.1389</code></pre>
<p>Based on the model, our confusion matrix above indicates that the model fits well for setosa class, given a classification error of 0. The virginica class shows a 0.1389 classification error.</p>
<p>Then, we plot the <strong>variable importance</strong>. It is worth mentioning that if the dataset is for regression, the importance of a variable is measured based on the increase of <strong>Node Impurity</strong> denoted by <strong>IncNodePurity</strong>; however, for classification, it is measured based on the mean decrease of <strong>Gini Index</strong> and <strong>Accuracy</strong> denoted by <strong>MeanDecreaseGini</strong> and <strong>MeanDecreaseAccuracy</strong> respectively.</p>
<div class="sourceCode" id="cb1522"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1522-1" data-line-number="1"><span class="kw">cbind</span>(<span class="kw">importance</span>(rf.model), <span class="st">&quot;varUsed&quot;</span> =<span class="st"> </span><span class="kw">varUsed</span>(rf.model))</a></code></pre></div>
<pre><code>##              setosa versicolor virginica MeanDecreaseAccuracy
## Sepal.Length  0.000    -0.3128    0.8773              1.02317
## Sepal.Width   0.000    -1.1180    1.1180             -0.08649
## Petal.Length  1.118     3.1824    1.6423              2.03300
## Petal.Width   4.372     3.7990    3.8253              4.58091
##              MeanDecreaseGini varUsed
## Sepal.Length           11.602      11
## Sepal.Width             1.091       4
## Petal.Length           17.058       9
## Petal.Width            59.251      17</code></pre>
<div class="sourceCode" id="cb1524"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1524-1" data-line-number="1"><span class="kw">varImpPlot</span>(rf.model)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:varImpPlot"></span>
<img src="DS_files/figure-html/varImpPlot-1.png" alt="Variable Importance" width="70%" />
<p class="caption">
Figure 10.31: Variable Importance
</p>
</div>
<p>It also helps to mention that the measures above explain how variables may contribute in importance the lower the values. In practice, however, the interpretation of such values gets lost in the complexity of how <strong>Random Forest</strong> builds trees, especially if we deal with thousands of trees.</p>
<p>Instead, we can look into the <strong>out-of-bag</strong> error. Note that it is not necessary for us to have a separate train set and test set because we are setting aside a portion of the dataset (called <strong>out-of-bag</strong>) for validation. We can use the entire dataset to fit our model while, internally, the algorithm sets aside an <strong>out-of-bag</strong> set to test the model. The error from the test is called the <strong>out-of-bag</strong> error rate or <strong>out-of-bag</strong> estimate. It reflects an estimate of what a prediction error may look like should we predict a class given new data.</p>
<div class="sourceCode" id="cb1525"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1525-1" data-line-number="1">rf.model<span class="op">$</span>err.rate</a></code></pre></div>
<pre><code>##          OOB setosa versicolor virginica
## [1,] 0.02381      0    0.06667    0.0000
## [2,] 0.10667      0    0.12500    0.1923
## [3,] 0.13000      0    0.20588    0.1875
## [4,] 0.11504      0    0.20000    0.1471
## [5,] 0.08333      0    0.12195    0.1389</code></pre>
<p>We also use <strong>OOB error rate</strong> to evaluate the performance of different configurations of our random Tree, e.g., using different <strong>mtry</strong> - this is the number of variables to try and test at split when building our random forest tree.</p>

<div class="sourceCode" id="cb1527"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1527-1" data-line-number="1">rf.model1 =<span class="st"> </span><span class="kw">randomForest</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">importance=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1527-2" data-line-number="2">                         <span class="dt">ntree=</span>ntree, <span class="dt">mtry=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1527-3" data-line-number="3">rf.model2 =<span class="st"> </span><span class="kw">randomForest</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">importance=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1527-4" data-line-number="4">                         <span class="dt">ntree=</span>ntree, <span class="dt">mtry=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1527-5" data-line-number="5">(<span class="dt">OOB.rates =</span> <span class="kw">cbind</span>(<span class="st">&quot;OOB (mtry=1)&quot;</span> =<span class="st"> </span>rf.model1<span class="op">$</span>err.rate[,<span class="dv">1</span>], </a>
<a class="sourceLine" id="cb1527-6" data-line-number="6">      <span class="st">&quot;OOB (mtry=2)&quot;</span> =<span class="st"> </span>rf.model<span class="op">$</span>err.rate[,<span class="dv">1</span>], </a>
<a class="sourceLine" id="cb1527-7" data-line-number="7">      <span class="st">&quot;OOB (mtry=3)&quot;</span> =<span class="st"> </span>rf.model2<span class="op">$</span>err.rate[,<span class="dv">1</span>]))</a></code></pre></div>
<pre><code>##      OOB (mtry=1) OOB (mtry=2) OOB (mtry=3)
## [1,]      0.06667      0.02381      0.06000
## [2,]      0.08974      0.10667      0.04706
## [3,]      0.10526      0.13000      0.06000
## [4,]      0.10619      0.11504      0.05405
## [5,]      0.09677      0.08333      0.06667</code></pre>
<div class="sourceCode" id="cb1529"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1529-1" data-line-number="1">min.rate.idx =<span class="st"> </span><span class="kw">which.min</span>(OOB.rates[ntree,])</a></code></pre></div>

<p>It shows that tuning the tree with 3 variable(s) (e.g.Â mtry=3) renders the least error at 6.67%.</p>
<p>Lastly, to complement our evaluation, we can still split our dataset and use a separate test set to measure the performance of predictions. Here, we make a prediction and generate the AUC score using the performance(.) function from the ROCR library. For prediction using our test set, we use the following code:</p>
<div class="sourceCode" id="cb1530"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1530-1" data-line-number="1">test.pred  =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(rf.model, <span class="dt">newdata=</span>test, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</a>
<a class="sourceLine" id="cb1530-2" data-line-number="2"><span class="kw">table</span>(<span class="dt">observed=</span>test.class,<span class="dt">predicted=</span>test.pred)</a></code></pre></div>
<pre><code>##             predicted
## observed     setosa versicolor virginica
##   setosa          5          0         0
##   versicolor      0          5         0
##   virginica       0          0         5</code></pre>
<p>We then evaluate the prediction performance by plotting and reviewing the <strong>AUC</strong>. See Figure <a href="10.3-multi-class-classification-supervised.html#fig:rfroc">10.32</a>.</p>

<div class="sourceCode" id="cb1532"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1532-1" data-line-number="1"><span class="kw">library</span>(ROCR)</a>
<a class="sourceLine" id="cb1532-2" data-line-number="2">color =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb1532-3" data-line-number="3">test.votes   =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(rf.model, <span class="dt">newdata=</span>test, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</a>
<a class="sourceLine" id="cb1532-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb1532-5" data-line-number="5">      <span class="dt">xlab=</span><span class="st">&quot;False Positive Rate&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;True Positive Rate&quot;</span>, </a>
<a class="sourceLine" id="cb1532-6" data-line-number="6">     <span class="dt">main=</span><span class="st">&quot;AUC (Random Forest Performance)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1532-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1532-8" data-line-number="8"><span class="kw">abline</span>( <span class="dt">a=</span><span class="dv">0</span>, <span class="dt">b=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb1532-9" data-line-number="9">auc =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(levs))</a>
<a class="sourceLine" id="cb1532-10" data-line-number="10"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(levs)) {</a>
<a class="sourceLine" id="cb1532-11" data-line-number="11">  truth   =<span class="st"> </span><span class="kw">ifelse</span>(test.class <span class="op">==</span><span class="st"> </span>levs[k], <span class="dv">1</span>, <span class="dv">0</span>) </a>
<a class="sourceLine" id="cb1532-12" data-line-number="12">  pred    =<span class="st"> </span><span class="kw">prediction</span>(test.votes[,k], truth)</a>
<a class="sourceLine" id="cb1532-13" data-line-number="13">  perf    =<span class="st"> </span><span class="kw">performance</span>(pred, <span class="st">&quot;tpr&quot;</span>, <span class="st">&quot;fpr&quot;</span>)</a>
<a class="sourceLine" id="cb1532-14" data-line-number="14">  auc[k]  =<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;auc&quot;</span>)<span class="op">@</span>y.values</a>
<a class="sourceLine" id="cb1532-15" data-line-number="15">  <span class="kw">plot</span>(perf, <span class="dt">col=</span>color[k], <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1532-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb1532-17" data-line-number="17"><span class="kw">legend</span>(<span class="fl">0.38</span>, <span class="fl">0.28</span>, <span class="dt">legend=</span><span class="kw">c</span>(  </a>
<a class="sourceLine" id="cb1532-18" data-line-number="18">      <span class="kw">paste0</span>(levs[<span class="dv">1</span>],<span class="st">&quot; (auc=&quot;</span>,auc[<span class="dv">1</span>],<span class="st">&quot;)&quot;</span>), </a>
<a class="sourceLine" id="cb1532-19" data-line-number="19">      <span class="kw">paste0</span>(levs[<span class="dv">2</span>],<span class="st">&quot; (auc=&quot;</span>,auc[<span class="dv">2</span>],<span class="st">&quot;)&quot;</span>),</a>
<a class="sourceLine" id="cb1532-20" data-line-number="20">      <span class="kw">paste0</span>(levs[<span class="dv">3</span>],<span class="st">&quot; (auc=&quot;</span>,auc[<span class="dv">3</span>],<span class="st">&quot;)&quot;</span>) ), </a>
<a class="sourceLine" id="cb1532-21" data-line-number="21">    <span class="dt">col=</span>color,  <span class="dt">pch=</span><span class="dv">20</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1532-22" data-line-number="22"><span class="kw">legend</span>(<span class="fl">0.65</span>, <span class="fl">0.70</span>, </a>
<a class="sourceLine" id="cb1532-23" data-line-number="23">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;0.90 - 1.00 = excellent&quot;</span>,  <span class="st">&quot;0.80 - 0.90 = good&quot;</span>, </a>
<a class="sourceLine" id="cb1532-24" data-line-number="24">              <span class="st">&quot;0.70 - 0.80 = fair&quot;</span>,   <span class="st">&quot;0.60 - 0.70 = poor&quot;</span>,</a>
<a class="sourceLine" id="cb1532-25" data-line-number="25">              <span class="st">&quot;0.50 - 0.60 = fail&quot;</span> ),</a>
<a class="sourceLine" id="cb1532-26" data-line-number="26">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>),  <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">16</span>,<span class="dv">16</span>,<span class="dv">16</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rfroc"></span>
<img src="DS_files/figure-html/rfroc-1.png" alt="AUC (Random Forest Performance)" width="70%" />
<p class="caption">
Figure 10.32: AUC (Random Forest Performance)
</p>
</div>

<p>The figure shows that the model predicts excellently, having <strong>AUC</strong> values within the range 0.90 - 1.00.</p>
</div>
<div id="AdaBoost" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.5</span> AdaBoost &amp; SAMME<a href="10.3-multi-class-classification-supervised.html#AdaBoost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We continue to discuss <strong>classification</strong> using one of the <strong>Ensemble</strong> methods called <strong>Boosting</strong>. Our next introductory classification algorithm is <strong>AdaBoost</strong> which we also covered under <strong>AdaBoost</strong> Subsection under <strong>Regression</strong> Section.  </p>
<p>Recall in <strong>AdaBoost Regression</strong> the introduction of <strong>AdaBoost.R2</strong> <span class="citation">(Drucker H. <a href="bibliography.html#ref-ref589h">1997</a>)</span>. For <strong>Regression</strong>, we also can use <strong>SAMME.R</strong> as an alternative algorithm which is not covered under <strong>Regression</strong> in <strong>Computational Learning I</strong>; however, in this section, we discuss <strong>SAMME</strong> for classification instead.</p>
<p>Also, we introduce <strong>AdaBoost.M2 (adam2)</strong> for classification. Though we do not cover <strong>AdaBoost.M1 (adam1)</strong> in this section, it helps to be aware of such precursors.</p>
<p>Here, we compare the two multi-classification algorithms, namely <strong>AdaBoost.M2</strong> and <strong>SAMME</strong>. Below is the <strong>AdaBoost.M2</strong> algorithm <span class="citation">(Yoav Freund, Robert E Schapire <a href="bibliography.html#ref-ref616y">1996</a>)</span>.</p>

<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,K\}\}_{i=1}^n\\
\ \ \ \text{Let} B = \{{(i, y)}:i\ \in\ \{1,...,n\}, y \ne y_i\}\\
\ \ \ \text{number of machines}: M\\
\mathbf{Algorithm}:\\
\ \ \ weights: D^{(0)}_i = \frac{1}{n},\ i=1,2,...,n\ \ \ \ \ \ \ \text{(initialize distribution)}\\
\ \ \ \text{loop}\ m\ in\ 1:\ M \\
\ \ \ \ \ \ \ S^{(m)} \leftarrow \text{Resample from original training set } X^{(0)} \text{ with }D^{(m)}\\
\ \ \ \ \ \ \ h^{(m)}\  \leftarrow \text{Train a weak classifier on } S^{(m)} \text{ then }\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{calculate the pseudo-loss of }  h^{(m)}: \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \epsilon^{(m)} = \frac{1}{2} \sum_{(i,y) \in B}^n D^{(m)}_{(i,y)} \left(1 - h^{(m)}(x_i, y_i) + h^{(m)}(x_i, y)\right)\\
\ \ \ \ \ \ \ \text{Compute contribution for this classifier}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \beta^{(m)} =   \epsilon^{(m)}/(1 - \epsilon^{(m)}) 
    \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \text{(Voting Power)}\\
\ \ \ \ \ \ \ \text{Update weights on training points}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \alpha^{(m)} = (1/2)  \left(1 + h^{(m)}(x_i, y_i) - h^{(m)}(x_i, y)\right)\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ D^{(m+1)}_{(i,y)} \leftarrow D^{(m)}_{(i,y)} \cdot (\beta^{(m)})^{\alpha^{(m)}}, i=1,2,...,n\\
\ \ \ \ \ \ \ \text{Normalize weights such that } \sum_{i=1}^n D^{(m+1)}_{(i,y)} = 1 \\
\ \ \ \text{end loop} \\
\ \ \ \text{Output }H(\mathbf{x}) = \text{arg}\ \underset{y \in Y}{\text{max}}\left(\sum_{m=1}^M \log_e \frac{1}{\beta^{(m)}}h^{(m)}(x, y)\right)
\end{array} 
\]</span>
</p>
<p>Moreover, below is the <strong>SAMME</strong> algorithm. The name is an acronym for <strong>S</strong>tagewise <strong>A</strong>dditive <strong>M</strong>odeling Using <strong>M</strong>ulti-class <strong>E</strong>xponential loss function. It is also a multi-classification algorithm <span class="citation">(Ji Zhu et al. <a href="bibliography.html#ref-ref602j">2006</a>)</span>.</p>

<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\ 
\ \ \ \text{dataset}: \{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,K\}\}_{i=1}^n\\
\ \ \ \text{number of machines}: M\\
\mathbf{Algorithm}:\\
\ \ \ weights: w^{(0)}_i = \frac{1}{n},\ i=1,2,...,n \ \ \ \ \ \ \ \ \text{(initialize equal weights)}\\
\ \ \ \text{loop}\ m\ in\ 1:\ M \\
\ \ \ \ \ \ \ S^{(m)} \leftarrow \text{Resample from original training set } X^{(0)} \text{ with }w^{(m)}\\
\ \ \ \ \ \ \ h^{(m)}\  \leftarrow \text{Train a weak classifier on } S^{(m)} \text{ then }\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{compute the following pseudo-loss of } h^{(m)}: \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \epsilon^{(m)} = 
    \sum_{i=1}^n w^{(m)}_i \mathbf{1}\{h^{(m)}(x_i) \ne y_i \} / \sum_{i=1}^n w_i^{(m)}\\
\ \ \ \ \ \ \ \text{Compute contribution for this classifier}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \beta^{(m)} = 
     \log_e \frac{1 - \epsilon^{(m)}}{\epsilon^{(m)}} + \log_e(K-1)
           \ \ \ \ \ \ \ \ \ \ \  \text{(Voting Power)}\\
\ \ \ \ \ \ \ \text{Update weights on training points}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \alpha^{(m)} = \beta^{(m)} \cdot \mathbf{1}\{h^{(m)}(x_i) \ne y_i \}) \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ w^{(m+1)}_i \leftarrow (w^{(m)}_i)^{\alpha^{(m)}}, i=1,2,...,n\\
\ \ \ \ \ \ \ \text{Normalize weights such that } \sum_{i=1}^n w^{(m+1)}_i = 1 \\
\ \ \ \text{end loop} \\
\ \ \ \text{Output }H(\mathbf{x}) = arg\ \underset{k}{max}\left(\sum_{m=1}^M \beta^{(m)} \cdot \mathbf{1}\{h^{(m)}(x_i) = k \}\right)
\end{array} 
\]</span>
</p>
<p>One important note is that a base learnerâs choice for <strong>h(x)</strong> is allowed for both algorithms. Any base learner classifier is supported, namely, Decision Trees, SVM, Logistic Regression, and so on. The idea is to use the base learner to fit models that are not required to be well fitted - hence, a weak fit - and we use our algorithm to <strong>boost</strong> for a better fit. In our case, we use our <strong>Classification Tree</strong> that we previously implemented and introduced as our base learner.</p>
<p><strong>First</strong>, we start with the idea that <strong>Boosting</strong> for <strong>Classification Trees</strong> uses weak base learners called <strong>Stumps</strong>. A <strong>stump</strong> is a tree with only one split and can be built using our classifier as an illustration. Note that our classifier uses <strong>Gini Index</strong> and <strong>Gini Gain</strong> to evaluate <strong>goodness of split</strong> for a <strong>stump</strong>. Alternatively, we can use <strong>Entropy</strong> and <strong>Entropy Gain</strong>, detailed under the <strong>Classification Trees</strong> section. </p>
<div class="sourceCode" id="cb1533"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1533-1" data-line-number="1"><span class="kw">library</span>(dequer)</a>
<a class="sourceLine" id="cb1533-2" data-line-number="2">h.learner =<span class="st"> </span>my.classification.tree <span class="co"># our base learner classifier</span></a></code></pre></div>
<p>For example, we write the following code to generate a stump considering only the <strong>Petal.Length</strong> features using the <strong>iris</strong> dataset:</p>

<div class="sourceCode" id="cb1534"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1534-1" data-line-number="1">my.petal.stump =<span class="st"> </span><span class="kw">h.learner</span>(<span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Species&quot;</span>, train, <span class="dt">minbucket=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1534-2" data-line-number="2">                           <span class="dt">maxdepth=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1534-3" data-line-number="3">my.tree.model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.petal.stump<span class="op">$</span>model, <span class="dt">display_mode=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##   N P      feature split obs  L  R  Pr      class  gain improve perc
## 1 1 0 Petal.Length  2.45 135 45 90  33     setosa 0.333      50   90
## 2 2 1       &lt;leaf&gt;     .  45  .  . 100     setosa 0.000       .   30
## 3 3 1       &lt;leaf&gt;     .  90 40 50  50 versicolor 0.361    72.2   60</code></pre>

<p>In terms of ranking the top feature to use for the split of a <strong>stump</strong>, we use the same <strong>rank.importance(.)</strong> function:</p>

<div class="sourceCode" id="cb1536"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1536-1" data-line-number="1">features    =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1536-2" data-line-number="2">target      =<span class="st"> &quot;Species&quot;</span></a>
<a class="sourceLine" id="cb1536-3" data-line-number="3">levs        =<span class="st"> </span><span class="kw">levels</span>(iris[,target])</a>
<a class="sourceLine" id="cb1536-4" data-line-number="4">datacars    =<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb1536-5" data-line-number="5">categories  =<span class="st"> </span><span class="kw">get.categories</span>(target, datacars)</a>
<a class="sourceLine" id="cb1536-6" data-line-number="6">data      =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;indices&quot;</span> =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, sample), <span class="st">&quot;dataset&quot;</span> =<span class="st"> </span>datacars)</a>
<a class="sourceLine" id="cb1536-7" data-line-number="7">r =<span class="st"> </span><span class="kw">rank.importance</span>(features, target, data, <span class="dt">minbucket=</span><span class="dv">1</span>, categories)</a>
<a class="sourceLine" id="cb1536-8" data-line-number="8">my.rank =<span class="st"> </span><span class="kw">as.data.frame</span>(r<span class="op">$</span>ranks)</a></code></pre></div>
<pre><code>##        feature split obs   L   R Pr  class  gain improve perc ntyp
## 3 Petal.Length  2.45 150  50 100 33 setosa 0.333   50.00  100 node
## 4  Petal.Width   0.8 150  50 100 33 setosa 0.333   50.00  100 node
## 1 Sepal.Length  5.45 150  52  98 33 setosa 0.228   34.16  100 node
## 2  Sepal.Width  3.35 150 113  37 33 setosa 0.127   19.04  100 node</code></pre>

<p>The result shows <strong>Petal.length</strong> feature as the top choice based on the <strong>Gini Gain</strong> metrics. Alternatively, we can use the <strong>improve</strong> metrics.</p>
<p><strong>Second</strong>, for <strong>prediction</strong>, we continue to use <strong>my.predict(.)</strong> with slight modification. Here, we expect a comparison of <strong>categorical</strong> output between the estimate and true value.</p>

<div class="sourceCode" id="cb1538"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1538-1" data-line-number="1">prediction.score &lt;-<span class="st"> </span><span class="cf">function</span>(y, y.hat) {</a>
<a class="sourceLine" id="cb1538-2" data-line-number="2">  result =<span class="st"> </span><span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span>y.hat, <span class="dv">1</span>, <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb1538-3" data-line-number="3">  <span class="kw">list</span>(<span class="st">&quot;fitted.values&quot;</span> =<span class="st"> </span>y.hat, <span class="st">&quot;result&quot;</span> =<span class="st"> </span>result)</a>
<a class="sourceLine" id="cb1538-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb1538-5" data-line-number="5">my.predict  &lt;-<span class="st"> </span><span class="cf">function</span>(my.model, x, y,  <span class="dt">resid =</span> <span class="ot">NULL</span>,</a>
<a class="sourceLine" id="cb1538-6" data-line-number="6">                        <span class="dt">tendency =</span> <span class="cf">function</span>(top, <span class="dt">resid =</span> <span class="ot">NULL</span>) { top<span class="op">$</span>class },</a>
<a class="sourceLine" id="cb1538-7" data-line-number="7">                        <span class="dt">method   =</span> prediction.score, <span class="dt">tabletree =</span> <span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb1538-8" data-line-number="8">  n          =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1538-9" data-line-number="9">  responses  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1538-10" data-line-number="10">  model      =<span class="st"> </span>my.model<span class="op">$</span>model</a>
<a class="sourceLine" id="cb1538-11" data-line-number="11">  categories =<span class="st"> </span>my.model<span class="op">$</span>categories</a>
<a class="sourceLine" id="cb1538-12" data-line-number="12">  <span class="cf">if</span> (tabletree <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) { model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.model<span class="op">$</span>model) }</a>
<a class="sourceLine" id="cb1538-13" data-line-number="13">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1538-14" data-line-number="14">    node =<span class="st"> </span>model[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb1538-15" data-line-number="15">    <span class="cf">while</span> (<span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1538-16" data-line-number="16">      <span class="cf">if</span> (node<span class="op">$</span>ntype <span class="op">==</span><span class="st"> &quot;leaf&quot;</span>) {</a>
<a class="sourceLine" id="cb1538-17" data-line-number="17">          <span class="cf">if</span> (tabletree <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span> ) { </a>
<a class="sourceLine" id="cb1538-18" data-line-number="18">            top          =<span class="st"> </span>my.model<span class="op">$</span>model[[node<span class="op">$</span>N]]<span class="op">$</span>top</a>
<a class="sourceLine" id="cb1538-19" data-line-number="19">            responses[i] =<span class="st"> </span><span class="kw">tendency</span>( top, resid )</a>
<a class="sourceLine" id="cb1538-20" data-line-number="20">          } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1538-21" data-line-number="21">            responses[i] =<span class="st"> </span>node<span class="op">$</span>class</a>
<a class="sourceLine" id="cb1538-22" data-line-number="22">          }</a>
<a class="sourceLine" id="cb1538-23" data-line-number="23">          <span class="cf">break</span></a>
<a class="sourceLine" id="cb1538-24" data-line-number="24">      }</a>
<a class="sourceLine" id="cb1538-25" data-line-number="25">      children    =<span class="st"> </span>model[<span class="kw">which</span>( model<span class="op">$</span>P <span class="op">==</span><span class="st"> </span>node<span class="op">$</span>N ),]</a>
<a class="sourceLine" id="cb1538-26" data-line-number="26">      cat         =<span class="st"> </span>categories[[node<span class="op">$</span>feature]]</a>
<a class="sourceLine" id="cb1538-27" data-line-number="27">      split       =<span class="st"> </span>node<span class="op">$</span>split</a>
<a class="sourceLine" id="cb1538-28" data-line-number="28">      val         =<span class="st"> </span>x[i, <span class="kw">c</span>(node<span class="op">$</span>feature)]</a>
<a class="sourceLine" id="cb1538-29" data-line-number="29">      <span class="cf">if</span> (<span class="kw">is.factor</span>(val)) {</a>
<a class="sourceLine" id="cb1538-30" data-line-number="30">        s         =<span class="st"> </span>base<span class="op">::</span><span class="kw">strsplit</span>(split,<span class="ot">NULL</span>)[[<span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb1538-31" data-line-number="31">        direction =<span class="st"> </span>s[<span class="kw">which</span>(cat <span class="op">%in%</span><span class="st"> </span>val )]</a>
<a class="sourceLine" id="cb1538-32" data-line-number="32">        direction =<span class="st"> </span><span class="kw">ifelse</span>(direction <span class="op">==</span><span class="st"> &#39;L&#39;</span>, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1538-33" data-line-number="33">      } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1538-34" data-line-number="34">        split     =<span class="st"> </span><span class="kw">as.numeric</span>(split)</a>
<a class="sourceLine" id="cb1538-35" data-line-number="35">        val       =<span class="st"> </span><span class="kw">as.numeric</span>(val)</a>
<a class="sourceLine" id="cb1538-36" data-line-number="36">        direction =<span class="st"> </span><span class="kw">ifelse</span>(val <span class="op">&lt;</span><span class="st"> </span>split, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1538-37" data-line-number="37">      }</a>
<a class="sourceLine" id="cb1538-38" data-line-number="38">      node =<span class="st"> </span>children[direction,]</a>
<a class="sourceLine" id="cb1538-39" data-line-number="39">    }</a>
<a class="sourceLine" id="cb1538-40" data-line-number="40">  }</a>
<a class="sourceLine" id="cb1538-41" data-line-number="41">  <span class="kw">method</span>(y,  responses) </a>
<a class="sourceLine" id="cb1538-42" data-line-number="42">}</a>
<a class="sourceLine" id="cb1538-43" data-line-number="43">h.score =<span class="st"> </span>my.predict </a></code></pre></div>

<p>To test our <strong>prediction</strong> function, we use one of our stump trees like so:</p>

<div class="sourceCode" id="cb1539"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1539-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1539-2" data-line-number="2"><span class="kw">h.score</span>(my.petal.stump, test, test.class)</a></code></pre></div>
<pre><code>## $fitted.values
##  [1] &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;    
##  [6] &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot;
## [11] &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot;
## 
## $result
##  [1]  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1</code></pre>

<p><strong>Third</strong>, we also use <strong>sampling.distribution(.)</strong>. Here, we pick a new set of samples from our original dataset; but the sample is a weighted sample based on the calculated distribution.</p>

<div class="sourceCode" id="cb1541"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1541-1" data-line-number="1">sampling.distribution &lt;-<span class="st"> </span><span class="cf">function</span>(sample.set, sample.weight, <span class="dt">seed=</span><span class="dv">142</span>) {</a>
<a class="sourceLine" id="cb1541-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">nrow</span>(sample.set)</a>
<a class="sourceLine" id="cb1541-3" data-line-number="3">  indices =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1541-4" data-line-number="4">  cumulative.weight =<span class="st"> </span><span class="kw">cumsum</span>(sample.weight) <span class="co"># Distribution</span></a>
<a class="sourceLine" id="cb1541-5" data-line-number="5">  <span class="kw">set.seed</span>(seed)</a>
<a class="sourceLine" id="cb1541-6" data-line-number="6">  random.number =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>n, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>)   </a>
<a class="sourceLine" id="cb1541-7" data-line-number="7">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1541-8" data-line-number="8">    indices[i] =<span class="st"> </span><span class="kw">which</span>(random.number[i] <span class="op">&lt;</span><span class="st"> </span>cumulative.weight)[<span class="dv">1</span>] </a>
<a class="sourceLine" id="cb1541-9" data-line-number="9">  }</a>
<a class="sourceLine" id="cb1541-10" data-line-number="10">  sample.set[indices,]</a>
<a class="sourceLine" id="cb1541-11" data-line-number="11">}</a></code></pre></div>

<p>Our sample weight, denoted as <strong>w</strong>, is used eventually to construct our distribution based on the cumulative weight with which we sample our original train set for the next fit.</p>

<div class="sourceCode" id="cb1542"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1542-1" data-line-number="1">sample.set =<span class="st"> </span>train</a>
<a class="sourceLine" id="cb1542-2" data-line-number="2">n =<span class="st"> </span><span class="kw">nrow</span>(sample.set); w =<span class="st"> </span><span class="dv">1</span><span class="op">/</span>n</a>
<a class="sourceLine" id="cb1542-3" data-line-number="3">sample.weight =<span class="st"> </span><span class="kw">rep</span>(w, n)</a>
<a class="sourceLine" id="cb1542-4" data-line-number="4">sampled.set =<span class="st"> </span><span class="kw">cbind</span>(sample.set, sample.weight)</a></code></pre></div>
<pre><code>##   Sepal.Len Sepa.Width Petal.Len Petal.Width Species Sampl.Weight
## 1       5.1        3.5       1.4         0.2  setosa     0.007407
## 2       4.9        3.0       1.4         0.2  setosa     0.007407
## 3       4.7        3.2       1.3         0.2  setosa     0.007407
## 4       4.6        3.1       1.5         0.2  setosa     0.007407
## 5       5.0        3.6       1.4         0.2  setosa     0.007407
## 6       5.4        3.9       1.7         0.4  setosa     0.007407</code></pre>

<p><strong>Fourth</strong>, calculate the <strong>pseudo-loss</strong> of our hypothesis, namely <span class="math inline">\(\mathbf{h^{(t)}}\)</span>. For <strong>AdaBoost.M2</strong>, we have:</p>
<p><span class="math display" id="eq:equate1120145">\[\begin{align}
\epsilon^{(m)} = \frac{1}{2} \sum_{(i,y) \in B}^n D^{(m)}_{(i,y)} \left(1 - h^{(m)}(x_i, y_i) + h^{(m)}(x_i, y)\right)  \tag{10.161} 
\end{align}\]</span></p>
<p>For <strong>SAMME</strong>, we have:</p>
<p><span class="math display" id="eq:equate1120146">\[\begin{align}
\epsilon^{(m)} = 
    \sum_{i=1}^n w^{(m)}_i \mathbf{1}\{h^{(m)}(x_i) \ne y_i \} / \sum_{i=1}^n w_i^{(m)}  \tag{10.162} 
\end{align}\]</span></p>
<p><strong>Fifth</strong>, calculate the contribution (or power of say). For <strong>AdaBoost.M2</strong>, we have:</p>
<p><span class="math display" id="eq:equate1120147">\[\begin{align}
\beta^{(m)} =   \epsilon^{(m)}/(1 - \epsilon^{(m)})  \tag{10.163} 
\end{align}\]</span></p>
<p>For <strong>SAMME</strong>, we have:</p>
<p><span class="math display" id="eq:equate1120148">\[\begin{align}
\beta^{(m)} = 
     \frac{1}{2} \log_e \frac{1 - \epsilon^{(m)}}{\epsilon^{(m)}} + \log_e(K-1) \tag{10.164} 
\end{align}\]</span></p>
<p><strong>Sixth</strong>, we now update the weights. For <strong>AdaBoost.M2</strong>, we have:</p>
<p><span class="math display" id="eq:equate1120150" id="eq:equate1120149">\[\begin{align}
\alpha^{(m)} = (1/2)  \left(1 + h^{(m)}(x_i, y_i) - h^{(m)}(x_i, y)\right) \tag{10.165} \\
D^{(m+1)}_{(i,y)} \leftarrow D^{(m)}_{(i,y)} \cdot (\beta^{(m)})^{\alpha^{(m)}}, i=1,2,...,n \tag{10.166} 
\end{align}\]</span></p>
<p>For <strong>SAMME</strong>, we have:</p>
<p><span class="math display" id="eq:equate1120152" id="eq:equate1120151">\[\begin{align}
\alpha^{(m)} = \beta^{(m)} \cdot \mathbf{1}\{h^{(m)}(x_i) \ne y_i \})  \tag{10.167} \\
w^{(m+1)}_i \leftarrow (w^{(m)}_i)^{\alpha^{(m)}}, i=1,2,...,n \tag{10.168} 
\end{align}\]</span></p>
<p><strong>Finally</strong>, we calculate our prediction output. For <strong>AdaBoost.M2</strong>, we have:</p>
<p><span class="math display" id="eq:equate1120153">\[\begin{align}
H(\mathbf{x}) = \text{arg}\ \underset{y \in Y}{\text{max}}\left(\sum_{m=1}^M \log_e \frac{1}{\beta^{(m)}}h^{(m)}(x, y)\right) \tag{10.169} 
\end{align}\]</span></p>
<p>For <strong>SAMME</strong>, we have:</p>
<p><span class="math display" id="eq:equate1120154">\[\begin{align}
H(\mathbf{x}) = arg\ \underset{k}{max}\left(\sum_{m=1}^M \beta^{(m)} \cdot \mathbf{1}\{h^{(m)}(x_i) = k \}\right) \tag{10.170} 
\end{align}\]</span></p>
<p>Now, for illustration, let us use <strong>SAMME</strong> as our algorithm for an example implementation of multi-classification.</p>

<div class="sourceCode" id="cb1544"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1544-1" data-line-number="1">my.adaboost.SAMME &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, D, <span class="dt">estimators =</span> <span class="dv">100</span>) {</a>
<a class="sourceLine" id="cb1544-2" data-line-number="2">    n        =<span class="st"> </span><span class="kw">nrow</span>(D)</a>
<a class="sourceLine" id="cb1544-3" data-line-number="3">    w        =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span>n, n)</a>
<a class="sourceLine" id="cb1544-4" data-line-number="4">    levs     =<span class="st"> </span><span class="kw">levels</span>(D[,<span class="kw">c</span>(target)])</a>
<a class="sourceLine" id="cb1544-5" data-line-number="5">    K        =<span class="st"> </span><span class="kw">length</span>(levs)</a>
<a class="sourceLine" id="cb1544-6" data-line-number="6">    K.log    =<span class="st"> </span><span class="kw">log</span>(K <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, <span class="kw">exp</span>(<span class="dv">1</span>)) <span class="co"># 2.718282</span></a>
<a class="sourceLine" id="cb1544-7" data-line-number="7">    model =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1544-8" data-line-number="8">    <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>estimators) {</a>
<a class="sourceLine" id="cb1544-9" data-line-number="9">        sample.set =<span class="st"> </span><span class="kw">sampling.distribution</span>(D, w, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1544-10" data-line-number="10">        x =<span class="st"> </span>sample.set[, <span class="kw">c</span>(features)]</a>
<a class="sourceLine" id="cb1544-11" data-line-number="11">        y =<span class="st"> </span><span class="kw">as.character</span>(sample.set[, <span class="kw">c</span>(target)])</a>
<a class="sourceLine" id="cb1544-12" data-line-number="12">        h.model =<span class="st"> </span><span class="kw">h.learner</span>(features, target, sample.set, </a>
<a class="sourceLine" id="cb1544-13" data-line-number="13">                            <span class="dt">minbucket=</span><span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1544-14" data-line-number="14">        h.pred  =<span class="st"> </span><span class="kw">h.score</span>(h.model, x, y)</a>
<a class="sourceLine" id="cb1544-15" data-line-number="15">        errors  =<span class="st"> </span>(h.pred<span class="op">$</span>fitted.values <span class="op">!=</span><span class="st"> </span>y) <span class="op">*</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1544-16" data-line-number="16">        epsilon =<span class="st"> </span><span class="kw">sum</span>(w <span class="op">*</span><span class="st"> </span>errors) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(w)</a>
<a class="sourceLine" id="cb1544-17" data-line-number="17">        beta    =<span class="st"> </span><span class="kw">log</span>( (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>epsilon) <span class="op">/</span><span class="st"> </span>epsilon, <span class="kw">exp</span>(<span class="dv">1</span>)) <span class="op">*</span><span class="st"> </span>K.log</a>
<a class="sourceLine" id="cb1544-18" data-line-number="18">        w       =<span class="st"> </span>w<span class="op">^</span>(beta <span class="op">*</span><span class="st"> </span>errors)</a>
<a class="sourceLine" id="cb1544-19" data-line-number="19">        w       =<span class="st"> </span>w <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(w) <span class="co"># normalize  </span></a>
<a class="sourceLine" id="cb1544-20" data-line-number="20">        model[[m]] =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;learner&quot;</span> =<span class="st"> </span>h.model, <span class="st">&quot;beta&quot;</span> =<span class="st"> </span>beta)</a>
<a class="sourceLine" id="cb1544-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb1544-22" data-line-number="22">    <span class="kw">list</span>(<span class="st">&quot;levs&quot;</span> =<span class="st"> </span>levs, <span class="st">&quot;model&quot;</span> =<span class="st"> </span>model)</a>
<a class="sourceLine" id="cb1544-23" data-line-number="23">}</a></code></pre></div>
<div class="sourceCode" id="cb1545"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1545-1" data-line-number="1">features =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1545-2" data-line-number="2">target   =<span class="st"> &quot;Species&quot;</span></a>
<a class="sourceLine" id="cb1545-3" data-line-number="3">my.model =<span class="st"> </span><span class="kw">my.adaboost.SAMME</span>(features, target, train, <span class="dt">estimators =</span> <span class="dv">5</span>)</a></code></pre></div>

<p>Let us also write our example implementation of a prediction function using <strong>SAMME</strong>.</p>

<div class="sourceCode" id="cb1546"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1546-1" data-line-number="1">H =<span class="st"> </span>my.predict.SAMME &lt;-<span class="st"> </span><span class="cf">function</span>(models, x, y) {</a>
<a class="sourceLine" id="cb1546-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1546-3" data-line-number="3">  y.max  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(y))</a>
<a class="sourceLine" id="cb1546-4" data-line-number="4">  levs   =<span class="st"> </span>models<span class="op">$</span>levs</a>
<a class="sourceLine" id="cb1546-5" data-line-number="5">  K      =<span class="st"> </span><span class="kw">length</span>(levs)</a>
<a class="sourceLine" id="cb1546-6" data-line-number="6">  class  =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1546-7" data-line-number="7">  <span class="cf">for</span> (model <span class="cf">in</span> models<span class="op">$</span>model) {</a>
<a class="sourceLine" id="cb1546-8" data-line-number="8">    pred =<span class="st"> </span><span class="kw">h.score</span>(model<span class="op">$</span>learner, x, y)</a>
<a class="sourceLine" id="cb1546-9" data-line-number="9">    beta.I =<span class="st"> </span><span class="kw">ifelse</span>(pred<span class="op">$</span>result <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, model<span class="op">$</span>beta, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1546-10" data-line-number="10">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1546-11" data-line-number="11">      h =<span class="st"> </span>(levs[k] <span class="op">==</span><span class="st"> </span>pred<span class="op">$</span>fitted.values)</a>
<a class="sourceLine" id="cb1546-12" data-line-number="12">      class[,k] =<span class="st"> </span>class[,k] <span class="op">+</span><span class="st">  </span>beta.I <span class="op">*</span><span class="st"> </span>h </a>
<a class="sourceLine" id="cb1546-13" data-line-number="13">    }</a>
<a class="sourceLine" id="cb1546-14" data-line-number="14">  }</a>
<a class="sourceLine" id="cb1546-15" data-line-number="15">  y.pred =<span class="st"> </span><span class="kw">apply</span>(class, <span class="dv">1</span>, which.max)</a>
<a class="sourceLine" id="cb1546-16" data-line-number="16">  <span class="kw">list</span>(<span class="st">&quot;y.hat&quot;</span> =<span class="st"> </span>levs[y.pred], <span class="st">&quot;classes&quot;</span> =<span class="st"> </span>levs)</a>
<a class="sourceLine" id="cb1546-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb1546-18" data-line-number="18">(<span class="dt">predicted =</span> <span class="kw">H</span>(my.model, test, test.class))</a></code></pre></div>
<pre><code>## $y.hat
##  [1] &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;    
##  [6] &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot;
## [11] &quot;virginica&quot;  &quot;virginica&quot;  &quot;virginica&quot;  &quot;virginica&quot;  &quot;virginica&quot; 
## 
## $classes
## [1] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;</code></pre>

<p>Then, to evaluate using <strong>confusion matrix</strong>, we have the following implementation:</p>

<div class="sourceCode" id="cb1548"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1548-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb1548-2" data-line-number="2">my.evaluate.SAMME &lt;-<span class="st"> </span><span class="cf">function</span>(target, predicted) {</a>
<a class="sourceLine" id="cb1548-3" data-line-number="3">   y.hat         =<span class="st"> </span><span class="kw">as.factor</span>(predicted<span class="op">$</span>y.hat)</a>
<a class="sourceLine" id="cb1548-4" data-line-number="4">   <span class="kw">levels</span>(y.hat) =<span class="st"> </span>predicted<span class="op">$</span>classes</a>
<a class="sourceLine" id="cb1548-5" data-line-number="5">   conf.tab      =<span class="st"> </span><span class="kw">table</span>(y.hat, <span class="kw">as.factor</span>(target))</a>
<a class="sourceLine" id="cb1548-6" data-line-number="6">   accuracy      =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">diag</span>(conf.tab)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(conf.tab)</a>
<a class="sourceLine" id="cb1548-7" data-line-number="7">   <span class="kw">list</span>(<span class="st">&quot;conf.table&quot;</span> =<span class="st"> </span>conf.tab, <span class="st">&quot;accuracy&quot;</span> =<span class="st"> </span><span class="kw">round</span>(accuracy <span class="op">*</span><span class="st"> </span><span class="dv">100</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1548-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb1548-9" data-line-number="9">(<span class="dt">my.outcome =</span> <span class="kw">my.evaluate.SAMME</span>(test.class, predicted))</a></code></pre></div>
<pre><code>## $conf.table
##             
## y.hat        setosa versicolor virginica
##   setosa          5          0         0
##   versicolor      0          5         0
##   virginica       0          0         5
## 
## $accuracy
## [1] 100</code></pre>

<p>where <strong>row-vectors</strong> are the predictions and <strong>column-vectors</strong> are the reference.</p>
<p>Our metrics are based on using a <strong>confusion matrix</strong> to derive our calculation for <strong>accuracy</strong> per class in a multi-classification setting. A multiclass <strong>accuracy</strong> is formulated below:</p>

<p><span class="math display" id="eq:equate1120155">\[\begin{align}
Accuracy_{(avg)} = \frac{1}{K} \sum_{k=1}^K \frac{TP_k + TN_k}{TP_k + TN_k + FP_k + FN_k} 
\ \ \ \ \ \text{where K is the number of classes}  \tag{10.171} 
\end{align}\]</span>
</p>
<p>The formula is calculated based on the <strong>confusion matrix</strong> above. Also, a simple way to calculate <strong>accuracy</strong> using a matrix is shown below:</p>
<div class="sourceCode" id="cb1550"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1550-1" data-line-number="1">cmat =<span class="st"> </span>my.outcome<span class="op">$</span>conf.table</a>
<a class="sourceLine" id="cb1550-2" data-line-number="2">accuracy =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">diag</span>(cmat)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(cmat)</a>
<a class="sourceLine" id="cb1550-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;accuracy&quot;</span> =<span class="st"> </span><span class="kw">round</span>(accuracy <span class="op">*</span><span class="st"> </span><span class="dv">100</span>, <span class="dv">2</span>), <span class="st">&quot;error&quot;</span> =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>accuracy)</a></code></pre></div>
<pre><code>## accuracy    error 
##      100        0</code></pre>
<p>Note that the accuracy above may seem too accurate if it is 100%. Overfitting can be avoided using regularization.</p>
<p>Other score metrics to consider for evaluation include <strong>precision</strong> and <strong>recall</strong>.</p>
<div class="sourceCode" id="cb1552"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1552-1" data-line-number="1">precision =<span class="st"> </span><span class="kw">diag</span>(cmat) <span class="op">/</span><span class="st"> </span><span class="kw">rowSums</span>(cmat)</a>
<a class="sourceLine" id="cb1552-2" data-line-number="2">recall    =<span class="st"> </span><span class="kw">diag</span>(cmat) <span class="op">/</span><span class="st"> </span><span class="kw">colSums</span>(cmat)</a>
<a class="sourceLine" id="cb1552-3" data-line-number="3"><span class="kw">rbind</span>(precision, recall)</a></code></pre></div>
<pre><code>##           setosa versicolor virginica
## precision      1          1         1
## recall         1          1         1</code></pre>
<p>As an exercise, we leave readers to play around with different random seeds for the random resampling, namely sampling.distribution(.), and see how it affects prediction performance.</p>
<p>Under the <strong>Regression</strong> Section for <strong>AdaBoost</strong>, the choice of sampling set is based on <strong>resampling</strong>. Here, we demonstrate the use of <strong>re-weighing</strong>.</p>
</div>
<div id="logitboost-j-classes" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.6</span> LogitBoost (J Classes)<a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us extend our discussion of <strong>Logistic Regression</strong> to cover <strong>Multi Classification</strong>. Here, we introduce <strong>LogitBoost</strong> as an <strong>ensemble method</strong> using <strong>Logistic Regression</strong> for <strong>Multi Classification</strong>. We tailor our discussion based on the <strong>LogitBoost (J classes)</strong> algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani <span class="citation">(<a href="bibliography.html#ref-ref685p">2012</a>)</span> - note here that we use <strong>K</strong> for the number of classes.</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,K\}\}_{i=1}^n\\
\ \ \ \text{number of machines}: M \\
\mathbf{Algorithm}:\\
\ \ \ F_{0,k}(x_i) = 0, P_{0,k}(x_i) = 1/K\ for\ k=1,...,K\ and\ i=1,...,n\ \ \ &amp;\text{(initialize)} \\
\ \ \ \text{loop}\ m\ in\ 1:\ M \\ 
\ \ \ \ \ \ \text{loop}\ k\ in\ 1:\ K \\
\ \ \ \ \ \ \ \ \ \ \ \ \text{Compute working responses and weights}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ z_{ik} = \frac{y_{ik} - P_{m-1,k}(x_i)}{P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) )} &amp; \text{i = 1,...,n}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ w_{ik} = P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) ) &amp; \text{i = 1,...,n}\\
\ \ \ \ \ \ \ \ \ \ \ \ \text{Fit model using } f_{m,k}(\{x_i,z_{ik},  w_{ik}\}_{i=1}^n)\ \text{by a weighted} &amp; \text{i = 1,...,n}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{least-squares regression of}\ z_{ik}\ \text{to}\ x_i\ \text{with weights}\ w_{ik}\\
\ \ \ \ \ \ \ \text{end loop} \\
\ \ \ \ \ \ \ \text{Set}\ f_{m,k}(x_i)\ = \frac{K-1}{K} \left(f_{m-1,k}(x_i) - \frac{1}{K}\sum_{l=1}^K f_{m-1,l}(x_i)\right) &amp;   \text{i = 1,...,n}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ F_{m,k}(x_i)\ = F_{m-1,k}(x_i) + f_{m,k}(x_i) &amp;   \text{i = 1,...,n}\\
\ \ \ \ \ \ \ P_{m,k}(x_i) = exp(F_{m,k}(x_i))/\sum_{l=1}^K exp(F_{m,l}(x_i))
  &amp;  \text{i = 1,...,n}\\
\ \ \ \text{end loop}\\
\ \ \ \text{Output}\ \text{arg}\ \underset{k}{\text{max}}\ F_{M,k}(x)
\end{array}
\]</span></p>
<p>The algorithm is based on <strong>Newton method</strong> for optimization. An alternative method called <strong>Gradient Descent</strong> is covered in the next section under <strong>Gradient Boost</strong>.</p>
<p>Here, we work on the basis of probabilistic estimates expressed like so:</p>
<p><span class="math display" id="eq:equate1120156">\[\begin{align}
\hat{p}_k = P(y_i=k|x_i) = \frac{exp(F_k(x_i))}{1 + exp(F_k(x_i))}, \tag{10.172} 
\end{align}\]</span></p>
<p>so that given a dataset, namely <span class="math inline">\(\{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,K\}\}_{i=1}^n\)</span> where <strong>K</strong> is the number of classes, our goal is to compute the probability of each of the <strong>K</strong> classes, e.g. <span class="math inline">\(\hat{p}_k \in \mathbb{R}^k\)</span>, such that one of them is the most likely to be the <strong>k</strong> class for <span class="math inline">\(y_i\)</span>. That is expressed as such:</p>
<p><span class="math display" id="eq:equate1120157">\[\begin{align}
\hat{y}_i|x_i = \text{arg}\ \underset{k}{\text{max}}\ \hat{p}_k(x_i)
\ \ \ \ \ where\ \sum_{k=1}^K \hat{p}_k = 1 \tag{10.173} 
\end{align}\]</span></p>
<p>For the <strong>LogitBoost Loss Function</strong>, we use <strong>negative log likelihood (NLL)</strong>:</p>
<p><span class="math display" id="eq:equate1120158">\[\begin{align}
\mathcal{L(y, F(x))} = \sum_{i=1}^n \left\{ - \sum_{k=1}^K y_{ik}\ \log_e \hat{p}_{k}(x_i)\right\} \tag{10.174} 
\end{align}\]</span></p>
<p>We minimize the <strong>Loss Function</strong> by taking the 1st (Jacobian) and second (Hessian) derivatives of the loss function with respect to <span class="math inline">\(\mathbf{F_k(x_i)}\)</span>.</p>
<p><span class="math display" id="eq:equate1120160" id="eq:equate1120159">\[\begin{align}
\underbrace{\nabla_{F_{m,l}(x_i)} Lik(\{y_{il}, F_{m,l}(x_i)\}^K_{l=1})}_{\text{negative gradient}} &amp;= -\left[\frac{\partial Lik(\{y_{il}, F_{m,l}(x_i)\}^K_{l=1})}{\partial F_{m,l}(x_i)}\right]_{\{F_{m,l}(x) = F_{m-1,l}(x)\}^K_{l=1}}  \tag{10.175} \\
&amp;= y_{ik} - P_{m-1,k}(x_i) \tag{10.176} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1120162" id="eq:equate1120161">\[\begin{align}
\underbrace{\nabla_{F_{m,l}(x_i)}^2 Lik(\{y_{il}, F_{m,l}(x_i)\}^K_{l=1})}_{\text{negative gradient}} &amp;= -\left[\frac{\partial^2 Lik(\{y_{il}, F_{m,l}(x_i)\}^K_{l=1})}{\partial F_{m,l}(x_i)^2}\right]_{\{F_{m,l}(x) = F_{m-1,l}(x)\}^K_{l=1}}  \tag{10.177} \\
&amp;= P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) ) \tag{10.178} 
\end{align}\]</span></p>
<p>Then we formulate our <strong>working response</strong>, namely <span class="math inline">\(\mathbf{z_{ik}}\)</span>, and <strong>weight</strong>, namely <span class="math inline">\(\mathbf{w_{ik}}\)</span>, based on the result of the derivatives:</p>
<p><span class="math display" id="eq:equate1120163">\[\begin{align}
z_{ik} = \frac{y_{ik} - P_{m-1,k}(x_i)}{P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) )}
\ \ \ \ \ \ \ \ \ \ \ \
w_{ik} = P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) ) \tag{10.179} 
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(w_{ik}\)</span> can become numerically unstable as it approaches zero. In our example implementation below, our quick patch is to apply the following condition:</p>
<p><span class="math display">\[\begin{align*}
w_{ik}\ \ \ \rightarrow
\begin{cases}
\text{1e-15}  &amp; if\ w_{ik} = 0\\
w_{ik} &amp; if\ w_{ik} \ne 0
\end{cases}
\end{align*}\]</span></p>
<p>Finally, <strong>LogitBoost</strong> is a greedy stage-wise additive model such that in terms of our base learner, namely <span class="math inline">\(\mathbf{f_{k,m}(x_i)}\)</span>, we perform the following expressions below iteratively:</p>
<p><span class="math display" id="eq:equate1120165" id="eq:equate1120164">\[\begin{align}
f_{m,k}(x_i)\ &amp;= &amp;\frac{K-1}{K} \left(f_{m-1,k}(x_i) - \frac{1}{K}\sum_{l=1}^K f_{m-1,l}(x_i)\right) \tag{10.180} \\
F_{m,k}(x_i) &amp;= &amp;F_{m-1,k}(x_i) + f_{m,k}(x_i)  \tag{10.181} 
\end{align}\]</span></p>
<p>Below is an example implementation of <strong>LogitBoost (J classes)</strong>. We use <strong>rpart(.)</strong> for the fit - a regression tree for the base learner.</p>

<div class="sourceCode" id="cb1554"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1554-1" data-line-number="1"><span class="kw">library</span>(rpart)</a>
<a class="sourceLine" id="cb1554-2" data-line-number="2">my.logitboost &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, data, <span class="dt">machines=</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb1554-3" data-line-number="3">  x       =<span class="st"> </span><span class="kw">as.matrix</span>(data[,<span class="kw">c</span>(features)])</a>
<a class="sourceLine" id="cb1554-4" data-line-number="4">  y       =<span class="st"> </span>data[,<span class="kw">c</span>(target)]</a>
<a class="sourceLine" id="cb1554-5" data-line-number="5">  n       =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1554-6" data-line-number="6">  M       =<span class="st"> </span>machines</a>
<a class="sourceLine" id="cb1554-7" data-line-number="7">  classes =<span class="kw">levels</span>(y)</a>
<a class="sourceLine" id="cb1554-8" data-line-number="8">  K       =<span class="st"> </span><span class="kw">length</span>(classes)</a>
<a class="sourceLine" id="cb1554-9" data-line-number="9">  F_k     =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1554-10" data-line-number="10">  p_k     =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span><span class="op">/</span>K, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1554-11" data-line-number="11">  y_ik    =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1554-12" data-line-number="12">  h       =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1554-13" data-line-number="13">  cntrl =<span class="st"> </span><span class="kw">rpart.control</span>(<span class="dt">cp=</span><span class="dv">0</span>, <span class="dt">maxdepth=</span><span class="dv">1</span>, <span class="dt">minsplit=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1554-14" data-line-number="14">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {   y_ik[,k]   =<span class="st"> </span><span class="kw">as.numeric</span>(y <span class="op">==</span><span class="st"> </span>classes[k]) }</a>
<a class="sourceLine" id="cb1554-15" data-line-number="15">  <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M) {</a>
<a class="sourceLine" id="cb1554-16" data-line-number="16">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1554-17" data-line-number="17">       w_ik    =<span class="st"> </span>( p_k <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p_k))</a>
<a class="sourceLine" id="cb1554-18" data-line-number="18">       z_ik    =<span class="st"> </span>(y_ik <span class="op">-</span><span class="st"> </span>p_k) <span class="op">/</span><span class="st"> </span><span class="kw">ifelse</span>( w_ik <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, w_ik, <span class="fl">1e-15</span>)</a>
<a class="sourceLine" id="cb1554-19" data-line-number="19">       f_model =<span class="st"> </span><span class="kw">rpart</span>(z_ik[,k] <span class="op">~</span><span class="st"> </span>x, <span class="dt">weights =</span> w_ik[,k], <span class="dt">control =</span> cntrl)</a>
<a class="sourceLine" id="cb1554-20" data-line-number="20">       h[,k]   =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(f_model)</a>
<a class="sourceLine" id="cb1554-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb1554-22" data-line-number="22">    f_mk  =<span class="st"> </span>(K<span class="dv">-1</span>)<span class="op">/</span>K <span class="op">*</span><span class="st"> </span>( h <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>K <span class="op">*</span><span class="st"> </span><span class="kw">apply</span>(h, <span class="dv">1</span>, sum))</a>
<a class="sourceLine" id="cb1554-23" data-line-number="23">    F_k  =<span class="st"> </span>F_k <span class="op">+</span><span class="st">  </span>f_mk</a>
<a class="sourceLine" id="cb1554-24" data-line-number="24">    p_k =<span class="st"> </span><span class="kw">exp</span>(F_k) <span class="op">/</span><span class="st"> </span><span class="kw">apply</span>(<span class="kw">exp</span>(F_k), <span class="dv">1</span>, sum) </a>
<a class="sourceLine" id="cb1554-25" data-line-number="25">  }</a>
<a class="sourceLine" id="cb1554-26" data-line-number="26">  <span class="kw">apply</span>(F_k, <span class="dv">1</span>, which.max)</a>
<a class="sourceLine" id="cb1554-27" data-line-number="27">}</a></code></pre></div>

<p>Let us use the implementation and compare the difference between the actual target values and the fitted values generated by <strong>logitBoost</strong> using m equal to 1, 5, and 10.</p>

<div class="sourceCode" id="cb1555"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1555-1" data-line-number="1">features =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1555-2" data-line-number="2">target   =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)</a>
<a class="sourceLine" id="cb1555-3" data-line-number="3">y        =<span class="st"> </span><span class="kw">as.numeric</span>(train[,<span class="kw">c</span>(target)])</a>
<a class="sourceLine" id="cb1555-4" data-line-number="4">fitted.values =<span class="st"> </span><span class="kw">my.logitboost</span>(features, target, train, <span class="dt">machines=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1555-5" data-line-number="5"><span class="kw">all.equal</span>(y, fitted.values)</a></code></pre></div>
<pre><code>## [1] &quot;Mean relative difference: 0.3529&quot;</code></pre>
<div class="sourceCode" id="cb1557"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1557-1" data-line-number="1">fitted.values =<span class="st"> </span><span class="kw">my.logitboost</span>(features, target, train, <span class="dt">machines=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb1557-2" data-line-number="2"><span class="kw">all.equal</span>(y, fitted.values)</a></code></pre></div>
<pre><code>## [1] &quot;Mean relative difference: 0.5&quot;</code></pre>
<div class="sourceCode" id="cb1559"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1559-1" data-line-number="1">fitted.values =<span class="st"> </span><span class="kw">my.logitboost</span>(features, target, train, <span class="dt">machines=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb1559-2" data-line-number="2"><span class="kw">all.equal</span>(y, fitted.values)</a></code></pre></div>
<pre><code>## [1] &quot;Mean relative difference: 0.3333&quot;</code></pre>

<p>As an experiment, we leave readers to tune maxdepth and maxsplit. Moreover, we leave readers to modify the implementation to preserve the <strong>weak learner models</strong>, e.g., <strong>f_model</strong>, and use them to predict using the test set. From there, we use <strong>Confusion Matrix</strong> to evaluate the performance of the <strong>logitBoost</strong> ensemble itself.</p>
<p>There are other variants of <strong>LogitBoost</strong> that address numerical stability. One is called <strong>Adaptive Base Class (ABC) LogitBoost</strong> formulated by Ping Li (from Cornell University). We leave readers to investigate <strong>ABC-LogitBoost</strong> and its application to <strong>image</strong> classification <span class="citation">(Ping Li <a href="bibliography.html#ref-ref685p">2012</a>)</span>.</p>
</div>
<div id="gradient-boost-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.7</span> Gradient Boost <a href="10.3-multi-class-classification-supervised.html#gradient-boost-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We continue to extend our concept of <strong>Boosting</strong> by using <strong>Gradient Boost</strong>. A detailed intuitive view of the concept of <strong>Gradient Boosting</strong> is covered under the <strong>Regression</strong> Section. The same intuition applies to <strong>Classification</strong>.</p>
<p>Recall in <strong>Gradient Boosting for Regression</strong> that we have the following <strong>square error</strong> in our <strong>loss function</strong>:</p>
<p><span class="math display">\[
\underbrace{Lik(y, F(x))}_{\text{loss function}} 
= \frac{1}{2}\left(y -\hat{y}\right)^2 \ \
and\ \ \underbrace{F(x) = \hat{y}}_{\text{y-hat}}.
\]</span></p>
<p>In this section, we discuss <strong>K-class Logistic Gradient Boost</strong> introduced by Jerome Friedman <span class="citation">(<a href="bibliography.html#ref-ref670j">1999</a><a href="bibliography.html#ref-ref670j">a</a>, <a href="bibliography.html#ref-ref677j">2001</a>, <a href="bibliography.html#ref-ref679j">2002</a>)</span> in the context of <strong>Multi-Classification</strong>. Here, we start with our <strong>loss function</strong> based on <strong>cross-entropy</strong> in the form below where we measure the <strong>loss</strong> for each <strong>k</strong> class - hence, it is notable to point out the notation: <span class="math inline">\(\{y_k, F_k(x)\}^K_{k=1}\)</span>. Our <strong>loss function</strong> is thus written as:</p>
<p><span class="math display" id="eq:equate1120166">\[\begin{align}
\underbrace{Lik(\{y_k, F_k(x)\}^K_{k=1})}_{\text{loss function}} 
= - \sum_{k=1}^K y_k\log_e P_k(x)  \tag{10.182} 
\end{align}\]</span></p>
<p>where <strong>K</strong> is the number of classes and <span class="math inline">\(P_k\)</span> is a <strong>softmax</strong> function expressed as:</p>
<p><span class="math display" id="eq:equate1120167">\[\begin{align}
\underbrace{P_k(x) = \frac{e^{o_k}}{\sum_{l=1}^K e^{o_l}}}_{\text{softmax}}
\ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ 
F_{(k\ or\ l)}(x) = \underbrace{o_{(k\ or\ l)} = \frac{P(y_{(k\ or\ l)} = 1|x)}{P (y_{(k\ or\ l)} = 0|x)}}_{\text{log odds}} \tag{10.183} 
\end{align}\]</span></p>
<p>Expanding our <strong>loss function</strong>, we get:</p>

<p><span class="math display" id="eq:eqnnumber419">\[\begin{align}
\underbrace{Lik(\{y_k, F_k(x)\}^K_1)}_{\text{loss function}} 
= -  \sum_{k=1}^K 
y_k \log_e \frac{e^{o_k}}{\sum_{l=1}^K e^{o_l}}
=  \sum_{i=1}^n \underbrace{ \left(-\sum_{k=1}^K 
\underbrace{\mathbf{1}\{class = k\}}_{y_k} \log_e \underbrace{ \frac{e^{o_k}}{\sum_{l=1}^K e^{o_l}}}_{\text{softmax}}\right)}_{\begin{array}{c}\text{negative log-likelihood loss}\\ \text{(cross-entropy)}\end{array}}   \tag{10.184}
\end{align}\]</span>
</p>
<p>Note that our <strong>log-odds</strong> undergo the following logistic transformation in reference to another <strong>Boosting method</strong> called <strong>LogitBoost</strong>, which uses <strong>Quasi-Newton (or Newton Raphson)</strong> method along with <strong>Hessian</strong> calculation <span class="citation">(Friedman J. <a href="bibliography.html#ref-ref670j">1999</a><a href="bibliography.html#ref-ref670j">a</a>)</span>. The multinomial log-likelihood loss is generalized into <strong>LogitBoost</strong> for multi-classification. </p>
<p><span class="math display" id="eq:equate1120168">\[\begin{align}
F_k(x) = \log_e p_k(x) - \frac{1}{K}\sum_{l=1}^K \log_e p_l(x) \tag{10.185} 
\end{align}\]</span></p>
<p>Using the <strong>loss function</strong>, we derive its <strong>negative gradient</strong>. Let us first take the derivative of our <strong>softmax</strong> with respect to <strong>log odds</strong>.</p>
<p><span class="math display" id="eq:equate1120170" id="eq:equate1120169">\[\begin{align}
\text{if }i=j\ \ \ \ \rightarrow
\frac{\partial P_i}{\partial o_k} = 
\frac{\partial \frac{e^{o_k}}{\sum_{j=1}^K e^{o_j}}}{\partial o_k} &amp;= 
   \frac{e^{o_k} \left(\sum_{k=1}^K e^{o_k} - e^{o_j} \right)}{(\sum_{k=1}^K)^2} =
   P_i \left(1 - P_j\right) \tag{10.186} \\
\text{if }i\ne j\ \ \ \ \rightarrow
\frac{\partial P_i}{\partial o_k} = 
\frac{\partial \frac{e^{o_k}}{\sum_{j=1}^K e^{o_j}}}{\partial o_k} &amp;=  
\frac{0 - e^o{_j} e^{o_i}}{\left(\sum_{k-1}^K e^{o_k}\right)} = - P_j P_i  \tag{10.187} 
\end{align}\]</span></p>
<p>therefore:</p>
<p><span class="math display" id="eq:eqnnumber420">\[\begin{align}
\frac{\partial P_i}{\partial o_k} =
\begin{cases} 
P_i( 1 - P_i) &amp; if\ i = j\\
- P_i P_j &amp; if\ i \ne j
\end{cases}  \tag{10.188}
\end{align}\]</span></p>
<p>We then take the <strong>negative derivative</strong> of our <strong>loss function</strong> with respect to <strong>log-odds</strong>.</p>
<p><span class="math display" id="eq:equate1120175" id="eq:equate1120174" id="eq:equate1120173" id="eq:equate1120172" id="eq:equate1120171">\[\begin{align}
r_{ik} = \underbrace{\nabla_{F_l(x_i)} Lik(\{y_{il}, F_l(x_i)\}^K_{l=1})}_{\text{negative gradient}} &amp;= -\left[\frac{\partial Lik(\{y_{il}, F_l(x_i)\}^K_{l=1})}{\partial F_l(x_i)}\right]_{\{F_l(x) = F_{l,m-1}(x)\}^K_{l=1}}  \tag{10.189} \\
&amp;= -\sum_{k=1}^K y_k(x) \left(\frac{ \partial \log_e P_k(x) }{\partial o_k}\right) \tag{10.190} \\
&amp;= - y_i ( 1- P_i) - \sum_{k\ne i}  y_k \frac{1}{P_k} ( - P_i P_j  ) \tag{10.191} \\
&amp;= y_{ik} - P \left(\sum_k y_k\right)\ \ \ \ \leftarrow\ \left(\sum_k y_k\right)  = 1 \tag{10.192} \\
&amp;= y_{ik} - P_{k,m-1}(x_i)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{(residual)} \tag{10.193} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(F_{l,m-1}(x_i) \equiv F_{l,0}(x_i) \equiv \hat{y}_l \equiv \gamma_l\)</span>.</p>
<p>Let us implement the <strong>log-odds function</strong> and <strong>softmax function</strong>. We perform a logistic transform, a.l.a <strong>softmax</strong>:</p>

<div class="sourceCode" id="cb1561"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1561-1" data-line-number="1">softmax &lt;-<span class="st"> </span><span class="cf">function</span>(F_k) {</a>
<a class="sourceLine" id="cb1561-2" data-line-number="2">    n   =<span class="st"> </span><span class="kw">nrow</span>(F_k)</a>
<a class="sourceLine" id="cb1561-3" data-line-number="3">    K   =<span class="st"> </span><span class="kw">ncol</span>(F_k)</a>
<a class="sourceLine" id="cb1561-4" data-line-number="4">    p_k =<span class="st"> </span><span class="kw">matrix</span>( <span class="dv">0</span>, <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1561-5" data-line-number="5">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1561-6" data-line-number="6">        p_k[,k] =<span class="st"> </span><span class="kw">exp</span>(F_k[,k]) <span class="op">/</span><span class="st"> </span><span class="kw">apply</span>(<span class="kw">exp</span>(F_k), <span class="dv">1</span>, sum)</a>
<a class="sourceLine" id="cb1561-7" data-line-number="7">    }</a>
<a class="sourceLine" id="cb1561-8" data-line-number="8">    p_k</a>
<a class="sourceLine" id="cb1561-9" data-line-number="9">}</a></code></pre></div>

<p>In the case of using our <strong>classification tree</strong> as our <strong>base learner</strong>, each leaf (or region) produces the following score transformation:</p>
<p><span class="math display">\[
\begin{array}{lrr}
\ \ \ \ \ \ \ \text{for each}\ (L)_j \in (T)_m\ &amp;\text{where (L)eaf and (T)ree}  \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \gamma_{jkm} = 
\frac{K - 1}{K} \frac{\sum_{x_i} \in {L_{jkm}}^{rim}}{\sum_{x_i} \in {L_{jkm}}^{|r_{im}|(1 - |r_{im}|)}} \\
\ \ \ \ \ \ \ \text{end loop} 
\end{array}
\]</span></p>
<p>The calculation of the score is implemented as such:</p>

<div class="sourceCode" id="cb1562"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1562-1" data-line-number="1">transform &lt;-<span class="st"> </span><span class="cf">function</span>(my.model, <span class="dt">prob =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb1562-2" data-line-number="2">    J     =<span class="st"> </span><span class="kw">length</span>(my.model<span class="op">$</span>model)</a>
<a class="sourceLine" id="cb1562-3" data-line-number="3">    numer =<span class="st"> </span><span class="dv">0</span>; denom =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1562-4" data-line-number="4">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>J) {</a>
<a class="sourceLine" id="cb1562-5" data-line-number="5">        top =<span class="st"> </span>my.model<span class="op">$</span>model[[j]]<span class="op">$</span>top</a>
<a class="sourceLine" id="cb1562-6" data-line-number="6">        <span class="cf">if</span> (top<span class="op">$</span>ntype <span class="op">==</span><span class="st"> &quot;leaf&quot;</span>) {</a>
<a class="sourceLine" id="cb1562-7" data-line-number="7">            indices  =<span class="st"> </span>top<span class="op">$</span>indices</a>
<a class="sourceLine" id="cb1562-8" data-line-number="8">            y_j      =<span class="st"> </span>top<span class="op">$</span>response</a>
<a class="sourceLine" id="cb1562-9" data-line-number="9">            p_j      =<span class="st"> </span>prob[indices]</a>
<a class="sourceLine" id="cb1562-10" data-line-number="10">            gamma    =<span class="st"> </span>(J<span class="dv">-1</span>) <span class="op">/</span><span class="st"> </span>J  <span class="op">*</span><span class="st"> </span>( <span class="kw">sum</span>(y_j) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span> (<span class="kw">abs</span>(p_j) <span class="op">*</span><span class="st"> </span><span class="kw">abs</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p_j)))</a>
<a class="sourceLine" id="cb1562-11" data-line-number="11">            my.model<span class="op">$</span>model[[j]]<span class="op">$</span>top<span class="op">$</span>ymean =<span class="st"> </span>gamma</a>
<a class="sourceLine" id="cb1562-12" data-line-number="12">        }</a>
<a class="sourceLine" id="cb1562-13" data-line-number="13">    }</a>
<a class="sourceLine" id="cb1562-14" data-line-number="14">    my.model</a>
<a class="sourceLine" id="cb1562-15" data-line-number="15">}</a></code></pre></div>

<p>Finally, <strong>Gradient Boost</strong> is a greedy stage-wise additive model such that in terms of our base learner, namely <span class="math inline">\(\mathbf{\gamma_{jkm}(x_i)}\)</span>, we perform the following expressions iteratively:</p>
<p><span class="math display" id="eq:equate1120176">\[\begin{align}
F_{k,m}(x) = F_{k,m-1}(x) + \sum_{j=1}^J \gamma_{jkm}\mathbf{1}\{x \in L_{jkm}\}
\ \ \ \ \ where\ \ \text{(where J is number of leafs)} \tag{10.194} 
\end{align}\]</span></p>
<p>The algorithm below is a variant of <strong>Gradient Boosting</strong> with modification to allow for <strong>Multi-Classification</strong> (See Algorithm 6: <span class="math inline">\(L_k\)</span>_TreeBoost, J. Friedman <span class="citation">(<a href="bibliography.html#ref-ref670j">1999</a><a href="bibliography.html#ref-ref670j">a</a>)</span>). Here, we use the term <strong>L</strong>eaf referring to <strong>R</strong>egion for a <strong>J-terminal</strong> to emphasize using decision trees for base learners.</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,K\}\}_{i=1}^n\\
\ \ \ \text{number of machines}: M\\
\mathbf{Algorithm}:\\
\ \ \ F_0(X) = \text{arg}\ \underset{\gamma}{min} \sum_{x=1}^n Lik(y_i, \gamma )
\ \ \ \text{(initialize)}\\
\ \ \ \text{loop}\ m\ in\ 1:\ M \\
\ \ \ \ \ \ P_k(x) = \frac{e^{F_k}}{\sum_{l=1}^K e^{F_l}},\ \ \ \  for\ k = 1,..,K \\
\ \ \ \ \ \ \text{loop}\ k\ in\ 1:\ K\\
\ \ \ \ \ \ \ \ \ \ \ \ \ r_{im} = y_{ik} - P_{k,m-1}(x_i)\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{fit model using } h_m(\{x_i,r_{im}\}_{i=1}^n)\ \ \ \ \ \ \ \text{(fit model to pseudo-residuals)} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{for each}\ (L)_j \in (T)_m\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{where (L)eaf and (T)ree}  \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \gamma_{jkm} = 
\frac{K - 1}{K} \frac{\sum_{x_i \in L_{jkm}}{rim}}{\sum_{x_i \in L_{jkm}}{|r_{im}|(1 - |r_{im}|)}}
\ \ \ \ \text{(leaf residual)}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{end loop} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ F_{k,m}(x) = F_{k,m-1}(x) + \sum_{j=1}^J \gamma_{jkm}\mathbf{1}\{x \in L_{jkm}\} \ \ \ \ \ \ \text{(J is number of leafs)}\\
\ \ \ \ \ \ \ \text{end loop} \\
\ \ \ \text{end loop}\\
\ \ \text{Output } F_{k,m}(x) 
\end{array}
\]</span></p>
<p>Below is our example implementation of Algorithm 6: <span class="math inline">\(L_k\)</span>_TreeBoost <span class="citation">(J. Friedman <a href="bibliography.html#ref-ref670j">1999</a><a href="bibliography.html#ref-ref670j">a</a>)</span>:</p>

<div class="sourceCode" id="cb1563"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1563-1" data-line-number="1"><span class="co"># Here, we use regression tree and prediction </span></a>
<a class="sourceLine" id="cb1563-2" data-line-number="2"><span class="co"># from Computational Learning I chapter.</span></a>
<a class="sourceLine" id="cb1563-3" data-line-number="3">h.learner &lt;-<span class="st"> </span>my.regression.tree  </a>
<a class="sourceLine" id="cb1563-4" data-line-number="4">h.score   &lt;-<span class="st"> </span>my.predict          </a>
<a class="sourceLine" id="cb1563-5" data-line-number="5">tendency  &lt;-<span class="st"> </span><span class="cf">function</span>(top, resid) { top<span class="op">$</span>ymean }</a>
<a class="sourceLine" id="cb1563-6" data-line-number="6">my.gradientboost &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, data, <span class="dt">machines=</span><span class="dv">50</span>, </a>
<a class="sourceLine" id="cb1563-7" data-line-number="7">                      <span class="dt">minbucket=</span><span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">4</span>) {</a>
<a class="sourceLine" id="cb1563-8" data-line-number="8">    x          =<span class="st"> </span>data[,<span class="kw">c</span>(features)]</a>
<a class="sourceLine" id="cb1563-9" data-line-number="9">    y          =<span class="st"> </span>data[,<span class="kw">c</span>(target)]</a>
<a class="sourceLine" id="cb1563-10" data-line-number="10">    n          =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1563-11" data-line-number="11">    classes    =<span class="st"> </span><span class="kw">levels</span>(y)</a>
<a class="sourceLine" id="cb1563-12" data-line-number="12">    K          =<span class="st"> </span><span class="kw">length</span>(classes)</a>
<a class="sourceLine" id="cb1563-13" data-line-number="13">    M          =<span class="st"> </span>machines</a>
<a class="sourceLine" id="cb1563-14" data-line-number="14">    y_ik       =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1563-15" data-line-number="15">    y.hat      =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1563-16" data-line-number="16">    F_k        =<span class="st"> </span><span class="kw">matrix</span>( <span class="dv">0</span>, <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1563-17" data-line-number="17">    f_model    =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1563-18" data-line-number="18">    <span class="kw">names</span>(F_k) =<span class="st"> </span>classes</a>
<a class="sourceLine" id="cb1563-19" data-line-number="19">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) { </a>
<a class="sourceLine" id="cb1563-20" data-line-number="20">      y_ik[,k]      =<span class="st"> </span><span class="kw">as.numeric</span>(y <span class="op">==</span><span class="st"> </span>classes[k])</a>
<a class="sourceLine" id="cb1563-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb1563-22" data-line-number="22">    <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M) {</a>
<a class="sourceLine" id="cb1563-23" data-line-number="23">      p_k =<span class="st"> </span><span class="kw">softmax</span>(F_k)</a>
<a class="sourceLine" id="cb1563-24" data-line-number="24">      <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1563-25" data-line-number="25">        y.hat[,k]        =<span class="st"> </span>residual =<span class="st"> </span>y_ik[,k] <span class="op">-</span><span class="st"> </span>p_k[,k] </a>
<a class="sourceLine" id="cb1563-26" data-line-number="26">        data[,<span class="kw">c</span>(target)] =<span class="st"> </span>residual</a>
<a class="sourceLine" id="cb1563-27" data-line-number="27">        f_model[[k]]     =<span class="st"> </span><span class="kw">h.learner</span>(features, target, data, minbucket, </a>
<a class="sourceLine" id="cb1563-28" data-line-number="28">                                     maxdepth)</a>
<a class="sourceLine" id="cb1563-29" data-line-number="29">        f_model[[k]]     =<span class="st"> </span><span class="kw">transform</span>(f_model[[k]], p_k[,k])</a>
<a class="sourceLine" id="cb1563-30" data-line-number="30">        gamma_jkm        =<span class="st"> </span><span class="kw">h.score</span>(f_model[[k]], x, y.hat[,k] )<span class="op">$</span>fitted.values</a>
<a class="sourceLine" id="cb1563-31" data-line-number="31">        F_k[,k]          =<span class="st"> </span>F_k[,k] <span class="op">+</span><span class="st"> </span>gamma_jkm </a>
<a class="sourceLine" id="cb1563-32" data-line-number="32">      }</a>
<a class="sourceLine" id="cb1563-33" data-line-number="33">    }</a>
<a class="sourceLine" id="cb1563-34" data-line-number="34">    p_k =<span class="st"> </span><span class="kw">softmax</span>(F_k)</a>
<a class="sourceLine" id="cb1563-35" data-line-number="35">    <span class="kw">apply</span>(p_k, <span class="dv">1</span>, which.max)</a>
<a class="sourceLine" id="cb1563-36" data-line-number="36">}</a></code></pre></div>

<p>Let us use the implementation and compare the difference between the actual target values and the fitted values generated by <strong>Gradient Boost</strong> using m equal to 1.</p>

<div class="sourceCode" id="cb1564"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1564-1" data-line-number="1">features =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1564-2" data-line-number="2">target   =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)</a>
<a class="sourceLine" id="cb1564-3" data-line-number="3">y        =<span class="st"> </span><span class="kw">as.numeric</span>(train[,<span class="kw">c</span>(target)])</a>
<a class="sourceLine" id="cb1564-4" data-line-number="4">fitted.values =<span class="st"> </span><span class="kw">my.gradientboost</span>(features, target, train, <span class="dt">machines=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1564-5" data-line-number="5"><span class="kw">all.equal</span>(y, fitted.values)</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>

<p>We leave readers to enhance the implementation with emphasis on fixing the overfitting.</p>
</div>
<div id="k-next-neighbors-knn" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.8</span> K-Next Neighbors (KNN)  <a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before we jump to a new topic, it helps to also finally give merit to the <strong>K-Next Neighbors (KNN)</strong> classification method being one of the traditional classification algorithms. <strong>KNN</strong> may be confused with <strong>K-means</strong>, which are two separate methods. The former is a classification method, and the latter is a clustering method.</p>
<p><strong>KNN</strong> has the following simple algorithm, which allows for unknown data, e.g., test data, to be classified given already learned or classified data. In the algorithm below, we are asked to determine the class to which the new data belongs. Note here that <strong>C</strong> is the number of categories (or classes) and <strong>K</strong> is the K nearest neighbors.</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,C\}\}_{i=1}^n\\
\ \ \ \text{unknown dataset}: \{{(z_i)}:z_i\ \in\ Z \}_{i=1}^m\\
\ \ \ \text{number of nearest data points}: K\\
\mathbf{Algorithm}:\\
\ \ \ \text{loop}\ j\ in\ 1:\ m\\
\ \ \ \ \ \ \ \text{loop}\ i\ in\ 1:\ n\\
\ \ \ \ \ \ \ \ \ \ \ \text{Calculate:}\ Distance(x_i, z_j) \\
\ \ \ \ \ \ \ \ \ \ \ \text{Sort the calculated distances in ascending order} \\
\ \ \ \ \ \ \ \ \ \ \ \text{Choose the top K least distances (nearest neighbors)} \\
\ \ \ \ \ \ \ \ \ \ \ \text{The most frequent class, e.g. c,  in the K nearest}\\
\ \ \ \ \ \ \ \ \ \ \ \text{neighbors gets assigned to}\ z_j \\
\ \ \ \ \ \ \ \text{end loop}\\
\ \ \ \text{end loop}\\
\end{array}
\]</span></p>
<p>In the algorithm, any data measurement as necessary can be used, e.g. <strong>Euclidean</strong>, <strong>Manhattan</strong>, etc.</p>

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="10.2-binary-classification-supervised.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="11-machinelearning3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
