<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.8 The Significance of Regression  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="6.8 The Significance of Regression  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.8 The Significance of Regression  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2023-03-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="6.7-regression-analysis.html"/>
<link rel="next" href="6.9-inference-for-regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat-and-disclaimer.html"><a href="caveat-and-disclaimer.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-poisson-equation"><i class="fa fa-check"></i><b>4.6.1</b> The Poisson Equation </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.4</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.6</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearson’s Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naïve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naïve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs. Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-learning-deep-rl.html"><a href="13.8-deep-reinforcement-learning-deep-rl.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Learning (Deep RL)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-significance-of-regression" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.8</span> The Significance of Regression <a href="6.8-the-significance-of-regression.html#the-significance-of-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall in previous sections that we deal with <strong>significance of difference</strong> of groups. In our examples, our <strong>null hypothesis</strong> claims that there is no difference between groups, while our <strong>alternative hypothesis</strong> supports the claim that there is a difference between groups:</p>
<p><span class="math display" id="eq:equate1080110">\[\begin{align}
H_0 : \mu = \mu_0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
H_1 : \mu \ne \mu_0 \tag{6.115} 
\end{align}\]</span></p>
<p>In this section, we focus on <strong>significance of regression</strong> in which we want to study the relationship, association, or effect of <strong>explanatory variables</strong> on <strong>response variables</strong>. We do this by <strong>examining the coefficients</strong> that are associated with the explanatory variables and then determining the significance based on a test of effect. Here, we are also required to formulate our own <strong>null hypothesis</strong> and <strong>alternative hypothesis</strong> using a <strong>linear equation</strong> starting with the following <strong>simple linear equation</strong>:</p>
<p><span class="math display" id="eq:equate1080111">\[\begin{align}
y_i = \beta_0 + \beta_1 x_i + \epsilon_i \tag{6.116} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i\)</span> is the response (dependent) variable</li>
<li><span class="math inline">\(B_0\)</span> and <span class="math inline">\(B_1\)</span> are coefficients</li>
<li><span class="math inline">\(x_i\)</span> is the explanatory (predictor or independent) variable</li>
<li><span class="math inline">\(e_i\)</span> is the unexplained residual</li>
</ul>
<p>Our <strong>null hypothesis</strong> claims that our response variable does not depend on the predictor variable.</p>
<p><span class="math display" id="eq:equate1080113" id="eq:equate1080112">\[\begin{align}
H_0 {}&amp;: \beta_1 = 0,\ \ \ \  \ y_i = \beta_0 + \epsilon_i \tag{6.117} \\
\nonumber \\
H_1 &amp;: \beta_1 \ne 0,\ \ \ \  \ y_i = \beta_0 + \beta_1 x_i + \epsilon_i \tag{6.118} 
\end{align}\]</span></p>
<p>While we can use <strong>ANOVA</strong> and <strong>Sum of Squares</strong> to measure <strong>Variance</strong>, in <strong>Significance of Regression</strong>, it also helps consider the effect of predictor variables on response variables. What we want to do is to settle over the least amount of error. Therefore, we measure error based on the distance between two data points deviating from one another. In the following sections, we introduce a few tests of <strong>null hypothesis</strong> for <strong>linear regression</strong>, starting with <strong>simple linear regression</strong>.</p>
<div id="simple-linear-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.1</span> Simple Linear Regression<a href="6.8-the-significance-of-regression.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>Simple Linear Regression</strong>, we deal with one continuous independent variable denoted as <span class="math inline">\(\mathbf{x_i}\)</span> and one continuous dependent variable denoted as <span class="math inline">\(\hat{y}\)</span>. Being linear, we try to find the best fit of a line into the data and determine if we can even fit a line - is there a relationship between the dependent and independent variables?</p>
<p>We need to determine if the <span class="math inline">\(H_0\)</span> supports the claim that the predicted outcome does not regress - is not close - to the actual value by any random chance (from an independent random variable).</p>
<p>To illustrate, let us continue to use the <strong>mtcars</strong> dataset. However, this time, we intend to know if there is an effect of the displacement, <strong>disp</strong>, to fuel consumption, <strong>mpg</strong>. We use a built-in R function called <strong>lm(.)</strong> to generate a linear model;</p>

<div class="sourceCode" id="cb698"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb698-1" data-line-number="1">(<span class="dt">simple.model =</span> <span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>disp, <span class="dt">data =</span> mtcars))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ disp, data = mtcars)
## 
## Coefficients:
## (Intercept)         disp  
##     29.5999      -0.0412</code></pre>

<p>Notice in the model that we have two coefficients: <span class="math inline">\(\beta_0\)</span> = 29.6 and <span class="math inline">\(\beta_1\)</span> = -0.041.</p>
<p>Let us run a summary against the linear model using <strong>summary(.)</strong>:</p>

<div class="sourceCode" id="cb700"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb700-1" data-line-number="1"><span class="kw">summary</span>(simple.model)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ disp, data = mtcars)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.892 -2.202 -0.963  1.627  7.231 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 29.59985    1.22972   24.07  &lt; 2e-16 ***
## disp        -0.04122    0.00471   -8.75  9.4e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.25 on 30 degrees of freedom
## Multiple R-squared:  0.718,  Adjusted R-squared:  0.709 
## F-statistic: 76.5 on 1 and 30 DF,  p-value: 9.38e-10</code></pre>

<p>Based on the summary, we can say that the displacement variable significantly affects fuel consumption. That is asserted because of the triple asterisk (***) for the coefficient of the variable <strong>disp</strong>. The code indicates a significant effect at <span class="math inline">\(\alpha=0.001\)</span>.</p>
<p>To determine if such a predictor variable <strong>regresses to</strong> a response variable, we may plot our dataset and determine if we can model the regression based on measuring the <strong>closeness of distance</strong> of each observed data point denoted as <span class="math inline">\(y_{i}\)</span> to a fitted line denoted as <span class="math inline">\(f(x_{i})\)</span>. In other words, we should be able to see the linear relationship pattern between <strong>disp</strong> and <strong>mpg</strong>:</p>

<div class="sourceCode" id="cb702"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb702-1" data-line-number="1"><span class="kw">plot</span>(mpg <span class="op">~</span><span class="st"> </span>disp, <span class="dt">data =</span> mtcars, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,</a>
<a class="sourceLine" id="cb702-2" data-line-number="2">     <span class="dt">main=</span><span class="st">&quot;Effect of Displacement to Fuel Consumption&quot;</span>)</a>
<a class="sourceLine" id="cb702-3" data-line-number="3"><span class="kw">abline</span>(simple.model, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>) <span class="co"># fit a line, f(xi)</span></a>
<a class="sourceLine" id="cb702-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb702-5" data-line-number="5"><span class="kw">arrows</span>(<span class="dv">200</span>,<span class="dv">23</span>, <span class="dv">250</span>,<span class="dv">28</span>, <span class="dt">code=</span><span class="dv">1</span>, <span class="dt">length=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb702-6" data-line-number="6"><span class="kw">text</span>(<span class="dv">350</span>, <span class="dv">28</span>, <span class="dt">label=</span><span class="st">&quot;fitted line, f(x), the linear model&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-304"></span>
<img src="DS_files/figure-html/unnamed-chunk-304-1.png" alt="Smoothing" width="70%" />
<p class="caption">
Figure 6.29: Smoothing
</p>
</div>

<p>In the plot, we fit a line to show the linear regression. The <span class="math inline">\(B_1\)</span> is negative which indicates a decreasing effect to the fuel consumption; meaning, for every unit increase of <span class="math inline">\(x\)</span> (disp), there is a 0.041 unit decrease of <span class="math inline">\(y\)</span> (mpg).</p>
We will see how to apply the formulas when we get into <strong>multilinear regression</strong>. Meanwhile, Figure <a href="6.8-the-significance-of-regression.html#fig:statistics">6.30</a> shows the relationships of the formulas:

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:statistics"></span>
<img src="DS_files/figure-html/statistics-1.png" alt="Statistics" width="0%" />
<p class="caption">
Figure 6.30: Statistics
</p>
</div>

<p>Note, based on Figure <a href="6.8-the-significance-of-regression.html#fig:statistics">6.30</a>, that <strong>RSS</strong> compares the regression line against the horizontal line - the null hypothesis (<strong>H0</strong>). Similarly, <strong>R squared</strong> is compared the same.</p>
<p><strong>RSS (Residual Sum of Squares)</strong>  </p>
<p>That is the sum of squares of the difference between <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y\)</span>. At times, in other literature, it may read as <strong>SSR</strong> (Sum of Squares Residual) or <strong>SSE</strong> (Sum of Squares Error).</p>
<p><span class="math display" id="eq:equate1080114">\[\begin{align}
RSS = \sum_{i=1}^{n}(y_{i} - \hat{y}_i)^2 \tag{6.119} 
\end{align}\]</span></p>
<p><strong>ESS (Explained Sum of Squares)</strong>  </p>
<p>That is the sum of squares of the difference between <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\bar{y}\)</span>. We may read this as <strong>RSS</strong> (Regression Sum of Squares) in other literature. Alternatively, it may read as <strong>SSR</strong> (Sum of Squares Regression) or <strong>SSE</strong> (Sum of Squares Explained).</p>
<p><span class="math display" id="eq:equate1080115">\[\begin{align}
ESS = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2 \tag{6.120} 
\end{align}\]</span></p>
<p><strong>TSS (Total Sum of Squares)</strong>  </p>
<p>That is the sum of squares of the difference between <span class="math inline">\(y\)</span> and <span class="math inline">\(\bar{y}\)</span>. We may sometimes read this as <strong>SST</strong> (Sum of Squares Total).</p>
<p><span class="math display" id="eq:equate1080116">\[\begin{align}
TSS = \sum_{i=1}^{n}(y_{i} - \bar{y})^2 \tag{6.121} 
\end{align}\]</span></p>
<p>The total sum of squares combines RSS (residual sum of squares) and ESS (explained sum of squares).</p>
<p><span class="math display" id="eq:equate1080117">\[\begin{align}
TSS = RSS + ESS \tag{6.122} 
\end{align}\]</span></p>
<p>We have the following implementation of the measures for the regression, for which we can demonstrate the usage in the next section:</p>

<div class="sourceCode" id="cb703"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb703-1" data-line-number="1">reg.summary &lt;-<span class="st"> </span><span class="cf">function</span>(A, coefficients, y) {</a>
<a class="sourceLine" id="cb703-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb703-3" data-line-number="3">  m =<span class="st"> </span><span class="kw">length</span>(coefficients)</a>
<a class="sourceLine" id="cb703-4" data-line-number="4">  y_hat =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>coefficients</a>
<a class="sourceLine" id="cb703-5" data-line-number="5">  df =<span class="st"> </span>n <span class="op">-</span><span class="st"> </span>m  <span class="co"># degrees of freedom</span></a>
<a class="sourceLine" id="cb703-6" data-line-number="6">  rss =<span class="st"> </span><span class="kw">sum</span>((y <span class="op">-</span><span class="st"> </span>y_hat)<span class="op">^</span><span class="dv">2</span>) <span class="co"># residual sum squared</span></a>
<a class="sourceLine" id="cb703-7" data-line-number="7">  ess =<span class="st"> </span><span class="kw">sum</span>((y_hat <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y))<span class="op">^</span><span class="dv">2</span>) <span class="co"># explained sum squared</span></a>
<a class="sourceLine" id="cb703-8" data-line-number="8">  tss =<span class="st"> </span><span class="kw">sum</span>((y <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y))<span class="op">^</span><span class="dv">2</span>) <span class="co"># total sum squared</span></a>
<a class="sourceLine" id="cb703-9" data-line-number="9">  se =<span class="st"> </span><span class="kw">sqrt</span>( rss <span class="op">/</span><span class="st"> </span>df ) <span class="co"># standard error</span></a>
<a class="sourceLine" id="cb703-10" data-line-number="10">  r2 =<span class="st"> </span>ess<span class="op">/</span><span class="st"> </span>tss <span class="co"># multiple r-squared</span></a>
<a class="sourceLine" id="cb703-11" data-line-number="11">  adj.r2 =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>((<span class="dv">1</span><span class="op">-</span>r2) <span class="op">*</span><span class="st"> </span>(n<span class="dv">-1</span>) <span class="op">/</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span>m))</a>
<a class="sourceLine" id="cb703-12" data-line-number="12">  out =<span class="st"> </span><span class="kw">c</span>(n, m, df, rss, ess, tss, se, r2, adj.r2)</a>
<a class="sourceLine" id="cb703-13" data-line-number="13">  <span class="kw">names</span>(out) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;n&quot;</span>, <span class="st">&quot;m&quot;</span>, <span class="st">&quot;df&quot;</span>, <span class="st">&quot;RSS&quot;</span>, <span class="st">&quot;ESS&quot;</span>, <span class="st">&quot;TSS&quot;</span>, <span class="st">&quot;SE&quot;</span>, </a>
<a class="sourceLine" id="cb703-14" data-line-number="14">                 <span class="st">&quot;R-squared&quot;</span>, <span class="st">&quot;Adj.R2&quot;</span>)</a>
<a class="sourceLine" id="cb703-15" data-line-number="15">  out</a>
<a class="sourceLine" id="cb703-16" data-line-number="16">}</a></code></pre></div>

</div>
<div id="multilinear-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.2</span> Multilinear Regression <a href="6.8-the-significance-of-regression.html#multilinear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>Multilinear Regression</strong>, we deal with more than one continuous independent variable, denoted as <strong>X</strong>, and one continuous dependent variable, denoted as <span class="math inline">\(\hat{y}\)</span>, using the following <strong>multilinear equation</strong>:</p>
<p><span class="math display" id="eq:equate1080118">\[\begin{align}
\hat{y} = \beta^T X \tag{6.123} 
\end{align}\]</span></p>
<p>which can be expanded like so:</p>
<p><span class="math display" id="eq:equate1080119">\[\begin{align}
y_i = \beta_0 + \beta_1 x_{1,i} +  \beta_2 x_{2,i}\ + ... +\ \beta_n x_{n,i} +  \epsilon_i \tag{6.124} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i\)</span> is the response (dependent) variable</li>
<li><span class="math inline">\(B_0\)</span> … <span class="math inline">\(B_n\)</span> are coefficients</li>
<li><span class="math inline">\(x_1\)</span> … <span class="math inline">\(X_n\)</span> are the explanatory (predictor or independent) variables</li>
<li><span class="math inline">\(e_i\)</span> is the unexplained residual</li>
</ul>
<p>Our <strong>null hypothesis</strong> claims that our response variable does not depend on any predictor variable. Otherwise, our <strong>alternative hypothesis</strong> claims that our response variable depends on at least one predictor variable.</p>
<p><span class="math display" id="eq:equate1080121" id="eq:equate1080120">\[\begin{align}
H_0 {}&amp;: \beta_1 =  \beta_2 =  \beta_3 =\ ...\ =\ \beta_n  = 0,\ \ \ \  \ y_i = \beta_0 + \epsilon_i \tag{6.125} \\
\nonumber \\
H_1 &amp;: at\ least\ one\ \ \beta_j \ne 0,\ \ \ \  \ y_i = \beta_0 + \beta_1 x_{1,i} +  \beta_2 x_{2,i}\ + ... +\ \beta_n x_{n,i} +  \epsilon_i \tag{6.126} 
\end{align}\]</span></p>
<p>We need to determine if the <span class="math inline">\(H_0\)</span> supports the claim that the predicted outcome does not regress - is not close - to the actual value by any number of random chances (from multiple independent random variables).</p>
<p>To illustrate, we now use more than one predictor variable from the <strong>mtcars</strong> dataset and model a multilinear regression. We choose four continuous predictor variables: displacement (disp), horsepower (hp), weight (wt), and 1/4 mile time (qsec).</p>

<div class="sourceCode" id="cb704"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb704-1" data-line-number="1"><span class="kw">str</span>(mtcars)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</code></pre>

<p>We generate our multilinear model:</p>

<div class="sourceCode" id="cb706"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb706-1" data-line-number="1">(<span class="dt">multi.model =</span>  <span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>disp <span class="op">+</span><span class="st"> </span>hp <span class="op">+</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>qsec, <span class="dt">data =</span> mtcars))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ disp + hp + wt + qsec, data = mtcars)
## 
## Coefficients:
## (Intercept)         disp           hp           wt         qsec  
##    27.32964      0.00267     -0.01867     -4.60912      0.54416</code></pre>

<p>and derive the following coefficients:</p>
<p><span class="math inline">\(\beta_0\)</span> = 27.3296,
<span class="math inline">\(\beta_1\)</span> = 0.0027,
<span class="math inline">\(\beta_2\)</span> = -0.0187,
<span class="math inline">\(\beta_3\)</span> = -4.6091,
<span class="math inline">\(\beta_4\)</span> = 0.5442</p>
<p>We can also use a built-in function called <strong>coef()</strong>:</p>

<div class="sourceCode" id="cb708"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb708-1" data-line-number="1"><span class="kw">coef</span>(multi.model)</a></code></pre></div>
<pre><code>## (Intercept)        disp          hp          wt        qsec 
##   27.329638    0.002666   -0.018666   -4.609123    0.544160</code></pre>

<p>Recall <strong>Polynomial Regression</strong> in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>). We use the same matrix manipulation to get the list of coefficients, where A is the matrix of equations (See also Equation <a href="6.7-regression-analysis.html#eq:eqnnumber3a">(6.105)</a>).</p>
<p><span class="math display">\[\begin{align*}
\hat{\beta} \approx (A^T \cdot A)^{-1} \cdot A^T \cdot y 
\end{align*}\]</span></p>
<p>For example:</p>

<div class="sourceCode" id="cb710"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb710-1" data-line-number="1">A =<span class="st"> </span><span class="kw">with</span>(mtcars, <span class="kw">cbind</span>(<span class="dv">1</span>, disp, hp, wt, qsec))</a>
<a class="sourceLine" id="cb710-2" data-line-number="2">y =<span class="st"> </span>mtcars<span class="op">$</span>mpg</a>
<a class="sourceLine" id="cb710-3" data-line-number="3">beta.hat =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>A) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb710-4" data-line-number="4"><span class="kw">colnames</span>(beta.hat) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;coefficients&quot;</span>)</a>
<a class="sourceLine" id="cb710-5" data-line-number="5"><span class="kw">rownames</span>(beta.hat) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;intercept&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>, <span class="st">&quot;wt&quot;</span>, <span class="st">&quot;qsec&quot;</span>)</a>
<a class="sourceLine" id="cb710-6" data-line-number="6"><span class="kw">t</span>(beta.hat)</a></code></pre></div>
<pre><code>##              intercept     disp       hp     wt   qsec
## coefficients     27.33 0.002666 -0.01867 -4.609 0.5442</code></pre>

<p>Note that there are other improvements to matrix manipulation we can numerically use to compute the coefficients. Please refer to Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>).</p>
<p>To get the approximate response outcome, <span class="math inline">\(\hat{y}\)</span>, we use the following equation:</p>
<p><span class="math display" id="eq:equate1080123" id="eq:equate1080122">\[\begin{align}
\hat{y} = A\times\hat{\beta}  \tag{6.127} \\
\nonumber \\
y_hat = A %*% beta.hat \tag{6.128} 
\end{align}\]</span></p>
<p>And to get the <strong>Standard Residual Error</strong> and <strong>R-squared</strong>, we use the following equations:</p>
<p><span class="math display" id="eq:equate1080124">\[\begin{align}
SE = \frac{RSS}{df}\ \ \ \ \ \ \leftarrow\ \ \ df = n - m,
\ \ \ \ \ \ \ \ \ \ \ \ \ \ R^2 = \frac{ESS}{TSS} \tag{6.129} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>RSS</strong> is the residual sum of squares</li>
<li><strong>n</strong> is size of the sample</li>
<li><strong>m</strong> is the number of coefficients (including intercept)</li>
<li><strong>df</strong> is the degrees of freedom</li>
</ul>

<div class="sourceCode" id="cb712"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb712-1" data-line-number="1">reg.out =<span class="st"> </span><span class="kw">round</span>( <span class="kw">reg.summary</span>(A, beta.hat, y), <span class="dv">4</span>)</a>
<a class="sourceLine" id="cb712-2" data-line-number="2">reg.out[<span class="kw">c</span>(<span class="st">&quot;n&quot;</span>, <span class="st">&quot;m&quot;</span>, <span class="st">&quot;df&quot;</span>, <span class="st">&quot;RSS&quot;</span>, <span class="st">&quot;ESS&quot;</span>, <span class="st">&quot;TSS&quot;</span>, <span class="st">&quot;SE&quot;</span>)]</a></code></pre></div>
<pre><code>##        n        m       df      RSS      ESS      TSS       SE 
##   32.000    5.000   27.000  185.635  940.412 1126.047    2.622</code></pre>
<div class="sourceCode" id="cb714"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb714-1" data-line-number="1">reg.out[<span class="kw">c</span>(<span class="st">&quot;R-squared&quot;</span>, <span class="st">&quot;Adj.R2&quot;</span>)]</a></code></pre></div>
<pre><code>## R-squared    Adj.R2 
##    0.8351    0.8107</code></pre>

<p>Here is the summary:</p>

<div class="sourceCode" id="cb716"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb716-1" data-line-number="1">(<span class="dt">summary.out =</span> <span class="kw">summary</span>(multi.model))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ disp + hp + wt + qsec, data = mtcars)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.866 -1.582 -0.379  1.171  5.647 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) 27.32964    8.63903    3.16   0.0038 **
## disp         0.00267    0.01074    0.25   0.8058   
## hp          -0.01867    0.01561   -1.20   0.2423   
## wt          -4.60912    1.26585   -3.64   0.0011 **
## qsec         0.54416    0.46649    1.17   0.2536   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.62 on 27 degrees of freedom
## Multiple R-squared:  0.835,  Adjusted R-squared:  0.811 
## F-statistic: 34.2 on 4 and 27 DF,  p-value: 3.31e-10</code></pre>

<p>As shown, the predictor variable <strong>wt</strong> has a significant effect to <strong>mpg</strong> at <span class="math inline">\(\alpha=0.01\)</span>.</p>
<p>Also, notice that our <strong>R-squared</strong> is at <span class="math inline">\(\sigma\)</span> = 0.8351. Any value closer to one indicates a better fit. However, while we consider this coefficient of determinant to measure the goodness of fit, it becomes less reliable as the number of predictors increases. </p>
<p>Let us calculate the adjusted <strong>R-squared</strong> using the equation below (with the intent to have an adjusted <span class="math inline">\(R^2\)</span> event at a high number of predictors - although, here, we are not going to perform that): </p>
<p><span class="math display" id="eq:equate1080125">\[\begin{align}
adj.R^2 = 1 - \frac{(1-R^2)(n-1)}{n-m } \tag{6.130} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb718"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb718-1" data-line-number="1">r2 =<span class="st"> </span><span class="kw">as.numeric</span>( reg.out[<span class="kw">c</span>(<span class="st">&quot;R-squared&quot;</span>)] )</a>
<a class="sourceLine" id="cb718-2" data-line-number="2">n  =<span class="st"> </span><span class="kw">as.numeric</span>(reg.out[<span class="kw">c</span>(<span class="st">&quot;n&quot;</span>)])</a>
<a class="sourceLine" id="cb718-3" data-line-number="3">m  =<span class="st"> </span><span class="kw">as.numeric</span>(reg.out[<span class="kw">c</span>(<span class="st">&quot;m&quot;</span>)])</a>
<a class="sourceLine" id="cb718-4" data-line-number="4">( <span class="dt">adj.r2 =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>r2)<span class="op">*</span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(n <span class="op">-</span><span class="st"> </span>m) )</a></code></pre></div>
<pre><code>## [1] 0.8107</code></pre>

</div>
<div id="logistic-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.3</span> Logistic Regression <a href="6.8-the-significance-of-regression.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We review <strong>Logistic Regression</strong> in the context of <strong>Generalized Linear Model (GLM)</strong>, emphasizing the use of a <strong>link function</strong>. Note that <strong>GLM</strong> and <strong>link function</strong> are covered in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>).  </p>
<p>In <strong>Logistic Regression</strong>, we deal with independent variables denoted by <span class="math inline">\(X\)</span> and a <strong>logit-transformed</strong> dependent variable that follows a binomial probability denoted by <span class="math inline">\(\hat{p}\)</span>. We express the distribution like so:</p>
<p><span class="math display">\[y \sim Bern(\hat{p})\]</span></p>
<p>The intuition behind <strong>Logistic Regression</strong> starts with two functions:</p>
<p><strong>Link Function (Logit Function)</strong>: </p>
<p>Note that a <strong>Logit Function</strong> is the inverse of a <strong>Sigmoid Function</strong> (also known as <strong>Logistic Function</strong>).</p>
<p><span class="math display" id="eq:equate1080126">\[\begin{align}
\text{logit}(\hat{p}) = \log_e\left(\frac{\hat{p}}{1 - \hat{p}}\right) = z
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{where z in }[-\infty,\infty] \tag{6.131} 
\end{align}\]</span></p>
<p><strong>Inverse Link Function (Sigmoid Function):</strong></p>
<p><span class="math display" id="eq:equate1080127">\[\begin{align}
\text{logit}^{-1}(z) = \hat{p} 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{where }\hat{p}\text{ in }[0,1] \tag{6.132} 
\end{align}\]</span></p>
<p>and where: <span class="math inline">\(z = \beta^TX\ \ \ \  \leftarrow\ \ \ \ \beta_0 + \sum_{i=1}^n\beta_i x_i\)</span>.</p>
<p>The <strong>Inverse Link Function</strong>, also called <strong>Inverse Logit Function</strong>, can be expanded and be shown to be inversely related to <strong>Logit function</strong> like so:</p>
<p><span class="math display" id="eq:equate1080128">\[\begin{align}
\hat{p} = \frac{\text{exp}(z)}{1 + \text{exp}(z)} = \frac{1}{1 + \text{exp}(-z)} 
= \frac{1}{1 + \text{exp}(-\text{logit}(z))}  \tag{6.133} 
\end{align}\]</span></p>
<p>where: <span class="math inline">\(\hat{p}\)</span> is interpreted as the <strong>probability</strong> of observing a successful event.</p>
<p><span class="math display" id="eq:equate1080129">\[\begin{align}
\underbrace{\hat{p} = P(y = 1| x)}_{\text{successful event}}\ \ \ \ \ \ \ \ \ 
\underbrace{\hat{q} = 1 - \hat{p} =P(y = 0| x) }_{\text{failed event}} \tag{6.134} 
\end{align}\]</span></p>
<p>The <strong>Logit Function</strong>, on the other hand, can be interpreted as the logarithm of the odds ratio of observing a successful event over observing a failed event. This function allows us to model regression linearly.</p>
<p><span class="math display" id="eq:equate1080130">\[\begin{align}
\text{logit}(\hat{p}) = \log_e\left(\frac{\hat{p}}{1 - \hat{p}}\right) 
= \log_e\left(\frac{\hat{p}}{\hat{q}}\right)  = \beta^TX \tag{6.135} 
\end{align}\]</span></p>
<p>Both functions can be implemented such that we have <strong>inverse.logit(.)</strong> and <strong>logit(.)</strong>.</p>

<div class="sourceCode" id="cb720"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb720-1" data-line-number="1">ln &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">log</span>(x, <span class="kw">exp</span>(<span class="dv">1</span>))}  <span class="co"># exp(1) = 2.718282</span></a>
<a class="sourceLine" id="cb720-2" data-line-number="2">inverse.logit &lt;-<span class="st"> </span><span class="cf">function</span>(z) {  <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>z)) }</a>
<a class="sourceLine" id="cb720-3" data-line-number="3">logit &lt;-<span class="st"> </span><span class="cf">function</span>(p) { <span class="kw">ln</span>(p <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p))  }</a></code></pre></div>

<p>Here, the base of our log is e = 2.7183.</p>
<p>Alternatively, R comes with two functions, namely <strong>plogis(.)</strong> and <strong>qlogis(.)</strong> respectively, e.g.:</p>

<div class="sourceCode" id="cb721"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb721-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;inverse.logit&quot;</span> =<span class="st"> </span><span class="kw">inverse.logit</span>(<span class="dv">8</span>), <span class="st">&quot;plogis&quot;</span> =<span class="st"> </span><span class="kw">plogis</span>(<span class="dv">8</span>))</a></code></pre></div>
<pre><code>## inverse.logit        plogis 
##        0.9997        0.9997</code></pre>
<div class="sourceCode" id="cb723"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb723-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;logit&quot;</span> =<span class="st"> </span><span class="kw">logit</span>(<span class="fl">0.90</span>), <span class="st">&quot;qlogis&quot;</span> =<span class="st"> </span><span class="kw">qlogis</span>(<span class="fl">0.90</span>))</a></code></pre></div>
<pre><code>##  logit qlogis 
##  2.197  2.197</code></pre>

<p>To visualize the logistic distribution using the plogis function, we show Figure <a href="6.8-the-significance-of-regression.html#fig:plogis">6.31</a>.</p>

<div class="sourceCode" id="cb725"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb725-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb725-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;Z (Log-Odds)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;plogis(z)&quot;</span>,</a>
<a class="sourceLine" id="cb725-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Inverse Logit (Log-Odds as Input)&quot;</span>)</a>
<a class="sourceLine" id="cb725-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb725-5" data-line-number="5"><span class="co"># Using plogis outcome</span></a>
<a class="sourceLine" id="cb725-6" data-line-number="6">z =<span class="st"> </span>x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dt">length.out=</span><span class="dv">50</span>) <span class="co"># z is log-odds.</span></a>
<a class="sourceLine" id="cb725-7" data-line-number="7"><span class="kw">curve</span>(<span class="kw">plogis</span>(x), <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">add=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:plogis"></span>
<img src="DS_files/figure-html/plogis-1.png" alt="Logistic Regression (CDF)" width="70%" />
<p class="caption">
Figure 6.31: Logistic Regression (CDF)
</p>
</div>

<p>To visualize the logistic distribution using qlogis function, we show Figure <a href="6.8-the-significance-of-regression.html#fig:qlogis">6.32</a>. The range is [<span class="math inline">\(-\infty, \infty\)</span>].</p>

<div class="sourceCode" id="cb726"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb726-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), </a>
<a class="sourceLine" id="cb726-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;P (Probability)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;qlogis(p)&quot;</span>, </a>
<a class="sourceLine" id="cb726-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Logit (Probabilities as Input)&quot;</span>)</a>
<a class="sourceLine" id="cb726-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb726-5" data-line-number="5"><span class="co"># Using qlogis outcome</span></a>
<a class="sourceLine" id="cb726-6" data-line-number="6">p =<span class="st"> </span>x =<span class="st">  </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">150</span>) <span class="co"># p is sequence of probabilities</span></a>
<a class="sourceLine" id="cb726-7" data-line-number="7"><span class="kw">curve</span>(<span class="kw">qlogis</span>(x), <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">add=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qlogis"></span>
<img src="DS_files/figure-html/qlogis-1.png" alt="Logit - Quantile" width="70%" />
<p class="caption">
Figure 6.32: Logit - Quantile
</p>
</div>

<p>To complement our understanding of <strong>Logistic Regression</strong>, let us review the concept of <strong>Proportionality</strong> and the <strong>Odds</strong> of observing a successful event. For example, suppose we flip a coin 200 times, then calculate the proportion of our experiment landing on heads:</p>

<div class="sourceCode" id="cb727"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb727-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb727-2" data-line-number="2">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb727-3" data-line-number="3">overall.outcome =<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb727-4" data-line-number="4">tosses =<span class="st"> </span><span class="kw">sample</span>(range, <span class="dt">size=</span>overall.outcome, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb727-5" data-line-number="5">heads =<span class="st"> </span><span class="kw">length</span>( <span class="kw">which</span>( tosses <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) ) <span class="co"># no of times coin landed on head</span></a>
<a class="sourceLine" id="cb727-6" data-line-number="6">tails =<span class="st"> </span><span class="kw">length</span>( <span class="kw">which</span>( tosses <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) ) <span class="co"># no of times coin landed on tail</span></a>
<a class="sourceLine" id="cb727-7" data-line-number="7"><span class="kw">c</span>(<span class="st">&quot;head&quot;</span> =<span class="st"> </span>heads <span class="op">/</span><span class="st"> </span>overall.outcome, <span class="st">&quot;tail&quot;</span> =<span class="st"> </span>tails <span class="op">/</span><span class="st"> </span>overall.outcome)</a></code></pre></div>
<pre><code>## head tail 
## 0.53 0.47</code></pre>

<p>Flipping a coin 200 times shows that the <strong>proportion</strong> of landing on heads overall outcome is around 0.53 and that the proportion of landing on tails overall outcome is around 0.47. So the total proportionally should sum to 1.</p>
<p>The <strong>odds</strong> of heads over tails is around 1.1277, and the <strong>odds</strong> of tails over heads is around 0.8868.</p>

<div class="sourceCode" id="cb729"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb729-1" data-line-number="1">p =<span class="st"> </span>heads <span class="op">/</span><span class="st"> </span>overall.outcome</a>
<a class="sourceLine" id="cb729-2" data-line-number="2"><span class="kw">c</span>(<span class="st">&quot;odds of heads&quot;</span> =<span class="st"> </span>p <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p),   <span class="st">&quot;odds of tails&quot;</span> =<span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>p) <span class="op">/</span><span class="st"> </span>p)</a></code></pre></div>
<pre><code>## odds of heads odds of tails 
##        1.1277        0.8868</code></pre>

<p>The result can be equivalently achieved using a very simple <strong>odds ratio</strong> formula:</p>

<div class="sourceCode" id="cb731"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb731-1" data-line-number="1"><span class="kw">c</span>( <span class="st">&quot;odds of heads&quot;</span> =<span class="st"> </span>heads<span class="op">/</span>tails,  <span class="st">&quot;odds of tails&quot;</span> =<span class="st"> </span>tails <span class="op">/</span><span class="st"> </span>heads)</a></code></pre></div>
<pre><code>## odds of heads odds of tails 
##        1.1277        0.8868</code></pre>

<p>To be able to transform the result of our <strong>odds</strong> to a symmetrical range, e.g. [<span class="math inline">\(-\infty,\infty\)</span>], we use <strong>log-odds</strong>.</p>

<div class="sourceCode" id="cb733"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb733-1" data-line-number="1"><span class="kw">c</span>( <span class="st">&quot;log-odds of heads&quot;</span> =<span class="st"> </span><span class="kw">ln</span>(heads<span class="op">/</span>tails),  </a>
<a class="sourceLine" id="cb733-2" data-line-number="2">   <span class="st">&quot;odds of tails&quot;</span>     =<span class="st"> </span><span class="kw">ln</span>(tails <span class="op">/</span><span class="st"> </span>heads))</a></code></pre></div>
<pre><code>## log-odds of heads     odds of tails 
##            0.1201           -0.1201</code></pre>

<p>Now, in terms of <strong>Logistic Regression</strong>, we need to determine if the <span class="math inline">\(H_0\)</span> supports the claim that the predicted outcome does not regress - is not close - to the actual value by any number of random chances (from independent random variables).</p>
<p>To illustrate, we choose a simple continuous independent variable, namely fuel consumption (<strong>mpg</strong>), from the <strong>mtcars</strong> dataset and model a logistic regression. Our dependent variable is <strong>dichotomous</strong> (binary), namely V/Straight Engine (vs).</p>
<p>In determining whether an independent variable <strong>regresses</strong> to a dependent variable, we use the <strong>logit link function</strong> to obtain the estimated <strong>logit</strong> values using <strong>glm(.)</strong>, generating our logistic model in the process. Note that we discuss <strong>Generalized Linear Models (GLM)</strong> in detail in Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under <strong>Regression</strong> Section.</p>

<div class="sourceCode" id="cb735"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb735-1" data-line-number="1"><span class="co"># binomial family with logit link</span></a>
<a class="sourceLine" id="cb735-2" data-line-number="2">logit.model =<span class="st"> </span><span class="kw">glm</span>(vs <span class="op">~</span><span class="st"> </span>mpg, <span class="dt">data=</span>mtcars, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span>logit)) </a>
<a class="sourceLine" id="cb735-3" data-line-number="3">logit.hat =<span class="st"> </span>logit.model<span class="op">$</span>fitted.values</a></code></pre></div>

<p>The model produces fitted values called <strong>logit values</strong> for the log odds. In other words, the values are the logarithm of the odds ratio of <strong>Straight Engine</strong> to <strong>V Engine</strong> for the <strong>vs</strong> dependent variable. See Figure <a href="6.8-the-significance-of-regression.html#fig:logit">6.33</a>.</p>

<div class="sourceCode" id="cb736"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb736-1" data-line-number="1">sort.mpg =<span class="st"> </span><span class="kw">sort</span>(mtcars<span class="op">$</span>mpg, <span class="dt">index.return =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb736-2" data-line-number="2">x =<span class="st"> </span>sort.mpg<span class="op">$</span>x</a>
<a class="sourceLine" id="cb736-3" data-line-number="3">y =<span class="st"> </span>logit.hat[sort.mpg<span class="op">$</span>ix]</a>
<a class="sourceLine" id="cb736-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,</a>
<a class="sourceLine" id="cb736-5" data-line-number="5">     <span class="dt">xlim =</span> <span class="kw">range</span>(x), <span class="dt">ylim =</span> <span class="kw">range</span>(y), <span class="dt">xlab =</span> <span class="st">&quot;mpg (Predictor)&quot;</span>, </a>
<a class="sourceLine" id="cb736-6" data-line-number="6">     <span class="dt">ylab =</span> <span class="st">&quot;logit(p)&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Logit&quot;</span>)</a>
<a class="sourceLine" id="cb736-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb736-8" data-line-number="8"><span class="kw">points</span>(x, y, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb736-9" data-line-number="9"><span class="kw">lines</span>(x, y, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb736-10" data-line-number="10"><span class="kw">points</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>vs, <span class="dt">col=</span></a>
<a class="sourceLine" id="cb736-11" data-line-number="11">         <span class="kw">ifelse</span>(mtcars<span class="op">$</span>vs <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;brown&quot;</span>), <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb736-12" data-line-number="12"><span class="kw">arrows</span>(<span class="fl">24.5</span>, <span class="fl">0.81</span>, <span class="fl">27.5</span>,<span class="fl">0.6</span>, <span class="dt">code=</span><span class="dv">1</span>, <span class="dt">length=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb736-13" data-line-number="13"><span class="kw">text</span>(<span class="dv">30</span>,  <span class="fl">0.55</span>, <span class="dt">label=</span><span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;logit(p)=&quot;</span>,beta<span class="op">^</span>T, <span class="st">&quot;X&quot;</span>)))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:logit"></span>
<img src="DS_files/figure-html/logit-1.png" alt="Logit (Estimated Values)" width="80%" />
<p class="caption">
Figure 6.33: Logit (Estimated Values)
</p>
</div>

<p>Note that the logit model in the figure seems to show a <strong>sigmoid</strong> curve. However, that is not the case. The model produces an outcome that follows a linear logit. That becomes more apparent if we feed the model with missing values and predict the outcome. See Figure <a href="6.8-the-significance-of-regression.html#fig:linearlogistic">6.34</a>.</p>

<div class="sourceCode" id="cb737"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb737-1" data-line-number="1">x =<span class="st"> </span>mpg.predictor =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">40</span>, <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb737-2" data-line-number="2">predicted.logit =<span class="st">  </span><span class="kw">predict.glm</span>(logit.model, </a>
<a class="sourceLine" id="cb737-3" data-line-number="3">                               <span class="kw">data.frame</span>(<span class="dt">mpg =</span> mpg.predictor))</a></code></pre></div>
<div class="sourceCode" id="cb738"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb738-1" data-line-number="1">y =<span class="st"> </span>predicted.logit  </a>
<a class="sourceLine" id="cb738-2" data-line-number="2"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,</a>
<a class="sourceLine" id="cb738-3" data-line-number="3">     <span class="dt">xlim =</span> <span class="kw">range</span>(x), <span class="dt">ylim =</span> <span class="kw">range</span>(y), <span class="dt">xlab =</span> <span class="st">&quot;mpg (Predictor)&quot;</span>, </a>
<a class="sourceLine" id="cb738-4" data-line-number="4">     <span class="dt">ylab =</span> <span class="st">&quot;logit(p)&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Logit (Linear)&quot;</span>)</a>
<a class="sourceLine" id="cb738-5" data-line-number="5"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb738-6" data-line-number="6"><span class="kw">lines</span>(x, y, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb738-7" data-line-number="7"><span class="kw">text</span>(<span class="dv">20</span>,  <span class="dv">3</span>, <span class="dt">label=</span><span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;logit(p)=&quot;</span>, beta<span class="op">^</span>T, <span class="st">&quot;X&quot;</span>)))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linearlogistic"></span>
<img src="DS_files/figure-html/linearlogistic-1.png" alt="Logit (Linear)" width="80%" />
<p class="caption">
Figure 6.34: Logit (Linear)
</p>
</div>

<p>Next, we convert the log odds into the inverse form (which becomes the <strong>P-hat</strong> <span class="math inline">\(\mathbf{\hat{p}}\)</span>). See Figure <a href="6.8-the-significance-of-regression.html#fig:inverseform">6.35</a>. The figure does show the expected <strong>sigmoid</strong> curve.</p>

<div class="sourceCode" id="cb739"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb739-1" data-line-number="1">p.hat =<span class="st"> </span>y =<span class="st"> </span><span class="kw">inverse.logit</span>(predicted.logit)</a></code></pre></div>
<div class="sourceCode" id="cb740"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb740-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,</a>
<a class="sourceLine" id="cb740-2" data-line-number="2">     <span class="dt">xlim =</span> <span class="kw">range</span>(x), <span class="dt">ylim =</span> <span class="kw">range</span>(y), <span class="dt">xlab =</span> <span class="st">&quot;mpg (Predictor)&quot;</span>, </a>
<a class="sourceLine" id="cb740-3" data-line-number="3">     <span class="dt">ylab =</span> <span class="st">&quot;inverse.logit(z) = p.hat&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Inverse Logit (Sigmoid)&quot;</span>)</a>
<a class="sourceLine" id="cb740-4" data-line-number="4"><span class="kw">abline</span>(simple.model, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>) <span class="co">#  </span></a>
<a class="sourceLine" id="cb740-5" data-line-number="5"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb740-6" data-line-number="6"><span class="kw">lines</span>(x, y, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb740-7" data-line-number="7"><span class="kw">text</span>(<span class="dv">25</span>,  <span class="fl">0.6</span>, <span class="dt">label=</span><span class="st">&quot;Sigmoid&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:inverseform"></span>
<img src="DS_files/figure-html/inverseform-1.png" alt="Inverse Logit (Sigmoid)" width="70%" />
<p class="caption">
Figure 6.35: Inverse Logit (Sigmoid)
</p>
</div>

<p>The coefficients of the fitted values show as follows:</p>

<div class="sourceCode" id="cb741"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb741-1" data-line-number="1">beta_<span class="dv">0</span> =<span class="st"> </span><span class="kw">as.numeric</span>(logit.model<span class="op">$</span>coefficients[<span class="dv">1</span>])  </a>
<a class="sourceLine" id="cb741-2" data-line-number="2">beta_<span class="dv">1</span> =<span class="st"> </span><span class="kw">as.numeric</span>(logit.model<span class="op">$</span>coefficients[<span class="dv">2</span>])  </a></code></pre></div>

<p><span class="math inline">\(\beta_0\)</span> = -8.8331 and <span class="math inline">\(\beta_1\)</span> = 0.4304.</p>
<p>If we review Figure <a href="6.8-the-significance-of-regression.html#fig:linearlogistic">6.34</a>, <span class="math inline">\(\beta_0\)</span> is the intercept that touches the y-axis.</p>
<p>We can re-formulate our log-odds model, recalculating <span class="math inline">\(\mathbf{z_i}\)</span>.</p>
<p><span class="math inline">\(z_i\)</span> = <span class="math inline">\(\beta_0\)</span> + <span class="math inline">\(\beta_1\)</span> <span class="math inline">\(x_i\)</span> = -8.8331 + 0.4304 <span class="math inline">\(x_i\)</span></p>

<div class="sourceCode" id="cb742"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb742-1" data-line-number="1">x.observed =<span class="st"> </span>new.mpg =<span class="st"> </span><span class="dv">30</span></a>
<a class="sourceLine" id="cb742-2" data-line-number="2">(<span class="dt">z =</span> beta_<span class="dv">0</span> <span class="op">+</span><span class="st"> </span>beta_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>x.observed)</a></code></pre></div>
<pre><code>## [1] 4.079</code></pre>
<div class="sourceCode" id="cb744"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb744-1" data-line-number="1">(<span class="dt">z =</span> <span class="kw">as.vector</span>(<span class="kw">predict.glm</span>(logit.model, </a>
<a class="sourceLine" id="cb744-2" data-line-number="2">                           <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">mpg =</span> x.observed))))</a></code></pre></div>
<pre><code>## [1] 4.079</code></pre>

<p>In terms of <strong>statistical significance</strong>, let us summarize the fitted model using <strong>glm(.)</strong>.</p>

<div class="sourceCode" id="cb746"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb746-1" data-line-number="1"><span class="kw">summary</span>(logit.model)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = vs ~ mpg, family = binomial(link = logit), data = mtcars)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.213  -0.512  -0.228   0.640   1.698  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)   -8.833      3.162   -2.79   0.0052 **
## mpg            0.430      0.158    2.72   0.0066 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 43.860  on 31  degrees of freedom
## Residual deviance: 25.533  on 30  degrees of freedom
## AIC: 29.53
## 
## Number of Fisher Scoring iterations: 6</code></pre>

<p>In the summary of the model, we see that the observed data (<strong>mpg</strong>) is also <strong>statistically significant</strong> at alpha <span class="math inline">\(\alpha = 0.01\)</span> (e.g., less than one in a hundred chance of being wrong). In other words, the <strong>fuel consumption</strong> has a significant association with the type of engine (<strong>vs</strong>) with 99% confidence.</p>
<p>To illustrate further, let us this time consider a <strong>multiple logistic regression</strong> - that is, with multiple independent variables. Let us use a few new functions; for example, <strong>mvrnorm(.)</strong> from a library called <strong>MASS</strong> to generate a multivariate normal distribution, <strong>rbinom(.)</strong> to generate our binomial response variable, and finally <strong>glm(.)</strong> using a generalized linear model with binomial family. Here is what it looks like:</p>

<div class="sourceCode" id="cb748"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb748-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb748-2" data-line-number="2">binary =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb748-3" data-line-number="3">range =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>, <span class="dt">length.out=</span><span class="dv">100</span>) </a>
<a class="sourceLine" id="cb748-4" data-line-number="4">sample_size =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb748-5" data-line-number="5">predictors =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb748-6" data-line-number="6"><span class="co"># simulate an unexplained residual (Gaussian)</span></a>
<a class="sourceLine" id="cb748-7" data-line-number="7">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dv">0</span>, <span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">20</span> </a>
<a class="sourceLine" id="cb748-8" data-line-number="8">mu =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, predictors)</a>
<a class="sourceLine" id="cb748-9" data-line-number="9">sigma =<span class="st"> </span><span class="kw">diag</span>(<span class="dv">1</span>, predictors, predictors)</a>
<a class="sourceLine" id="cb748-10" data-line-number="10">x_observed =<span class="st"> </span>x =<span class="st">  </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> sample_size, <span class="dt">mu=</span>mu, <span class="dt">Sigma=</span>sigma)</a>
<a class="sourceLine" id="cb748-11" data-line-number="11">y_observed =<span class="st"> </span><span class="kw">inverse.logit</span>(x_observed) <span class="op">+</span><span class="st"> </span>e</a>
<a class="sourceLine" id="cb748-12" data-line-number="12"><span class="co"># try some initial beta values and inject some bias</span></a>
<a class="sourceLine" id="cb748-13" data-line-number="13">b0 =<span class="st"> </span><span class="fl">2.3</span>; b1 =<span class="st"> </span><span class="dv">3</span>; b2 =<span class="st"> </span><span class="dv">1</span>; b3 =<span class="st"> </span><span class="fl">1.5</span> </a>
<a class="sourceLine" id="cb748-14" data-line-number="14">z =<span class="st"> </span>b0 <span class="op">+</span><span class="st"> </span>b1 <span class="op">*</span><span class="st"> </span>x[,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>b2 <span class="op">*</span><span class="st"> </span>x[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>b3 <span class="op">*</span><span class="st"> </span>x[,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb748-15" data-line-number="15">y_expected =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> sample_size, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span><span class="kw">inverse.logit</span>(z))</a>
<a class="sourceLine" id="cb748-16" data-line-number="16">x_obs =<span class="st"> </span>x_observed; y_exp =<span class="st"> </span>y_expected</a>
<a class="sourceLine" id="cb748-17" data-line-number="17">(<span class="dt">glm.model =</span> <span class="kw">glm</span>(y_exp  <span class="op">~</span><span class="st"> </span>x_obs,  <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>)))</a></code></pre></div>
<pre><code>## 
## Call:  glm(formula = y_exp ~ x_obs, family = binomial(link = &quot;logit&quot;))
## 
## Coefficients:
## (Intercept)       x_obs1       x_obs2       x_obs3  
##        2.26         4.04         0.99         1.95  
## 
## Degrees of Freedom: 99 Total (i.e. Null);  96 Residual
## Null Deviance:       120 
## Residual Deviance: 49.9  AIC: 57.9</code></pre>

<p>Here, we see four <strong>beta hat</strong> coefficients (including the intercept) which allows the logistic model to fit:</p>
<p><span class="math inline">\(\hat{\beta}_0\)</span> = 2.2559 <span class="math inline">\(\hat{\beta}_1\)</span> = 4.0386 <span class="math inline">\(\hat{\beta}_2\)</span> = 0.9902 <span class="math inline">\(\hat{\beta}_3\)</span> = 1.948.</p>
<p>Let us compare the outcome of running <strong>summary()</strong> vs <strong>anova()</strong>:</p>

<div class="sourceCode" id="cb750"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb750-1" data-line-number="1"><span class="kw">summary</span>(glm.model)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y_exp ~ x_obs, family = binomial(link = &quot;logit&quot;))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1530  -0.0481   0.0826   0.3347   2.4757  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    2.256      0.572    3.94 0.000081 ***
## x_obs1         4.039      0.998    4.05 0.000052 ***
## x_obs2         0.990      0.427    2.32   0.0205 *  
## x_obs3         1.948      0.664    2.93   0.0033 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 120.43  on 99  degrees of freedom
## Residual deviance:  49.94  on 96  degrees of freedom
## AIC: 57.94
## 
## Number of Fisher Scoring iterations: 7</code></pre>

<p>The outcome of our summary shows that the first coefficient <span class="math inline">\(x\_obs1\)</span> has three asterisks (***), indicating that this coefficient is statistically significant at <span class="math inline">\(\alpha=0.001\)</span>. The second coefficient <span class="math inline">\(x\_obs1\)</span> has one asterisk (*) indicating that this coefficient is significant at <span class="math inline">\(\alpha=0.05\)</span>. The third coefficient is significant at <span class="math inline">\(\alpha=0.10\)</span>. In other words, the first predictor has a significant effect on the outcome with 99% confidence. The second predictor has a significant association (or effect) on the outcome with 95% confidence. Lastly, the third predictor has a significant association (or effect) on the outcome with 90% confidence.</p>
<p>The result rejects the <strong>null hypothesis</strong> at their respective alphas.</p>

<div class="sourceCode" id="cb752"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb752-1" data-line-number="1"><span class="kw">anova</span>(glm.model, <span class="dt">test=</span><span class="st">&quot;Chisq&quot;</span>)</a></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: y_exp
## 
## Terms added sequentially (first to last)
## 
## 
##       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
## NULL                     99      120.4             
## x_obs  3     70.5        96       49.9  3.4e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>Overall, an <strong>ANOVA</strong> using the <strong>Chi-square</strong> test shows that the observed predictors have a significant association or effect on the outcome with 99% confidence (at <span class="math inline">\(\alpha=0.001\)</span>).</p>
<p>One that is almost identical to the <strong>logit</strong> link function is what we call the <strong>probit</strong> link function. In the <strong>inverse-logit</strong> (CDF) function, we generate an S-shaped non-linear curve and determine the dichotomous nature of the outcome in which we say that the upper half of the curve corresponds to the positive half (e.g., YES, TRUE, ONE, and so on) versus the lower half corresponds to the negative half (e.g., NO, FALSE, ZERO, and so on). On the other hand, a bell-shaped non-linear curve is generated by the <strong>inverse-probit</strong> (CDF) function, which is a Gaussian Cumulative Density function.</p>
<p>When using <strong>glm()</strong>, we merely replace the link with <strong>probit</strong> like so:</p>

<div class="sourceCode" id="cb754"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb754-1" data-line-number="1">(<span class="dt">probit.model =</span> <span class="kw">glm</span>(dependent.y <span class="op">~</span><span class="st"> </span>independent.x, </a>
<a class="sourceLine" id="cb754-2" data-line-number="2">                    <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;probit&quot;</span>)))</a></code></pre></div>

<p>We continue to expand on the concept of <strong>Logistic Regression</strong> and <strong>Generalized Regression Models (GLM)</strong> using <strong>glm(.)</strong> in Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under <strong>Poisson Regression</strong> Subsection under <strong>Regression</strong>.</p>
</div>
<div id="poisson-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.4</span> Poisson Regression <a href="6.8-the-significance-of-regression.html#poisson-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to <strong>Logistic Regression</strong>, we also review <strong>Poisson Regression</strong> in the context of <strong>Generalized Linear Model (GLM)</strong>, emphasizing the use of a <strong>link function</strong>.</p>
<p><strong>Poisson Regression</strong> models a regression that describes the number of incidents occurring during a <strong>follow-up period</strong> (per hour, per day, per week). <strong>Poisson models</strong> are suitable for data in which, as examples, we count the number of tweets a person submits per day, the number of eye blinks per minute, or the number of cars paying in a toll plaza every hour.</p>
<p>Here, we deal with Poisson distribution, which is expressed as such:</p>
<p><span class="math display">\[y \sim Pois(\hat{\lambda}) \equiv N(\mu = \hat{\lambda}, \epsilon)\]</span></p>
<p>The intuition behind <strong>Poisson Regression</strong> also starts with two functions similar to <strong>Logistic Regression</strong>.</p>
<p><strong>Link Function</strong>: </p>
<p>A <strong>Link Function</strong> allows us to model regression in linear fashion.</p>
<p><span class="math display" id="eq:equate1080131">\[\begin{align}
\text{link}(\hat{\lambda}) = \log_e\left(y\right) = z \tag{6.136} 
\end{align}\]</span></p>
<p><strong>Inverse Link Function:</strong></p>
<p><span class="math display" id="eq:equate1080132">\[\begin{align}
\text{link}^{-1}(z) = \hat{\lambda} =  \text{exp}(z) \tag{6.137} 
\end{align}\]</span></p>
<p>where: <span class="math inline">\(z = \beta^TX\ \ \ \  \leftarrow\ \ \ \ \beta_0 + \sum_{i=1}^n\beta_i x_i\)</span>.</p>
<p>The Lambda, denoted by <span class="math inline">\(\lambda\)</span>, represents the mean of a <strong>Poisson distribution</strong>. The expectation is that lambda grows in an exponential non-linear fashion, as seen in the <strong>Inverse Link Function</strong>. However, our goal is to linearize the function; thus, we have the <strong>Link Function</strong>.</p>
<p>A detailed discussion of <strong>Poisson Regression</strong> is covered in Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under <strong>Regression</strong> Section.</p>
</div>
<div id="cox-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.5</span> Cox Regression <a href="6.8-the-significance-of-regression.html#cox-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Cox Regression</strong>, also known as <strong>Cox proportional hazard</strong> regression, models a regression describing the length of time an event occurs, also called <strong>time-to-event (TTE)</strong> occurrence. That applies to cases such as survival rate and hazard rate of subjects <span class="citation">(Cox D. R. <a href="bibliography.html#ref-ref921d">1972</a>)</span>.</p>
<p>Let us review Figure <a href="6.8-the-significance-of-regression.html#fig:survival">6.36</a> to illustrate.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:survival"></span>
<img src="DS_files/figure-html/survival-1.png" alt="Survival Probability" width="70%" />
<p class="caption">
Figure 6.36: Survival Probability
</p>
</div>

<p>Suppose from the figure that we split a group of subjects into two environments - one environment with treatment and another environment with no treatment. We then measure the probability of survival of the two groups.</p>
<p>Below is a summary of the <strong>survival probability</strong> in the figure:</p>

<pre><code>##                                           T1   T2   T3   T4 T5
## Total Subjects in Treatment             9.00 7.00 6.00 2.00  1
## Total Subjects not in Treatment        11.00 9.00 9.00 6.00  4
## Treated Subjects Failing to Survive     2.00 1.00 4.00 1.00  1
## Untreated Subjects Failing to Survive   2.00 0.00 3.00 2.00  4
## Survival Probab. of Treated Subjects    0.78 0.67 0.22 0.11  0
## Survival Probab. of Untreated Subjects  0.82 0.82 0.55 0.36  0</code></pre>

<p>Survival probability is the probability of surviving through time (t) and is expressed as:</p>
<p><span class="math display" id="eq:equate1080133">\[\begin{align}
S(t) = \prod_{i = 1}^t \frac{n_i - d_i}{n_i} \tag{6.138} 
\end{align}\]</span></p>
<p>For example, there are a total of 9 subjects in treatment at time one, <strong>T1</strong> and a total of 11 subjects that are not in treatment. Now, at time three, <strong>T3</strong>, <strong>because some subjects failed to survive previous to</strong> <strong>T3</strong>, we are left with 6 subjects in treatment and 9 subjects not in treatment. At time one, <strong>T1</strong>, we have 2 subjects in treatment that failed to survive and 2 subjects not in treatment that failed to survive. At time three, <strong>T3</strong>, we have 4 subjects in treatment that failed to survive and 3 subjects not in treatment that failed to survive. The survival probability at time one, <strong>T1</strong>, of the group with treatment and the group with no treatment are 0.78 and 0.82, respectively. At time three, <strong>T3</strong>, the survival probabilities of the group with treatment and the group without treatment are 0.22 and 0.55, respectively.</p>
<p>We calculate the survival probability at <strong>T1</strong> like so:</p>

<p><span class="math display" id="eq:equate1080134">\[\begin{align}
S_{(treated)}(t=1) = \left(\frac{9-4}{9}\right) = 0.56
\ \ \ \ \ \ \ \ 
S_{(not treated)}(t=1) = \left(\frac{11 - 0}{11}\right) = 1.00 \tag{6.139} 
\end{align}\]</span>
</p>
<p>The survival probability at <strong>T3</strong> is calculated like so:</p>
<p><span class="math display">\[\begin{align*}
S_{(treated)}(t=3) {}&amp;= \left(\frac{9-4}{9}\right)_{T1} \times
 \left(\frac{5-0}{5}\right)_{T2}  \times
 \left(\frac{5-3}{5}\right)_{T3}  = 0.22\\
S_{(not\ treated)}(t=3) &amp;= \left(\frac{11-0}{11}\right)_{T1} \times
 \left(\frac{11-1}{11}\right)_{T2}  \times
 \left(\frac{10-4}{10}\right)_{T3}  = 0.55
\end{align*}\]</span></p>
<p>Also, it helps to understand the <strong>Hazard Ratio</strong> (or risk ratio) which is expressed as:</p>
<p><span class="math display" id="eq:equate1080135">\[\begin{align}
R(t) = \frac{risk_{(treatment)}}{risk_{(control)}} \tag{6.140} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>treatment</strong> refers to the exposure of a group to treatment (this is an experimental group).</li>
<li><strong>control</strong> refers to a baseline group in a sample not exposed to treatment (this is a control group).</li>
</ul>
<p>For example, suppose we have a sample of plants which we expose to a type of soil with special treatment. Our experiment is to determine if the plant survives under such conditions.</p>

<div class="sourceCode" id="cb756"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb756-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb756-2" data-line-number="2">sample_size =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb756-3" data-line-number="3"><span class="co"># T - with treatment, N - with no treatment</span></a>
<a class="sourceLine" id="cb756-4" data-line-number="4">treatment_range =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;T&quot;</span>, <span class="st">&quot;N&quot;</span>) </a>
<a class="sourceLine" id="cb756-5" data-line-number="5">population =<span class="st"> </span><span class="kw">sample</span>( treatment_range, <span class="dt">size =</span> <span class="dv">300</span>, <span class="dt">replace=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb756-6" data-line-number="6">sample =<span class="st">  </span><span class="kw">sample</span>( population , <span class="dt">size =</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span> )</a>
<a class="sourceLine" id="cb756-7" data-line-number="7">treated =<span class="st"> </span>sample[<span class="kw">which</span>(sample<span class="op">==</span><span class="st">&quot;T&quot;</span>)]</a>
<a class="sourceLine" id="cb756-8" data-line-number="8">not_treated =<span class="st"> </span>sample[<span class="kw">which</span>(sample<span class="op">==</span><span class="st">&quot;N&quot;</span>)]</a></code></pre></div>

<p>In terms of risk ratio, we have the following computation:</p>
<div class="sourceCode" id="cb757"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb757-1" data-line-number="1">(<span class="dt">risk_ratio =</span> <span class="kw">length</span>(treated) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(not_treated))</a></code></pre></div>
<pre><code>## [1] 1.222</code></pre>
<p>A risk ratio gives the following interpretation:</p>
<p><span class="math display">\[\begin{align*}
R {}&amp;= 1\ \ \ \ \text{exposure to treatment has no impact on risk of an incident}\\
R &amp;&gt; 1\ \ \ \ \text{suggests an increased risk of an incident in the treatment group} \\
R &amp;&lt; 1\ \ \ \ \ \text{suggests a reduced risk of an incident in the treatment group}
\end{align*}\]</span></p>
<p>Because <strong>cox regression</strong> covers a wide range of topics, we only cover here <strong>significance of regression</strong>.</p>
<p>The Cox regression model at time (t) is expressed as:</p>
<p><span class="math display" id="eq:equate1080136">\[\begin{align}
ln \frac{h(t|x_1, x_2,...,x_n)}{h_0(t)} = \sum_{i=1}^n \beta_i x_i  \tag{6.141} 
\end{align}\]</span></p>
<p>which is derived from the following equation:</p>
<p><span class="math display" id="eq:equate1080137">\[\begin{align}
h(t) = h_0(t|x_1, x_2,...,x_n)\times exp \left(\sum_{i=1}^n \beta_i x_i \right) \tag{6.142} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(h(\mathbf{t|x_1, x_2,...,x_n)}\)</span> is the expected incidence (hazard) at time (t) given independent variables.</li>
<li><span class="math inline">\(h_0(t)\)</span> is the baseline incidence (hazard) at time (t) when <span class="math inline">\(x_1 = x_2 = ... = x_n = 0\)</span>.</li>
<li><span class="math inline">\(\mathbf{\beta_i}\)</span> are the coefficients,</li>
<li><span class="math inline">\(\mathbf{x_i}\)</span> are the predictive variables.</li>
</ul>
<p>To illustrate, we create a dataset with survival range, environment type, and status. The idea is to understand the survival pattern of a specie exposed to three different types of environments where status indicates the following (1=survived, 0=failed to survive):</p>

<div class="sourceCode" id="cb759"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb759-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb759-2" data-line-number="2">sample_size =<span class="st"> </span><span class="dv">300</span></a>
<a class="sourceLine" id="cb759-3" data-line-number="3">survival_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb759-4" data-line-number="4">status_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb759-5" data-line-number="5">environment_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb759-6" data-line-number="6">time =<span class="st"> </span><span class="kw">sample</span>( survival_range, <span class="dt">size =</span>sample_size, <span class="dt">replace=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb759-7" data-line-number="7">status =<span class="st">  </span><span class="kw">sample</span>( status_range , <span class="dt">size =</span>sample_size, <span class="dt">replace=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb759-8" data-line-number="8">environment =<span class="st"> </span><span class="kw">sample</span>( environment_range , <span class="dt">size =</span>sample_size, </a>
<a class="sourceLine" id="cb759-9" data-line-number="9">                      <span class="dt">replace=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb759-10" data-line-number="10">data =<span class="st"> </span><span class="kw">cbind</span>(time, status) </a>
<a class="sourceLine" id="cb759-11" data-line-number="11">data =<span class="st"> </span><span class="kw">cbind</span>(data,  environment  )</a>
<a class="sourceLine" id="cb759-12" data-line-number="12">data =<span class="st"> </span><span class="kw">as.data.frame</span>(data)</a>
<a class="sourceLine" id="cb759-13" data-line-number="13">data<span class="op">$</span>environment =<span class="st"> </span><span class="kw">as.factor</span>(data<span class="op">$</span>environment)</a>
<a class="sourceLine" id="cb759-14" data-line-number="14"><span class="kw">head</span>(data)</a></code></pre></div>
<pre><code>##   time status environment
## 1   33      0           3
## 2   20      0           3
## 3   31      0           2
## 4   24      0           3
## 5    7      0           3
## 6    4      1           1</code></pre>

<p>Let us use a built-in function called <strong>coxph(.)</strong> from <strong>survival</strong> library to model our <strong>Cox regression</strong>:</p>

<div class="sourceCode" id="cb761"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb761-1" data-line-number="1"><span class="kw">library</span>(survival)</a>
<a class="sourceLine" id="cb761-2" data-line-number="2">(<span class="dt">cox.model =</span> <span class="kw">coxph</span>( <span class="kw">Surv</span>(time, status) <span class="op">~</span><span class="st"> </span>environment,  <span class="dt">data =</span> data ))</a></code></pre></div>
<pre><code>## Call:
## coxph(formula = Surv(time, status) ~ environment, data = data)
## 
##              coef exp(coef) se(coef)    z   p
## environment2 -0.1       0.9      0.2 -0.5 0.6
## environment3  0.2       1.2      0.2  1.0 0.3
## 
## Likelihood ratio test=2  on 2 df, p=0.3
## n= 300, number of events= 152</code></pre>

<p>Now, we summarize the model for analysis:</p>

<div class="sourceCode" id="cb763"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb763-1" data-line-number="1"><span class="kw">summary</span>(cox.model)</a></code></pre></div>
<pre><code>## Call:
## coxph(formula = Surv(time, status) ~ environment, data = data)
## 
##   n= 300, number of events= 152 
## 
##                coef exp(coef) se(coef)     z Pr(&gt;|z|)
## environment2 -0.104     0.901    0.194 -0.54     0.59
## environment3  0.204     1.226    0.209  0.98     0.33
## 
##              exp(coef) exp(-coef) lower .95 upper .95
## environment2     0.901      1.110     0.616      1.32
## environment3     1.226      0.816     0.814      1.85
## 
## Concordance= 0.528  (se = 0.027 )
## Likelihood ratio test= 2.31  on 2 df,   p=0.3
## Wald test            = 2.36  on 2 df,   p=0.3
## Score (logrank) test = 2.38  on 2 df,   p=0.3</code></pre>

<p>The <strong>z</strong> is a wald statistics computed based on <span class="math inline">\(z = \frac{coef}{se(coef)}\)</span>. In the example above, our <strong>p-value</strong> is greater than our typical alpha values, and so our <strong>null hypothesis</strong> holds in which our coefficient does not have any significant effect (or the different environment does not contribute to survivability).</p>
<p><span class="math display" id="eq:equate1080139" id="eq:equate1080138">\[\begin{align}
H_0 {}&amp;: \beta_1 = 0  \tag{6.143} \\
H_1 &amp;: \beta_1 \ne 0 \tag{6.144} 
\end{align}\]</span></p>
<p>However, if we are to consider <strong>inference for regression</strong>, which we discuss further in the next section, we may at least be able to predict the probability of survivability.</p>
<p>We use a built-in function called <strong>survfit(.)</strong> to fit our <strong>cox model</strong>, then we capture a predicted probability at time (t) (where i=20):</p>

<div class="sourceCode" id="cb765"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb765-1" data-line-number="1">p =<span class="st"> </span><span class="kw">survfit</span>(cox.model)</a>
<a class="sourceLine" id="cb765-2" data-line-number="2">i =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb765-3" data-line-number="3">t =<span class="st"> </span>p<span class="op">$</span>time[i];  r =<span class="st"> </span>p<span class="op">$</span>n.risk[i]; e =<span class="st"> </span>p<span class="op">$</span>n.event[i]; s =<span class="st"> </span>p<span class="op">$</span>surv[i]</a>
<a class="sourceLine" id="cb765-4" data-line-number="4">l =<span class="st"> </span>p<span class="op">$</span>lower[i]; u =<span class="st"> </span>p<span class="op">$</span>upper[i]; c =<span class="st"> </span>p<span class="op">$</span>conf.int</a>
<a class="sourceLine" id="cb765-5" data-line-number="5"><span class="kw">t</span>(<span class="kw">as.matrix</span>(<span class="kw">round</span>(<span class="kw">c</span>(<span class="st">&quot;survival prob.&quot;</span>=<span class="kw">round</span>(s,<span class="dv">2</span>), <span class="st">&quot;time&quot;</span>=<span class="st"> </span>t, </a>
<a class="sourceLine" id="cb765-6" data-line-number="6">    <span class="st">&quot;lower&quot;</span>=l, <span class="st">&quot;upper&quot;</span>=u, <span class="st">&quot;conf&quot;</span>=c), <span class="dv">2</span>)))</a></code></pre></div>
<pre><code>##      survival prob. time lower upper conf
## [1,]           0.81   20  0.76  0.86 0.95</code></pre>

<p>Let us plot the survival probability (see Figure <a href="6.8-the-significance-of-regression.html#fig:coxreg">6.37</a>).</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:coxreg"></span>
<img src="DS_files/figure-html/coxreg-1.png" alt="Kaplan-Meier Estimate" width="70%" />
<p class="caption">
Figure 6.37: Kaplan-Meier Estimate
</p>
</div>

<p>In the next few sections, let us cover <strong>Polynomial</strong> regressions as we continue to emphasize on significance of regression.</p>
</div>
<div id="polynomial-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.6</span> Polynomial Regression <a href="6.8-the-significance-of-regression.html#polynomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, we have discussed data points that follow <strong>normal distribution</strong> and use <strong>linear regression</strong> to fit a model to data. We also discussed data points that follow <strong>exponential distribution</strong> and use <strong>generalized linear regression</strong> to fit.</p>
<p>Now in cases in which the distribution of data may not necessarily reflect a simple straight line or a simple curve line but rather curvier or deeply curvy, it helps to carefully obtain a model that is not too <strong>wiggly</strong> causing an overfit, or that is not approaching a straight line causing an underfit. Both situations offset the power of inference or prediction.</p>
<p>Therefore, in this section, we discuss the importance of fitting a model - ensuring our model regresses to the actual data set. The more representative our model is, the more we can use it for prediction in general. However, an overfitted model may not be able to apply to a new data set; likewise, one that is underfitted may just as well not apply.</p>
<p>This section talks about higher-degree polynomials as an extended discussion of <strong>simple linear regression</strong> - one-degree polynomial.</p>
<p>A good understanding of this section requires a good review of <strong>B-spline regression</strong> in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>). It may help to re-iterate the four knobs that we can use to manipulate and influence the curvature of B-splines, especially when fitting a model:</p>
<ul>
<li>the coefficients</li>
<li>the number of knots</li>
<li>the placements of knots</li>
<li>the basis function</li>
</ul>
<p>For illustration, let us use <strong>Chebyshev polynomial of the first kind</strong> as a template for our next discussion. <strong>Chebyshev polynomial</strong> is introduced in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) under the <strong>Higher Degree Polynomials</strong> Section. Out of four polynomials, let us use the quintic polynomial and generate the response variables: one for observed, one for expected:</p>

<div class="sourceCode" id="cb767"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb767-1" data-line-number="1">quadratic &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">2</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb767-2" data-line-number="2">cubic     &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">4</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">3</span><span class="op">*</span>x }</a>
<a class="sourceLine" id="cb767-3" data-line-number="3">quartic   &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">8</span><span class="op">*</span>x<span class="op">^</span><span class="dv">4</span> <span class="op">-</span><span class="st"> </span><span class="dv">8</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span>  }</a>
<a class="sourceLine" id="cb767-4" data-line-number="4">quintic   &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">16</span><span class="op">*</span>x<span class="op">^</span><span class="dv">5</span> <span class="op">-</span><span class="st"> </span><span class="dv">20</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">5</span><span class="op">*</span>x  }</a>
<a class="sourceLine" id="cb767-5" data-line-number="5">x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">150</span>)</a>
<a class="sourceLine" id="cb767-6" data-line-number="6">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">5</span>  <span class="co"># simulate Gaussian residual</span></a>
<a class="sourceLine" id="cb767-7" data-line-number="7">y_observed =<span class="st"> </span><span class="kw">quintic</span>(x) <span class="op">+</span><span class="st"> </span>e</a>
<a class="sourceLine" id="cb767-8" data-line-number="8">y_expected =<span class="st"> </span><span class="kw">quintic</span>(x) </a></code></pre></div>

<p>Here, to create a model that fits the quintic polynomial above for <strong>Polynomial regression</strong>, let us use a new function called <strong>bs(.)</strong> from the <strong>splines</strong> library. The <strong>bs</strong> stands for B-spline. Notice that we use five degrees of freedom. That is to attempt to render enough <strong>wiggles</strong> or <strong>oscillation</strong> to form a quintic polynomial curve, as we see after plotting.</p>

<div class="sourceCode" id="cb768"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb768-1" data-line-number="1"><span class="kw">library</span>(splines)</a>
<a class="sourceLine" id="cb768-2" data-line-number="2">poly.model =<span class="st"> </span><span class="kw">lm</span>(y_expected <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(x, <span class="dt">df=</span><span class="dv">5</span>))</a></code></pre></div>

<p>Now, let us plot (See Figure <a href="6.8-the-significance-of-regression.html#fig:bsplineregress">6.38</a>). Notice the polynomial generated by <span class="math inline">\(bs(x,df=5)\)</span>. It can generate a quintic polynomial that fits close to the actual.</p>

<div class="sourceCode" id="cb769"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb769-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb769-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis (quintic)&quot;</span>,</a>
<a class="sourceLine" id="cb769-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Chebyshev polynomial of the first kind (Quintic poly)&quot;</span>)</a>
<a class="sourceLine" id="cb769-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb769-5" data-line-number="5"><span class="kw">points</span>(x, y_observed, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb769-6" data-line-number="6"><span class="kw">lines</span>(x, y_expected, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb769-7" data-line-number="7"><span class="kw">lines</span>(x, <span class="kw">fitted</span>(poly.model), <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb769-8" data-line-number="8"><span class="kw">legend</span>(<span class="dv">0</span>, <span class="fl">-0.5</span>, </a>
<a class="sourceLine" id="cb769-9" data-line-number="9">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;expected fit&quot;</span>, <span class="st">&quot;poly.model&quot;</span>),</a>
<a class="sourceLine" id="cb769-10" data-line-number="10">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>), <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bsplineregress"></span>
<img src="DS_files/figure-html/bsplineregress-1.png" alt="Chebyshev polynomial of the first kind (Quintic poly)" width="70%" />
<p class="caption">
Figure 6.38: Chebyshev polynomial of the first kind (Quintic poly)
</p>
</div>

<p>Let us evaluate the <strong>significance of regression</strong> for our polynomial model:</p>

<div class="sourceCode" id="cb770"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb770-1" data-line-number="1"><span class="kw">summary</span>(poly.model)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y_expected ~ bs(x, df = 5))
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -0.377 -0.098  0.000  0.098  0.377 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -0.6227     0.0520   -12.0   &lt;2e-16 ***
## bs(x, df = 5)1   3.4213     0.0992    34.5   &lt;2e-16 ***
## bs(x, df = 5)2  -3.6255     0.0669   -54.2   &lt;2e-16 ***
## bs(x, df = 5)3   4.8710     0.0910    53.5   &lt;2e-16 ***
## bs(x, df = 5)4  -2.1758     0.0740   -29.4   &lt;2e-16 ***
## bs(x, df = 5)5   1.2455     0.0757    16.4   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.118 on 144 degrees of freedom
## Multiple R-squared:  0.973,  Adjusted R-squared:  0.972 
## F-statistic: 1.04e+03 on 5 and 144 DF,  p-value: &lt;2e-16</code></pre>

<p>Notice that we base the number of coefficients (excluding intercept) on the number of degrees of freedom provided to our model. Looking at the geometrical fit of our model, it may seem that the fitted model using bs(x, df=5) may not be close enough to the actual polynomial curve - it does not regress enough. Nonetheless, our coefficients are significant at <span class="math inline">\(\alpha = 0.001\)</span>. We may accept the model or continue to adjust and improve.</p>
<p>In the later section, we show how to adjust the model by covering <strong>power of prediction</strong> under <strong>Inference for Regression</strong>.</p>
</div>
<div id="b-splines-and-natural-splines" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.7</span> B-Splines and Natural Splines  <a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We extend our discussion of <strong>B-Splines</strong> and <strong>Natural Splines</strong> from Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) under <strong>Polynomial Regression</strong> Section and <strong>Polynomial Interpolation</strong> Section.</p>
<p>In the previous section, we show how to interpret the <strong>significance of regression</strong>. In this section, we continue <strong>B-spline regression</strong> from the perspective of <strong>prediction</strong>. We can attain Better prediction if only we can adjust a <strong>B-spline model</strong> to fit better. Here, we show two adjustments to fit a <strong>B-SPline</strong> and one adjustment to fit a <strong>Natural Spline</strong>. From there, we obtain three models which we can use to predict.</p>
<p>Let us continue to use the same dataset (using one of the <strong>Chebyshev polynomials</strong>): </p>

<div class="sourceCode" id="cb772"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb772-1" data-line-number="1">quadratic &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">2</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb772-2" data-line-number="2">cubic     &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">4</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">3</span><span class="op">*</span>x }</a>
<a class="sourceLine" id="cb772-3" data-line-number="3">quartic   &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">8</span><span class="op">*</span>x<span class="op">^</span><span class="dv">4</span> <span class="op">-</span><span class="st"> </span><span class="dv">8</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span>  }</a>
<a class="sourceLine" id="cb772-4" data-line-number="4">quintic   &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">16</span><span class="op">*</span>x<span class="op">^</span><span class="dv">5</span> <span class="op">-</span><span class="st"> </span><span class="dv">20</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">5</span><span class="op">*</span>x  }</a>
<a class="sourceLine" id="cb772-5" data-line-number="5">x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">150</span>)</a>
<a class="sourceLine" id="cb772-6" data-line-number="6">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">5</span>  <span class="co"># simulate Gaussian residual</span></a>
<a class="sourceLine" id="cb772-7" data-line-number="7">y_observed =<span class="st"> </span><span class="kw">quintic</span>(x) <span class="op">+</span><span class="st"> </span>e</a>
<a class="sourceLine" id="cb772-8" data-line-number="8">y_expected =<span class="st"> </span><span class="kw">quintic</span>(x) </a></code></pre></div>

<p>Let us now try to adjust:</p>
<ul>
<li>use df=3 (3 degrees of freedom) - for the sake of showing insufficient oscillation (underfit):</li>
</ul>

<div class="sourceCode" id="cb773"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb773-1" data-line-number="1">adj1.model =<span class="st"> </span><span class="kw">lm</span>(y_expected <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(x, <span class="dt">df=</span><span class="dv">3</span>))</a></code></pre></div>

<ul>
<li>still use df=5 (5 degrees of freedom) with the following number of knots and placements - to show that the wrong number of knots and placement could create greater displacement of the oscillation (with possible signs of rank deficiency - see next topic around knob adjustments):</li>
</ul>

<div class="sourceCode" id="cb774"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb774-1" data-line-number="1">x.df5.knots4 =<span class="st"> </span><span class="kw">bs</span>(x, <span class="dt">df=</span><span class="dv">5</span>, <span class="dt">knots=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.6</span>, <span class="fl">-0.2</span>, <span class="dv">5</span>, <span class="fl">0.9</span>))</a>
<a class="sourceLine" id="cb774-2" data-line-number="2">adj2.model =<span class="st"> </span><span class="kw">lm</span>(y_expected <span class="op">~</span><span class="st"> </span>x.df5.knots4)</a></code></pre></div>

<ul>
<li>use another function called <strong>ns()</strong> - <strong>natural spline</strong> with the following parameters - to show that even a natural spline, we can also find ways to fit a model:</li>
</ul>

<div class="sourceCode" id="cb775"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb775-1" data-line-number="1">adj3.model =<span class="st"> </span><span class="kw">lm</span>(y_expected <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(x, <span class="dt">df=</span><span class="dv">5</span>))</a></code></pre></div>

<p>Let us try to plot (this time coloring the actual fit with grey color to emphasize the other adjusted models). See Figure <a href="6.8-the-significance-of-regression.html#fig:knobsregress">6.39</a>.</p>

<div class="sourceCode" id="cb776"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb776-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb776-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis (quintic)&quot;</span>,</a>
<a class="sourceLine" id="cb776-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Fitting Adjusted B-Spline Models&quot;</span>)</a>
<a class="sourceLine" id="cb776-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb776-5" data-line-number="5"><span class="kw">points</span>(x, y_observed, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb776-6" data-line-number="6"><span class="kw">lines</span>(x, y_expected, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb776-7" data-line-number="7"><span class="kw">lines</span>(x, <span class="kw">fitted</span>(adj1.model), <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb776-8" data-line-number="8"><span class="kw">lines</span>(x, <span class="kw">fitted</span>(adj2.model), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb776-9" data-line-number="9"><span class="kw">lines</span>(x, <span class="kw">fitted</span>(adj3.model), <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb776-10" data-line-number="10"><span class="kw">legend</span>(<span class="fl">0.1</span>, <span class="fl">-0.5</span>, </a>
<a class="sourceLine" id="cb776-11" data-line-number="11">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;expected fit&quot;</span>, <span class="st">&quot;adj1.model&quot;</span>, </a>
<a class="sourceLine" id="cb776-12" data-line-number="12">              <span class="st">&quot;adj2.model&quot;</span>,  <span class="st">&quot;adj3.model&quot;</span>),</a>
<a class="sourceLine" id="cb776-13" data-line-number="13">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;brown&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>), <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:knobsregress"></span>
<img src="DS_files/figure-html/knobsregress-1.png" alt="Fitting Adjusted B-Spline Models" width="70%" />
<p class="caption">
Figure 6.39: Fitting Adjusted B-Spline Models
</p>
</div>

<p>As one can imagine, there are more ways than just one to generate a linear model with multiple knobs available to adjust, allowing us to try to tune a polynomial model so it can fit well to a given data.</p>
<p>Now, using the adjusted models, we demonstrate three predictions:</p>

<div class="sourceCode" id="cb777"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb777-1" data-line-number="1"><span class="co"># predict via interpolation</span></a>
<a class="sourceLine" id="cb777-2" data-line-number="2">x.new =<span class="st">  </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">150</span>) </a>
<a class="sourceLine" id="cb777-3" data-line-number="3">y1.predict =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(adj1.model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> x.new))</a>
<a class="sourceLine" id="cb777-4" data-line-number="4">y2.predict =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(adj2.model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> x.new))</a></code></pre></div>
<pre><code>## Warning in predict.lm(adj2.model, newdata = data.frame(x = x.new)): prediction
## from a rank-deficient fit may be misleading</code></pre>
<div class="sourceCode" id="cb779"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb779-1" data-line-number="1">y3.predict =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(adj3.model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> x.new))</a></code></pre></div>

<p>We should also notice that the second prediction renders a warning about <strong>rank deficiency</strong>. If we are to run a summary of the model, it turns out that the estimated value of one of the <strong>coefficients</strong> is not available due to <strong>singularity</strong>, effectively rendering a linear combination with one being deficient in forming a <strong>full rank</strong>.   </p>

<div class="sourceCode" id="cb780"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb780-1" data-line-number="1"><span class="kw">summary</span>(adj2.model)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y_expected ~ x.df5.knots4)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -0.659 -0.271 -0.013  0.285  0.972 
## 
## Coefficients: (1 not defined because of singularities)
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -1.271      0.194   -6.54  1.0e-09 ***
## x.df5.knots41    4.207      0.349   12.05  &lt; 2e-16 ***
## x.df5.knots42   -0.342      0.218   -1.57     0.12    
## x.df5.knots43    1.607      0.308    5.22  6.2e-07 ***
## x.df5.knots44    2.650      0.271    9.80  &lt; 2e-16 ***
## x.df5.knots45   -0.261      0.250   -1.05     0.30    
## x.df5.knots46    2.929      0.348    8.43  3.5e-14 ***
## x.df5.knots47       NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.362 on 143 degrees of freedom
## Multiple R-squared:  0.75,   Adjusted R-squared:  0.739 
## F-statistic: 71.3 on 6 and 143 DF,  p-value: &lt;2e-16</code></pre>

<p>Let us plot the predictions (see Figure <a href="6.8-the-significance-of-regression.html#fig:knobsregress1">6.40</a>).</p>

<div class="sourceCode" id="cb782"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb782-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb782-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis (quintic)&quot;</span>,</a>
<a class="sourceLine" id="cb782-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Plotting Predicted B-Spline Lines&quot;</span>)</a>
<a class="sourceLine" id="cb782-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb782-5" data-line-number="5"><span class="kw">points</span>(x, y_observed, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb782-6" data-line-number="6"><span class="kw">lines</span>(x, y_expected, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb782-7" data-line-number="7"><span class="kw">lines</span>(x.new, y1.predict, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb782-8" data-line-number="8"><span class="kw">lines</span>(x.new, y2.predict, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb782-9" data-line-number="9"><span class="kw">lines</span>(x.new, y3.predict, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb782-10" data-line-number="10"><span class="kw">legend</span>(<span class="fl">0.1</span>, <span class="fl">-0.5</span>, </a>
<a class="sourceLine" id="cb782-11" data-line-number="11">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;expected fit&quot;</span>, <span class="st">&quot;y1.predict&quot;</span>, </a>
<a class="sourceLine" id="cb782-12" data-line-number="12">              <span class="st">&quot;y2.predict&quot;</span>,  <span class="st">&quot;y3.predict&quot;</span>),</a>
<a class="sourceLine" id="cb782-13" data-line-number="13">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;brown&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>), <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:knobsregress1"></span>
<img src="DS_files/figure-html/knobsregress1-1.png" alt="Plotting Predicted B-Spline Lines" width="80%" />
<p class="caption">
Figure 6.40: Plotting Predicted B-Spline Lines
</p>
</div>

<p>Notice that the curves in the plot correspond to those in Figure <a href="6.8-the-significance-of-regression.html#fig:knobsregress1">6.40</a>. That is only because the predicted curves are already the fitted curves. Any new data always predicts a value in the y-axis inside the fitted curve.</p>
</div>
<div id="spline-smoothing" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.8</span> Spline Smoothing <a href="6.8-the-significance-of-regression.html#spline-smoothing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are times when curves seem rough - rather than smooth. Hence, another form of adjustment to make is by smoothing splines. In terms of the formula, we reference Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) under <strong>B-Spline Regression</strong> Subsection under <strong>Approximating Polynomial Functions by Interpolation</strong> Section.</p>
<p>Note here that if done optimally, smoothing a curve allows us to also <strong>fit a smooth spline</strong>.</p>
<p>To illustrate, we intentionally reduce our dataset to a sample size of maybe only 10. That creates a crooked line instead of a smooth line. We use a function in R called <strong>approx(.)</strong> to simulate a crooked line.</p>

<div class="sourceCode" id="cb783"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb783-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb783-2" data-line-number="2">sample_size =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb783-3" data-line-number="3">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,sample_size)</a>
<a class="sourceLine" id="cb783-4" data-line-number="4">y =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb783-5" data-line-number="5">crooked.line =<span class="st"> </span><span class="kw">approx</span>(x, y, <span class="dt">method=</span><span class="st">&quot;linear&quot;</span>, <span class="dt">n=</span>sample_size)</a></code></pre></div>

<p>We then apply a few smoothing adjustments to the crooked line by using two built-in R functions called <strong>spline()</strong> and <strong>smooth.spline()</strong> to demonstrate our case.</p>

<div class="sourceCode" id="cb784"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb784-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb784-2" data-line-number="2">spline =<span class="st"> </span><span class="kw">spline</span>(x, y, <span class="dt">n=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb784-3" data-line-number="3">spline1.model =<span class="st"> </span><span class="kw">smooth.spline</span>(x, y, <span class="dt">spar=</span><span class="fl">0.30</span>)</a>
<a class="sourceLine" id="cb784-4" data-line-number="4">spline2.model =<span class="st"> </span><span class="kw">smooth.spline</span>(x, y, <span class="dt">spar=</span><span class="fl">1.00</span>)</a>
<a class="sourceLine" id="cb784-5" data-line-number="5">spline3.model =<span class="st"> </span><span class="kw">smooth.spline</span>(x, y, <span class="dt">df =</span> <span class="dv">9</span>)</a></code></pre></div>

<p>We are now ready to predict.</p>

<div class="sourceCode" id="cb785"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb785-1" data-line-number="1">x.interpolate =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, sample_size, <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb785-2" data-line-number="2">smooth1.line =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(spline1.model, x.interpolate)</a>
<a class="sourceLine" id="cb785-3" data-line-number="3">smooth2.line =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(spline2.model, x.interpolate)</a>
<a class="sourceLine" id="cb785-4" data-line-number="4">smooth3.line =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(spline3.model, x.interpolate)</a></code></pre></div>

<p>Let us plot our prediction (see Figure <a href="6.8-the-significance-of-regression.html#fig:splineapprox">6.41</a>).</p>

<div class="sourceCode" id="cb786"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb786-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">1</span>,sample_size), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>), </a>
<a class="sourceLine" id="cb786-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis (approx)&quot;</span>,</a>
<a class="sourceLine" id="cb786-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Fitting a Smooth Spline&quot;</span>)</a>
<a class="sourceLine" id="cb786-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb786-5" data-line-number="5"><span class="kw">lines</span>(crooked.line, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb786-6" data-line-number="6"><span class="kw">lines</span>(spline, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb786-7" data-line-number="7"><span class="kw">lines</span>(smooth1.line, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb786-8" data-line-number="8"><span class="kw">lines</span>(smooth2.line, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb786-9" data-line-number="9"><span class="kw">lines</span>(smooth3.line, <span class="dt">col=</span><span class="st">&quot;purple&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb786-10" data-line-number="10"><span class="kw">points</span>(x, y, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb786-11" data-line-number="11"><span class="kw">legend</span>(<span class="dv">7</span>, <span class="fl">-0.5</span>, </a>
<a class="sourceLine" id="cb786-12" data-line-number="12">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;crooked.line&quot;</span>, <span class="st">&quot;spline&quot;</span>, </a>
<a class="sourceLine" id="cb786-13" data-line-number="13">              <span class="st">&quot;smooth1.line&quot;</span>,  <span class="st">&quot;smooth2.line&quot;</span>,</a>
<a class="sourceLine" id="cb786-14" data-line-number="14">              <span class="st">&quot;smooth3.line&quot;</span>),</a>
<a class="sourceLine" id="cb786-15" data-line-number="15">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;brown&quot;</span>, </a>
<a class="sourceLine" id="cb786-16" data-line-number="16">           <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;purple&quot;</span>), <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:splineapprox"></span>
<img src="DS_files/figure-html/splineapprox-1.png" alt="Fitting a Smooth Spline" width="80%" />
<p class="caption">
Figure 6.41: Fitting a Smooth Spline
</p>
</div>

<p>The spline shows a smooth curve that travels almost through all the points touched by the crooked line. Therefore, we can say that spline is just a smoothing method for a crooked line. However, modeling a spline to fit our data gets overfitting this way.</p>
<p>The spline1.model has a smooth parameter (spar) of 0.30. That is a way to force the spline to fit about 30% through the data. So it does not overfit much. On the other hand, spline2.model has a spar of 1.0, which simulates a straight <strong>linear regression</strong> fitting. As we can see, fitting this way could end up underfitting.</p>
<p>Overfitting may risk including outliers while, at the same time, the model cannot be generalized in usage. On the other hand, underfitting may eliminate outliers but exclude significant influencers.</p>
<p>Overall, spline1.model is a better polynomial regression model than the others but may still be improved.</p>
<p>Other parameters can be tuned using the <strong>smooth.spline(.)</strong> function. To view a list of parameters, we can just run it like so (notice how it calculates the other parameters by default):</p>

<div class="sourceCode" id="cb787"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb787-1" data-line-number="1"><span class="co"># output of a spline model</span></a>
<a class="sourceLine" id="cb787-2" data-line-number="2"><span class="kw">smooth.spline</span>(x, y, <span class="dt">spar=</span><span class="fl">0.30</span>)</a></code></pre></div>
<pre><code>## Call:
## smooth.spline(x = x, y = y, spar = 0.3)
## 
## Smoothing Parameter  spar= 0.3  lambda= 0.00001489
## Equivalent Degrees of Freedom (Df): 12.87
## Penalized Criterion (RSS): 5.869
## GCV: 2.308</code></pre>

<p>The output includes two performance metrics:</p>
<ul>
<li><strong>RSS</strong> - calculates a <strong>Penalized criterion</strong>. Recall <strong>residual sum squares</strong> covered in <strong>least squares</strong>.</li>
<li><strong>GCV</strong> - generalized cross-validation. Recall <strong>bandwidth selection</strong> criterion covered in <strong>KDE</strong>. Similarly, we use <strong>GCV</strong> for <strong>parameter selection</strong> for <strong>optimal spline smoothing</strong>. There are three parameters shown: <strong>Spar</strong>, <strong>Lambda</strong>, and <strong>Degrees of Freedom</strong>.</li>
</ul>
<p>We can use degrees of freedom like so (notice how it calculates the other parameters by default). Here, fewer degrees of freedom generate a <strong>linear regression</strong>:</p>

<div class="sourceCode" id="cb789"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb789-1" data-line-number="1"><span class="kw">smooth.spline</span>(x, y, <span class="dt">df=</span><span class="dv">6</span>)</a></code></pre></div>
<pre><code>## Call:
## smooth.spline(x = x, y = y, df = 6)
## 
## Smoothing Parameter  spar= 0.5182  lambda= 0.0005617 (12 iterations)
## Equivalent Degrees of Freedom (Df): 6.001
## Penalized Criterion (RSS): 25.63
## GCV: 2.616</code></pre>

<p>Notice in Figure <a href="6.8-the-significance-of-regression.html#fig:splineapprox">6.41</a>, the parameter spar=0.30 generates a polynomial curve that closes matches one with the parameter df=6.873 and vice-versa. A <strong>linear regression</strong> happens if spar=1.00 or df=2.</p>
<p>Next, we also can use lambda like so:</p>

<div class="sourceCode" id="cb791"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb791-1" data-line-number="1"><span class="kw">smooth.spline</span>(x, y,  <span class="dt">lambda=</span> <span class="fl">0.0001300587</span>)</a></code></pre></div>
<pre><code>## Call:
## smooth.spline(x = x, y = y, lambda = 0.0001300587)
## 
## Smoothing Parameter  spar= NA  lambda= 0.0001301
## Equivalent Degrees of Freedom (Df): 8.163
## Penalized Criterion (RSS): 18.58
## GCV: 2.653</code></pre>

<p>We can use a combination like so (spar and lambda are mutually exclusive):</p>

<div class="sourceCode" id="cb793"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb793-1" data-line-number="1"><span class="kw">smooth.spline</span>(x, y,  <span class="dt">df=</span><span class="fl">6.78</span>, <span class="dt">lambda=</span> <span class="fl">0.00013</span>)</a></code></pre></div>
<pre><code>## Call:
## smooth.spline(x = x, y = y, df = 6.78, lambda = 0.00013)
## 
## Smoothing Parameter  spar= NA  lambda= 0.00013
## Equivalent Degrees of Freedom (Df): 8.164
## Penalized Criterion (RSS): 18.58
## GCV: 2.653</code></pre>

<p>Note that any parameter adjustments affect the <strong>power of prediction</strong>. Our optimal choices should allow a more general model to fit a broader set of cases.</p>
</div>
<div id="loess-and-lowess" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.9</span> LOESS and LOWESS  <a href="6.8-the-significance-of-regression.html#loess-and-lowess" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two common smoothers discussed in other literature are <strong>LOESS</strong> and <strong>LOWESS</strong>. We discussed the theory and math in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) under the <strong>Polynomial Smoothing</strong> Section.</p>
<p>Here, we show a simple use of both smoothers in R code (this time, we once again use the built-in R function <strong>predict(.)</strong> for inference):</p>

<div class="sourceCode" id="cb795"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb795-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb795-2" data-line-number="2"><span class="co"># degree of polynomial is 2</span></a>
<a class="sourceLine" id="cb795-3" data-line-number="3">loess.model =<span class="st"> </span><span class="kw">loess</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">span=</span><span class="fl">0.70</span>, <span class="dt">degree=</span><span class="dv">2</span>) </a>
<a class="sourceLine" id="cb795-4" data-line-number="4">loess.line =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(loess.model)</a>
<a class="sourceLine" id="cb795-5" data-line-number="5"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">1</span>,sample_size), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>), </a>
<a class="sourceLine" id="cb795-6" data-line-number="6">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis (approx)&quot;</span>,</a>
<a class="sourceLine" id="cb795-7" data-line-number="7">     <span class="dt">main=</span><span class="st">&quot;Fitting using LOWESS and LOESS&quot;</span>)</a>
<a class="sourceLine" id="cb795-8" data-line-number="8"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb795-9" data-line-number="9"><span class="kw">lines</span>(<span class="kw">lowess</span>(x, y, <span class="dt">f=</span><span class="fl">0.20</span>), <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb795-10" data-line-number="10"><span class="kw">lines</span>(<span class="kw">lowess</span>(x, y, <span class="dt">f=</span><span class="fl">1.70</span>), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb795-11" data-line-number="11"><span class="kw">lines</span>(smooth1.line, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb795-12" data-line-number="12"><span class="kw">lines</span>(loess.line , <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb795-13" data-line-number="13"><span class="kw">points</span>(x, y, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb795-14" data-line-number="14"><span class="kw">legend</span>(<span class="dv">7</span>, <span class="dv">-1</span>, </a>
<a class="sourceLine" id="cb795-15" data-line-number="15">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;lowess(f=0.20)&quot;</span>, <span class="st">&quot;lowess(f=1.70)&quot;</span>, </a>
<a class="sourceLine" id="cb795-16" data-line-number="16">              <span class="st">&quot;smooth1.line&quot;</span>,  <span class="st">&quot;loess.line(span=0.70)&quot;</span>),</a>
<a class="sourceLine" id="cb795-17" data-line-number="17">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;brown&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>), </a>
<a class="sourceLine" id="cb795-18" data-line-number="18">    <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:loessapprox"></span>
<img src="DS_files/figure-html/loessapprox-1.png" alt="Fitting using LOWESS and LOESS" width="80%" />
<p class="caption">
Figure 6.42: Fitting using LOWESS and LOESS
</p>
</div>

<p>We use the <strong>loess(.)</strong> function to model a fit for our dataset. We then use the generated model to <strong>predict</strong> a new set of data (equivalently performing interpolation) which we then use to plot a <strong>smooth curve</strong>.</p>
<p>In the previous discussion, spline1.model is chosen to be the better smoothing model to fit our data. Now, in Figure <a href="6.8-the-significance-of-regression.html#fig:loessapprox">6.42</a>, we compare spline1.model with loess.model to show how both are trying to fit data. Even though it may show that spline1.model is still a better model than loess.model; we can still tune loess.model to get a good fit. We leave readers to experiment and perhaps tune the smoothing span (span) parameter.</p>
<p>On the other hand, a lowess model with f=0.70 renders a line and is slightly smooth; but still able to fit our data. We can increase the parameter, but while it may not <strong>regress</strong> ideally, it nevertheless tries to do so, risking underfitting.</p>
<p>Also, the span parameter in <strong>loess(.)</strong> function settles into a <strong>linear regression</strong> similar to a higher <strong>f</strong> parameter in <strong>lowess(.)</strong> function. Otherwise, if tuned to ideal lower values, both parameters can fit a polynomial model.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="6.7-regression-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="6.9-inference-for-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
