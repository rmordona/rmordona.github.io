<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.6 Bayesian Inference | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="7.6 Bayesian Inference | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.6 Bayesian Inference | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-03-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="7.5-information-theory.html"/>
<link rel="next" href="8-bayesian2.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat-and-disclaimer.html"><a href="caveat-and-disclaimer.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><em>Introduction</em></a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i><em>Mathematical Notation</em></a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i><em>Bibliography</em></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesianinference" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.6</span> Bayesian Inference<a href="7.6-bayesianinference.html#bayesianinference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Bayesian Inference</strong> mostly, if not all cases, operates in the context of <strong>optimization problems</strong> where the object of interest is around the <strong>posterior</strong>.</p>
<p>We begin this section by recalling the discussion around <strong>Linear Regression</strong> in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>). See <strong>Linear Regression</strong> Figure under <strong>Approximating Polynomial Functions by Regression</strong> Section in Chapter <strong>3</strong>. A review of <strong>Linear Regression</strong> shows a <strong>deterministic linear model</strong> expressed as such:</p>
<p><span class="math display" id="eq:equate1090348">\[\begin{align}
\hat{y}_i = \beta_0 + \beta_1 x_i \tag{7.366} 
\end{align}\]</span></p>
<p>However, this model is assumed to be a perfect model that is not mostly and not practically applicable in real-world situations. Most of our observed data are mixed with <strong>random noise</strong> (<span class="math inline">\(\mathbf{\epsilon_i}\)</span>), and thus, we model in a stochastic manner by adding noise into the equation to form a <strong>stochastic linear model</strong>; thus, we have the following:</p>
<p><span class="math display" id="eq:equate1090349">\[\begin{align}
\hat{y}_i = \beta_0 + \beta_1 x_i + \epsilon_i\ \ \ \ \ \ \ \ \epsilon_i \sim \mathcal{N}(\mu, \sigma^2),\ \ \ \ \ \ \ i = 1,...,n \tag{7.367} 
\end{align}\]</span></p>
<p>There are two notes to mention here.</p>
<p><strong>First</strong>, our response variable - being <span class="math inline">\(\mathbf{\hat{y}_i}\)</span>, given value <span class="math inline">\(\mathbf{x_i}\)</span> - is a <strong>point-estimate</strong>; meaning, our estimate renders one single value.</p>
<p><strong>Second</strong>, <strong>linear regression</strong> is accomplished by optimizing the <span class="math inline">\(\beta\)</span> parameters, namely <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, by minimizing the <strong>error</strong> (or loss) function, e.g. <strong>least square</strong> based on the residual (<span class="math inline">\(\epsilon_i\)</span>).</p>
<p>In this section, we also deal with two notes corresponding to the response variable and parameter estimation.</p>
<p><strong>First</strong>, instead of dealing with <strong>point-estimates</strong>, we deal with <strong>stochastic estimates</strong> in which our response variable - being <span class="math inline">\(\mathbf{\hat{y}_i}\)</span> - assumes a <strong>random variable</strong>; meaning, our estimate does not render a single value, but rather a random sampling that follows a <strong>normal posterior distribution</strong> and therefore it is expressed as such:</p>
<p><span class="math display" id="eq:equate1090350">\[\begin{align}
\hat{y}_i|x_i \sim \mathcal{N}(\mu, \sigma^2) \tag{7.368} 
\end{align}\]</span></p>
<p>For every <span class="math inline">\(\mathbf{x_i}\)</span> in the X space, there is a corresponding <span class="math inline">\(\mathbf{\hat{y}_i}\)</span> <strong>normal posterior</strong> distribution.</p>
<p>Another way to get the intuition is to use Figure <a href="7.6-bayesianinference.html#fig:stochasticestimate">7.14</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:stochasticestimate"></span>
<img src="stochasticestimate.png" alt="Point-Estimate vs Stochastic Estimate" width="70%" />
<p class="caption">
Figure 7.14: Point-Estimate vs Stochastic Estimate
</p>
</div>

<p><strong>Second</strong>, recall <span class="math inline">\(\theta\)</span> parameter in the <strong>Likelihood</strong> section. In this case, the theta <span class="math inline">\(\theta\)</span> parameter is a vector that contains <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. For clear notation, let us use <span class="math inline">\(\alpha\)</span> for <span class="math inline">\(\beta_1\)</span> and use <span class="math inline">\(\beta\)</span> for <span class="math inline">\(\beta_1\)</span>. To optimize theta <span class="math inline">\(\theta\)</span> parameter, we focus on the <strong>likelihood</strong> term in the normalized <strong>Bayes Theorem</strong> and perform <strong>maximum likelihood estimation (MLE)</strong>:</p>
<p><span class="math display" id="eq:equate1090351">\[\begin{align}
\underbrace{P(\theta|X)}_\text{posterior}\ \propto\ \underbrace{Lik(\theta|X)}_\text{likelihood} \times \underbrace{P(\theta)}_\text{prior} \tag{7.369} 
\end{align}\]</span></p>
<p>Taking Figure <a href="7.6-bayesianinference.html#fig:stochasticestimate">7.14</a> as an example in point, there are four <strong>sampling densities</strong> in the Y <strong>posterior space</strong>: <span class="math inline">\(Y = (\hat{y}_1, \hat{y}_2, \hat{y}_3, \hat{y}_4 )\)</span>. Such list of sampling densities forms the following notation:</p>
<p><span class="math display" id="eq:equate1090352">\[\begin{align}
\forall y:   \hat{y}_i|x_i;\alpha,\beta,\sigma \sim \overbrace{ \underbrace{ 
   \mathcal{N}(\alpha_i + \beta_i x_i\ , \sigma^2) }_\text{likelihood}}^{sampling\ density} \tag{7.370} 
\end{align}\]</span></p>
<p>Note that each of the <strong>sampling densities</strong> is independent and thus we can form a <strong>joint distribution</strong> like so:</p>
<p><span class="math display" id="eq:equate1090353">\[\begin{align}
Lik(X; \alpha, \beta, \sigma^2|Y) = Lik(x_1,...,x_n; \alpha, \beta, \sigma^2|y_1,...,y_n) = \prod_{i=1}^n P(y_i|x_i; \alpha_i, \beta_i, \sigma^2)  \tag{7.371} 
\end{align}\]</span></p>
<p>To avoid underflows and overflows, we use log-likelihood:</p>
<p><span class="math display" id="eq:equate1090354">\[\begin{align}
-\log_e Lik(X; \alpha, \beta, \sigma^2|Y) = - \sum_{i=1}^n ln\ P(y_i|x_i; \alpha_i, \beta_i, \sigma^2)  \tag{7.372} 
\end{align}\]</span></p>
<p>Note that it is common to minimize a <strong>loss</strong> or <strong>cost</strong> function. We minimize the log-likelihood by negating it to conform to the same practice. Therefore, to maximize the likelihood estimate <strong>(MLE)</strong> is also to minimize the <strong>negative log-likelihood (NLL)</strong>. Here is the <strong>minimization</strong> equation:</p>
<p><span class="math display" id="eq:equate1090355">\[\begin{align}
\hat{y} = \underset{\alpha,\beta,\sigma^2}{argmin}\ -\log_e Lik(X; \alpha, \beta, \sigma^2|Y) = \underset{\alpha,\beta,\sigma^2}{argmin}\ -\sum_{i=1}^n ln\ P(y_i|x_i; \alpha_i, \beta_i, \sigma^2)  \tag{7.373} 
\end{align}\]</span></p>
<p>As for the variance <span class="math inline">\(\sigma^2\)</span> parameter, if we lack assumptions, we can use uniform distribution:</p>
<p><span class="math display" id="eq:equate1090356">\[\begin{align}
\sigma^2 \sim \mathcal{U}(1,1) \tag{7.374} 
\end{align}\]</span></p>
<div id="maximum-likelihood-mle" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.6.1</span> Maximum Likelihood (MLE)  <a href="7.6-bayesianinference.html#maximum-likelihood-mle" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>Linear regression</strong>, we look for <strong>a model that fits</strong> our data. The <strong>goodness of fit</strong> is determined using <strong>RMSE</strong> or <span class="math inline">\(\mathbf{R^2}\)</span>. The model is expressed as a <strong>line function</strong> (or a <strong>curve function</strong> for non-linear regression) described by the <span class="math inline">\(\beta\)</span> parameters. Fitting a model depends upon optimizing the <span class="math inline">\(\beta\)</span> parameters.</p>
<p>The same concept applies to <strong>Stochastic regression</strong> in which we also look for a <strong>model that fits</strong> our data. And the <strong>goodness of fit</strong> is determined using <strong>MLE</strong> or <strong>NLL</strong>. The model is expressed as a <strong>likelihood function</strong> and is described by the <span class="math inline">\(\theta\)</span> parameters. Fitting a model depends upon optimizing the <span class="math inline">\(\theta\)</span> parameters.</p>
<p>See Figure <a href="7.6-bayesianinference.html#fig:modeling">7.15</a> for reference.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:modeling"></span>
<img src="modeling.png" alt="Modeling by Parameter Estimation" width="80%" />
<p class="caption">
Figure 7.15: Modeling by Parameter Estimation
</p>
</div>

<p>In this section, the idea here is to optimize the <span class="math inline">\(\theta\)</span> parameters to maximize the likelihood of observing data. That is called <strong>maximum likelihood estimation (MLE)</strong>. An <strong>MLE</strong> takes the following general form:</p>
<p><span class="math display" id="eq:equate1090357">\[\begin{align}
\hat{\theta} = \underset{\theta}{argmax}\ P(X|\theta) = \underset{\theta}{argmax} \prod_{i=1}^n f( x_i ; \theta) \tag{7.375} 
\end{align}\]</span></p>
<p><strong>MLE</strong> for <strong>Normal Distribution</strong></p>
<p>Let us derive the equation to find the optimal mean <span class="math inline">\(\mu^*\)</span>, given a known <span class="math inline">\(\sigma^{2*}\)</span>. Here, the likelihood function takes multivariates as input and performs a multiplication of the normal <strong>PDF</strong> for each variate.</p>
<p><span class="math display" id="eq:equate1090360" id="eq:equate1090359" id="eq:equate1090358">\[\begin{align}
{}&amp;Lik(\mu, \sigma^2 | x_1,...,x_n) {} \equiv P(x_1,...,x_n\ |\ \mu, \sigma^2) \tag{7.376} \\
&amp;= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} exp\left[-\frac{(x_i - \mu)^2}{2\sigma^2}\right] \tag{7.377} \\
&amp;= \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n
    exp\left[-\frac{\sum_{i=1}^n(x_i - \mu)^2}{2\sigma^2}\right] \tag{7.378} 
\end{align}\]</span></p>
<p>Use log-likelihood and simplify:</p>
<p><span class="math display" id="eq:equate1090363" id="eq:equate1090362" id="eq:equate1090361">\[\begin{align}
{}&amp; \log_e Lik(\mu, \sigma^2 | x_1,...,x_n)   \tag{7.379} \\
&amp;= ln \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n + ln\ 
    exp\left[-\frac{\sum_{i=1}^n(x_i - \mu)^2}{2\sigma^2}\right] \tag{7.380} \\
&amp;= \sum_{i=1}^n ln\ \left[\frac{1}{\sqrt{2\pi\sigma^2}} \right] + \left[-\frac{\sum_{i=1}^n(x_i - \mu)^2}{2\sigma^2}\right] \tag{7.381} 
\end{align}\]</span></p>
<p>Now, take the partial derivative with respect to <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display" id="eq:equate1090366" id="eq:equate1090365" id="eq:equate1090364">\[\begin{align}
{}&amp; \frac{ \partial\ \log_e Lik(\mu, \sigma^2 | x_1,...,x_n) }{\partial \mu} \tag{7.382} \\
&amp;=  -\frac{1}{2\sigma^2} \frac{\partial}{\partial\mu}  \left[\sum_{i=1}^n(x_i - \mu)^2\right]   &amp; \text{(drop 1st term constant)} \tag{7.383} \\
&amp;= -\frac{2}{2\sigma^2}  \left[\sum_{i=1}^n x_i - n\mu \right] \tag{7.384} 
\end{align}\]</span></p>
<p>Set left-side of equation to zero and solve for <span class="math inline">\(\mu_{(MLE)}\)</span>:</p>
<p><span class="math display" id="eq:equate1090369" id="eq:equate1090368" id="eq:equate1090367">\[\begin{align}
{}&amp;\rightarrow -\frac{1}{\sigma^2}  \left[\sum_{i=1}^n x_i - n\mu \right] = 0 \tag{7.385} \\
&amp;\rightarrow  \left[\sum_{i=1}^n x_i - n\mu \right] = 0 \tag{7.386} \\
\nonumber \\ 
&amp;\mu_{(MLE)} = \frac{1}{n}\sum_{i=1}^n x_i \tag{7.387} 
\end{align}\]</span></p>
<p>Notice that <strong>MLE</strong> for the mean for a <strong>normal distribution</strong> gets simplified to a simple average computation.</p>
<p>Using the same log-likelihood, let us take the partial derivative with respect to <span class="math inline">\(\sigma^2\)</span>, given <span class="math inline">\(\mu\)</span>. In what follows, we temporarily use placeholder variables:</p>
<p><span class="math display" id="eq:equate1090370">\[\begin{align}
v = \sigma^2\ \ \ \ \ \ \ \ \ \ \ \ \ \ X_u = \sum_{i=1}^n(x_i - \mu)^2 \tag{7.388} 
\end{align}\]</span></p>
<p>Therefore, we get:</p>
<p><span class="math display" id="eq:equate1090372" id="eq:equate1090371">\[\begin{align}
\frac{ \partial\ ln\ Lik(\mu, v | x_1,...,x_n) }{\partial v} 
{}&amp;= \frac{\partial}{\partial v}\sum_{i=1}^n ln\ \left[\frac{1}{\sqrt{2\pi v}} \right] + 
   \frac{\partial}{\partial v}\left[-\frac{X_u}{2 v}\right] \tag{7.389} \\
&amp;= -\frac{n}{2v} + \frac{X_u}{2v^2} \tag{7.390} 
\end{align}\]</span></p>
<p>Set left-side of equation to zero and solve for <span class="math inline">\(\sigma^2_{(MLE)}\)</span>:</p>
<p><span class="math display" id="eq:equate1090375" id="eq:equate1090374" id="eq:equate1090373">\[\begin{align}
{}&amp;\rightarrow -\frac{1}{2}\left[\frac{n}{v} - \frac{X_u}{v^2} \right]= 0 \tag{7.391} \\
{}&amp;\rightarrow v^* = \frac{X_u}{n},\ \ \ \ \ \ \ \ n \ne 0 \tag{7.392} \\
\nonumber \\
\sigma^2_{(MLE)} &amp;= \frac{\sum_{i=1}^n(x_i - \mu)^2}{n} &amp; \text{(substitute placeholders)} \tag{7.393} 
\end{align}\]</span></p>
<p><strong>MLE</strong> for <strong>Binomial Distribution</strong></p>
<p>Let us use <strong>binomial distribution</strong> to illustrate a case. Here, we perform the following:</p>
<p><span class="math display" id="eq:equate1090376">\[\begin{align}
Lik(n,\rho|x_1,...,x_m) = \prod_{i=1}^m \binom{n}{x_i}  \rho^{x_i}(1-\rho)^{n-x_i}
\ \ \ \ \ \ \ \ \ \ where:\ 
\binom{n}{x_i}\ \text{is a constant} \tag{7.394} 
\end{align}\]</span></p>
<p>Note that the <strong>constant</strong> <span class="math inline">\(\binom{n}{x_i}\)</span> gets dropped in the partial derivatives eventually; therefore, we can drop the constant up-front given data.</p>
<p>We then calculate the log-likelihood:</p>
<p><span class="math display" id="eq:equate1090379" id="eq:equate1090378" id="eq:equate1090377">\[\begin{align}
\log_e  Lik(n,\rho|x_1,...,x_m) {}&amp;=  ln\left( \prod_{i=1}^m \binom{n}{x_i} \rho^{x_i}(1-\rho)^{n-x_i} \right) \tag{7.395} \\
&amp;= \sum_{i=1}^m  \left( ln\binom{n}{x_i}  + ln(\rho)^{x_i} + ln(1-\rho)^{n-x_i} \right) \tag{7.396} \\
&amp;=  \sum_{i=1}^m  \left( ln \binom{n}{x_i} + x_i \cdot ln(\rho) + (n-x_i)\cdot ln(1-\rho) \right) \tag{7.397} 
\end{align}\]</span></p>
<p>The <strong>key to optimization</strong> in this respect is based on taking the partial derivatives with respect to the <span class="math inline">\(\rho\)</span> parameter for the log-likelihood:</p>
<p><span class="math display" id="eq:equate1090382" id="eq:equate1090381" id="eq:equate1090380">\[\begin{align}
\frac{\partial}{\partial_\rho} \log_e  Lik(n,\rho|x_1,...,x_m) {}&amp;= \sum_{i=1}^m  \left( ln \binom{n}{x_i} + x_i \cdot \frac{\partial}{\partial_\rho}ln(\rho) + (n-x_i)\cdot \frac{\partial}{\partial_\rho}ln(1-\rho) \right)  \tag{7.398} \\
&amp;= \sum_{i=1}^m  \left( 0 + x_i \cdot \frac{\partial}{\partial_\rho}ln(\rho) + (n-x_i)\cdot \frac{\partial}{\partial_\rho}ln(1-\rho) \right)  \tag{7.399} \\
&amp;= \sum_{i=1}^m \left( \frac{1}{\rho} x_i -  \frac{1}{1 - \rho}(n - x_i) \right) . \tag{7.400} 
\end{align}\]</span></p>
<p>Then we simplify:</p>
<p><span class="math display" id="eq:equate1090383">\[\begin{align}
\sum_{i=1}^m  \left[ \frac{1}{\rho} x_i - \frac{1}{1 - \rho}(n - x_i) \right] = 0  \tag{7.401} 
\end{align}\]</span></p>
<p>With a few algebraic calculations, we obtain the following optimized parameter (<strong>MLE</strong>):</p>
<p>For <strong>marginal-density (univariate)</strong> likelihood:</p>
<p><span class="math display" id="eq:equate1090384">\[\begin{align}
\hat{\rho}_{(MLE)} = \underset{\rho}{argmax}\ \log_e Lik(n,\rho|X = x) = \frac{x}{n} \tag{7.402} 
\end{align}\]</span></p>
<p>For <strong>joint-density (multivariate)</strong> likelihood:</p>
<p><span class="math display" id="eq:equate1090385">\[\begin{align}
\hat{\rho}_{(MLE)} = \underset{\rho}{argmax}\ \log_e Lik(n,\rho|x_1,...,x_m) = \frac{\sum^m x}{mn} \tag{7.403} 
\end{align}\]</span></p>
<p>Below is a family of distributions with their corresponding maximum likelihood estimates. See Table <a href="7.6-bayesianinference.html#tab:familymle">7.4</a>.</p>

<table>
<caption><span id="tab:familymle">Table 7.4: </span>Maximum Likelihood Estimate</caption>
<thead>
<tr class="header">
<th align="left">Family</th>
<th align="left">Parameters (<span class="math inline">\(\hat{\theta}\)</span>)</th>
<th align="left">Derived Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Uniform</td>
<td align="left"><span class="math inline">\(\hat{\theta}\)</span></td>
<td align="left"><span class="math inline">\(X_n\)</span></td>
</tr>
<tr class="even">
<td align="left">Normal</td>
<td align="left"><span class="math inline">\(\hat{\theta_1} = \mu\)</span></td>
<td align="left"><span class="math inline">\(\mu\)</span> = <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><span class="math inline">\(\hat{\theta_2} = \sigma^2\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{n} \sum_{i=1}^n(x_i - \hat{\mu})^2\)</span></td>
</tr>
<tr class="even">
<td align="left">Binomial</td>
<td align="left"><span class="math inline">\(\hat{\rho}\)</span></td>
<td align="left"><span class="math inline">\(\frac{\sum^m x}{mn}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Geometric</td>
<td align="left"><span class="math inline">\(\hat{\rho}\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{X}\)</span> = <span class="math inline">\(\frac{n}{\sum_{i=1}^n x_i}\)</span></td>
</tr>
<tr class="even">
<td align="left">Poisson</td>
<td align="left"><span class="math inline">\(\hat{\lambda}\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{n} \sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="odd">
<td align="left">Exponential</td>
<td align="left"><span class="math inline">\(\hat{\lambda}\)</span></td>
<td align="left"><span class="math inline">\(\bar{X}\)</span> = <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n x_i\)</span></td>
</tr>
</tbody>
</table>

<p>In the following two sections, we discuss the <strong>Expectation-Maximization</strong> technique in dealing with multiple latent distributions instead of just one, which we can solve with <strong>MLE</strong>.</p>
<p>Also, in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>), we expand further on the concept of <strong>MLE</strong> (<strong>NLL</strong>) in the context of <strong>Generalized Linear Model (GLM)</strong> and <strong>Logistic Regression</strong>.</p>
<p>For further reading, we encourage readers to investigate MLE in the context of misspecified models <span class="citation">(White H. <a href="bibliography.html#ref-ref1065h">1982</a>)</span>.</p>
</div>
<div id="maximum-a-posteriori-map" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.6.2</span> Maximum A-posteriori (MAP)  <a href="7.6-bayesianinference.html#maximum-a-posteriori-map" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>Gaussian distribution</strong> can be characterized by its mean (the average) and variance (the spread). In some cases, we may also want to characterize (or estimate) the distribution in terms of mode. For example, in a unimodal Gaussian distribution, we may want to estimate the most common value or the highest peak in the distribution - this is the mode, denoted as <span class="math inline">\(\mu_{(MAP)}\)</span>, with the most number of expected values. Additionally, we also want to identify the variance <span class="math inline">\(\sigma^2_{(MAP)}\)</span> between the mode <span class="math inline">\(\mu_{(MAP)}\)</span> and certain error levels, <span class="math inline">\(\epsilon_0\)</span>, for our confidence level.</p>
<p>In the previous section, we try to optimize <span class="math inline">\(\theta\)</span> parameters for <strong>likelihood</strong>. This section aims to optimize the <span class="math inline">\(\theta\)</span> parameters to maximize the <strong>posterior</strong>. That is called <strong>maximum a-posteriori estimation (MAP)</strong>. The same concept applies in which our goal is to find the closest approximating posterior distribution proportional to the actual posterior distribution.</p>
<p><strong>MAP</strong> takes the following general form:</p>
<p><span class="math display" id="eq:equate1090386">\[\begin{align}
\theta^* = \underset{\theta}{argmax}\ P(\theta | X)  \tag{7.404} 
\end{align}\]</span></p>
<p>Here, we require a proper <strong>posterior distribution</strong>. Based on the derived closed-form conjugate posterior from the <strong>Conjugacy</strong> section, let us evaluate the posterior from the Normal-Normal conjugacy and Binomial-Beta conjugacy and try to maximize the posterior.</p>
<p><strong>MAP</strong> for <strong>Normal Posterior</strong></p>
<p>To illustrate, let us use the <strong>Normal</strong> family of distribution to compute for <strong>MAP</strong>. However, it helps to review the <strong>Normal-Normal conjugacy</strong> for the posterior equation derived in the <strong>Conjugacy</strong> section.</p>

<p><span class="math display" id="eq:equate1090389" id="eq:equate1090388" id="eq:equate1090387">\[\begin{align}
P(\mu,\sigma^2|x1,...,x_n) {}&amp;\propto Lik_X(\mu, \sigma^2|x_1,...,x_n) \times P(\mu_0,\sigma^2_0) \tag{7.405} \\
&amp;\propto  \underbrace{\prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}} exp\left[-\frac{(x_i - \mu)^2}{2\sigma^2}\right] }_\text{normal likelihood} \times 
\underbrace{ \frac{1}{\sqrt{2\pi\sigma^2_0}} exp\left[-\frac{(\mu - \mu_0)^2}{2\sigma^2_0}\right]}_\text{normal prior}  \tag{7.406} \\
&amp;\propto  \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n exp\left[-\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}\right] \times 
 \frac{1}{\sqrt{2\pi\sigma^2_0}} exp\left[-\frac{(\mu - \mu_0)^2}{2\sigma^2_0}\right] \tag{7.407} 
\end{align}\]</span>
</p>
<p>We then calculate for the log a-posteriori (dropping constants up-front):</p>
<p><span class="math display" id="eq:equate1090390">\[\begin{align}
{}&amp;\log_e P(\mu,\sigma^2|x1,...,x_n) \nonumber \\
&amp;\propto \sum_{i=1}^nln\ \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) + \left[-\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}\right] + 
 ln\ \frac{1}{\sqrt{2\pi\sigma^2_0}} + \left[-\frac{(\mu - \mu_0)^2}{2\sigma^2_0}\right] \tag{7.408} 
\end{align}\]</span></p>
<p>Next, we take the partial derivative with respect to <span class="math inline">\(\mu\)</span> and simplify:</p>
<p><span class="math display" id="eq:equate1090393" id="eq:equate1090392" id="eq:equate1090391">\[\begin{align}
{}&amp;\frac{\partial\ \log_e P(\mu,\sigma^2|x1,...,x_n)}{\partial \mu} \nonumber \\
&amp;\propto   \frac{\partial}{\partial \mu}\left[-\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}\right]
+  \frac{\partial}{\partial \mu} \left[-\frac{(\mu - \mu_0)^2}{2\sigma^2_0}\right]
&amp; \text{(drop constants)} \tag{7.409} \\
&amp;\propto  -\frac{2}{2} \left[\frac{n\mu - \sum_{i=1}^n x_i}{\sigma^2}\right]
 -\frac{2}{2} \left[\frac{(\mu - \mu_0)}{\sigma^2_0}\right] \tag{7.410} \\
&amp;\propto  - \left[\frac{ \sigma^2_0 ( n\mu - \sum_{i=1}^n x_i ) + \sigma^2(\mu - \mu_0)}{\sigma^2 \sigma^2_0}\right] \tag{7.411} 
\end{align}\]</span></p>
<p>Then we set the left-side of equation to zero to solve for <span class="math inline">\(\mu_{(MAP)}\)</span> :</p>
<p><span class="math display" id="eq:equate1090396" id="eq:equate1090395" id="eq:equate1090394">\[\begin{align}
{}&amp;\rightarrow - \left[\frac{ \sigma^2_0 ( n\mu - \sum_{i=1}^n x_i ) + \sigma^2(\mu - \mu_0)}{\sigma^2 \sigma^2_0}\right] = 0 \tag{7.412} \\
&amp;\rightarrow \sigma^2_0 \left( n\mu - \sum_{i=1}^n x_i \right) + \sigma^2(\mu - \mu_0)  = 0  \tag{7.413} \\
\nonumber \\
\mu_{(MAP)} &amp;= \frac{ \sigma^2_0 \sum_{i=1}^n x_i + \sigma^2 \mu_0  }{n\sigma^2_0 + \sigma^2} \tag{7.414} 
\end{align}\]</span></p>
<p>Let us now take the partial derivative with respect to <span class="math inline">\(\sigma^2\)</span> and simplify. In what follows, we temporarily use placeholder variables:</p>
<p><span class="math display" id="eq:equate1090397">\[\begin{align}
v = \sigma^2\ \ \ \ \ \ \ \ \ \ \ \ \ Q_x = \sum_{i=1}^n (x_i - \mu)^2 \tag{7.415} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1090399" id="eq:equate1090398">\[\begin{align}
{}&amp;\frac{\partial\ \log_e P(\mu,v|x1,...,x_n)}{\partial v} \nonumber \\
&amp;\propto  \frac{\partial}{\partial v} \sum_{i=1}^nln\ \left(\frac{1}{\sqrt{2\pi v}}\right) + \frac{\partial}{\partial v}\left[-\frac{Q_x}{2v}\right]
&amp; \text{(drop constants)} \tag{7.416} \\
&amp;\propto -\frac{n}{2v} +  \frac{Q_x}{2v^2} \tag{7.417} 
\end{align}\]</span></p>
<p>Then we set the left-side of equation to zero to solve for <span class="math inline">\(\sigma^2_{(MAP)}\)</span> :</p>
<p><span class="math display" id="eq:equate1090402" id="eq:equate1090401" id="eq:equate1090400">\[\begin{align}
{}&amp;\rightarrow -\frac{n}{2v} +  \frac{Q_x}{2v^2} = 0 \tag{7.418} \\
&amp;\rightarrow  v = \frac{Q_x}{n} \tag{7.419} \\
\nonumber \\
\sigma^2_{(MAP)} &amp;= \frac{\sum_{i=1}^n (x_i - \mu)^2}{n} &amp; \text{(substitute placeholders)} \tag{7.420} 
\end{align}\]</span></p>
<p>It is worth mentioning that in a case where we do not have a piece of prior knowledge, we then can use a uniform prior, e.g., <span class="math inline">\(P(\theta) \propto 1\)</span> where <span class="math inline">\(\theta \sim U(1,1)\)</span>. That renders the prior with lesser or no influence on the posterior, so then the maximum posterior (<strong>MAP</strong>) becomes proportional to maximum likelihood (<strong>MLE</strong>). In that regard, we can say that <strong>MAP</strong> is a regularization of <strong>ML</strong> because of the influence of the prior.</p>
<p><strong>MAP</strong> for <strong>Beta Posterior</strong></p>
<p>To illustrate, let us use the <strong>Binomial</strong> family of distribution to compute for <strong>MAP</strong>. For this, it helps to review the <strong>Binomial-Beta conjugacy</strong> for the posterior equation as derived in the <strong>Conjugacy</strong> section.</p>
<p>Here, we calculate using the following equation (where the choice of our <strong>prior</strong> is a <strong>beta prior</strong> - we discuss more of <strong>prior</strong> probability in subsequent sections):</p>
<p><span class="math display" id="eq:equate1090404" id="eq:equate1090403">\[\begin{align}
P(n,\rho|x_1,...,x_m) {}&amp;\propto Lik(n,\rho|x_1,...,x_m) \times P(n,\rho)  \tag{7.421} \\
&amp;\propto \underbrace{ \prod_{i=1}^m \binom{n}{x_i}  \rho^{x_i}(1-\rho)^{n-x_i}}_\text{binomial likelihood}  \times \underbrace{ \frac{1}{\mathcal{B}(\alpha_0,\beta_0)}\rho^{\alpha_0 - 1} (1 - \rho)^{\beta_0 - 1} }_\text{beta prior} \tag{7.422} 
\end{align}\]</span></p>
<p>We then calculate for the log a-posteriori (dropping constants up-front):</p>
<p><span class="math display" id="eq:equate1090407" id="eq:equate1090406" id="eq:equate1090405">\[\begin{align}
\log_e  P(n,\rho|x_1,...,x_m)  {}&amp;\propto  ln\left[ \prod_{i=1}^m \binom{n}{x_i}  \rho^{x_i}(1-\rho)^{n-x_i}  \times \frac{1}{\mathcal{B}(\alpha_0,\beta_0)}\rho^{\alpha_0 - 1} (1 - \rho)^{\beta_0 - 1} \right] \tag{7.423} \\
&amp;\propto   ln\left[ \prod_{i=1}^m \binom{n}{x_i}  \rho^{x_i}(1-\rho)^{n-x_i} \right] +  ln \left[ \frac{1}{\mathcal{B}(\alpha_0,\beta_0)}\rho^{\alpha_0 - 1} (1 - \rho)^{\beta_0 - 1} \right] \tag{7.424} \\
&amp;\propto   \sum_{i=1}^m ln(\rho)^{x_i} + \sum_{i=1}^mln(1-\rho)^{n-x_i}  + ln(\rho)^{\alpha_0 - 1}+  ln (1 - \rho)^{\beta_0 - 1}  \tag{7.425} 
\end{align}\]</span></p>
<p>Note that we zero-out the constant up-front.</p>
<p>We then take the partial derivative with respect to <span class="math inline">\(\rho\)</span> and simplify:</p>
<p><span class="math display" id="eq:equate1090410" id="eq:equate1090409" id="eq:equate1090408">\[\begin{align}
{}&amp;\frac{\partial}{\partial_\rho}\log_e  P(n,\rho|X = x) \nonumber \\
&amp;\propto  \sum_{i=1}^m \frac{\partial}{\partial_\rho}ln(\rho)^{x_i} + \sum_{i=1}^m \frac{\partial}{\partial_\rho}ln(1-\rho)^{n-x_i} + \frac{\partial}{\partial_\rho} ln(\rho)^{\alpha_0 - 1} + \frac{\partial}{\partial_\rho} ln (1 - \rho)^{\beta_0 - 1} \tag{7.426} \\
&amp;\propto \sum_{i=1}^m \frac{x_i}{\rho} - \sum_{i=1}^m \frac{n - x_i}{1 - \rho}  + \frac{\alpha_0-1}{\rho} - \frac{\beta_0 - 1}{1 - \rho} \tag{7.427} \\
&amp;\propto \frac{(1-\rho)(\sum_{i-1}^m x_i + \alpha_0 - 1) - \rho(\sum_{i=1}^m (n-x_i) + \beta_0 - 1)}{\rho(1 - \rho)} \tag{7.428} 
\end{align}\]</span></p>
<p>Then we set the left-side of equation to zero to solve for <span class="math inline">\(\rho_{(MAP)}\)</span>:</p>
<p><span class="math display" id="eq:equate1090412" id="eq:equate1090411">\[\begin{align}
{}&amp;\rightarrow (1-\rho)(\sum_{i-1}^m x_i + \alpha_0 - 1) - \rho(\sum_{i=1}^m (n-x_i) + \beta_0 - 1)  = 0 \tag{7.429} \\
{}&amp;\rightarrow (1-\rho)(m\bar{x} + \alpha_0 - 1) - \rho( (mn - m\bar{x}) + \beta_0 - 1)  = 0 \tag{7.430} 
\end{align}\]</span></p>
<p>Therefore, for <strong>joint posterior</strong>:</p>
<p><span class="math display" id="eq:equate1090413">\[\begin{align}
\rho_{(MAP)} = \frac{m \bar{x} + \alpha - 1}{\alpha + \beta + m \bar{x} + (mn - m\bar{x}) - 2} \tag{7.431} 
\end{align}\]</span></p>
<p>For <strong>marginal posterior</strong>:</p>
<p><span class="math display" id="eq:equate1090414">\[\begin{align}
\rho_{(MAP)} = \frac{x + \alpha - 1}{\alpha + \beta + x + (n- x) - 2} \tag{7.432} 
\end{align}\]</span></p>
<p>We leave readers to investigate the <strong>maximum a-posteriori</strong> for other a-posteriori distributions.</p>
</div>
<div id="laplace-approximation" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.6.3</span> Laplace Approximation <a href="7.6-bayesianinference.html#laplace-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to <strong>Maximum a-posteriori</strong>, the <strong>Laplace Approximation</strong> method is another technique used to approximate an actual <strong>posterior distribution</strong>. As always, we start with the <strong>Bayes theorem</strong>:</p>
<p><span class="math display" id="eq:equate1090415">\[\begin{align}
P(\theta|X) = \frac{P(X|\theta)P(\theta)}
{P(X)},\ \ \ \ where\ P(X)= \int P(X|\theta)P(\theta) d\theta \tag{7.433} 
\end{align}\]</span></p>
<p>The object of interest is still the true <strong>posterior</strong>, which is rendered with no closed-form solution in cases, for example, where its <strong>marginal likelihood</strong>, namely <span class="math inline">\(P(X)\)</span>, tends to be intractable.</p>
<p><strong>Laplace approximation</strong> uses an <strong>approximating distribution</strong> denoted as <span class="math inline">\(\mathcal{Q}(X)\)</span> for the true <strong>posterior</strong>. To illustrate, let us consider the following steps <span class="citation">(Murphy K. section 8.4.1 <a href="bibliography.html#ref-ref224k">2012</a>; Bishop C.M., section 4.4 <a href="bibliography.html#ref-ref482c">2006</a>)</span>:</p>
<p><strong>First</strong>, to approximate the <strong>posterior distribution</strong>, we replace the equation with the following <strong>approximating distribution</strong>:</p>
<p><span class="math display" id="eq:equate1090416">\[\begin{align}
\mathcal{Q}(\mu) = \frac{q(u)}{\int q(u)du}\ \ \ \ \ \ \text{let Z = } \int q(u)du \tag{7.434} 
\end{align}\]</span></p>
<p>Here, <strong>Z</strong> takes the role of a <strong>normalizing constant</strong> for <span class="math inline">\(P(X)\)</span> - the denominator. And <span class="math inline">\(q(u)\)</span> assumes the relation between the <strong>likelihood</strong> and the <strong>prior</strong>, namely <span class="math inline">\(P(X|\theta)P(\theta)\)</span>, - the numerator.</p>
<p><strong>Second</strong>, we assume that <span class="math inline">\(\mathcal{Q}(\mu)\)</span> follows a unimodal Gaussian distribution but with a shape based on unknown mean and unknown variance such that <span class="math inline">\(\mathcal{Q} \sim \mathcal{N}(X; \mu, \sigma^2)\)</span>. Naturally, to approximate a <strong>Gaussian distribution</strong> for <span class="math inline">\(\mathcal{Q}(\mu)\)</span>, we need to find the peak instead (or the mode), which we denote as <span class="math inline">\(\mu_0\)</span> for which the slope (first derivative) is zero, e.g., <span class="math inline">\(q&#39;(\mu_0) = 0\)</span>.</p>
<p><span class="math display" id="eq:equate1090417">\[\begin{align}
\left.\frac{d}{d\mu}q(\mu)\right|_{\mu={\mu_0}} \tag{7.435} 
\end{align}\]</span></p>
<p><strong>Third</strong>, let us derive a gaussian-like equation for <span class="math inline">\(\mathcal{Q}(\mu)\)</span> by using <strong>second-order Taylor series expansion</strong> for the log of <span class="math inline">\(q(u)\)</span>, centered at the mode (<span class="math inline">\(\mu_0\)</span>):</p>
<p><span class="math display" id="eq:equate1090425" id="eq:equate1090424" id="eq:equate1090423" id="eq:equate1090422" id="eq:equate1090421" id="eq:equate1090420" id="eq:equate1090419" id="eq:equate1090418">\[\begin{align}
\log_e q(u) {}&amp; = \log_e \left[P(X|\theta = \mu) P(\theta =\mu)\right]  \tag{7.436} \\
\nonumber \\
{}&amp;\approx \sum_{n=0}^{N=2} \left(\frac{q^{(n)}(\mu_0)(\mu - \mu_0)^{(n)}}{n!}\right) &amp; \text{(2nd-order Taylor series)} \tag{7.437} \\
&amp;= q(\mu_0) + q&#39;(\mu_0)(\mu - \mu_0) + \frac{1}{2}q&#39;&#39;(\mu_0)(\mu - \mu_0)^2  \tag{7.438} \\
&amp;= q(\mu_0)  + \frac{1}{2}q&#39;&#39;(\mu_0)(\mu - \mu_0)^2 &amp; \text{(1st-order vanishes at slope=0)} \tag{7.439} \\
&amp;= q(\mu_0)  + \left(-\frac{1}{2}\right)(-q&#39;&#39;(\mu_0))(\mu - \mu_0)^2 &amp; \text{(negate for concave quadratic)} \tag{7.440} \\
&amp;= q(\mu_0)  + \left(-\frac{1}{2}\right)\frac{(\mu - \mu_0)^2}{\Sigma} &amp; \Sigma=-q&#39;&#39;(\mu_0)^{-1} \leftarrow\ \text{(precision)}  \tag{7.441} \\
q(\mu) &amp;= exp\left[q(\mu_0)  + \left(-\frac{1}{2}\right)\frac{(\mu - \mu_0)^2}{\Sigma}\right] &amp; \text{(exp-log)}  \tag{7.442} \\
&amp;= exp(q(\mu_0))  \times exp \left[-\frac{1}{2}\left(\frac{(\mu - \mu_0)^2}{\Sigma}\right)\right] \tag{7.443} 
\end{align}\]</span></p>
<p><strong>Fourth</strong>, let us also solve for the <strong>normalizing constant</strong>, namely <strong>Z</strong> = <span class="math inline">\(\int q(\mu)d\mu\)</span>.</p>
<p><span class="math display" id="eq:equate1090427" id="eq:equate1090426">\[\begin{align}
Z {}&amp;= \int exp(q(\mu_0))  \times exp \left[-\frac{1}{2}\left(\frac{(\mu - \mu_0)^2}{\Sigma}\right)\right] d\mu &amp; \text{(normalizing constant)}  \tag{7.444} \\
&amp;= exp(q(\mu_0))  \int  exp\left[-\frac{1}{2}\left(\frac{(\mu - \mu_0)^2}{\Sigma}\right) \right] d\mu \tag{7.445} 
\end{align}\]</span></p>
<p>Now, recall the equation for <strong>gaussian integral with polar coordinates</strong> used to derive the following sample equation (See Chapter <strong>5</strong> (<strong>Numerical Probability and Distribution</strong>) under <strong>Normal Distribution</strong> Subsection). Similarly, with some integration, we arrive at the following:</p>
<p><span class="math display" id="eq:equate1090428">\[\begin{align}
\int  exp\left[-\frac{1}{2}\left(\frac{(\mu - \mu_0)^2}{\Sigma}\right) \right] d\mu = \sqrt{2\pi\Sigma}\ \ \ \ \ \ \ \ \ \ \leftarrow\ \ 
\int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi} \tag{7.446} 
\end{align}\]</span></p>
<p>Therefore:</p>
<p><span class="math display" id="eq:equate1090429">\[\begin{align}
Z = exp(q(\mu_0)) \times \sqrt{2\pi\Sigma}\ \ \ \ \text{(normalizing constant)} \tag{7.447} 
\end{align}\]</span></p>
<p>That simplifies our <strong>approximating distribution for posterior</strong>, namely <span class="math inline">\(\mathcal{Q}(\mu)\)</span>, by avoiding the use of integration. Therefore, we end up with the following:</p>
<p><span class="math display" id="eq:equate1090431" id="eq:equate1090430">\[\begin{align}
\mathcal{Q}(\mu) {}&amp;= \frac{1}{\mathbf{Z}} \times q(u)\  = 
\frac{1}{exp(q(\mu_0)) \times  \sqrt{2\pi\Sigma}}\times
exp(q(\mu_0)) \times exp\left[-\frac{1}{2}\left(\frac{(\mu - \mu_0)^2}{\Sigma}\right) \right] \tag{7.448} \\
\nonumber \\
&amp;= \frac{1}{\sqrt{2\pi\Sigma}}exp\left[-\frac{1}{2}\left(\frac{(\mu - \mu_0)^2}{\Sigma}\right) \right] \tag{7.449} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display" id="eq:equate1090432">\[\begin{align}
\mu_0 = \underbrace{q&#39;(\mu_0) = \left.\frac{d}{d\mu}q(\mu)\right|_{\mu={\mu_0}}}_\text{the mode}\ \ 
\ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ 
\Sigma^{-1} = \underbrace{-q&#39;&#39;(\mu_0) = -\left.\frac{d^2}{d\mu^2}q(\mu)\right|_{\mu={\mu_0}}}_\text{the precision} \tag{7.450} 
\end{align}\]</span></p>
<p>In terms of M-dimensional <strong>multivariate gaussian distribution</strong>, we leave the readers to derive the following equation:</p>
<p><span class="math display" id="eq:equate1090433">\[\begin{align}
\mathcal{Q}_X(\mu)= \frac{1}{\sqrt{(2\pi)^M |H|^{-1}}} exp\left[-\frac{1}{2} (\mu - \mu_0)^T H (\mu - \mu_0)  \right]\ \ \ \leftarrow \text{(H is Hessian Matrix)} \tag{7.451} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mu = 
\left[\begin{array}{r}\mu_0 \\ \mu_1 \\ \vdots \\ \mu_m\end{array}\right],\ \ \ \  \ 
H^{-1} = \left[\begin{array}{rrrr} 
\sigma_{1,1} &amp; \sigma_{1,2} &amp; ... &amp; \sigma_{1,m}\\
\sigma_{2,1} &amp; \sigma_{2,2} &amp;... &amp; \sigma_{2,m}\\
\vdots &amp; \ldots &amp; \ddots &amp; \vdots \\
\sigma_{m,1} &amp; \sigma_{m,2} &amp; \ldots &amp; \sigma_{m,m} \\
\end{array}\right]
\]</span></p>
<p>Depending on the sample size, <strong>Laplace approximation</strong> is adjustable to the <strong>n-order</strong> of a <strong>Taylor series expansion</strong> necessary to improve the estimation. That is called <strong>Bayesian Information Criterion (BIC)</strong>.</p>
<p><strong>Laplace approximation</strong> is an old technique demonstrating approximation of simple unimodal <strong>posterior</strong> distribution, and it performs well if the mean and mode are not far apart. However, for complex models, it may help look at other techniques such as <strong>Expectation-Maximization</strong> and <strong>Variational Bayes</strong>.</p>
</div>
<div id="expectation-maximization-em" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.6.4</span> Expectation-Maximization (EM)  <a href="7.6-bayesianinference.html#expectation-maximization-em" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are real-world applications in object recognition such as facial and speech recognition, behavior and gesture recognition, and hand-writing recognition that deal with latent or hidden data. These applications provide solutions for identifying, describing, or recognizing latent data, such as recognizing speech by evaluating speech patterns. There are powerful techniques used, in whole or in part, to solve such problems, and one of them uses the <strong>EM</strong> algorithm.</p>
<p><strong>Expectation-Maximization (EM)</strong> (Dempster, Laird, Rubin 1977) is an iterative and recursive technique that extends <strong>MLE</strong> and <strong>MAP</strong>, optimizing parameter models.</p>
<p>We begin the discussion of the <strong>EM</strong> algorithm by enumerating a few items (Note here that our discussion is driven by the use of the <strong>Gaussian mixture model (GMM)</strong>):</p>
<ul>
<li>A cluster of unknown distributions (or components) that can be classified, e.g. <span class="math inline">\(Y \in \{\ y_1, y_2,...,\ y_k\ \}\)</span>.</li>
<li>A sampling of data from an unknown distribution, e.g. <span class="math inline">\(X \in \{\ x_1,\ x_2,\ ...,\ x_n\ \}\)</span></li>
<li>A set of classification to label cluster components, e.g., <span class="math inline">\(c_j \in \{\ A, B, C\ \}\)</span></li>
<li>A set of proportionality (to serve as weight), e.g. <span class="math inline">\(\omega \in \{\ \omega_1,\ \omega_2,\ ...,\ \omega_k\ \}\)</span>.</li>
<li>An unknown parameter model denoted as theta <span class="math inline">\(\theta\)</span> that models each unknown distribution.</li>
</ul>
<p>Note that in <strong>mixture models</strong>, we also call the <strong>unobserved distributions</strong> as <strong>latent components</strong> (or latent samplings). We may use the first component referring to the first unobserved distribution from time to time.</p>
<p>Now <strong>EM</strong> starts with an initialization step and then iterates between the expectation and maximization steps.</p>
<p><strong>Model initialization (Initialization step)</strong></p>
<p>Unlike our discussion around <strong>MLE</strong> in characterizing a single latent distribution, we illustrate how to characterize latent mixture distributions.</p>
<p>For illustration, assume we have three latent distributions, <span class="math inline">\(k = 3\)</span>, with corresponding parameter models <span class="math inline">\(\theta = \{ \theta_1, \theta_2, \theta_3 \}\)</span> where:</p>
<p><span class="math display">\[
\theta_j = 
\begin{cases}
(\mu, \sigma^2) &amp; \text{(gaussian distribution)}\\
(n, \rho) &amp; \text{(binomial distribution)}\\
... &amp; \text{(other types)}
\end{cases}\ \ \ \ \ \ \ \ \forall j : 1,...,k
\]</span></p>
<p>Suppose that each latent distribution, namely <span class="math inline">\(y_j\)</span>, can be classified accordingly: <span class="math inline">\(c_j \in \{\ A,\ B,\ C\ \}\)</span>.</p>
<p>Here, we characterize each of our <strong>k</strong> models with <strong>gaussian distribution</strong> with corresponding gaussian parameters, namely <span class="math inline">\(\theta_j = (\mu_j, \sigma^2_j)\)</span>.</p>
<p>We start <strong>EM</strong> by initializing our model parameters for each unobserved distribution at time <span class="math inline">\(t=0\)</span>.</p>
<p><span class="math display">\[
\theta_1^0 = (25, 2)\ \ \ \ \ \ \ \ \ \ \ \ 
\theta_2^0 = (30, 2)\ \ \ \ \ \ \ \ \ \ \ \  
\theta_3^0 = (35, 2)
\]</span></p>
<p>As for our observed data, suppose we have <span class="math inline">\(X = \{\ 27,\ 23,\ 18,\ ...,\ x_n\ \}\)</span> where <span class="math inline">\(n = sample\ size\)</span>.</p>
<p>We assume the following proportionality (or <strong>prior</strong> probability) of three samplings from the <strong>mixture</strong> distribution. Here, we arbitrarily assign the proportions like so:</p>
<p><span class="math display">\[
P(\theta_1) = \omega_1 = 33\%\ \ \ \ \ \ \ \ \ 
P(\theta_2) = \omega_2 = 33\%\ \ \ \ \ \ \ \ \ 
P(\theta_3) = \omega_3 = 34\%
\]</span></p>
<p>The <strong>proportionality</strong>, also called <strong>weight</strong>, sums up to 1, for example, <span class="math inline">\(\sum_{j=1}^k \omega_j\)</span> = 1. We can use uniform distribution equally spread across all <strong>prior</strong> probabilities if we do not make any assumptions using the following <strong>proportionality</strong> formula:</p>
<p><span class="math display">\[
w_j = \frac{n_j}{n}\ \ \ \ \ \ \text{where}\ n_j \text{= size of jth component and n = total size of X}
\]</span></p>
<p><strong>Expectation Step (E-STEP)</strong></p>
<p>In this step, we use the initialized (and eventually the optimized) model to calculate the <strong>posterior</strong> probability using the <strong>Bayes Theorem</strong> equation.</p>
<p><span class="math display" id="eq:equate1090434">\[\begin{align}
P(x_i \in  y_j|x_i) = \frac{P(x_i | x_i \in  y_j) P(y_j)}{P(x_i) } \tag{7.452} 
\end{align}\]</span></p>
<p>The idea is to evaluate every observation, namely (<span class="math inline">\(x_i\)</span>), and estimate the probability that each observation in the sample <strong>belongs to</strong> or <strong>exists</strong> (<span class="math inline">\(\in\)</span>) <strong>in</strong> a particular sampling component, namely (<span class="math inline">\(y_i\)</span>). However, it may help show how the model theta <span class="math inline">\(\theta\)</span> contributes to the equations.</p>
<p>Therefore, we can express the <strong>posterior</strong> term like so:</p>
<p><span class="math display" id="eq:equate1090435">\[\begin{align}
P(x_i \in  y_j|x_i)
\equiv \underbrace{ P\left(Y = y_j|X = x_i,\ \theta_j^{(t)}\right)  }_\text{posterior} \tag{7.453} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\theta_j^t\)</span> models (or characterizes) <span class="math inline">\(\mathbf{y_j}\)</span>.</p>
<p>The <strong>likelihood</strong> term can be defined as:</p>
<p><span class="math display" id="eq:equate1090436">\[\begin{align}
P(x_i | x_i \in  y_j) \equiv \underbrace{P\left(X=x_i|\theta_j^{(t)}\right)}_\text{likelihood} = \frac{1}{\sqrt{2\pi\sigma_j^{2{(t)}}}} exp\left[-\frac{\left(x_i - \mu_j^{(t)}\right)^2}{2\sigma_j^{2{(t)}}}\right] \tag{7.454} 
\end{align}\]</span></p>
<p>Here, we use <strong>gaussian pdf</strong> and is implemented like so:</p>

<div class="sourceCode" id="cb849"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb849-1" data-line-number="1">norm.prob &lt;-<span class="st"> </span><span class="cf">function</span>(x, mean, sd) { <span class="kw">dnorm</span>(x, <span class="dt">mean=</span> mean, <span class="dt">sd =</span> sd) }</a></code></pre></div>

<p>Finally, our normalizer - the marginal likelihood - can be written as:</p>
<p><span class="math display" id="eq:equate1090437">\[\begin{align}
P(x_i) \equiv \sum_{l=1}^k P\left(X=x_i|\theta_l^{(t)}\right)\times\omega_l \tag{7.455} 
\end{align}\]</span></p>
<p>We use the <strong>log marginal likelihood</strong>, namely <span class="math inline">\(\log_eP(x_i)\)</span>, to evaluate convergence during iteration. Because of the intractable nature of the normalizer for more complex models, we discuss a way to solve the intractability problem in the next section under <strong>variational EM</strong>.</p>
<p>For now, here is the equivalent equation for our <strong>vanilla EM</strong>.</p>
<p><span class="math display" id="eq:equate1090438">\[\begin{align}
P(Y = y_j|X = x_i,\ \theta_j^{(t)}) = 
\frac{P\left(X=x_i|\theta_j^{(t)}\right)\times\omega_j}{\sum_{l=1}^k P\left(X=x_i|\theta_l^{(t)}\right)\times\omega_l}
,\ \ \ \ \ \ \forall j : 1,...,k \tag{7.456} 
\end{align}\]</span></p>
<p>Now, let us use a placeholder variable for the outcome of the calculation in the above equation which we use in the <strong>maximization</strong> step:</p>
<p><span class="math display" id="eq:equate1090439">\[\begin{align}
\Upsilon_{j,x_i}^{(t)}\ \ \leftarrow P\left(Y = y_j|X = x_i,\ \theta_j^{(t)}\right) \tag{7.457} 
\end{align}\]</span></p>
<p>Note that this variable represents a posterior proportion of the jth component. Also, it is essential to note further that the proportionality of all components must sum up to one. In the context of <strong>ML</strong> classification, calculating the proportion of every value over the sum of all values in a vector is called <strong>Softmax</strong>. A <strong>Softmax</strong> function performs proportion evaluation (especially in neural networks) to decide the path with the highest proportion. See our simple classification implementation later in this section.</p>
<p><strong>Maximization Step (M-STEP)</strong></p>
<p>In this step, we maximize the parameter model. In the case of <strong>gaussian mixture</strong>, we maximize <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display" id="eq:equate1090441" id="eq:equate1090440">\[\begin{align}
\mu_j^{(t+1)} {}&amp;= \frac{\sum_{i=1}^n x_i \Upsilon_{j,x_i}^{(t)}}{\sum_{i=1}^n  \Upsilon_{j,x_i}^{(t)}} 
\ \ \ \ \ \ \ \ \ \ \ \ \sigma_j^{t+1} = \frac{\sum_{i=1}^n \left(x_i - \mu_j^{(t)}\right)^2 \Upsilon_{j,x_i}^{(t)}}{\sum_{i=1}^n  \Upsilon_{j,x_i}^{(t)}} \tag{7.458} \\
\theta_j^{(t+1)} &amp;= (\mu_j^{(t+1)}  , \sigma_j^{t+1})
\ \ \ \ \ \ \ \ \ \ \ \ 
\omega_j^{t+1} = \frac{ \sum_{i=1}^n \Upsilon_{j,x_i}^{(t)}}{n} \tag{7.459} 
\end{align}\]</span></p>
<p>We then iterate between <strong>E-step</strong> and <strong>M-step</strong> until convergence. Note that we use the log of the normalizer being computed in <strong>E-STEP</strong> to evaluate convergence, for example:</p>
<p><span class="math display" id="eq:equate1090442">\[\begin{align}
| \underbrace{\log_e P(x_i)^{(t)}}_\text{new loglik} - \underbrace{\log_e P(x_i)^{(t-1)}}_\text{old loglik}  | &lt; tolerance \tag{7.460} 
\end{align}\]</span></p>
<p>To illustrate, let us perform the <strong>initialization step</strong>:</p>

<div class="sourceCode" id="cb850"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb850-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb850-2" data-line-number="2">k =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb850-3" data-line-number="3">sample_size =<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb850-4" data-line-number="4">n =<span class="st"> </span>k <span class="op">*</span><span class="st"> </span>sample_size</a>
<a class="sourceLine" id="cb850-5" data-line-number="5">theta =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>(  <span class="kw">c</span>(<span class="dv">25</span>, <span class="dv">2</span>), <span class="kw">c</span>(<span class="dv">30</span>, <span class="fl">1.5</span>), <span class="kw">c</span>(<span class="dv">35</span>, <span class="fl">1.5</span>) ), <span class="dt">nrow=</span>k, </a>
<a class="sourceLine" id="cb850-6" data-line-number="6">                <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb850-7" data-line-number="7">x1 =<span class="st"> </span>sample1 =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> sample_size, <span class="dt">mean =</span> theta[<span class="dv">1</span>,<span class="dv">1</span>],   </a>
<a class="sourceLine" id="cb850-8" data-line-number="8">                     <span class="dt">sd =</span> theta[<span class="dv">1</span>,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb850-9" data-line-number="9">x2 =<span class="st"> </span>sample2 =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> sample_size, <span class="dt">mean =</span> theta[<span class="dv">2</span>,<span class="dv">1</span>],   </a>
<a class="sourceLine" id="cb850-10" data-line-number="10">                     <span class="dt">sd =</span> theta[<span class="dv">2</span>,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb850-11" data-line-number="11">x3 =<span class="st"> </span>sample3 =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> sample_size, <span class="dt">mean =</span> theta[<span class="dv">3</span>,<span class="dv">1</span>],   </a>
<a class="sourceLine" id="cb850-12" data-line-number="12">                     <span class="dt">sd =</span> theta[<span class="dv">3</span>,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb850-13" data-line-number="13">w1 =<span class="st"> </span><span class="kw">length</span>(sample1) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(X)  </a>
<a class="sourceLine" id="cb850-14" data-line-number="14">w2 =<span class="st"> </span><span class="kw">length</span>(sample2) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(X)  </a>
<a class="sourceLine" id="cb850-15" data-line-number="15">w3 =<span class="st"> </span><span class="kw">length</span>(sample3) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(X)  </a>
<a class="sourceLine" id="cb850-16" data-line-number="16">W =<span class="st"> </span><span class="kw">c</span>(w1, w2, w3)    <span class="co"># prior probabilities</span></a>
<a class="sourceLine" id="cb850-17" data-line-number="17">X =<span class="st"> </span><span class="kw">c</span>(x1, x2, x3)    <span class="co"># mixture distribution</span></a>
<a class="sourceLine" id="cb850-18" data-line-number="18">Y =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;C&quot;</span>) <span class="co"># components</span></a></code></pre></div>

<p>Albeit we have generated three samples, they are merely used to simulate a <strong>gaussian mixture</strong> for our observed data, namely <strong>X</strong>. For demonstration, the three samples (or three components) become latent but labeled as <strong>A</strong>, <strong>B</strong>, and <strong>C</strong> accordingly. The parameters for each component, namely theta <span class="math inline">\(\theta_j\)</span> and prior weight <span class="math inline">\(\omega_j\)</span>, are kept unknown but arbitrarily initialized. See Figure <a href="7.6-bayesianinference.html#fig:mixturecomponents">7.16</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mixturecomponents"></span>
<img src="DS_files/figure-html/mixturecomponents-1.png" alt="Gaussian Mixture Model - 3 hidden components" width="70%" />
<p class="caption">
Figure 7.16: Gaussian Mixture Model - 3 hidden components
</p>
</div>

<p>Before we show the implementation of each step, let us first define the <strong>natural log</strong> function.</p>

<div class="sourceCode" id="cb851"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb851-1" data-line-number="1">ln &lt;-<span class="st"> </span><span class="cf">function</span>(n) { <span class="kw">log</span>(n, <span class="kw">exp</span>(<span class="dv">1</span>))} <span class="co"># exp(1) = 2.718282</span></a></code></pre></div>

<p>For the <strong>Expectation Step</strong>, we calculate the <strong>weighted likelihood</strong> of each observation belonging to a component. Notice that each weighted likelihood gets a higher weighting score the more relevant the corresponding data points become to their component.</p>

<div class="sourceCode" id="cb852"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb852-1" data-line-number="1">weight.likelihd1 =<span class="st"> </span><span class="kw">norm.prob</span>(X, <span class="dt">mean=</span>theta[<span class="dv">1</span>,<span class="dv">1</span>], <span class="dt">sd=</span>theta[<span class="dv">1</span>,<span class="dv">2</span>]) <span class="op">*</span><span class="st"> </span>W[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb852-2" data-line-number="2">weight.likelihd2 =<span class="st"> </span><span class="kw">norm.prob</span>(X, <span class="dt">mean=</span>theta[<span class="dv">2</span>,<span class="dv">1</span>], <span class="dt">sd=</span>theta[<span class="dv">2</span>,<span class="dv">2</span>]) <span class="op">*</span><span class="st"> </span>W[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb852-3" data-line-number="3">weight.likelihd3 =<span class="st"> </span><span class="kw">norm.prob</span>(X, <span class="dt">mean=</span>theta[<span class="dv">3</span>,<span class="dv">1</span>], <span class="dt">sd=</span>theta[<span class="dv">3</span>,<span class="dv">2</span>]) <span class="op">*</span><span class="st"> </span>W[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb852-4" data-line-number="4"><span class="kw">round</span>(<span class="kw">matrix</span>( <span class="kw">c</span>(weight.likelihd1, weight.likelihd2, weight.likelihd3),</a>
<a class="sourceLine" id="cb852-5" data-line-number="5">     <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>),<span class="dv">4</span>)[,<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>] <span class="co"># display 1st 8 columns</span></a></code></pre></div>
<pre><code>##        [,1]   [,2]   [,3]   [,4]  [,5]   [,6]   [,7]   [,8]
## [1,] 0.0464 0.0477 0.0273 0.0263 0.001 0.0005 0.0003 0.0033
## [2,] 0.0012 0.0009 0.0000 0.0000 0.000 0.0513 0.0428 0.0648
## [3,] 0.0000 0.0000 0.0000 0.0000 0.000 0.0022 0.0038 0.0001</code></pre>

<p>Then we calculate for the normalizer - the joint probability of the <strong>Gaussian mixture</strong>.</p>

<div class="sourceCode" id="cb854"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb854-1" data-line-number="1">normalizer =<span class="st"> </span>weight.likelihd1 <span class="op">+</span><span class="st"> </span>weight.likelihd2 <span class="op">+</span><span class="st"> </span>weight.likelihd3</a>
<a class="sourceLine" id="cb854-2" data-line-number="2"><span class="kw">round</span>(normalizer,<span class="dv">3</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>] <span class="co"># display 1st 10 columns</span></a></code></pre></div>
<pre><code>##  [1] 0.048 0.049 0.027 0.026 0.001 0.054 0.047 0.068 0.033 0.068</code></pre>

<p>Finally, we calculate the <strong>normalized posterior</strong> of each component. Let us show the posterior result of the 1st component.</p>

<div class="sourceCode" id="cb856"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb856-1" data-line-number="1">posterior1 =<span class="st"> </span>weight.likelihd1 <span class="op">/</span><span class="st"> </span>normalizer </a>
<a class="sourceLine" id="cb856-2" data-line-number="2">posterior2 =<span class="st"> </span>weight.likelihd2 <span class="op">/</span><span class="st"> </span>normalizer </a>
<a class="sourceLine" id="cb856-3" data-line-number="3">posterior3 =<span class="st"> </span>weight.likelihd3 <span class="op">/</span><span class="st"> </span>normalizer </a>
<a class="sourceLine" id="cb856-4" data-line-number="4"><span class="kw">round</span>(posterior1,<span class="dv">2</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>] <span class="co"># display 1st 10 columns</span></a></code></pre></div>
<pre><code>##  [1] 0.97 0.98 1.00 1.00 1.00 0.01 0.01 0.05 0.00 0.03</code></pre>

<p>For <strong>Maximization step</strong>:</p>
<p>We first calculate the normalizer for <strong>mean</strong>, <strong>standard deviation</strong>, and <strong>prior weight</strong>.</p>

<div class="sourceCode" id="cb858"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb858-1" data-line-number="1">cnm1 =<span class="st"> </span>comp.normalizer1 =<span class="st"> </span><span class="kw">sum</span>( posterior1 )</a>
<a class="sourceLine" id="cb858-2" data-line-number="2">cnm2 =<span class="st"> </span>comp.normalizer2 =<span class="st"> </span><span class="kw">sum</span>( posterior2 )</a>
<a class="sourceLine" id="cb858-3" data-line-number="3">cnm3 =<span class="st"> </span>comp.normalizer3 =<span class="st"> </span><span class="kw">sum</span>( posterior3 )</a>
<a class="sourceLine" id="cb858-4" data-line-number="4"><span class="kw">c</span>(<span class="st">&quot;comp.norm1&quot;</span> =<span class="st"> </span>cnm1, <span class="st">&quot;comp.norm2&quot;</span> =<span class="st"> </span>cnm2, <span class="st">&quot;comp.norm3&quot;</span> =<span class="st"> </span>cnm3)</a></code></pre></div>
<pre><code>## comp.norm1 comp.norm2 comp.norm3 
##      5.047      4.330      5.623</code></pre>

<p>Then we calculate the parameter <strong>mean</strong> <span class="math inline">\(\mu_j^{(t=1)}\)</span> itself for each component at time=1 (or 1st iteration).</p>

<div class="sourceCode" id="cb860"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb860-1" data-line-number="1">mu1 =<span class="st"> </span><span class="kw">sum</span>( posterior1 <span class="op">*</span><span class="st"> </span>X ) <span class="op">/</span><span class="st"> </span>comp.normalizer1 </a>
<a class="sourceLine" id="cb860-2" data-line-number="2">mu2 =<span class="st"> </span><span class="kw">sum</span>( posterior2 <span class="op">*</span><span class="st"> </span>X ) <span class="op">/</span><span class="st"> </span>comp.normalizer2 </a>
<a class="sourceLine" id="cb860-3" data-line-number="3">mu3 =<span class="st"> </span><span class="kw">sum</span>( posterior3 <span class="op">*</span><span class="st"> </span>X ) <span class="op">/</span><span class="st"> </span>comp.normalizer3 </a>
<a class="sourceLine" id="cb860-4" data-line-number="4"><span class="kw">c</span>(<span class="st">&quot;mean1&quot;</span> =<span class="st"> </span>mu1, <span class="st">&quot;mean2&quot;</span> =<span class="st"> </span>mu2, <span class="st">&quot;mean3&quot;</span> =<span class="st"> </span>mu3)</a></code></pre></div>
<pre><code>## mean1 mean2 mean3 
## 23.36 30.79 34.89</code></pre>

<p>We do the same for standard deviation by calculating for <strong>sd</strong> <span class="math inline">\(\sigma_j^{(t=1)}\)</span>.</p>

<div class="sourceCode" id="cb862"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb862-1" data-line-number="1">sd1 =<span class="st"> </span><span class="kw">sum</span>( posterior1 <span class="op">*</span><span class="st"> </span>(X <span class="op">-</span><span class="st"> </span>mu1)<span class="op">^</span><span class="dv">2</span> ) <span class="op">/</span><span class="st"> </span>comp.normalizer1 </a>
<a class="sourceLine" id="cb862-2" data-line-number="2">sd2 =<span class="st"> </span><span class="kw">sum</span>( posterior2 <span class="op">*</span><span class="st"> </span>(X <span class="op">-</span><span class="st"> </span>mu2)<span class="op">^</span><span class="dv">2</span> ) <span class="op">/</span><span class="st"> </span>comp.normalizer2 </a>
<a class="sourceLine" id="cb862-3" data-line-number="3">sd3 =<span class="st"> </span><span class="kw">sum</span>( posterior3 <span class="op">*</span><span class="st"> </span>(X <span class="op">-</span><span class="st"> </span>mu3)<span class="op">^</span><span class="dv">2</span> ) <span class="op">/</span><span class="st"> </span>comp.normalizer3 </a>
<a class="sourceLine" id="cb862-4" data-line-number="4"><span class="kw">c</span>(<span class="st">&quot;sd1&quot;</span> =<span class="st"> </span>sd1, <span class="st">&quot;sd2&quot;</span> =<span class="st"> </span>sd2, <span class="st">&quot;mean3&quot;</span> =<span class="st"> </span>sd3)</a></code></pre></div>
<pre><code>##   sd1   sd2 mean3 
## 6.145 1.252 2.124</code></pre>

<p>We also re-calculate for the next value of our <strong>prior weights</strong> <span class="math inline">\(\omega_j^{(t=1)}\)</span>:</p>

<div class="sourceCode" id="cb864"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb864-1" data-line-number="1">w1 =<span class="st"> </span>comp.normalizer1 <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(X) </a>
<a class="sourceLine" id="cb864-2" data-line-number="2">w2 =<span class="st"> </span>comp.normalizer2 <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(X) </a>
<a class="sourceLine" id="cb864-3" data-line-number="3">w3 =<span class="st"> </span>comp.normalizer3 <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(X) </a>
<a class="sourceLine" id="cb864-4" data-line-number="4"><span class="kw">c</span>(<span class="st">&quot;prior1&quot;</span> =<span class="st"> </span>w1, <span class="st">&quot;prior2&quot;</span> =<span class="st"> </span>w2, <span class="st">&quot;prior3&quot;</span> =<span class="st"> </span>w3)</a></code></pre></div>
<pre><code>## prior1 prior2 prior3 
## 0.3364 0.2887 0.3749</code></pre>

<p>After computing for <strong>mean</strong>, <strong>sd</strong>, and <strong>prior weight</strong> we continue to iterate until convergence.</p>
<p>Below is an example of <strong>EM</strong> in R code (Here, we change the sample size to 500). Note that the implementation and samples are based on <strong>gaussian distribution</strong>.</p>



<div class="sourceCode" id="cb866"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb866-1" data-line-number="1">expectation &lt;-<span class="st"> </span><span class="cf">function</span>(X, theta, W) {</a>
<a class="sourceLine" id="cb866-2" data-line-number="2">  posterior =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>k, <span class="dt">ncol=</span>n, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb866-3" data-line-number="3">  normalizer =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb866-4" data-line-number="4">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {</a>
<a class="sourceLine" id="cb866-5" data-line-number="5">    posterior[j,] =<span class="st"> </span><span class="kw">norm.prob</span>(X, <span class="dt">mean=</span> theta[j,<span class="dv">1</span>], </a>
<a class="sourceLine" id="cb866-6" data-line-number="6">                              <span class="dt">sd =</span> theta[j,<span class="dv">2</span>]) <span class="op">*</span><span class="st"> </span>W[j]</a>
<a class="sourceLine" id="cb866-7" data-line-number="7">    normalizer =<span class="st"> </span>normalizer <span class="op">+</span><span class="st"> </span>posterior[j,]</a>
<a class="sourceLine" id="cb866-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb866-9" data-line-number="9">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {</a>
<a class="sourceLine" id="cb866-10" data-line-number="10">    posterior[j,] =<span class="st"> </span>posterior[j,] <span class="op">/</span><span class="st"> </span>normalizer</a>
<a class="sourceLine" id="cb866-11" data-line-number="11">  }</a>
<a class="sourceLine" id="cb866-12" data-line-number="12">  <span class="kw">list</span>(<span class="st">&quot;posterior&quot;</span> =<span class="st"> </span>posterior, <span class="st">&quot;loglik&quot;</span> =<span class="kw">sum</span>(<span class="kw">log</span>(normalizer, <span class="kw">exp</span>(<span class="dv">1</span>))))</a>
<a class="sourceLine" id="cb866-13" data-line-number="13">}</a>
<a class="sourceLine" id="cb866-14" data-line-number="14">maximization &lt;-<span class="st"> </span><span class="cf">function</span>(X, posterior ) { </a>
<a class="sourceLine" id="cb866-15" data-line-number="15">  comp.normalizer =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, k)</a>
<a class="sourceLine" id="cb866-16" data-line-number="16">  mu =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, k); var =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, k); W =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, k)</a>
<a class="sourceLine" id="cb866-17" data-line-number="17">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) { </a>
<a class="sourceLine" id="cb866-18" data-line-number="18">    comp.normalizer[j] =<span class="st"> </span><span class="kw">sum</span>( posterior[j,] )</a>
<a class="sourceLine" id="cb866-19" data-line-number="19">    mu[j] =<span class="st"> </span><span class="kw">sum</span>( posterior[j,] <span class="op">*</span><span class="st"> </span>X ) <span class="op">/</span><span class="st"> </span>comp.normalizer[j] </a>
<a class="sourceLine" id="cb866-20" data-line-number="20">    var[j] =<span class="st"> </span><span class="kw">sum</span>( posterior[j,] <span class="op">*</span><span class="st"> </span></a>
<a class="sourceLine" id="cb866-21" data-line-number="21"><span class="st">                        </span>(X <span class="op">-</span><span class="st"> </span>mu[j])<span class="op">^</span><span class="dv">2</span> ) <span class="op">/</span><span class="st"> </span>comp.normalizer[j] </a>
<a class="sourceLine" id="cb866-22" data-line-number="22">    W[j]  =<span class="st"> </span>comp.normalizer[j] <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(X)</a>
<a class="sourceLine" id="cb866-23" data-line-number="23">  }</a>
<a class="sourceLine" id="cb866-24" data-line-number="24">  theta =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>(mu, <span class="kw">sqrt</span>(var)), <span class="dt">nrow=</span>k, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb866-25" data-line-number="25">  <span class="kw">list</span>(<span class="st">&quot;theta&quot;</span>=theta, <span class="st">&quot;W&quot;</span>=<span class="st"> </span>W)</a>
<a class="sourceLine" id="cb866-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb866-27" data-line-number="27">iterate &lt;-<span class="st"> </span><span class="cf">function</span>(x, theta, <span class="dt">limit=</span><span class="dv">1000</span>) {</a>
<a class="sourceLine" id="cb866-28" data-line-number="28">  tol =<span class="st"> </span><span class="fl">1e-10</span>; err =<span class="st"> </span>loglik.old =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb866-29" data-line-number="29">  sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>k <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb866-30" data-line-number="30">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb866-31" data-line-number="31">    component &lt;-<span class="st"> </span><span class="kw">expectation</span>(x, theta, W)        <span class="co"># new posterior</span></a>
<a class="sourceLine" id="cb866-32" data-line-number="32">    model =<span class="st"> </span><span class="kw">maximization</span>(X, component<span class="op">$</span>posterior) <span class="co"># new model parameters </span></a>
<a class="sourceLine" id="cb866-33" data-line-number="33">    loglik =<span class="st"> </span>component<span class="op">$</span>loglik</a>
<a class="sourceLine" id="cb866-34" data-line-number="34">    err =<span class="st"> </span><span class="kw">abs</span> ( loglik <span class="op">-</span><span class="st"> </span>loglik.old )</a>
<a class="sourceLine" id="cb866-35" data-line-number="35">    sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(t, <span class="kw">round</span>(theta[,<span class="dv">1</span>],<span class="dv">2</span>), </a>
<a class="sourceLine" id="cb866-36" data-line-number="36">                                 <span class="kw">round</span>(theta[,<span class="dv">2</span>],<span class="dv">2</span>), err ))</a>
<a class="sourceLine" id="cb866-37" data-line-number="37">    <span class="cf">if</span> ( err <span class="op">&lt;</span><span class="st"> </span>tol ) { <span class="cf">break</span> }</a>
<a class="sourceLine" id="cb866-38" data-line-number="38">    theta =<span class="st"> </span>model<span class="op">$</span>theta  <span class="co"># old model parameter update</span></a>
<a class="sourceLine" id="cb866-39" data-line-number="39">    W =<span class="st"> </span>model<span class="op">$</span>W          <span class="co"># old prior update</span></a>
<a class="sourceLine" id="cb866-40" data-line-number="40">    loglik.old =<span class="st"> </span>loglik  <span class="co"># old likelihood update</span></a>
<a class="sourceLine" id="cb866-41" data-line-number="41">  }</a>
<a class="sourceLine" id="cb866-42" data-line-number="42">  <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Iteration&quot;</span>, <span class="kw">paste0</span>(<span class="st">&quot;mu&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,k)), </a>
<a class="sourceLine" id="cb866-43" data-line-number="43">                         <span class="kw">paste0</span>(<span class="st">&quot;sd&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,k)), <span class="st">&quot;error&quot;</span> )</a>
<a class="sourceLine" id="cb866-44" data-line-number="44">  iteration =<span class="st"> </span><span class="kw">as.data.frame</span>( <span class="kw">tail</span>( sequence))</a>
<a class="sourceLine" id="cb866-45" data-line-number="45">  <span class="kw">list</span>(<span class="st">&quot;theta&quot;</span>=theta, <span class="st">&quot;iteration&quot;</span>=iteration, </a>
<a class="sourceLine" id="cb866-46" data-line-number="46">       <span class="st">&quot;posterior&quot;</span> =<span class="st"> </span>component<span class="op">$</span>posterior)</a>
<a class="sourceLine" id="cb866-47" data-line-number="47">}</a></code></pre></div>

<p>By executing the <strong>EM steps</strong>, we can iterate and converge with an optimized value for <strong>mean</strong> and <strong>standard deviation</strong> parameters. Note that parameters are more optimized with more observations.</p>

<div class="sourceCode" id="cb867"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb867-1" data-line-number="1">em =<span class="st"> </span><span class="kw">iterate</span>(X, theta, <span class="dt">limit=</span><span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb867-2" data-line-number="2">em<span class="op">$</span>iteration</a></code></pre></div>
<pre><code>##        Iteration   mu1   mu2   mu3  sd1  sd2  sd3     error
## [180,]       180 24.96 29.92 34.95 2.14 1.32 1.57 1.601e-10
## [181,]       181 24.96 29.92 34.95 2.14 1.32 1.57 1.446e-10
## [182,]       182 24.96 29.92 34.95 2.14 1.32 1.57 1.291e-10
## [183,]       183 24.96 29.92 34.95 2.14 1.32 1.57 1.155e-10
## [184,]       184 24.96 29.92 34.95 2.14 1.32 1.57 1.037e-10
## [185,]       185 24.96 29.92 34.95 2.14 1.32 1.57 9.277e-11</code></pre>

<p>Also, we can now perform stratification by classifying each observation according to the component it belongs. The table below shows the number of observations and proportions belonging to each component after the <strong>EM iteration</strong>.</p>

<div class="sourceCode" id="cb869"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb869-1" data-line-number="1">classify &lt;-<span class="st"> </span><span class="cf">function</span>(theta, posterior) {</a>
<a class="sourceLine" id="cb869-2" data-line-number="2">    component =<span class="st"> </span><span class="kw">apply</span>(posterior, <span class="dv">2</span>, which.max)</a>
<a class="sourceLine" id="cb869-3" data-line-number="3">    strata =<span class="st"> </span><span class="kw">table</span> (component) <span class="co"># groupings or classes</span></a>
<a class="sourceLine" id="cb869-4" data-line-number="4">    model =<span class="st"> </span><span class="kw">cbind</span>(theta, strata, strata <span class="op">/</span><span class="st"> </span>n)</a>
<a class="sourceLine" id="cb869-5" data-line-number="5">    <span class="kw">colnames</span>(model) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mean&quot;</span>, <span class="st">&quot;sd&quot;</span>, <span class="st">&quot;count&quot;</span>, <span class="st">&quot;proportion&quot;</span>)</a>
<a class="sourceLine" id="cb869-6" data-line-number="6">    <span class="kw">rownames</span>(model) =<span class="st"> </span>Y <span class="co"># components</span></a>
<a class="sourceLine" id="cb869-7" data-line-number="7">    <span class="kw">round</span>( model, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb869-8" data-line-number="8">    <span class="kw">list</span>(<span class="st">&quot;model&quot;</span>=model, <span class="st">&quot;component&quot;</span>=component)</a>
<a class="sourceLine" id="cb869-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb869-10" data-line-number="10">classified =<span class="st"> </span><span class="kw">classify</span>(em<span class="op">$</span>theta, em<span class="op">$</span>posterior)</a>
<a class="sourceLine" id="cb869-11" data-line-number="11">classified<span class="op">$</span>model</a></code></pre></div>
<pre><code>##    mean    sd count proportion
## A 24.96 2.142   492     0.3280
## B 29.92 1.320   488     0.3253
## C 34.95 1.570   520     0.3467</code></pre>

<p>Finally, we also can show how the proportions are broken down into their <strong>Gaussian components</strong>. See Figure <a href="7.6-bayesianinference.html#fig:mixturecomponents1">7.17</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mixturecomponents1"></span>
<img src="DS_files/figure-html/mixturecomponents1-1.png" alt="Four mixture densities" width="70%" />
<p class="caption">
Figure 7.17: Four mixture densities
</p>
</div>

<p>We leave readers to experiment on <strong>EM</strong> by using other parameter models of other familiar distributions.</p>
</div>
<div id="variational-inference" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.6.5</span> Variational Inference <a href="7.6-bayesianinference.html#variational-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In a previous section, we used <strong>Laplace approximation</strong> to estimate our posterior with ease. That is possible only because the problem statement in our case involves a unimodal Gaussian distribution that may be easy to compute. Also, in the last section, we use <strong>Expectation-Maximization</strong> to solve for a more complex distribution; and that is possible only because the problem statement allows for a tractable posterior. However, if our Gaussian mixture distribution consists of a larger number of components with a larger number of observations than as seen in Figure <a href="7.6-bayesianinference.html#fig:mixturecomponents1">7.17</a>, then the <strong>marginal likelihood</strong> - the denominator in the Bayes theorem - becomes intractable. The equation below demonstrates the intractability of the <strong>marginal likelihood</strong> <span class="citation">(Blei D et al., <a href="bibliography.html#ref-ref393d">n.d.</a>)</span>:</p>

<p><span class="math display" id="eq:equate1090443">\[\begin{align}
P(\mu_{1:k}, y_{1:n}|x_{1:n}) =
\frac{\prod_{j=1}^kP(\mu_j) \prod_{i=1}^n P(y_i)P(x_i|y_i,\mu_{1:k})}
{\int_{\mu_{1:k}}\sum_{y_{1:n}} \prod_{j=1}^k P(\mu_j)\prod_{i=1}^nP(y_i)P(x_i|y_i,\mu_{1:k})} \tag{7.461} 
\end{align}\]</span>
</p>
<p>Consequently, our <strong>posterior distribution</strong> is rendered intractable. That is also true if none of the standard <strong>Conjugacy</strong> methods apply, otherwise allowing us to compute for the <strong>posterior distribution</strong> exactly (in closed-form).</p>
<p>Additionally, the inference techniques discussed in previous sections, such as <strong>MLE</strong> and <strong>MAP</strong>, fail to operate on complex models. Therefore, it behooves us to find other techniques in mitigating the challenge.</p>
<p>Here, we introduce <strong>Variational Bayes</strong>. Note that our discussion references some of the works of Bishop C.M <span class="citation">(<a href="bibliography.html#ref-ref482c">2006</a>)</span>, Blei D. et al. <span class="citation">(<a href="bibliography.html#ref-ref381d">2017</a>)</span>, and C.W., Roberts S.J. <span class="citation">(<a href="bibliography.html#ref-ref1048f">2012</a>)</span>. </p>
<p>The motivation in this section is to find an <strong>approximating distribution</strong> denoted as <span class="math inline">\(\mathcal{Q}(y)\)</span> to eventually take the place of our intractable <strong>posterior distribution</strong> denoted as <span class="math inline">\(P(y|x)\)</span>. In the context of <strong>Bayesian inference</strong>, also referenced as <strong>variational inference (VI)</strong>, this approximating distribution is also called <strong>variational distribution</strong> with parameters optimized based on an approach different from one offered by <strong>Laplace approximation</strong> for reasons that become apparent later.</p>
<p>One essential concept in this section is the <strong>Bayesian Network</strong>, which depicts the dependency between variables. For example, Figure <a href="7.6-bayesianinference.html#fig:bayesnetwork">7.18</a> demonstrates a joint probability that follows the product rule <span class="citation">(Nguyen L. <a href="bibliography.html#ref-ref948l">2013</a>; Horny M. <a href="bibliography.html#ref-ref939m">2014</a>)</span>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bayesnetwork"></span>
<img src="bayesnetwork.png" alt="Bayesian Network" width="80%" />
<p class="caption">
Figure 7.18: Bayesian Network
</p>
</div>

<p>Let us now discuss how <strong>variational Bayes</strong> works.</p>
<p><strong>First</strong>, our goal is essentially to develop a model for our inference. This model represents a family of distributions. We define this family of distributions <strong>depending on the problem statement</strong>. In our case, let us scheme to use a model that involves a multivariate Gaussian mixture distribution:</p>

<p><span class="math display">\[\begin{align*}
\underbrace{x_i|y_i \sim \mathcal{N}_p(\mu_k, \Lambda_k)}_\text{observed variable}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\underbrace{y_i  \sim Multi(n,\pi_k)}_\text{latent variable}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\underbrace{\pi_k \sim Dirichlet(\omega_0)}_\text{mixing weight/coefficient}\\
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\underbrace{\mu_k\ \sim \mathcal{N}_p(\alpha_0, \beta_0) }_\text{1st latent conjugate prior}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\underbrace{\Lambda_k\ \sim Wishart(\nu_0, \Sigma_0) }_\text{2nd latent conjugate prior}
\end{align*}\]</span>
</p>
<p>where <span class="math inline">\(\beta_0\)</span> is scaling factor, <span class="math inline">\(\pi_k\)</span> is a <strong>mixture coefficient</strong>, and <span class="math inline">\(\Lambda_k\)</span> is a <strong>positive-definite precision matrix</strong>.</p>
<p>The observed variable, namely <strong>x</strong>, has a corresponding latent (classifying) variable, namely <strong>y</strong>, which contains a hot-encoding of a binary vector of K-size, namely <span class="math inline">\(\mathbf{\tau_k}\)</span> (to be further explained), describing which cluster each corresponding observation belongs.</p>
<p>We also can revisit Figure <a href="7.3-bayes-theorem.html#fig:bayestheorem">7.1</a> discussed in <strong>Bayes Theorem</strong> section as reference along with Figure <a href="7.6-bayesianinference.html#fig:mixturebayes">7.19</a> for two models. Albeit here, we operate on <strong>p-variate</strong> gaussian mixture models.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mixturebayes"></span>
<img src="mixturebayes.png" alt="Multivariate Gaussian Mixture Model" width="70%" />
<p class="caption">
Figure 7.19: Multivariate Gaussian Mixture Model
</p>
</div>

<p>From Figure <a href="7.6-bayesianinference.html#fig:mixturebayes">7.19</a>, the <strong>Bayesian model</strong> captures five hyperparameters, namely <span class="math inline">\(\{\ \omega_0, \alpha_0, (\beta_0\Lambda_k), \nu_0, \Sigma_0 \}\)</span>, for which we calculate the corresponding <strong>conjugacy</strong>. Note here that we assume a conjugacy relationship between parameters, hinting at a closed-form solution for our model, which is useful.</p>
<p>In <strong>Bayesian Network</strong> notation, the dependency in the <strong>Bayesian model</strong> (excluding hyperparameters) is written as: </p>

<p><span class="math display" id="eq:equate1090444">\[\begin{align}
P(x_i, y_i, \pi_k, \mu_k, \Lambda_k) {}&amp;= P(x_i | y_i, \mu_k, \Lambda_k)   &amp; x_i \text{ depends on } \{y_i, \mu_k, \Lambda_k \} \nonumber \\
&amp;\times P(y_i|\pi_k)   &amp; y_i \text{ depends on } \{\pi_k \} \nonumber \\
&amp;\times P(\pi_k) \nonumber \\
&amp;\times P(\mu_k|\Lambda_k)   &amp; \mu_k \text{ depends on } \{\Lambda_k \} \nonumber \\
&amp;\times P(\Lambda_k)  \tag{7.462} 
\end{align}\]</span>
</p>
<p>For posterior distribution with <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\Lambda_k\)</span> being both unknown, recall the derivation for a <strong>normal-Wishart</strong> conjugacy we made in the <strong>Conjugacy</strong> section:</p>

<p><span class="math display" id="eq:equate1090445">\[\begin{align}
\mu_k, \Lambda_k|x\sim \ \mathcal{NW}_p(\alpha_1, \beta_1, \nu_1,\Sigma_1) 
= \mathcal{N}_p(\alpha_1, \beta_1)\times\mathcal{W}_p(\nu_1, \Sigma_1) \tag{7.463} 
\end{align}\]</span>
</p>
<p>obtaining the following parameters:</p>

<p><span class="math display" id="eq:equate1090449" id="eq:equate1090448" id="eq:equate1090447" id="eq:equate1090446">\[\begin{align}
\alpha_1 &amp;=  \frac{(\alpha_0\beta_0 + n\bar{x})}{(\beta_0 + n)} \tag{7.464} \\
\beta_1 &amp;= \beta_0 + n  \tag{7.465} \\
\nu_1 {}&amp;= \nu_0 + n  \tag{7.466} \\
\Sigma_1 &amp;=  \frac{ n\beta_0}{(\beta_0 + n)}(\bar{x} - \alpha_0)( \bar{x} - \alpha_0 )^T + S  + \Sigma_0^{-1}  \tag{7.467} 
\end{align}\]</span>
</p>
<p>In the case in which <span class="math inline">\(\mu_k\)</span> is known and <span class="math inline">\(\Lambda_k\)</span> is unknown, we can use the <strong>normal inverse Wishart</strong> conjugacy instead:</p>

<p><span class="math display" id="eq:equate1090450">\[\begin{align}
\mu_k|x\sim \ \mathcal{N}_p(\alpha_1, \beta_1)\ \ \ \ \ \ 
\Lambda_k|x\sim \ \mathcal{IW}_p(\nu_1, \Sigma_1) \tag{7.468} 
\end{align}\]</span>
</p>
<p>obtaining the following parameters:</p>

<p><span class="math display" id="eq:equate1090451">\[\begin{align}
\Sigma_1 = (\Sigma_0 + \Sigma)^{-1}\ \ \ \ \ \ \ \ \nu_1 = \nu_0 + n  \tag{7.469} 
\end{align}\]</span>
</p>
<p>For the posterior distribution of our category, <span class="math inline">\(\pi_k\)</span>, let us recall the derivation for a multinomial-Dirichlet conjugacy:</p>

<p><span class="math display" id="eq:equate1090452">\[\begin{align}
\pi_k \sim \mathcal{Dir}\left(\omega_1\right) = \frac{1}{\mathcal{B}\left(\omega_1\right)}\prod_{k=1}^K x_i^{\omega_1-1} \tag{7.470} 
\end{align}\]</span>
</p>
<p>obtaining the following:</p>

<p><span class="math display" id="eq:equate1090453">\[\begin{align}
\omega_1 = \sum_{i=1}^k\left(x_i + \omega_0\right) \tag{7.471} 
\end{align}\]</span>
</p>
<p>A natural step to take here is to initialize the hyperparameters and iteratively optimize the model parameters <span class="math inline">\(\{\ \pi_k, \mu_k, \Lambda_k\ \}\)</span>, including the latent variable <strong>y</strong>. We have shown this in the <strong>Expectation-Maximization</strong> section. However, to illustrate <strong>variational inference</strong>, let us skip the <strong>analytical operations</strong> and the dependency to <strong>conjugate</strong> priors. Instead, we move on to the <strong>Mean-Field modeling</strong> for approximation.</p>
<p>For the <strong>Mean-Field model</strong>, the family of distributions above forms our <strong>variational distribution</strong> that approximates our true <strong>posterior</strong> such that <span class="math inline">\(P(y,\pi, \mu, \Lambda |x) \approx \mathcal{Q}(y,\pi, \mu, \Lambda)\)</span>. For mathematical convenience, let us temporarily use <strong>z</strong> as placeholder for <span class="math inline">\(\{\ y, \pi, \mu, \Lambda\ \}\)</span> so that we have the following: </p>

<p><span class="math display" id="eq:equate1090454">\[\begin{align}
P(z|x) \approx \mathcal{Q}(z)\ \ \ \ \leftarrow\ \ \ \ \ P(y,\pi, \mu, \Lambda |x) \approx \mathcal{Q}(y,\pi, \mu, \Lambda). \tag{7.472} 
\end{align}\]</span>
</p>
<p>Then, to generalize, we factorize into a simpler tractable granular composition (a factor) such that we have the following:</p>

<p><span class="math display" id="eq:equate1090455">\[\begin{align}
\mathcal{Q}(z) = \prod_{j=1} \mathcal{Q}_i(z_i) \tag{7.473} 
\end{align}\]</span>
</p>
<p>That is called <strong>mean-field variational family</strong> <span class="citation">(Blei D. et al. <a href="bibliography.html#ref-ref381d">2017</a>)</span>. We explain the reason for factorization in a few steps.</p>
<p>It is important to assume that the latent variable and its parameters are independent (by virtue of mean-field variational property), and so we can model a joint distribution - <strong>our variational distribution</strong> - like so:</p>

<p><span class="math display" id="eq:equate1090456">\[\begin{align}
\mathcal{Q}(z)  = \mathcal{Q}(y,\pi, \mu, \Lambda ) = 
\prod_{i=1}^N \mathcal{Q}_y(y_i) \times \prod_{k=1}^K\left[\mathcal{Q}_\pi(\pi_k) \times \mathcal{Q}_\mu(\mu_k)\times\mathcal{Q}_{\Lambda}\left(\Lambda_k\right)\right]  \tag{7.474} 
\end{align}\]</span>
</p>
<p><strong>Second</strong>, because our objective is to optimize our <strong>variational distribution</strong>, we need to <strong>develop an objective function</strong> that we can maximize. Let us show a couple of derivations that we need later for our inference. We start by deriving a <strong>variational lower bound</strong> equation, also called <strong>evidence lower bound (ELBO)</strong>, and a <strong>KL divergence</strong> equation <span class="citation">(Yang X. <a href="bibliography.html#ref-ref1057x">2017</a>)</span>. Both are derived from the <strong>marginal likelihood</strong> factor in the <strong>Bayes Theorem</strong> - the <strong>evidence</strong>. </p>

<p><span class="math display" id="eq:equate1090461" id="eq:equate1090460" id="eq:equate1090459" id="eq:equate1090458" id="eq:equate1090457">\[\begin{align}
P(x) {}&amp;= \int_z P(x, z)\ dz &amp; \text{(marginal/sum rule)} \tag{7.475} \\
\log_e P(x) &amp;= \log_e \int_z P(x, z)\ dz &amp; \text{(logarithm)} \tag{7.476} \\
&amp;= \log_e \int_z \frac{\mathcal{Q}(z)}{\mathcal{Q}(z)}  P(x,z)\ dz &amp;  \text{(variational distribution)} \tag{7.477} \\
&amp;\equiv \log_e\left(\mathbb{E}_{\mathcal{Q}(z)} \frac{P(x,z)}{\mathcal{Q}(z)}\right) &amp; \text{(expectation)} \tag{7.478} \\
&amp;\ge \mathbb{E}_{\mathcal{Q}(z)} \left( \log_e \frac{P(x,z)}{\mathcal{Q}(z)}\right) &amp; \text{(jensen&#39;s inequality)} \tag{7.479} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1090468" id="eq:equate1090467" id="eq:equate1090466" id="eq:equate1090465" id="eq:equate1090464" id="eq:equate1090463" id="eq:equate1090462">\[\begin{align}
&amp; \text{(focus on the equal sign of the inequality,} &amp; \nonumber \\
&amp; \text{extract the lower bound)} &amp; \nonumber \\
\nonumber \\
\mathcal{LB}[Q(z)] &amp;= \mathbb{E}_{\mathcal{Q}(z)} \left( \log_e \frac{P(x,z)}{\mathcal{Q}(z)}\right)\ \ \ \ (\text{lower bound - } \mathbf{\text{ELBO}}) \tag{7.480} \\
&amp;\equiv \int_z {\mathcal{Q}(z)} \left( \log_e \frac{P(x,z)}{\mathcal{Q}(z)}\right) dz &amp; \text{(equivalent)} \tag{7.481} \\
&amp;= \int_z {\mathcal{Q}(z)} \left( \log_e \frac{P(z|x)P(x)}{\mathcal{Q}(z)}\right) dz &amp; \text{(chain rule)} \tag{7.482} \\
&amp;= \int_z {\mathcal{Q}(z)} \left( \log_e \frac{P(z|x)}{\mathcal{Q}(z)}  +
 \log_e P(x)\right) dz &amp; \text{(simplify)} \tag{7.483} \\
&amp;= \underbrace{\int_z \mathcal{Q}(z)  \log_e \frac{P(z|x)}{\mathcal{Q}(z)}\ dz }_\text{-KL divergence} +
 \underbrace{\int_z  \mathcal{Q}(z) \log_e P(x)\  dz }_\text{marginal log likelihood} &amp; \text{(logarithm)} \tag{7.484} \\
\nonumber \\
\mathcal{LB}[Q(z)] &amp;=  -KL(\mathcal{Q}(z) || P(z|x)) + \log_e  P(x) &amp; \text{(ELBO eq. 1)} \tag{7.485} \\
\nonumber \\
\mathcal{KL}(\mathcal{Q}(z) || P(z|x))  &amp;=   -\mathcal{LB}[Q(z)] + \log_e  P(x) &amp; \text{(KL divergence)} \tag{7.486} 
\end{align}\]</span>
</p>
<p>The second term in the <strong>ELBO</strong> equation can be treated as a constant, and because it is independent of the <strong>approximating distribution</strong>, it can therefore be ignored.</p>
<p>Note that the <strong>lower bound (ELBO)</strong> is a functional expression as it accepts a probability function, namely <span class="math inline">\(Q(z)\)</span>.</p>
<p>We now have an <strong>objective function</strong> we can use in the form of the <strong>ELBO</strong>. Both <strong>ELBO</strong> and <strong>KL divergence</strong> can be used as an objective function. See below.</p>

<p><span class="math display" id="eq:equate1090469">\[\begin{align}
\mathcal{LB}(\mathcal{Q}(z)) = \int_z {\mathcal{Q}(z)} \left( \log_e \frac{P(x,z)}{\mathcal{Q}(z)}\right) dz
\ \ \ \ \ \ \ \ \
\mathcal{KL}(\mathcal{Q}(z)||\mathcal{Q}(z|x)) = 
- \int_z \mathcal{Q}(z)  \log_e \frac{P(z|x)}{\mathcal{Q}(z)}\ dz \tag{7.487} 
\end{align}\]</span>
</p>
<p>By maximizing <strong>ELBO</strong>, we are effectively minimizing <strong>KL divergence</strong> which means that our <strong>variational distribution</strong> is closer to our <strong>posterior</strong>, e.g. <span class="math inline">\(\mathcal{Q}(z) \approx P(z|x)\)</span>. See Figure <a href="7.6-bayesianinference.html#fig:tightbound">7.20</a> for the balance:</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tightbound"></span>
<img src="tightbound.png" alt="KL Divergence vs Lower Bound" width="70%" />
<p class="caption">
Figure 7.20: KL Divergence vs Lower Bound
</p>
</div>

<p>However, notice that the <strong>KL divergence</strong> has a conditional distribution that can potentially be intractable. That also effectively renders <strong>ELBO</strong> intractable.</p>
<p>Let us show an alternative <strong>ELBO</strong> measurement. We use <strong>Entropy</strong> over <strong>KL divergence</strong>, which we can also derive from the lower bound. We avoid the use of the chain rule in what follows.</p>

<p><span class="math display" id="eq:equate1090473" id="eq:equate1090472" id="eq:equate1090471" id="eq:equate1090470">\[\begin{align}
\mathcal{LB}[Q(z)] {}&amp;= \int_z {\mathcal{Q}(z)} \left( \log_e \frac{P(x,z)}{\mathcal{Q}(z)}\right) dz &amp; \text{(lower bound)} \tag{7.488} \\
&amp;= \int_z \mathcal{Q}(z) \left( \log_e  P(x,z) - \log_e \mathcal{Q}(z)  \right) \ dz &amp; \text{(logarithm rule)} \tag{7.489} \\
&amp;= \underbrace{\int_z \mathcal{Q}(z)\log_e P(x,z)\ dz}_\text{expected energy}\ 
  \underbrace{-\int_z \mathcal{Q}(z)\log_e \mathcal{Q}(z) \ dz}_\text{Entropy} &amp; \text{(simplify)} \tag{7.490} \\
\nonumber \\
\mathcal{LB}[Q(z)] &amp;= \mathbb{E}_{Q(z)}\left[\log_e P(x,z)\right] + \mathcal{H}(z) &amp; \text{(ELBO eq. 2)} \tag{7.491} 
\end{align}\]</span>
</p>
<p><strong>Third</strong>, now that we have an objective function in the form of <strong>ELBO eq. 2</strong>, we also need to generate an <strong>update function</strong> to optimize our model parameters. That is where we need to perform more derivation. So let us expand <strong>ELBO eq. 2</strong> <span class="citation">(Yuling Yao et al <a href="bibliography.html#ref-ref457y">2018</a>; Keng B. <a href="bibliography.html#ref-ref443b">2018</a>)</span>.</p>

<p><span class="math display" id="eq:equate1090477" id="eq:equate1090476" id="eq:equate1090475" id="eq:equate1090474">\[\begin{align}
{}&amp;\mathcal{LB}[Q(z)] = \mathbb{E}_{Q(z)}(P(x,z)) + \mathcal{H}(z)  \tag{7.492} \\
&amp;= \int_z \mathcal{Q}(z)\log_e P(x,z)\ dz -\int_z \mathcal{Q}(z)\log_e \mathcal{Q}(z) \ dz \tag{7.493} \\
&amp;= \int_z \prod_{i=1} \mathcal{Q_i}(z_i)\log_e P(x,z)\ dz -\int_z \prod_{i=1} \mathcal{Q_i}(z_i)\log_e \prod_{i=1} \mathcal{Q_i}(z_i) \ dz &amp; \text{(mean-field assumption)} \tag{7.494} \\
&amp;\rightarrow  \prod_{i=1} \mathcal{Q}_i(z_i) = \mathcal{Q}_j(z_j) \times \prod_{i \ne j} \mathcal{Q}_i(z_i) &amp; \text{(extract jth component)} \tag{7.495} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1090482" id="eq:equate1090481" id="eq:equate1090480" id="eq:equate1090479" id="eq:equate1090478">\[\begin{align}
&amp;= \int_{z_j}  \mathcal{Q}_j(z_j) \int_{z_{-j}} \left[ \prod_{i \ne j} \mathcal{Q}_i(z_i) \log_e P(x,z)  \right] d_{z_{-j}} d_{z_j} &amp; \text{(notation:} -j \equiv i\ne j ) \nonumber \\
&amp;-  \int_{z_j} \mathcal{Q}_j(z_j) \int_{z_{-j}} \prod_{i \ne j} \mathcal{Q}_i(z_i) \left( \log_e \mathcal{Q}_j(z_j) + \log_e \prod_{i \ne j} \mathcal{Q}_i(z_i)\right)  d_{z_{-j}}d_{z_j} \tag{7.496} \\
&amp;\rightarrow \int_{z_{-j}} \ \prod_{i \ne j} \mathcal{Q}_i(z_i) d_{z_{-j}} = 1 &amp; \text{(simplify)} \tag{7.497} \\
&amp;\rightarrow \int_{z_{-j}} \ \prod_{i \ne j} \mathcal{Q}_i(z_i) \log_e \prod_{-j} \mathcal{Q}_i(z_i) d_{z_{i\ne j}} = const &amp; \text{(simplify)} \tag{7.498} \\
&amp;= \int_{z_j}  \mathcal{Q}_j(z_j) \mathbb{E}_{Q_{z_{i\ne j}}} \left( \log_e P(x,z) \right)  d_{z_j}   &amp; \text{(expectation)} \nonumber \\
&amp;-  \int_{z_j} \mathcal{Q}_j(z_j)  \left( \log_e \mathcal{Q}_j(z_j) + const \right)d_{z_j} \tag{7.499} \\
\nonumber \\
&amp;= \int_{z_j} \mathcal{Q}_j(z_j) \left(\mathbb{E}_{Q_{z_{i\ne j}}}(\log_e P(x,z)) -  \log_e \mathcal{Q}_j(z_j)\right) dz_j &amp; \text{(factorized ELBO)} \tag{7.500} 
\end{align}\]</span>
</p>
<p>Notice that we have factorized the lower bound. We also can generate the factorized version of the <strong>KL divergence</strong> (which we may not be using in our discussion):</p>

<p><span class="math display" id="eq:equate1090484" id="eq:equate1090483">\[\begin{align}
\mathcal{LB}[\mathcal{Q}(z)]
&amp;= \int_{z_j} \mathcal{Q}_j(z_j) \log_e \frac{\mathbb{E}_{Q_{z_{i\ne j}}}(\log_e P(x,z)) } {\mathcal{Q}_j(z_j)} dz_j &amp; \text{(-KL divergence)} \tag{7.501} \\
&amp;= -KL(\mathcal{Q}_j(z_j)||\mathbb{E}_{Q_{z_{i\ne j}}}(\log_e P( x,z)) ) \tag{7.502} 
\end{align}\]</span>
</p>
<p>To get the <strong>update function</strong>, we maximize the factors. We use <strong>Lagrange multiplier</strong> and partial derivative with respect to the function <span class="math inline">\(\mathcal{Q}_j\)</span> to extract (formulate) the equation for any arbitrary jth component - this uses <strong>functional differential</strong>:</p>

<p><span class="math display" id="eq:equate1090490" id="eq:equate1090489" id="eq:equate1090488" id="eq:equate1090487" id="eq:equate1090486" id="eq:equate1090485">\[\begin{align}
\frac{ \partial\ \mathcal{LB}[\mathcal{Q_j}(z_j)]}{\partial\ \mathcal{Q}_j(z_j)} {}&amp;\equiv
\frac{ \partial}{\partial\ \mathcal{Q}_j(z_j)} \left[ \int_{z_j} \mathcal{Q}_j(z_j) \left(\mathbb{E}_{Q_{z_{i \ne j}}}(\log_e P(x,z)) -  \log_e \mathcal{Q}_j(z_j)\right) dz_j  \right] =  0 \tag{7.503} \\
&amp;\equiv \mathbb{E}_{Q_{z_{i \ne j}}}(\log_e P(x,z)) - \log_e \mathcal{Q}_j(z_j) - 1 = 0 
\nonumber\\
&amp;\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{(entropy functional derivative)} \tag{7.504} \\
&amp;\equiv exp\left[\mathbb{E}_{Q_{z_{i \ne j}}}(\log_e P(x,z)) - \log_e \mathcal{Q}_j(z_j)  \right] = const \ \ \  \text{(exp-log)} \tag{7.505} \\
&amp;\equiv exp\ \mathbb{E}_{Q_{z_{i \ne j}}}(\log_e P(x,z)) -  \mathcal{Q}_j(z_j)  ) = const\ \ \  \text{(exp-log)} \tag{7.506} \\
\nonumber \\
\mathcal{Q}_j(z_j) &amp;\propto exp\left[\mathbb{E}_{Q_{z_{i \ne j}}}(\log_e P(x,z))\right] + const \tag{7.507} \\
&amp;\propto exp\left[\mathbb{E}_{Q_{z_{i \ne j}}}(\log_e P(x,z))\right] + const\ \ \ \ \text{(factorized; mean-field property)} \tag{7.508} 
\end{align}\]</span>
</p>
<p>Herein lies a template for our <strong>update function</strong> that we can use to update individual factors.</p>

<p><span class="math display" id="eq:equate1090491">\[\begin{align}
\mathcal{Q}_j(z_j) \propto exp\left[\mathbb{E}_{Q_{z_{i \ne j}}}(\log_e P(x,z))\right] + const \tag{7.509} 
\end{align}\]</span>
</p>
<p>Equivalently, we have the following expectation equation for the log factor:</p>

<p><span class="math display" id="eq:equate1090492">\[\begin{align}
\log_e \mathcal{Q}_j(z_j) \propto \mathbb{E}_{Q_{z_{i \ne j}}}\left[\log_e P(x,z)\right] + const \tag{7.510} 
\end{align}\]</span>
</p>
<p>That naturally completes the picture in which we have a <strong>factor-level update function</strong> and an <strong>ELBO eq 2</strong> to use for convergence.</p>
<p><strong>Fourth</strong>, at this point, we still do not have a shape for each factor of our <strong>variational distribution</strong>, namely <span class="math inline">\(\mathcal{Q}(z) = \mathcal{Q}(y, \pi, \mu, \Lambda)\)</span>. Our next goal is to pick or make assumptions on the closest type of distribution to use for each factor. For that, we use Figure <a href="7.6-bayesianinference.html#fig:mixturebayes">7.19</a> around the conditional dependency on the <strong>Bayesian model</strong>.</p>
<p>Notice that both our <strong>ELBO eq. 2</strong> and <strong>update function</strong> rely on <span class="math inline">\(\log_e P(x, z)\)</span>. Let us expand the log probability function:</p>

<p><span class="math display" id="eq:equate1090495" id="eq:equate1090494" id="eq:equate1090493">\[\begin{align}
\log_e P(x, z) {}&amp;= \log_e P(x, y, \pi, \mu, \Lambda) \tag{7.511} \\ 
&amp;= \log_e \left[ P(x| y, \mu, \Lambda)
           P(y|\pi)P(\pi)P(\mu|\Lambda)P(\Lambda)\right]  \tag{7.512} \\
&amp;= \sum_{k=1}^K \biggl[ 
   \log_e \prod_{i=1}^N P(x_i| y_{ik}, \mu_k, \Lambda_k) + 
   \log_e \prod_{i=1}^N P(y_{ik}|\pi_k) +  \log_e P(\pi_k)
   +\log_e P(\mu_k,\Lambda_k) \biggr] \tag{7.513} 
\end{align}\]</span>
</p>
<p>Equivalently, the approximate joint distribution is shown below.</p>

<p><span class="math display" id="eq:equate1090496">\[\begin{align}
P(x, y, \pi, \mu, \Lambda) \approx \mathcal{Q}(y, \pi, \mu, \Lambda) = 
\mathcal{Q}_y(y)\mathcal{Q}_\pi(\pi) \prod_{k=1}^K\mathcal{Q}(\mu_k, \Lambda_k)  \tag{7.514} 
\end{align}\]</span>
</p>
<p>Both <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\Lambda_k\)</span> are not split apart into their conditional form, e.g.:</p>

<p><span class="math display" id="eq:equate1090497">\[\begin{align}
\log_e P(\mu_k,\Lambda_k) = \log_e P(\mu_k|\Lambda_k) + \log_e P(\Lambda_k). \tag{7.515} 
\end{align}\]</span>
</p>
<p>Note that the shape of the joint distribution is not the one of interest, but we use the factors above to approximate the shape of the closest distribution we pick.</p>
<p><strong>For</strong> <span class="math inline">\(\mathbf{\mathcal{Q}_y(y)}\)</span>, we absorb terms with respect to <strong>y</strong>:</p>

<p><span class="math display" id="eq:equate1090500" id="eq:equate1090499" id="eq:equate1090498">\[\begin{align}
\log_e \mathcal{Q}^*_y(y) {}&amp;\propto \mathbb{E}_{Q_{-y}}\left[\log_e P(x,z)\right] + const \tag{7.516} \\
&amp;\propto  \mathbb{E}_{Q_{\mu,\Lambda}}\left[ \log_e P(x| y, \mu, \Lambda)\right] + 
   \mathbb{E}_{Q_{\pi}}\left[\log_e P(y|\pi)\right] + const \tag{7.517} \\
&amp;\propto \sum_{k=1}^K \sum_{i=1}^N (y_{ik}) \left[ 
    \log_e \mathcal{N}_p(\ x_i\ ;\ \mu_k, {\Lambda_k}^{-1}\ ) + \log_e {\pi_k} \right]  + const \tag{7.518} 
\end{align}\]</span>
</p>
<p>Operate on the first term, recalling that <span class="math inline">\(\Lambda_k\)</span> is a <strong>positive-definite precision matrix</strong>:</p>

<p><span class="math display" id="eq:equate1090501">\[\begin{align}
\log_e \mathcal{N}_p(\ x_i\ ;\ \mu_k, {\Lambda_k}^{-1}\ ) &amp;= \log_e \left[\frac{|\Lambda_k|^{\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}} exp\left( -\frac{1}{2}(x_i - \mu_k)^T\Lambda_k(x_i - \mu_k)\right)\right]  \tag{7.519} 
\end{align}\]</span>
</p>
<p>Note that <span class="math inline">\((\log_e \pi_k)\)</span> is a <strong>categorical weight or mixture coefficient</strong> and thus should not to be confused with the <strong>gaussian normalizing constant</strong>, namely <span class="math inline">\((2\pi)^{\frac{p}{2}}\)</span>.</p>
<p>Combine the two terms and use a placeholder, namely <span class="math inline">\(\log_e\ \rho_{ik}\)</span>.</p>

<p><span class="math display" id="eq:equate1090503" id="eq:equate1090502">\[\begin{align}
\log_e\ \rho_{ik} {}&amp;=  
   \log_e \left[\frac{|\Lambda_k|^{\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}} exp\left( -\frac{1}{2}(x_i - \mu_k)^T\Lambda_k(x_i - \mu_k)\right)\right]      +  \log_e \pi_k \tag{7.520} \\
&amp;=\frac{1}{2}\log_e |\Lambda_k| - \frac{p}{2}\log_e (2\pi) - \frac{1}{2}(x_i - \mu_k)^T\Lambda_k(x_i - \mu_k) + \log_e \pi_k \tag{7.521} 
\end{align}\]</span>
</p>
<p>Also, take a note of the following expectation and its vector of cluster sizes, namely <span class="math inline">\(\tau_{k}\)</span>, used by other variational factors:</p>

<p><span class="math display" id="eq:equate1090504">\[\begin{align}
 \mathbb{E}_{Q_{y}}\left[y_{ik} \right]  = \tau_{ik} = \frac{\rho_{ik}}{\sum_{j=1}^K \rho_{ij}}
 \ \ \ \ \ \ \ and \ \ \ \ \ \ \ \
 \tau_k = \sum_{i=1}^N \tau_{ik} \tag{7.522} 
\end{align}\]</span>
</p>
<p>where:</p>

<p><span class="math display" id="eq:equate1090505">\[\begin{align}
\rho_{ik} = 
   \frac{\pi_k|\Lambda_k|^{\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}} exp\left(-\frac{1}{2}\biggl[ (x_i - \mu_k)^T\Lambda_k(x_i - \mu_k)\biggr]\right)   \tag{7.523} 
\end{align}\]</span>
</p>
<p><strong>Now</strong>, combine the two terms and solve for <span class="math inline">\(\mathcal{Q}^*_y(y)\)</span>:</p>

<p><span class="math display" id="eq:eqnnumber317" id="eq:eqnnumber316">\[\begin{align}
\log_e \mathcal{Q}^*_y(y) {}&amp;=  \sum_{k=1}^K \sum_{n=1}^N (y_{ik})  \log_e \rho_{ik}
    = \sum_{k=1}^K \sum_{i=1}^N (y_{ik})  \log_e \tau_{ik} \tag{7.524}
   \\
 \mathcal{Q}^*_y({{y_{ik}}; \tau_{ik}})  &amp;= Multi( {{y_{ik}}; \tau_{ik}})  = \prod_{i=1}^N \prod_{k=1}^K  {\tau_{ik}}^{y_{ik}} &amp;
 \begin{array}{rr}
 \text{(exponentiate)}\\
 \text{(Multinomial PDF)}
 \end{array} \tag{7.525}
\end{align}\]</span>
</p>
<p><strong>For</strong> <span class="math inline">\(\mathbf{\mathcal{Q}_\pi(\pi)}\)</span>, we absorb terms with respect to <span class="math inline">\(\pi\)</span>:</p>

<p><span class="math display" id="eq:equate1090509" id="eq:equate1090508" id="eq:equate1090507" id="eq:equate1090506">\[\begin{align}
\log_e \mathcal{Q}_\pi(\pi) {}&amp;\propto \mathbb{E}_{Q_{-\pi}}\left[\log_e P(x,z)\right] + const \tag{7.526} \\ 
&amp;\propto  \mathbb{E}_{Q_{y}}\left[\log_e P(z|\pi)\right] +  \log_e P(\pi)  + const \tag{7.527} \\
&amp;\propto  \sum_{k=1}^K \left[\sum_{i=1}^N (y_{ik}) \log_e {\pi_k} +  \log_e \mathcal{Dir}(\pi_{k}; \omega_0)  \right] + const \tag{7.528} \\
&amp;\propto  \left[\sum_{k=1}^K \sum_{i=1}^N (y_{ik}) \log_e {\pi_k} +  \log_e \mathcal{Dir}(\pi_{1:k}; \omega_0)  \right] + const \tag{7.529} 
\end{align}\]</span>
</p>
<p>Operate on the first term:</p>

<p><span class="math display" id="eq:equate1090510">\[\begin{align}
\left[\sum_{k=1}^K  \sum_{i=1}^N (y_{ik}) \log_e {\pi_k} \right] =
\sum_{k=1}^K  \sum_{i=1}^N (\tau_{ik}) \log_e {\pi_k} \ \leftarrow \ {\tau}_{ik} &amp; \text{  (normalized)} \tag{7.530} 
\end{align}\]</span>
</p>
<p>Operate on the second term (See Dirichlet distribution in Chapter <strong>5</strong> (<strong>Numerical Probability and Distribution</strong>) for <span class="math inline">\(\mathcal{B}(w_0)\)</span>):</p>

<p><span class="math display" id="eq:equate1090514" id="eq:equate1090513" id="eq:equate1090512" id="eq:equate1090511">\[\begin{align}
\log_e \mathcal{Dir}(\pi_{1:k}; \omega_0) {}&amp;\rightarrow 
\log_e \left[\frac{1}{\mathcal{B}(\omega_0)}\prod_{k=1}^K {\pi_k}^{\omega_0 - 1}\right] \tag{7.531} \\
&amp;= \log_e  \frac{1}{\mathcal{B}(\omega_0)} + \log_e \prod_{k=1}^K  {\pi_k}^{\omega_0 - 1} &amp; \text{(logarithm)} \tag{7.532} \\
&amp;= \log_e \prod_{k=1}^K  {\pi_k}^{\omega_0 - 1} + const   &amp; \text{(constant)} \tag{7.533} \\
&amp;= (\omega_0 - 1)\sum_{k=1}^K \log_e  {\pi_k}   + const   \tag{7.534} 
\end{align}\]</span>
</p>
<p><strong>Now</strong>, combine the two terms and solve for <span class="math inline">\(\mathcal{Q}^*_\pi(\pi)\)</span>:</p>

<p><span class="math display" id="eq:eqnnumber321" id="eq:eqnnumber320" id="eq:eqnnumber319" id="eq:eqnnumber318">\[\begin{align}
\log_e \mathcal{Q}_\pi(\pi) {}&amp;= \sum_{k=1}^K  \sum_{i=1}^N (\tau_{ik}) \log_e {\pi_k}  + (\omega_0 - 1)\sum_{k=1}^K \log_e  {\pi_k}  + const \tag{7.535}\\ 
&amp;= \sum_{k=1}^K  \left[\sum_{i=1}^N (\tau_{ik})  + (\omega_0 - 1) \right] \log_e  {\pi_k} + const \tag{7.536}\\ 
\mathcal{Q}_\pi^*(\pi)  &amp;= \sum_{k=1}^K  {\pi_k} \times exp \left[\left( \tau_{k}  + \omega_0 \right) - 1 \right] + const &amp; 
\begin{array}{rr}
\text{(exponentiate)}  \\
\text{(Dirichlet PDF)}  \\
\end{array}    \tag{7.537}
\\ 
\nonumber \\
\mathcal{Q}_\pi^*(\pi_{1:k}; \omega_1) &amp;=  \text{Dir}\left(\pi_{1:k}| \omega_1 \right) 
      \ \ \ \ \ \ \ \ \ where\ \omega_1 =  \tau_{k} + \omega_0  \tag{7.538}
\end{align}\]</span>
</p>
<p><strong>For</strong> <span class="math inline">\(\mathbf{\mathcal{Q}(\mu, \Lambda) }\)</span>, we absorb terms with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Lambda\)</span>. Here, we choose to compute both parameters via <strong>normal-Wishart distribution</strong>; albeit, one can choose to calculate the two parameters independently.</p>

<p><span class="math display" id="eq:equate1090520" id="eq:equate1090519" id="eq:equate1090518" id="eq:equate1090517" id="eq:equate1090516" id="eq:equate1090515">\[\begin{align}
\log_e {}&amp;\mathcal{Q}^*_{\mu,\Lambda}(\mu, \Lambda) \propto \mathbb{E}_{Q_{-\mu,\Lambda}}\left[\log_e P(x,z)\right] + const \tag{7.539} \\
&amp;\propto  \mathbb{E}_{Q_{y}}\left[ 
   \log_e P(x| y, \mu, \Lambda) + \log_e P(\mu,\Lambda) \right] + const \tag{7.540} \\
&amp;\propto  \left[ \sum_{k=1}^K \sum_{i=1}^N 
   (y_{ik})\log_e \mathcal{N}_p(x_i;  \mu_k, {\Lambda_k}^{-1}) \right] + \nonumber  \\
&amp;\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left[\sum_{k=1}^K \log_e\ \mathcal{NW}_p(\mu_k, \Lambda_k; \alpha_0, (\beta_0\Lambda_k)^{-1}, \nu_0, \Sigma_0) \right] + c \tag{7.541} \\
\nonumber \\
\rightarrow&amp; \text{Recalling that } \mathbb{E}_{Q_{y}}\left[y_{ik} \right] =  \tau_{ik}. \nonumber \\
&amp;\propto  \left[ \sum_{k=1}^K \sum_{i=1}^N 
   (\tau_{ik})\log_e \mathcal{N}_p(x_i;  \mu_k, {\Lambda_k}^{-1}) \right] + \nonumber \\ 
&amp;\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left[\sum_{k=1}^K \log_e\ \mathcal{NW}_p(\mu_k, \Lambda_k; \alpha_0, (\beta_0\Lambda_k)^{-1}, \nu_0, \Sigma_0) \right] + c \tag{7.542} \\
\nonumber \\
\rightarrow &amp;\text{Let } \tau_k = \sum_{i=1}^N  (\tau_{ik}) 
      \text{ and } \bar{x}_k = \frac{1}{\tau_k} \sum_{i=1}^N \left[ (\tau_{ik}) x_i\right].  \tag{7.543} \\
&amp;\propto  \left[ \sum_{k=1}^K 
   (\tau_{k}) \log_e \mathcal{N}_p(\bar{x}_k;  \mu_k, {\Lambda_k}^{-1}) \right] + \nonumber \\ 
&amp;\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left[\sum_{k=1}^K \log_e\ \mathcal{NW}_p(\mu_k, \Lambda_k; \alpha_0, (\beta_0\Lambda_k)^{-1}, \nu_0, \Sigma_0) \right] + c \tag{7.544} 
\end{align}\]</span>
</p>
<p>Operate on the first term (dropping constants):</p>

<p><span class="math display" id="eq:equate1090523" id="eq:equate1090522" id="eq:equate1090521">\[\begin{align}
\log_e \mathcal{N}&amp;(\ \bar{x}_k \ ;\ \mu_k, {\Lambda_k}^{-1}\ ) \nonumber \\
 &amp;= \log_e \left[\frac{|\Lambda_k|^{\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}} exp\left( -\frac{1}{2}(\bar{x}_k - \mu_k)^T\Lambda_k(\bar{x}_k - \mu_k)\right)\right] 
 \tag{7.545} \\ 
&amp;=  (\log_e |\Lambda_k|^{ \frac{1}{2}} ) - \frac{p}{2}(\log_e  2\pi )  -\frac{1}{2}(\bar{x} - \mu_k)^T\Lambda_k(\bar{x} - \mu_k)  &amp; \text{(logarithm)} \tag{7.546} \\
&amp;\propto  (\log_e |\Lambda_k|^{ \frac{1}{2}} )  -\frac{1}{2}\Lambda_k(\bar{x}_k \bar{x}_k^T - \bar{x}_k {\mu_k}^T - 
  \mu_k \bar{x}_k^T + \mu_k {\mu_k}^T)  &amp; \text{(logarithm)} \tag{7.547} 
\end{align}\]</span>
</p>
<p>Operate on the second term (dropping constants).</p>

<p><span class="math display" id="eq:equate1090526" id="eq:equate1090525" id="eq:equate1090524">\[\begin{align}
\log_e &amp;\mathcal{NW}_p{}(\mu_k, \Lambda_k ;\ \alpha_0, (\beta_0 \Lambda_k)^{-1}, \nu_0, \Sigma_0\ ) \nonumber \\ 
&amp;\propto log_e \left( \ \underbrace{ |\beta_0\Lambda_k|^{\frac{1}{2}} 
  exp\left[-\frac{1}{2}(\mu_k - \alpha_0)^T\beta_0\Lambda_k(\mu_k - \alpha_0)\right]}_\text{gaussian}
      \times
       \underbrace{ |\Lambda_k|^{\frac{\nu_0-p-1}{2}}  
  exp\left[-\frac{1}{2}tr(\Sigma_0^{-1}\Lambda_k)\right]  }_\text{wishart } \right)  \tag{7.548} \\
&amp;\propto  log_e \left(  |\beta_0|^\frac{1}{2} |\Lambda_k|^{\frac{1}{2}}  |\Lambda_k|^{\frac{\nu_0-p-1}{2}}  exp\left[-\frac{\beta_0}{2}\Lambda_k(\mu_k - \alpha_0)(\mu_k - \alpha_0)^T + -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda_k) \right] \right)  \tag{7.549} \\
&amp;\propto log_e \left[ |\Lambda_k|^{\frac{\nu_0-p}{2}}  exp\left[-\frac{\beta_0}{2}\Lambda_k(\mu_k\mu_k^T - \mu{\alpha_0}^T - {\alpha_0}\mu^T + \alpha_0{\alpha_0}^T) + -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda_k) \right] \right] \tag{7.550} 
\end{align}\]</span>
</p>
<p>Combine the two terms:</p>

<p><span class="math display" id="eq:equate1090527">\[\begin{align}
\log_e \mathcal{Q}^*_{\mu,\Lambda}(\mu, \Lambda) {}&amp;\propto \left[ \sum_{k=1}^K  
   \left( \tau_{k}\log_e |\Lambda_k|^{ \frac{1}{2}}  -\frac{\tau_k}{2}\Lambda_k(\bar{x}_k \bar{x}_k^T - \bar{x}_k {\mu_k}^T - \mu_k \bar{x}_k^T  + \mu_k {\mu_k}^T) \right)\right] \nonumber \\
&amp;+  \sum_{k=1}^K \left[log_e  |\Lambda_k|^{\frac{\nu_0 - p}{2}} -\frac{1}{2}\left(\beta_0 \Lambda_k(\mu_k\mu_k^T - \mu_k{\alpha_0}^T - {\alpha_0}\mu_k^T + \alpha_0{\alpha_0}^T)   -tr(\Sigma_0^{-1}\Lambda_k) \right) \right] \tag{7.551} 
\end{align}\]</span>
</p>
<p>Exponentiate:</p>

<p><span class="math display" id="eq:equate1090529" id="eq:equate1090528">\[\begin{align}
\mathcal{Q}^*_{\mu,\Lambda}(\mu, \Lambda)  
{}&amp;\propto \left[ \prod_{k=1}^K 
  \left(|\Lambda_k|^{ (\tau_{k})/2} exp \left[ -\frac{\tau_k}{2}\Lambda_k\left(\bar{x}_k \bar{x}_k^T - \bar{x}_k {\mu_k}^T - \mu_k \bar{x}_k^T  + \mu_k {\mu_k}^T\right) \right]\right)\right] \nonumber \\
&amp;\times  \prod_{k=1}^K \left[ |\Lambda_k|^{\frac{\nu_0 - p}{2}} exp \left( -\frac{1}{2}\biggl[\beta_0 \Lambda_k(\mu_k\mu_k^T - \mu_k{\alpha_0}^T - {\alpha_0}\mu_k^T + \alpha_0{\alpha_0}^T)   -tr(\Sigma_0^{-1}\Lambda_k) \biggr] \right)\right] \tag{7.552} \\
\nonumber \\
&amp;\propto \prod_{k=1}^K 
    |\Lambda_k|^{(\nu_0 +  \tau_{k} - p)/2}  exp \biggl[ -\frac{1}{2}\biggl(\Lambda_k \biggl(\Upsilon \biggr) - tr\biggl(\Sigma_0^{-1}\Lambda_k\biggr) \biggr)  \biggr] \tag{7.553} 
\end{align}\]</span>
</p>
<p><span class="math display">\[
\text{let }\Upsilon = \tau_k(\bar{x}_k \bar{x}_k^T - \bar{x}_k {\mu_k}^T - \mu_k \bar{x}_k^T  + \mu_k {\mu_k}^T) +  \beta_0(\mu_k\mu_k^T - \mu_k{\alpha_0}^T -  {\alpha_0}\mu_k^T + \alpha_0{\alpha_0}^T)
\]</span></p>
<p><strong>Now</strong>, solving for <span class="math inline">\(\mathcal{Q}^*_{\mu,\Lambda}(\mu, \Lambda)\)</span> from here, recall the simplification of the exponent under the <strong>Normal Wishart Conjugacy</strong> Section. That leads to the following:</p>

<p><span class="math display" id="eq:equate1090530">\[\begin{align}
\mathcal{Q}^*_{\mu,\Lambda}(\mu, \Lambda\ ) {}&amp;\propto   \prod_{k=1}^K  |\Lambda_k|^{(\nu_0+\tau_{k}-p)/2} \nonumber \\
&amp;\underbrace{ exp\biggl[-\frac{(\beta_0 + \tau_{k}) }{2}\biggl( \mu_k - \frac{(\alpha_0\beta_0 + \tau_{k}\bar{x}_k)}{(\beta_0 + \tau_{k})} \biggr)^T\Lambda_k\biggl( \mu_k - \frac{(\alpha_0\beta_0 + \tau_{k}\bar{x}_k)}{(\beta_0 + \tau_{k})} \biggr) \biggr]}_\text{gaussian} \times \nonumber \\
&amp;\underbrace{exp\biggl[-\frac{1}{2}tr\biggl(  \frac{ \tau_k\beta_0}{(\beta_0 + \tau_{k})}(\tau_k\bar{x}_k - \alpha_0)( \tau_k\bar{x}_k - \alpha_0 )^T + (\tau_{k}) S_k  + \Sigma_0^{-1} \biggr) \Lambda_k \biggr]}_\text{wishart} \tag{7.554} 
\end{align}\]</span>
</p>
<p>Reparameterize the prior hyperparameters:</p>

<p><span class="math display" id="eq:equate1090535" id="eq:equate1090534" id="eq:equate1090533" id="eq:equate1090532" id="eq:equate1090531">\[\begin{align}
\alpha_1 {}&amp;=  \frac{(\alpha_0\beta_0 + \tau_{k}\bar{x}_k)}{(\beta_0 + \tau_{k})} \tag{7.555} \\
\beta_1 &amp;= \beta_0 + \tau_{k}  \tag{7.556} \\
\nu_1 {}&amp;= \nu_0 + \tau_{k}  \tag{7.557} \\
\Sigma_1 &amp;=  \frac{ \tau_k \beta_0}{(\beta_0 + \tau_{k})}(\bar{x}_k - \alpha_0)( \bar{x}_k - \alpha_0 )^T + \tau_{k} S_k  + \Sigma_0^{-1}  \tag{7.558} \\
\nonumber \\
&amp;where: S_k = \frac{1}{\tau_{k}} \sum_{i=1}^n (\tau_{ik}) (x_i - \bar{x}_k)(x_i - \bar{x}_k)^T. \tag{7.559} 
\end{align}\]</span>
</p>
<p>We then get the following equation:</p>

<p><span class="math display" id="eq:equate1090536">\[\begin{align}
\mathcal{Q}^*_{\mu,\Lambda}(\mu, \Lambda) &amp;= \prod_{k=1}^K  \mathcal{N}_p(\mu_k; \alpha_1, (\beta_1\Lambda_k)^{-1})\times \mathcal{W}_p(\Lambda_k; \nu_1, \Sigma_1) &amp; \text{(Normal-Wishart PDF)} \tag{7.560} 
\end{align}\]</span>
</p>
<p><strong>Finally</strong>, our <strong>variational distribution</strong> has the following equation:</p>

<p><span class="math display" id="eq:equate1090538" id="eq:equate1090537">\[\begin{align}
\mathcal{Q}(z) {}&amp;=  \mathcal{Q}^*_y(y) \times \mathcal{Q}_\pi^*(\pi) \times  \prod_{k=1}^K  \mathcal{Q}^*_{\mu,\Lambda}(\mu, \Lambda)  \tag{7.561} \\
&amp;= \text{Multi}( {{y_{ik}}; \tau_{ik}}) \times \text{Dir}\left(\pi_{1:k}| \omega_1 \right) \times \prod_{k=1}^K  \mathcal{N}_p(\mu_k; \alpha_1, (\beta_1\Lambda_k)^{-1})\times \mathcal{W}_p(\Lambda_k; \nu_1, \Sigma_1) \tag{7.562} 
\end{align}\]</span>
</p>
<p><strong>Fifth</strong>, we need to use an <strong>iterative</strong> algorithm to optimize our model. We choose to use the <strong>Variational EM</strong> algorithm or a straight-forward <strong>Coordinate Ascent Variational Inference (CAVI)</strong>.</p>
<p>As for <strong>CAVI</strong>, we have the below algorithm <span class="citation">(Bishop C.M <a href="bibliography.html#ref-ref482c">2006</a>; Blei D. et al <a href="bibliography.html#ref-ref381d">2017</a>)</span>:</p>

<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\mathcal{LB}[Q(z)] = \mathbb{E}_{Q(z)}\left[\log_e P(x,z)\right] + \mathcal{H}(z) &amp; \text{(ELBO eq. 2)}\\
\text{while}\ \mathbf{ELBO}\ \text{has not converged}\\
\ \ \ \ \text{for j in 1,...,m}\\
\ \ \ \ \ \ \ \ \ \text{set } \log_e \mathcal{Q}_j(z_j) \propto \mathbb{E}_{Q_{z_{i \ne j}}}\left[\log_e P(x,z)\right] \\
\ \ \ \ \text{end}\\
\ \ \ \ \text{compute}\ \mathbf{ELBO}\\
\text{end}
\end{array}
\end{align*}\]</span>
</p>
<p>On the other hand, to illustrate the use of <strong>variational EM</strong> (which has a different arrangement), we first initialize the parameters.</p>
<p><strong>Initialization</strong>:</p>
<p>We start <strong>VEM</strong> by initializing the parameters for each variational factor, given the following assumptions:</p>
<ul>
<li><strong>p</strong> is the number of random variables (p-dimensions).</li>
<li><strong>n</strong> is the number of observations for each random variable.</li>
<li><strong>k</strong> is the number of classes or clusters (e.g., tri-modal mixture).</li>
</ul>
<p>For <strong>model parameters</strong>:</p>
<p><span class="math display">\[
\mathbf{x} = \left[
\begin{array}{c}
x_{1n} \\
x_{2n} \\
\end{array}
\right]_{p=2}\ \ \ \ 
\mathbf{\mu} = \left[
\begin{array}{c}
\mu_{1} = \bar{x}_1 \\
\mu_{2} = \bar{x}_2\\
\end{array}
\right]_{p=2}
\]</span></p>
<p><span class="math display">\[
\Lambda = \left(\left[
\begin{array}{cc} \sigma_{11} &amp;  \sigma_{1p}\\ \sigma_{p1} &amp;  \sigma_{pp} \end{array}
\right]_{pxp}^{-1}
\left[
\begin{array}{cc} \sigma_{11} &amp;  \sigma_{1p}\\ \sigma_{p1} &amp;  \sigma_{pp} \end{array}
\right]_{pxp}^{-1}
\left[
\begin{array}{cc} \sigma_{11} &amp;  \sigma_{1p}\\ \sigma_{p1} &amp;  \sigma_{pp} \end{array}
\right]_{pxp}^{-1}
\right)_{k=3}^T
\]</span></p>
<p>Note that <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\Lambda_k\)</span> are unknown.</p>
<p>For <strong>hyperparameters</strong>:</p>

<p><span class="math display">\[\begin{align*}
\omega_0 &amp;= \left[\begin{array}{lll}1/K &amp; 1/K &amp; 1/K \end{array}\right] &amp; \text{(hyper-proportionality of } \pi_k \text{)}\\
\alpha_0 &amp;= \left[\begin{array}{ll}0 &amp; 0 \end{array}\right] &amp; \text{(hyper-mean of } \mu_k \text{)} \\
\beta_0 &amp;=  1 &amp; \text{(hyper-variance of } \mu_k \text{)}  \\ 
\nu_0 &amp;= p -1 &amp; \text{(degrees of freedom of } \Lambda_k \text{)} \\
\Sigma_0 &amp;= 
\left[\begin{array}{cc} 1 &amp;  0\\ 0 &amp;  1\\ \end{array}\right]_{pxp}
&amp; \text{(hyper-covariance of } \Lambda_k  \text{)}\\
\end{align*}\]</span>
</p>
<p><strong>Variational Estimation Step (VE-Step)</strong>:</p>
<p>In this step, we use the initialized (and eventually the optimized) parameters to calculate expectations and <span class="math inline">\(\mathcal{Q}^*(y)\)</span>.</p>
<p>For <strong>expectation of log-determinant of Wishart covariance and log of the mixture coefficient</strong>, we can reference the following use of <strong>digamma function</strong> for an estimation. Also, let us use an asterisk to denote estimation for the following parameters, namely <span class="math inline">\(\Lambda_k^*\)</span> and <span class="math inline">\(\pi_k^*\)</span>. We reference Bishop C.M., pp.475-479 <span class="citation">(<a href="bibliography.html#ref-ref482c">2006</a>)</span> for the expectation formulas. The hyperparameters, namely <span class="math inline">\(\omega_1, \alpha_1, \beta_1, \nu_1, \Sigma_1\)</span>, are available after calculating the other factors.</p>

<p><span class="math display" id="eq:equate1090541" id="eq:equate1090540" id="eq:equate1090539">\[\begin{align}
\log_e\ \Lambda_k^* \equiv \mathbb{E}_{\Lambda}[\log_e\ |\Lambda_k|] {}&amp;= p\log_e2 + \log_e|\Sigma_1| + \sum_{i=1}^p\Psi\left(\frac{\nu_1 - i + 1}{2}\right) \tag{7.563} \\
\log_e\ \pi_k^* \equiv \mathbb{E}_{\pi}[\log_e\ \pi_k] &amp;= 
  \Psi(\omega_1) - \Psi\left( \sum_{k} \omega_1 \right) \tag{7.564} \\
\mathbb{E}_{\mu,\Lambda}\left[(x_i - \mu_k)^T\Lambda_k(x_i - \mu_k)\right] &amp;= 
  p\beta_1^{-1} + \nu_1(x_i - \alpha_1)^T\Sigma_1(x_i - \alpha_1) \tag{7.565} \\
\nonumber \\
\text{where } \Psi(.)\text{ is the } &amp;\text{digamma function.} \nonumber
\end{align}\]</span>
</p>
<p>Therefore, given the expectations, we formulate the equation below for the responsibilities:</p>

<p><span class="math display" id="eq:equate1090542">\[\begin{align}
\log_e\ \rho_{ik} =\frac{1}{2}\log_e \Lambda_k^* - \frac{p}{2}\log_e (2\pi) - \frac{1}{2}\left[p\beta_1^{-1} + \nu_1(x_i - \alpha_1)^T\Sigma_1(x_i - \alpha_1)\right] + \log_e \pi_k^* \tag{7.566} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1090543">\[\begin{align}
\rho_{ik} \propto 
   \pi_k^*|\Lambda_k^*|^{\frac{1}{2}} exp\left(-\frac{1}{2}\biggl[ p\beta_1^{-1} + \nu_1(x_i - \alpha_1)^T\Sigma_1(x_i - \alpha_1)\biggr]\right) \tag{7.567} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1090544">\[\begin{align}
 \mathbb{E}_{Q_{y}}\left[y_{ik} \right]  = \tau_{ik} = \frac{\rho_{ik}}{\sum_{j=1}^K \rho_{ij}}
 \ \ \ \ \ \ \ and \ \ \ \ \ \ \ \
 \tau_k = \sum_{i=1}^N \tau_{ik} \tag{7.568} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1090545">\[\begin{align}
\mathcal{Q}^*_y(y)  &amp;= \text{Multi}( {{y_{ik}}; \tau_{ik}}) = \prod_{i=1}^N \prod_{k=1}^K  {\tau_{ik}}^{y_{ik}} \tag{7.569} 
\end{align}\]</span>
</p>
<p><strong>Update Cluster Statistics</strong>:</p>
<p>The following statistics depend on the <span class="math inline">\(\tau_{ik}\)</span> as <strong>one-hot encoding</strong> for classification (clustering) and are structured as so:</p>
<p><span class="math display">\[
\tau_{ik} = \left[ \begin{array}{cccccccc} 
1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; ... &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; ... &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; ... &amp; 0 \\
\end{array}
\right]_{kxn}^T\ \ \ \ \ \ \ \ where\ k = 3
\]</span></p>

<p><span class="math display" id="eq:eqnnumber324" id="eq:eqnnumber323" id="eq:eqnnumber322">\[\begin{align}
\tau_k  &amp;= \sum_{i=1}^n \tau_{ik}  = 
\left[\begin{array}{c} n_1 \\ n_2 \\ n_3 \end{array}\right]_k^T
\ \ \ \ \ \ \ \ 
\begin{array}{l}
\text{ where } n_i \text{ is number of observations per cluster - }\\
\text{ the responsibilities}\\
\end{array}  \tag{7.570} \\
\bar{x}_k &amp;= \frac{1}{\tau_k} \sum_{i=1}^N \left[ (\tau_{ik}) x_i\right]  \tag{7.571}\\
S_k &amp;= \frac{1}{\tau_{k}} \sum_{i=1}^n (\tau_{ik}) (x_i - \bar{x}_k)(x_i - \bar{x}_k)^T. \tag{7.572}
\end{align}\]</span>
</p>
<p><strong>Variational Maximization Step (VM-Step)</strong>:</p>
<p>In this step, we optimize the parameters and calculate <span class="math inline">\(\mathcal{Q}^*(\pi)\)</span> and <span class="math inline">\(\mathcal{Q}^*(\mu,\Lambda)\)</span>.</p>

<p><span class="math display" id="eq:equate1090552" id="eq:equate1090551" id="eq:equate1090550" id="eq:equate1090549" id="eq:equate1090548" id="eq:equate1090547" id="eq:equate1090546">\[\begin{align}
\mathcal{Q}_\pi^*(\pi_{1:k}; \omega_1) {}&amp;=  \text{Dir}\left(\pi_{1:k}| \omega_1 \right) \tag{7.573} \\
\nonumber \\
\omega_1 &amp;=  \tau_{k} + \omega_0 \tag{7.574} \\
\nonumber \\
\mathcal{Q}^*_{\mu,\Lambda}(\mu, \Lambda) &amp;= \prod_{k=1}^K  \mathcal{N}_p(\mu_k; \alpha_1, (\beta_1\Lambda_k)^{-1})\times \mathcal{W}_p(\Lambda_k; \nu_1, \Sigma_1)  \tag{7.575} \\
\nonumber \\
\alpha_1 &amp;=  \frac{(\alpha_0\beta_0 + \tau_{k}\bar{x}_k)}{(\beta_0 + \tau_{k})} \tag{7.576} \\
\beta_1 &amp;= \beta_0 + \tau_{k}  \tag{7.577} \\
\nu_1 {}&amp;= \nu_0 + \tau_{k}  \tag{7.578} \\
\Sigma_1 &amp;=  \frac{ \tau_k \beta_0}{(\beta_0 + \tau_{k})}(\bar{x}_k - \alpha_0)( \bar{x}_k - \alpha_0 )^T + \tau_{k} S_k  + \Sigma_0^{-1}  \tag{7.579} 
\end{align}\]</span>
</p>
<p>From here, we perform iteration until <strong>ELBO</strong> converges.</p>
<p><strong>Sixth</strong>, we need to calculate <strong>ELBO eq. 2 for convergence</strong>. Recall <strong>log marginal likelihood</strong>, namely <span class="math inline">\(\log_e P(X)\)</span>, which we use for convergence as illustrated in the previous section for <strong>EM</strong>. Here, we use the derived <strong>ELBO eq. 2</strong> to evaluate convergence.</p>

<p><span class="math display" id="eq:equate1090557" id="eq:equate1090556" id="eq:equate1090555" id="eq:equate1090554" id="eq:equate1090553">\[\begin{align}
\mathcal{LB}[Q(z)]  {}&amp;= \int_z \mathcal{Q}(z)\log_e P(x,z)\ dz\ 
  -\int_z \mathcal{Q}(z)\log_e \mathcal{Q}(z) \ dz \tag{7.580} \\
 &amp;= \mathbb{E}_{Q(z)}\left[\log_e P(x,z)\right] + \mathcal{H}(z) 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{(ELBO eq. 2)} \tag{7.581} \\
 &amp;= \mathbb{E}_{Q(z)}\left[\log_e P(x,z)\right] - \mathbb{E}_{Q(z)}\left[\log_e \mathcal{Q}(z)\right] \tag{7.582} \\
 &amp;= \mathbb{E}_{Q(z)}\left[\log_e P(x,y,\pi,\mu,\Lambda)\right] - \mathbb{E}_{Q(z)}\left[\log_e \mathcal{Q}(y,\pi,\mu,\Lambda)\right] \tag{7.583} \\
 &amp;= \mathbb{E}\left[\log_e P(x|y,\mu,\Lambda)\right] +
 \mathbb{E}\left[\log_e P(y,\pi)\right] +
 \mathbb{E}\left[\log_e P(\pi)\right] +
 \mathbb{E}\left[\log_e P(\mu,\Lambda)\right] \nonumber \\
 &amp;\ \ \ \ \ - \mathbb{E}\left[\log_e \mathcal{Q}(y)\right]
      - \mathbb{E}\left[\log_e \mathcal{Q}(\pi)\right]
      - \mathbb{E}\left[\log_e \mathcal{Q}(\mu,\Lambda)\right] \tag{7.584} 
\end{align}\]</span>
</p>
<p>We then have to calculate the individual <strong>expectations</strong>. We leave readers to derive the individual terms of the <strong>lower bound</strong> as exercise. For reference, see Bishop C.M., section 10.2.2 <span class="citation">(<a href="bibliography.html#ref-ref482c">2006</a>)</span>.</p>
<p>Let us go through the process with an example implementation of <strong>Variational Bayes</strong> in R code using the <strong>Variational EM</strong> algorithm.</p>
<p><strong>First</strong>, let us generate our dataset (bivariate trimodal) like so:</p>

<div class="sourceCode" id="cb871"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb871-1" data-line-number="1">K =<span class="st"> </span><span class="dv">3</span>  <span class="co"># number of clusters (tri-modal)</span></a>
<a class="sourceLine" id="cb871-2" data-line-number="2">P =<span class="st"> </span><span class="dv">2</span>  <span class="co"># number of random variables (p-variate)</span></a>
<a class="sourceLine" id="cb871-3" data-line-number="3">N =<span class="st"> </span><span class="dv">60</span> <span class="co"># number of observations per random variable</span></a>
<a class="sourceLine" id="cb871-4" data-line-number="4">ksample &lt;-<span class="st"> </span><span class="cf">function</span>(m, mu, sd, seed) {</a>
<a class="sourceLine" id="cb871-5" data-line-number="5">  <span class="kw">set.seed</span>(seed);  <span class="kw">rnorm</span>(<span class="dt">n=</span>m, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd)</a>
<a class="sourceLine" id="cb871-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb871-7" data-line-number="7">dataset &lt;-<span class="st"> </span><span class="cf">function</span>() {</a>
<a class="sourceLine" id="cb871-8" data-line-number="8">  <span class="co"># simulate bivariate tri-modal mixture model (cluster: A, B, C)</span></a>
<a class="sourceLine" id="cb871-9" data-line-number="9">  <span class="co">#set.seed(2020)</span></a>
<a class="sourceLine" id="cb871-10" data-line-number="10">  mu =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>)    <span class="co"># assume true mean of all three clusters</span></a>
<a class="sourceLine" id="cb871-11" data-line-number="11">  sd =<span class="st"> </span><span class="kw">c</span>(<span class="fl">2.0</span>, <span class="fl">1.5</span>, <span class="fl">1.5</span>) <span class="co"># assume true std dev of all three clusters</span></a>
<a class="sourceLine" id="cb871-12" data-line-number="12">  m =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb871-13" data-line-number="13">  A.x1  =<span class="st"> </span><span class="kw">ksample</span>(m, mu[<span class="dv">1</span>], sd[<span class="dv">1</span>], <span class="dv">150</span> ) </a>
<a class="sourceLine" id="cb871-14" data-line-number="14">  A.x2  =<span class="st"> </span><span class="kw">ksample</span>(m, mu[<span class="dv">1</span>], sd[<span class="dv">1</span>], <span class="dv">180</span> )</a>
<a class="sourceLine" id="cb871-15" data-line-number="15">  B.x1  =<span class="st"> </span><span class="kw">ksample</span>(m, mu[<span class="dv">2</span>], sd[<span class="dv">2</span>], <span class="dv">160</span> ) </a>
<a class="sourceLine" id="cb871-16" data-line-number="16">  B.x2  =<span class="st"> </span><span class="kw">ksample</span>(m, mu[<span class="dv">2</span>], sd[<span class="dv">2</span>], <span class="dv">190</span> )</a>
<a class="sourceLine" id="cb871-17" data-line-number="17">  C.x1  =<span class="st"> </span><span class="kw">ksample</span>(m, mu[<span class="dv">3</span>], sd[<span class="dv">3</span>], <span class="dv">170</span> ) </a>
<a class="sourceLine" id="cb871-18" data-line-number="18">  C.x2  =<span class="st"> </span><span class="kw">ksample</span>(m, mu[<span class="dv">3</span>], sd[<span class="dv">3</span>], <span class="dv">200</span> )</a>
<a class="sourceLine" id="cb871-19" data-line-number="19">  x1 =<span class="st"> </span><span class="kw">c</span>(A.x1 , B.x1 , C.x1)</a>
<a class="sourceLine" id="cb871-20" data-line-number="20">  x2 =<span class="st"> </span><span class="kw">c</span>(A.x2 , B.x2 , C.x2)</a>
<a class="sourceLine" id="cb871-21" data-line-number="21">  x =<span class="st"> </span><span class="kw">cbind</span>(x1, x2)</a>
<a class="sourceLine" id="cb871-22" data-line-number="22">  <span class="kw">list</span>(<span class="st">&quot;m&quot;</span> =<span class="st"> </span>m, <span class="st">&quot;x&quot;</span> =<span class="st"> </span>x )</a>
<a class="sourceLine" id="cb871-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb871-24" data-line-number="24">data =<span class="st"> </span><span class="kw">dataset</span>()</a>
<a class="sourceLine" id="cb871-25" data-line-number="25"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">40</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">40</span>), </a>
<a class="sourceLine" id="cb871-26" data-line-number="26">     <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>,</a>
<a class="sourceLine" id="cb871-27" data-line-number="27">     <span class="dt">main=</span><span class="st">&quot;Bivariate tri-modal mixture model&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb871-28" data-line-number="28"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb871-29" data-line-number="29"><span class="kw">points</span>(data<span class="op">$</span>x[,<span class="dv">1</span>], data<span class="op">$</span>x[,<span class="dv">2</span>], <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>), <span class="dt">pch=</span><span class="dv">16</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bivartrimodel1"></span>
<img src="DS_files/figure-html/bivartrimodel1-1.png" alt="Bivariate tri-modal mixture model" width="70%" />
<p class="caption">
Figure 7.21: Bivariate tri-modal mixture model
</p>
</div>

<p>Figure <a href="7.6-bayesianinference.html#fig:bivartrimodel1">7.21</a> shows the data points that are intentionally colored with black and distributed in three clusters with centroid points (10,10), (20,20), and (30,30). The goal is to see if we can correctly identify the cluster that each data point belongs to by assigning colors to each.</p>
<p><strong>Second</strong>, let us perform initialization.</p>

<div class="sourceCode" id="cb872"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb872-1" data-line-number="1">initialize &lt;-<span class="st"> </span><span class="cf">function</span>(data) {</a>
<a class="sourceLine" id="cb872-2" data-line-number="2">  omega =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span>K, K)               <span class="co"># dirichlet hyper-parameter</span></a>
<a class="sourceLine" id="cb872-3" data-line-number="3">  alpha =<span class="st"> </span><span class="kw">kmeans</span>(data<span class="op">$</span>x, K)<span class="op">$</span>centers <span class="co"># mean approximation using k-means</span></a>
<a class="sourceLine" id="cb872-4" data-line-number="4">  beta =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, K)                  <span class="co"># variance  hyper-parameter</span></a>
<a class="sourceLine" id="cb872-5" data-line-number="5">  v    =<span class="st"> </span><span class="kw">rep</span>(P,K)                   <span class="co"># degrees of freedom hyperparameter</span></a>
<a class="sourceLine" id="cb872-6" data-line-number="6">  sigma =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(P,P,K)) </a>
<a class="sourceLine" id="cb872-7" data-line-number="7">  sigma[,,] =<span class="st"> </span><span class="kw">rep</span>(<span class="kw">diag</span>(P),K)        <span class="co"># covariance hyperparameter</span></a>
<a class="sourceLine" id="cb872-8" data-line-number="8"></a>
<a class="sourceLine" id="cb872-9" data-line-number="9">  loge_pi     =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, K)</a>
<a class="sourceLine" id="cb872-10" data-line-number="10">  loge_lambda =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, K)</a>
<a class="sourceLine" id="cb872-11" data-line-number="11">  E_mulambda =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, N, K)</a>
<a class="sourceLine" id="cb872-12" data-line-number="12">  </a>
<a class="sourceLine" id="cb872-13" data-line-number="13">  <span class="kw">list</span>( <span class="st">&quot;omega&quot;</span> =<span class="st"> </span>omega, <span class="st">&quot;alpha&quot;</span>=<span class="st"> </span>alpha, <span class="st">&quot;beta&quot;</span> =<span class="st"> </span>beta, </a>
<a class="sourceLine" id="cb872-14" data-line-number="14">        <span class="st">&quot;v&quot;</span> =<span class="st"> </span>v, <span class="st">&quot;sigma&quot;</span> =<span class="st"> </span>sigma, </a>
<a class="sourceLine" id="cb872-15" data-line-number="15">        <span class="st">&quot;loge_pi&quot;</span>=loge_pi, <span class="st">&quot;loge_lambda&quot;</span> =<span class="st"> </span>loge_lambda,</a>
<a class="sourceLine" id="cb872-16" data-line-number="16">        <span class="st">&quot;E_mulambda&quot;</span> =<span class="st"> </span>E_mulambda)  </a>
<a class="sourceLine" id="cb872-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb872-18" data-line-number="18">params =<span class="st"> </span><span class="kw">initialize</span>(data)</a></code></pre></div>

<p><strong>Third</strong>, let us implement <strong>variational EM</strong>. Note that the VEM implementation references an R code published in public by Jean Arreola <span class="citation">(<a href="bibliography.html#ref-ref546j">2018</a>)</span>. We made a few re-arrangement and slight modifications to reflect corresponding notations in the discussion above. Additionally, our implementation is motivated by an R code from Fabian Dablander <span class="citation">(<a href="bibliography.html#ref-ref453b">2018</a>)</span>). As always, our implementation and the referenced implementations are examples only that should be used only to supplement our understanding, and thus they may not be production-proof):</p>

<div class="sourceCode" id="cb873"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb873-1" data-line-number="1">ln &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">log</span>(x, <span class="kw">exp</span>(<span class="dv">1</span>))} <span class="co"># exp(1) = 2.718282</span></a>
<a class="sourceLine" id="cb873-2" data-line-number="2">one_hot_encoding &lt;-<span class="st"> </span><span class="cf">function</span>(n) { <span class="co"># using log-sum-exp</span></a>
<a class="sourceLine" id="cb873-3" data-line-number="3">  <span class="kw">t</span>( <span class="kw">apply</span>(n, <span class="dv">1</span>, <span class="cf">function</span>(x) { offset =<span class="st"> </span><span class="kw">max</span>(x); y =<span class="st"> </span>x <span class="op">-</span><span class="st"> </span>offset</a>
<a class="sourceLine" id="cb873-4" data-line-number="4">    <span class="kw">return</span>  ( <span class="kw">exp</span>(y)<span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">exp</span>(y) ))</a>
<a class="sourceLine" id="cb873-5" data-line-number="5">  }) )</a>
<a class="sourceLine" id="cb873-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb873-7" data-line-number="7">update_xmean &lt;-<span class="st"> </span><span class="cf">function</span>(x, rik, rk) {</a>
<a class="sourceLine" id="cb873-8" data-line-number="8">  xm =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, K, P)</a>
<a class="sourceLine" id="cb873-9" data-line-number="9">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) { xm[k,] =<span class="st"> </span><span class="kw">colSums</span>(rik[,k] <span class="op">*</span><span class="st"> </span>x <span class="op">/</span><span class="st"> </span>rk[k]) }</a>
<a class="sourceLine" id="cb873-10" data-line-number="10">  xm</a>
<a class="sourceLine" id="cb873-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb873-12" data-line-number="12">update_S &lt;-<span class="st"> </span><span class="cf">function</span>(x, rik, rk, xm) {</a>
<a class="sourceLine" id="cb873-13" data-line-number="13">  <span class="co"># Update covariance</span></a>
<a class="sourceLine" id="cb873-14" data-line-number="14">  S =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(P,P,K))</a>
<a class="sourceLine" id="cb873-15" data-line-number="15">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb873-16" data-line-number="16">    sum_sk =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb873-17" data-line-number="17">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb873-18" data-line-number="18">      sum_sk =<span class="st"> </span>sum_sk <span class="op">+</span><span class="st"> </span>rik[i,k] <span class="op">*</span><span class="st"> </span>(x[i,] <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb873-19" data-line-number="19"><span class="st">                                  </span>xm[k,]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x[i,] <span class="op">-</span><span class="st"> </span>xm[k,])</a>
<a class="sourceLine" id="cb873-20" data-line-number="20">    }</a>
<a class="sourceLine" id="cb873-21" data-line-number="21">    S[,,k] =<span class="st"> </span>sum_sk <span class="op">/</span><span class="st"> </span>rk[k]</a>
<a class="sourceLine" id="cb873-22" data-line-number="22">  }</a>
<a class="sourceLine" id="cb873-23" data-line-number="23">  S</a>
<a class="sourceLine" id="cb873-24" data-line-number="24">}</a>
<a class="sourceLine" id="cb873-25" data-line-number="25">VEM &lt;-<span class="st"> </span><span class="cf">function</span>(x, params) {</a>
<a class="sourceLine" id="cb873-26" data-line-number="26">  <span class="co"># consider k = 3 ( tri-modal )</span></a>
<a class="sourceLine" id="cb873-27" data-line-number="27">  omega<span class="fl">.0</span> =<span class="st"> </span><span class="dv">1</span><span class="op">/</span>K          <span class="co"># mixing weight hyperparameter</span></a>
<a class="sourceLine" id="cb873-28" data-line-number="28">  alpha<span class="fl">.0</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, P)    <span class="co"># mean hyperparameter</span></a>
<a class="sourceLine" id="cb873-29" data-line-number="29">  beta<span class="fl">.0</span>  =<span class="st"> </span><span class="dv">1</span>            <span class="co"># variance hyperparameter</span></a>
<a class="sourceLine" id="cb873-30" data-line-number="30">  v<span class="fl">.0</span>     =<span class="st"> </span>P            <span class="co"># degrees of freedom hyperparameter</span></a>
<a class="sourceLine" id="cb873-31" data-line-number="31">  sigma<span class="fl">.0</span> =<span class="st"> </span><span class="kw">diag</span>(P)      <span class="co"># lambda - covariance hyperparameter</span></a>
<a class="sourceLine" id="cb873-32" data-line-number="32">  <span class="co"># hyperparameters</span></a>
<a class="sourceLine" id="cb873-33" data-line-number="33">  omega      =<span class="st"> </span>params<span class="op">$</span>omega</a>
<a class="sourceLine" id="cb873-34" data-line-number="34">  alpha      =<span class="st"> </span>params<span class="op">$</span>alpha</a>
<a class="sourceLine" id="cb873-35" data-line-number="35">  beta       =<span class="st"> </span>params<span class="op">$</span>beta  </a>
<a class="sourceLine" id="cb873-36" data-line-number="36">  v          =<span class="st"> </span>params<span class="op">$</span>v     </a>
<a class="sourceLine" id="cb873-37" data-line-number="37">  sigma     =<span class="st"> </span>params<span class="op">$</span>sigma </a>
<a class="sourceLine" id="cb873-38" data-line-number="38">  <span class="co"># expectation estimations</span></a>
<a class="sourceLine" id="cb873-39" data-line-number="39">  loge_pi     =<span class="st"> </span>params<span class="op">$</span>loge_pi</a>
<a class="sourceLine" id="cb873-40" data-line-number="40">  loge_lambda =<span class="st"> </span>params<span class="op">$</span>loge_lambda</a>
<a class="sourceLine" id="cb873-41" data-line-number="41">  E_mulambda  =<span class="st"> </span>params<span class="op">$</span>E_mulambda</a>
<a class="sourceLine" id="cb873-42" data-line-number="42">  pik         =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, N, K)</a>
<a class="sourceLine" id="cb873-43" data-line-number="43">  <span class="co">############### Variational E-Step ###########################</span></a>
<a class="sourceLine" id="cb873-44" data-line-number="44">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb873-45" data-line-number="45">    loge_lambda[k] =<span class="st"> </span><span class="dv">0</span>  </a>
<a class="sourceLine" id="cb873-46" data-line-number="46">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>P) {</a>
<a class="sourceLine" id="cb873-47" data-line-number="47">      loge_lambda[k] =<span class="st"> </span>loge_lambda[k] <span class="op">+</span><span class="st"> </span><span class="kw">digamma</span>( (v[k] <span class="op">-</span><span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb873-48" data-line-number="48">    }</a>
<a class="sourceLine" id="cb873-49" data-line-number="49">    loge_lambda[k] =<span class="st"> </span>P <span class="op">*</span><span class="st"> </span><span class="kw">ln</span>(<span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ln</span>(<span class="kw">det</span>(sigma[,,k])) <span class="op">+</span><span class="st"> </span>loge_lambda[k]</a>
<a class="sourceLine" id="cb873-50" data-line-number="50">    loge_pi[k] =<span class="st"> </span><span class="kw">digamma</span>(omega[k]) <span class="op">-</span><span class="st"> </span><span class="kw">digamma</span>(<span class="kw">sum</span>(omega))</a>
<a class="sourceLine" id="cb873-51" data-line-number="51">    </a>
<a class="sourceLine" id="cb873-52" data-line-number="52">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb873-53" data-line-number="53">      E_mulambda[i,k] =<span class="st"> </span>(P <span class="op">/</span><span class="st"> </span>beta[k]) <span class="op">+</span></a>
<a class="sourceLine" id="cb873-54" data-line-number="54"><span class="st">         </span>v[k] <span class="op">*</span><span class="st"> </span><span class="kw">t</span>(x[i,] <span class="op">-</span><span class="st"> </span>alpha[k,]) <span class="op">%*%</span><span class="st"> </span>sigma[,,k] <span class="op">%*%</span></a>
<a class="sourceLine" id="cb873-55" data-line-number="55"><span class="st">                          </span>(x[i,] <span class="op">-</span><span class="st"> </span>alpha[k,])</a>
<a class="sourceLine" id="cb873-56" data-line-number="56">      </a>
<a class="sourceLine" id="cb873-57" data-line-number="57">      pik[i,k] =<span class="st"> </span>loge_pi[k]  <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>loge_lambda[k] <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb873-58" data-line-number="58"><span class="st">                 </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>E_mulambda[i,k] <span class="op">-</span><span class="st"> </span>(P<span class="op">/</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">ln</span>(<span class="dv">2</span><span class="op">*</span>pi)  </a>
<a class="sourceLine" id="cb873-59" data-line-number="59">    }</a>
<a class="sourceLine" id="cb873-60" data-line-number="60">  }</a>
<a class="sourceLine" id="cb873-61" data-line-number="61">  <span class="co">############### Update Cluster Statistics #####################</span></a>
<a class="sourceLine" id="cb873-62" data-line-number="62">  rik =<span class="st"> </span><span class="kw">one_hot_encoding</span>(pik)</a>
<a class="sourceLine" id="cb873-63" data-line-number="63">  rk =<span class="st"> </span><span class="kw">apply</span>(rik, <span class="dv">2</span>, sum) <span class="co"># capture no of obsv per cluster</span></a>
<a class="sourceLine" id="cb873-64" data-line-number="64">  xm =<span class="st"> </span><span class="kw">update_xmean</span>(x, rik, rk)</a>
<a class="sourceLine" id="cb873-65" data-line-number="65">  S  =<span class="st"> </span><span class="kw">update_S</span>(x, rik, rk, xm)</a>
<a class="sourceLine" id="cb873-66" data-line-number="66">  <span class="co">############### Variational M-Step ###########################</span></a>
<a class="sourceLine" id="cb873-67" data-line-number="67">  <span class="co"># Update hyperparameters</span></a>
<a class="sourceLine" id="cb873-68" data-line-number="68">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb873-69" data-line-number="69">    <span class="co"># beta1</span></a>
<a class="sourceLine" id="cb873-70" data-line-number="70">    beta[k] =<span class="st"> </span>beta<span class="fl">.0</span> <span class="op">+</span><span class="st"> </span>rk[k]</a>
<a class="sourceLine" id="cb873-71" data-line-number="71">    <span class="co"># alpha1</span></a>
<a class="sourceLine" id="cb873-72" data-line-number="72">    alpha[k,] =<span class="st">  </span>(alpha<span class="fl">.0</span> <span class="op">*</span><span class="st"> </span>beta<span class="fl">.0</span> <span class="op">+</span><span class="st"> </span>rk[k] <span class="op">*</span><span class="st"> </span>xm[k,]) <span class="op">/</span><span class="st"> </span>beta[k]</a>
<a class="sourceLine" id="cb873-73" data-line-number="73">    <span class="co"># v1</span></a>
<a class="sourceLine" id="cb873-74" data-line-number="74">    v[k] =<span class="st"> </span>v<span class="fl">.0</span> <span class="op">+</span><span class="st"> </span>rk[k]</a>
<a class="sourceLine" id="cb873-75" data-line-number="75">    <span class="co"># sigma1</span></a>
<a class="sourceLine" id="cb873-76" data-line-number="76">    sigma[,,k] =<span class="st">  </span>((beta<span class="fl">.0</span> <span class="op">*</span><span class="st"> </span>rk[k]) <span class="op">/</span><span class="st"> </span>beta[k ]) <span class="op">*</span></a>
<a class="sourceLine" id="cb873-77" data-line-number="77"><span class="st">                   </span>(xm[k,] <span class="op">-</span><span class="st"> </span>alpha<span class="fl">.0</span>) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(xm[k,] <span class="op">-</span><span class="st"> </span>alpha<span class="fl">.0</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb873-78" data-line-number="78"><span class="st">                   </span><span class="op">+</span><span class="st"> </span>rk[k] <span class="op">*</span><span class="st"> </span>S[,,k] <span class="op">+</span><span class="st">  </span><span class="kw">solve</span>(sigma<span class="fl">.0</span>)</a>
<a class="sourceLine" id="cb873-79" data-line-number="79">  }</a>
<a class="sourceLine" id="cb873-80" data-line-number="80">  <span class="co"># hyperparameters</span></a>
<a class="sourceLine" id="cb873-81" data-line-number="81">  params<span class="op">$</span>omega       =<span class="st"> </span>omega</a>
<a class="sourceLine" id="cb873-82" data-line-number="82">  params<span class="op">$</span>alpha        =<span class="st"> </span>alpha</a>
<a class="sourceLine" id="cb873-83" data-line-number="83">  params<span class="op">$</span>beta        =<span class="st"> </span>beta</a>
<a class="sourceLine" id="cb873-84" data-line-number="84">  params<span class="op">$</span>v           =<span class="st"> </span>v</a>
<a class="sourceLine" id="cb873-85" data-line-number="85">  params<span class="op">$</span>sigma    =<span class="st"> </span>sigma</a>
<a class="sourceLine" id="cb873-86" data-line-number="86">  <span class="co"># expectation estimations</span></a>
<a class="sourceLine" id="cb873-87" data-line-number="87">  params<span class="op">$</span>loge_pi     =<span class="st"> </span>loge_pi</a>
<a class="sourceLine" id="cb873-88" data-line-number="88">  params<span class="op">$</span>loge_lambda =<span class="st"> </span>loge_lambda</a>
<a class="sourceLine" id="cb873-89" data-line-number="89">  params<span class="op">$</span>E_mulambda  =<span class="st"> </span>E_mulambda</a>
<a class="sourceLine" id="cb873-90" data-line-number="90">  <span class="kw">list</span>( <span class="st">&quot;params&quot;</span> =<span class="st"> </span>params, <span class="st">&quot;rik&quot;</span>=rik, <span class="st">&quot;rk&quot;</span>=rk,  <span class="st">&quot;S&quot;</span> =<span class="st"> </span>S, <span class="st">&quot;xm&quot;</span> =<span class="st"> </span>xm)</a>
<a class="sourceLine" id="cb873-91" data-line-number="91">}</a></code></pre></div>



<p><strong>Finally</strong>, we step through the iteration until <strong>ELBO</strong> converges.</p>

<div class="sourceCode" id="cb874"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb874-1" data-line-number="1">tol =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb874-2" data-line-number="2">limit =<span class="st">  </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb874-3" data-line-number="3">old_elbo =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb874-4" data-line-number="4">params =<span class="st"> </span><span class="kw">initialize</span>(data)</a>
<a class="sourceLine" id="cb874-5" data-line-number="5"><span class="cf">for</span> (iterate <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb874-6" data-line-number="6">  vem =<span class="st"> </span><span class="kw">VEM</span>(data<span class="op">$</span>x, params)</a>
<a class="sourceLine" id="cb874-7" data-line-number="7">  elbo =<span class="st"> </span><span class="kw">with</span>(vem, <span class="kw">ELBO</span>(params, rik, rk, S, xm))</a>
<a class="sourceLine" id="cb874-8" data-line-number="8">  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.nan</span>(elbo)) {</a>
<a class="sourceLine" id="cb874-9" data-line-number="9">    err =<span class="st"> </span><span class="kw">abs</span>(elbo <span class="op">-</span><span class="st"> </span>old_elbo)</a>
<a class="sourceLine" id="cb874-10" data-line-number="10">    <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb874-11" data-line-number="11">  }</a>
<a class="sourceLine" id="cb874-12" data-line-number="12">  old_elbo =<span class="st"> </span>elbo</a>
<a class="sourceLine" id="cb874-13" data-line-number="13">  params =<span class="st"> </span>vem<span class="op">$</span>params</a>
<a class="sourceLine" id="cb874-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb874-15" data-line-number="15"><span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Number of Iterations : &quot;</span>, iterate))</a></code></pre></div>
<pre><code>## [1] &quot;Number of Iterations : 3&quot;</code></pre>

<p>And now we plot the data points with the assigned colors (see Figure <a href="7.6-bayesianinference.html#fig:bivartrimodel2">7.22</a>).</p>

<div class="sourceCode" id="cb876"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb876-1" data-line-number="1">color =<span class="st"> </span><span class="kw">apply</span>(vem<span class="op">$</span>rik, <span class="dv">1</span>, which.max) <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb876-2" data-line-number="2"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">40</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">40</span>), </a>
<a class="sourceLine" id="cb876-3" data-line-number="3">     <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>,</a>
<a class="sourceLine" id="cb876-4" data-line-number="4">     <span class="dt">main=</span><span class="st">&quot;Bivariate tri-modal mixture model&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb876-5" data-line-number="5"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb876-6" data-line-number="6"><span class="kw">points</span>(data<span class="op">$</span>x[,<span class="dv">1</span>], data<span class="op">$</span>x[,<span class="dv">2</span>], <span class="dt">col=</span>color, <span class="dt">pch=</span><span class="dv">16</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bivartrimodel2"></span>
<img src="DS_files/figure-html/bivartrimodel2-1.png" alt="Approximate Bivariate tri-modal mixture model" width="70%" />
<p class="caption">
Figure 7.22: Approximate Bivariate tri-modal mixture model
</p>
</div>


</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="7.5-information-theory.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="8-bayesian2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
