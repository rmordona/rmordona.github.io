<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.5 Information Theory  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="3.5 Information Theory  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.5 Information Theory  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="conjugacy.html"/>
<link rel="next" href="bayesianinference.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i><b>0.2</b> Caveat</a></li>
<li class="chapter" data-level="0.3" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i><b>0.3</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.4" data-path="notation.html"><a href="notation.html"><i class="fa fa-check"></i><b>0.4</b> Notation</a></li>
<li class="chapter" data-level="0.5" data-path="number-system.html"><a href="number-system.html"><i class="fa fa-check"></i><b>0.5</b> Number System</a></li>
<li class="chapter" data-level="0.6" data-path="implementation.html"><a href="implementation.html"><i class="fa fa-check"></i><b>0.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numericalprobability.html"><a href="numericalprobability.html"><i class="fa fa-check"></i><b>1</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="1.1" data-path="approximation-based-on-random-chances.html"><a href="approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>1.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="1.2" data-path="distribution.html"><a href="distribution.html"><i class="fa fa-check"></i><b>1.2</b> Distribution</a></li>
<li class="chapter" data-level="1.3" data-path="mass-and-density.html"><a href="mass-and-density.html"><i class="fa fa-check"></i><b>1.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="1.4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1.4</b> Probability  </a></li>
<li class="chapter" data-level="1.5" data-path="probability-density-function-pdf.html"><a href="probability-density-function-pdf.html"><i class="fa fa-check"></i><b>1.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="1.6" data-path="probability-mass-function-pmf.html"><a href="probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>1.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="1.7" data-path="cumulative-distribution-function-cdf.html"><a href="cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>1.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="1.8" data-path="special-functions.html"><a href="special-functions.html"><i class="fa fa-check"></i><b>1.8</b> Special Functions</a><ul>
<li class="chapter" data-level="1.8.1" data-path="special-functions.html"><a href="special-functions.html#gamma-function"><i class="fa fa-check"></i><b>1.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="1.8.2" data-path="special-functions.html"><a href="special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>1.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="1.8.3" data-path="special-functions.html"><a href="special-functions.html#digamma-function"><i class="fa fa-check"></i><b>1.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="1.8.4" data-path="special-functions.html"><a href="special-functions.html#beta-function"><i class="fa fa-check"></i><b>1.8.4</b> Beta function </a></li>
<li class="chapter" data-level="1.8.5" data-path="special-functions.html"><a href="special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>1.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="1.8.6" data-path="special-functions.html"><a href="special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>1.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="1.8.7" data-path="special-functions.html"><a href="special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>1.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="1.8.8" data-path="special-functions.html"><a href="special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>1.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="1.8.9" data-path="special-functions.html"><a href="special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>1.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="1.8.10" data-path="special-functions.html"><a href="special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>1.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="distributiontypes.html"><a href="distributiontypes.html"><i class="fa fa-check"></i><b>1.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="1.9.1" data-path="distributiontypes.html"><a href="distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>1.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="1.9.2" data-path="distributiontypes.html"><a href="distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>1.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="1.9.3" data-path="distributiontypes.html"><a href="distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>1.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="1.9.4" data-path="distributiontypes.html"><a href="distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>1.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="1.9.5" data-path="distributiontypes.html"><a href="distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>1.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="1.9.6" data-path="distributiontypes.html"><a href="distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="1.9.7" data-path="distributiontypes.html"><a href="distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>1.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="1.9.8" data-path="distributiontypes.html"><a href="distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>1.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="1.9.9" data-path="distributiontypes.html"><a href="distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>1.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="1.9.10" data-path="distributiontypes.html"><a href="distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>1.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="1.9.11" data-path="distributiontypes.html"><a href="distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>1.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="1.9.12" data-path="distributiontypes.html"><a href="distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>1.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="1.9.13" data-path="distributiontypes.html"><a href="distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>1.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="1.9.14" data-path="distributiontypes.html"><a href="distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>1.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="1.9.15" data-path="distributiontypes.html"><a href="distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>1.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="1.9.16" data-path="distributiontypes.html"><a href="distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>1.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="1.9.17" data-path="distributiontypes.html"><a href="distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>1.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="1.9.18" data-path="distributiontypes.html"><a href="distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>1.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="1.9.19" data-path="distributiontypes.html"><a href="distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>1.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="1.9.20" data-path="distributiontypes.html"><a href="distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>1.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="1.9.21" data-path="distributiontypes.html"><a href="distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>1.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="1.9.22" data-path="distributiontypes.html"><a href="distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>1.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="1.9.23" data-path="distributiontypes.html"><a href="distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>1.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="1.9.24" data-path="distributiontypes.html"><a href="distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>1.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>2</b> Statistical Computation</a><ul>
<li class="chapter" data-level="2.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html"><i class="fa fa-check"></i><b>2.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>2.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="2.1.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>2.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="2.1.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>2.1.3</b> Variability </a></li>
<li class="chapter" data-level="2.1.4" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>2.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="2.1.5" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>2.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="inferential-statistics.html"><a href="inferential-statistics.html"><i class="fa fa-check"></i><b>2.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="2.3" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html"><i class="fa fa-check"></i><b>2.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="2.3.1" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>2.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>2.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="2.3.3" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>2.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="2.3.4" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>2.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="2.3.5" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>2.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="2.3.6" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>2.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="2.3.7" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>2.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="2.3.8" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>2.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="2.3.9" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>2.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="2.3.10" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>2.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="post-hoc-analysis.html"><a href="post-hoc-analysis.html"><i class="fa fa-check"></i><b>2.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="2.4.1" data-path="post-hoc-analysis.html"><a href="post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>2.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="2.4.2" data-path="post-hoc-analysis.html"><a href="post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>2.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html"><i class="fa fa-check"></i><b>2.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="2.5.1" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>2.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="2.5.2" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>2.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="2.5.3" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>2.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="2.5.4" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>2.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="2.5.5" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>2.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="2.5.6" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>2.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="2.5.7" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>2.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="2.5.8" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>2.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistical-modeling.html"><a href="statistical-modeling.html"><i class="fa fa-check"></i><b>2.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="2.6.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>2.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="2.6.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>2.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="2.6.3" data-path="statistical-modeling.html"><a href="statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>2.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="2.6.4" data-path="statistical-modeling.html"><a href="statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>2.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="regression-analysis.html"><a href="regression-analysis.html"><i class="fa fa-check"></i><b>2.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="2.7.1" data-path="regression-analysis.html"><a href="regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>2.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="2.7.2" data-path="regression-analysis.html"><a href="regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>2.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="2.7.3" data-path="regression-analysis.html"><a href="regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>2.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="2.7.4" data-path="regression-analysis.html"><a href="regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>2.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="2.7.5" data-path="regression-analysis.html"><a href="regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>2.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="2.7.6" data-path="regression-analysis.html"><a href="regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>2.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="2.7.7" data-path="regression-analysis.html"><a href="regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>2.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html"><i class="fa fa-check"></i><b>2.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="2.8.1" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>2.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="2.8.3" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>2.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="2.8.4" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>2.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="2.8.5" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>2.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="2.8.6" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>2.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="2.8.7" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>2.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="2.8.8" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>2.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="2.8.9" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>2.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="inference-for-regression.html"><a href="inference-for-regression.html"><i class="fa fa-check"></i><b>2.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="2.9.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>2.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="2.9.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>2.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="2.9.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>2.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>2.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>3</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="3.1" data-path="probability-1.html"><a href="probability-1.html"><i class="fa fa-check"></i><b>3.1</b> Probability </a><ul>
<li class="chapter" data-level="3.1.1" data-path="probability-1.html"><a href="probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>3.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="3.1.2" data-path="probability-1.html"><a href="probability-1.html#joint-probability"><i class="fa fa-check"></i><b>3.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="3.1.3" data-path="probability-1.html"><a href="probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>3.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="3.1.4" data-path="probability-1.html"><a href="probability-1.html#negation-probability"><i class="fa fa-check"></i><b>3.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="3.1.5" data-path="probability-1.html"><a href="probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>3.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="probability-rules.html"><a href="probability-rules.html"><i class="fa fa-check"></i><b>3.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="3.2.1" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>3.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>3.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>3.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="3.2.4" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>3.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="3.2.5" data-path="probability-rules.html"><a href="probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>3.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="3.2.6" data-path="probability-rules.html"><a href="probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>3.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="3.2.7" data-path="probability-rules.html"><a href="probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>3.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="3.2.8" data-path="probability-rules.html"><a href="probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>3.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="3.2.9" data-path="probability-rules.html"><a href="probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>3.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bayes-theorem.html"><a href="bayes-theorem.html"><i class="fa fa-check"></i><b>3.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="3.3.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>3.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="3.3.2" data-path="bayes-theorem.html"><a href="bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>3.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="3.3.3" data-path="bayes-theorem.html"><a href="bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>3.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="3.3.4" data-path="bayes-theorem.html"><a href="bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>3.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="conjugacy.html"><a href="conjugacy.html"><i class="fa fa-check"></i><b>3.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="3.4.1" data-path="conjugacy.html"><a href="conjugacy.html#precision"><i class="fa fa-check"></i><b>3.4.1</b> Precision </a></li>
<li class="chapter" data-level="3.4.2" data-path="conjugacy.html"><a href="conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>3.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="3.4.3" data-path="conjugacy.html"><a href="conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>3.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="3.4.4" data-path="conjugacy.html"><a href="conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>3.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="3.4.5" data-path="conjugacy.html"><a href="conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>3.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="3.4.6" data-path="conjugacy.html"><a href="conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>3.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="3.4.7" data-path="conjugacy.html"><a href="conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>3.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="3.4.8" data-path="conjugacy.html"><a href="conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>3.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="3.4.9" data-path="conjugacy.html"><a href="conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>3.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="3.4.10" data-path="conjugacy.html"><a href="conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>3.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="3.4.11" data-path="conjugacy.html"><a href="conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>3.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="3.4.12" data-path="conjugacy.html"><a href="conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>3.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="3.4.13" data-path="conjugacy.html"><a href="conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>3.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="3.4.14" data-path="conjugacy.html"><a href="conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>3.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="information-theory.html"><a href="information-theory.html"><i class="fa fa-check"></i><b>3.5</b> Information Theory </a><ul>
<li class="chapter" data-level="3.5.1" data-path="information-theory.html"><a href="information-theory.html#information"><i class="fa fa-check"></i><b>3.5.1</b> Information </a></li>
<li class="chapter" data-level="3.5.2" data-path="information-theory.html"><a href="information-theory.html#entropy"><i class="fa fa-check"></i><b>3.5.2</b> Entropy </a></li>
<li class="chapter" data-level="3.5.3" data-path="information-theory.html"><a href="information-theory.html#gini-index"><i class="fa fa-check"></i><b>3.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="3.5.4" data-path="information-theory.html"><a href="information-theory.html#information-gain"><i class="fa fa-check"></i><b>3.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="3.5.5" data-path="information-theory.html"><a href="information-theory.html#mutual-information"><i class="fa fa-check"></i><b>3.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="3.5.6" data-path="information-theory.html"><a href="information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>3.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="3.5.7" data-path="information-theory.html"><a href="information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>3.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="bayesianinference.html"><a href="bayesianinference.html"><i class="fa fa-check"></i><b>3.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="3.6.1" data-path="bayesianinference.html"><a href="bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>3.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="3.6.2" data-path="bayesianinference.html"><a href="bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>3.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="3.6.3" data-path="bayesianinference.html"><a href="bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>3.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="3.6.4" data-path="bayesianinference.html"><a href="bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>3.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="3.6.5" data-path="bayesianinference.html"><a href="bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>3.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bayesian2.html"><a href="bayesian2.html"><i class="fa fa-check"></i><b>4</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="4.1" data-path="bayesian-models.html"><a href="bayesian-models.html"><i class="fa fa-check"></i><b>4.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="4.1.1" data-path="bayesian-models.html"><a href="bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>4.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="4.1.2" data-path="bayesian-models.html"><a href="bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>4.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="4.1.3" data-path="bayesian-models.html"><a href="bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>4.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="4.1.4" data-path="bayesian-models.html"><a href="bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>4.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="4.1.5" data-path="bayesian-models.html"><a href="bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>4.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="4.1.6" data-path="bayesian-models.html"><a href="bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>4.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="4.1.7" data-path="bayesian-models.html"><a href="bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>4.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="4.1.8" data-path="bayesian-models.html"><a href="bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>4.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="4.1.9" data-path="bayesian-models.html"><a href="bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>4.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="4.1.10" data-path="bayesian-models.html"><a href="bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>4.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="4.1.11" data-path="bayesian-models.html"><a href="bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>4.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html"><i class="fa fa-check"></i><b>4.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="4.2.1" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>4.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="4.2.2" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="4.2.3" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>4.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="4.2.4" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>4.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="4.2.5" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>4.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="4.2.6" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>4.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="4.2.7" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>4.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="4.2.8" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>4.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="4.2.9" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>4.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html"><i class="fa fa-check"></i><b>4.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>4.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="4.3.2" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>4.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="4.3.3" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>4.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="4.3.4" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>4.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>5</b> Computational Learning I</a><ul>
<li class="chapter" data-level="5.1" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html"><i class="fa fa-check"></i><b>5.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="5.1.1" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>5.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="5.1.2" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>5.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="5.1.3" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>5.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="5.1.4" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>5.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="input-data.html"><a href="input-data.html"><i class="fa fa-check"></i><b>5.2</b> Input Data</a><ul>
<li class="chapter" data-level="5.2.1" data-path="input-data.html"><a href="input-data.html#structured-data"><i class="fa fa-check"></i><b>5.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="5.2.2" data-path="input-data.html"><a href="input-data.html#non-structured-data"><i class="fa fa-check"></i><b>5.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="5.2.3" data-path="input-data.html"><a href="input-data.html#statistical-data"><i class="fa fa-check"></i><b>5.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="5.2.4" data-path="input-data.html"><a href="input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>5.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="5.2.5" data-path="input-data.html"><a href="input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>5.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="5.2.6" data-path="input-data.html"><a href="input-data.html#data-lake"><i class="fa fa-check"></i><b>5.2.6</b> Data lake</a></li>
<li class="chapter" data-level="5.2.7" data-path="input-data.html"><a href="input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>5.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="5.2.8" data-path="input-data.html"><a href="input-data.html#multimedia-md"><i class="fa fa-check"></i><b>5.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="primitive-methods.html"><a href="primitive-methods.html"><i class="fa fa-check"></i><b>5.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="5.3.1" data-path="primitive-methods.html"><a href="primitive-methods.html#weighting"><i class="fa fa-check"></i><b>5.3.1</b> Weighting</a></li>
<li class="chapter" data-level="5.3.2" data-path="primitive-methods.html"><a href="primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>5.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="5.3.3" data-path="primitive-methods.html"><a href="primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>5.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="5.3.4" data-path="primitive-methods.html"><a href="primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>5.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="5.3.5" data-path="primitive-methods.html"><a href="primitive-methods.html#centering"><i class="fa fa-check"></i><b>5.3.5</b> Centering </a></li>
<li class="chapter" data-level="5.3.6" data-path="primitive-methods.html"><a href="primitive-methods.html#scaling"><i class="fa fa-check"></i><b>5.3.6</b> Scaling </a></li>
<li class="chapter" data-level="5.3.7" data-path="primitive-methods.html"><a href="primitive-methods.html#transforming"><i class="fa fa-check"></i><b>5.3.7</b> Transforming</a></li>
<li class="chapter" data-level="5.3.8" data-path="primitive-methods.html"><a href="primitive-methods.html#clipping"><i class="fa fa-check"></i><b>5.3.8</b> Clipping </a></li>
<li class="chapter" data-level="5.3.9" data-path="primitive-methods.html"><a href="primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>5.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="distance-metrics.html"><a href="distance-metrics.html"><i class="fa fa-check"></i><b>5.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="5.4.1" data-path="distance-metrics.html"><a href="distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>5.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="5.4.2" data-path="distance-metrics.html"><a href="distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>5.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="5.4.3" data-path="distance-metrics.html"><a href="distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>5.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="5.4.4" data-path="distance-metrics.html"><a href="distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>5.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="5.4.5" data-path="distance-metrics.html"><a href="distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>5.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="5.4.6" data-path="distance-metrics.html"><a href="distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>5.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="5.4.7" data-path="distance-metrics.html"><a href="distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>5.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="5.4.8" data-path="distance-metrics.html"><a href="distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>5.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>5.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="5.5.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>5.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="5.5.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>5.5.2</b> Association</a></li>
<li class="chapter" data-level="5.5.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>5.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="5.5.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>5.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="5.5.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>5.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="5.5.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>5.5.6</b> Covariance </a></li>
<li class="chapter" data-level="5.5.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>5.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="5.5.8" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>5.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="5.5.9" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>5.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="5.5.10" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>5.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="5.5.11" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>5.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="5.5.12" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>5.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="5.5.13" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>5.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="5.5.14" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>5.5.14</b> Discretization </a></li>
<li class="chapter" data-level="5.5.15" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>5.5.15</b> Stratification </a></li>
<li class="chapter" data-level="5.5.16" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>5.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="5.5.17" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>5.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="featureengineering.html"><a href="featureengineering.html"><i class="fa fa-check"></i><b>5.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="5.6.1" data-path="featureengineering.html"><a href="featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>5.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="5.6.2" data-path="featureengineering.html"><a href="featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>5.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="5.6.3" data-path="featureengineering.html"><a href="featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>5.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="5.6.4" data-path="featureengineering.html"><a href="featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>5.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="5.6.5" data-path="featureengineering.html"><a href="featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>5.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="5.6.6" data-path="featureengineering.html"><a href="featureengineering.html#featureselection"><i class="fa fa-check"></i><b>5.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="5.6.7" data-path="featureengineering.html"><a href="featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>5.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="5.6.8" data-path="featureengineering.html"><a href="featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>5.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="general-modeling.html"><a href="general-modeling.html"><i class="fa fa-check"></i><b>5.7</b> General Modeling</a><ul>
<li class="chapter" data-level="5.7.1" data-path="general-modeling.html"><a href="general-modeling.html#training-learning"><i class="fa fa-check"></i><b>5.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="5.7.2" data-path="general-modeling.html"><a href="general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>5.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="5.7.3" data-path="general-modeling.html"><a href="general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>5.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="5.7.4" data-path="general-modeling.html"><a href="general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>5.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="5.7.5" data-path="general-modeling.html"><a href="general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>5.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="5.7.6" data-path="general-modeling.html"><a href="general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>5.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="5.7.7" data-path="general-modeling.html"><a href="general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>5.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="5.7.8" data-path="general-modeling.html"><a href="general-modeling.html#regularization"><i class="fa fa-check"></i><b>5.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="supervised-vs.unsupervised-learning.html"><a href="supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>5.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="5.9" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>5.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>6</b> Appendix</a><ul>
<li class="chapter" data-level="6.1" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i><b>6.1</b> Appendix A</a><ul>
<li class="chapter" data-level="6.1.1" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i><b>6.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="6.1.2" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i><b>6.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="6.1.3" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i><b>6.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i><b>6.2</b> Appendix B</a><ul>
<li class="chapter" data-level="6.2.1" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i><b>6.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="6.2.2" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i><b>6.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="6.2.3" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>6.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="6.2.4" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>6.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="6.2.5" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>6.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="6.2.6" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>6.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="6.2.7" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>6.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i><b>6.3</b> Appendix C</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="information-theory" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.5</span> Information Theory <a href="information-theory.html#information-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we introduce <strong>Information Theory</strong>. Some concepts are helpful when we cover <strong>Variational Bayes</strong>, especially around quantifying information. There are cases in which we need to transform our data set, and it is natural for us to compare the original (actual) data set and the transformed data set. To do that, we use measures such as <strong>Entropy</strong>, <strong>Information gain</strong>, <strong>Mutual Information</strong>, <strong>Gini Index</strong>, <strong>Kullback-Leibler divergence</strong>, and many others, which are metrics used to quantify information, mainly optimized by a <strong>cost function</strong> commonly denoted as <span class="math inline">\(\mathcal{J}(\theta)\)</span> in machine learning. Here, we reference the great works of Cover T.M. <span class="citation">(<a href="bibliography.html#ref-ref1039t">2006</a>)</span> and Ebrahimi N. et al. <span class="citation">(<a href="bibliography.html#ref-ref1029n">2010</a>)</span>.</p>
<p>For most of the discussions in this section, we settle only on <strong>Gaussian</strong> and <strong>Binomial</strong> processes; albeit, we do not restrict ourselves to only those distributions. Other familiar distributions should apply. Equivalently, <strong>continuous</strong> random variables do apply as well.</p>
<div id="information" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.5.1</span> Information <a href="information-theory.html#information" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, <strong>information</strong> is based on quantifying the uncertainty of random events. Rare events tend to be more uncertain, and when they happen, they become more <strong>surprising</strong> (more <strong>impure</strong>). Such rare events require additional information.</p>
<p>The amount of information to measure is called <strong>Shannon information</strong> and can be expressed as such:</p>
<p><span class="math display">\[\begin{align}
\mathcal{I}(X) = - \log_e P(X)
\end{align}\]</span></p>
</div>
<div id="entropy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.5.2</span> Entropy <a href="information-theory.html#entropy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We describe <strong>Entropy</strong>, also called <strong>Shannon Entropy</strong>, in terms of the degree of randomness, uncertainty, impurity, or disorder based on the amount of information. <strong>Entropy</strong> is also discussed in <strong>Physics</strong> and <strong>Thermodynamics</strong>. It measures the amount of information required to eliminate the degree of randomness or uncertainty (see Figure ). </p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:entropy"></span>
<img src="entropy.png" alt="Entropy" width="60%" />
<p class="caption">
Figure 3.9: Entropy
</p>
</div>

<p><strong>Entropy</strong> is denoted by the symbol <span class="math inline">\(\mathcal{H}(X)\)</span> and is expressed as:</p>
<p><span class="math display">\[\begin{align}
\mathcal{H}(X) = 
\begin{cases}
- \sum_x   P_X(x)\ \log_e  P_X(x) &amp; \text{(discrete entropy)}\\
\\
- \int_x   P_X(x)\ \log_e  P_X(x) dx  &amp; \text{(continuous differential entropy)}\\
\end{cases} \label{eqn:eqnnumber313}
\end{align}\]</span></p>
<p>Entropy (<span class="math inline">\(\mathcal{H}\)</span>) measures the level of impurity (or surprise) of the probability of an outcome. If the expected information of a random event is always 100% to the point of perfection (or purity), then it becomes unsurprising. The <strong>entropy</strong> is therefore zero. For example, if we have a coin with a head on both sides, no matter how many times we flip the coin, there is always a 100% probability that it lands on a head. Thus, there are zero surprises right there.</p>
<p><span class="math display">\[\begin{align}
\mathcal{H}(X) = -P(x) log P(x) = -1 \times \log_e (1) = 0,\ \ \ \ \ \ where\ P(x) = 100\%
\end{align}\]</span></p>
<p>Now, let us assume that we have four coins. As strange as it may sound, let us suppose three coins have heads on both sides, and the last coin has tails on both sides. Let us then compute the entropy of the set.</p>
<p><span class="math display">\[\begin{align}
\mathcal{H}(X) &amp;= - P(x_1) \log_e P(x_1) - P(x_2) \log_e P(x_2)  \\
&amp;= - \frac{3}{4} \log_e \left(\frac{3}{4} \right) - \frac{1}{4} \log_e \left( \frac{1}{4} \right)  \nonumber \\
&amp;= - (-0.2157616) - (-0.3465736 ) \nonumber \\
&amp;= 0.5623352 \nonumber
\end{align}\]</span></p>
<p>Notice that the higher the <strong>entropy</strong>, the more we see some information content. This measurement is proper when we split a dataset into corresponding features, similar to the techniques in decision trees in <strong>machine learning</strong>. Another contending measurement is the <strong>Gini Index</strong> which we discuss in the following subsection. </p>
<p>Now, let us consider <strong>Joint Entropy</strong>, which has the following equation: </p>
<p><span class="math display">\[\begin{align}
\mathcal{H}(X, Y) =  - \sum_{x \in X} \sum_{y \in Y} P(x,y) \log_e P(x,y)
\end{align}\]</span></p>
<p><strong>Joint Entropy</strong> computes the entropy of all possible pairs of two events. For example, if <span class="math inline">\(X \in \{A,B\}\)</span> and <span class="math inline">\(Y \in \{C,D\}\)</span>, and we have the following probabilities of each pair of combination: </p>
<p><span class="math display">\[\begin{align*}
P(X=A,Y=C) {}&amp;= 0.30\ \ \ \ \ \ \
P(X=A,Y=D) = 0.30\ \ \ \ \ \ \\
P(X=B,Y=C) &amp;= 0.20\ \ \ \ \ \ \
P(X=B,Y=D) = 0.20
\end{align*}\]</span>
then we can compute for the <strong>Joint Entropy</strong>:</p>
<p><span class="math display">\[\begin{align*}
\mathcal{H}(X, Y) {}&amp;= - \left[\ 0.30 \log_e (0.30) + 0.30 \log_e (0.30) + 0.20 \log_e (0.20) + 0.20 \log_e (0.20)\ \right]\\
&amp;= 1.366159
\end{align*}\]</span></p>
<p><strong>Conditional Entropy</strong> computes the entropy of one possible event given another event which has the following equation: </p>
<p><span class="math display">\[\begin{align}
\mathcal{H}(X|Y) = - \sum_{x \in X} P(x) \sum_{y \in Y} P(y|x) \log_e P(y|x)
\end{align}\]</span></p>
<p><strong>Cross-Entropy</strong> is another concept for measuring the difference between two distributions: one being the actual distribution, namely <span class="math inline">\(P(x)\)</span>, and the other being a training (approximating) distribution, namely <span class="math inline">\(\mathcal{Q}(x)\)</span>. It has the following equation:</p>
<p><span class="math display">\[\begin{align}
\mathcal{H}_Q(P) \equiv \mathcal{H}(P,Q) = 
\begin{cases}
- \sum_x   P_X(x)\ \log_e  \mathcal{Q}_X(x) &amp; \text{(discrete)}\\
\\
- \int_x   P_X(x)\ \log_e  \mathcal{Q}_X(x) dx  &amp; \text{(continuous)}\\
\end{cases} \label{eqn:eqnnumber314}
\end{align}\]</span></p>
<p>A <strong>cross-entropy</strong> value of zero means that the two distributions are almost identical. The higher the value, the farther away the two distributions are alike. In a later section, we discuss <strong>KL divergence</strong>, which is another measurement of <strong>closeness</strong> between two distributions such that we can express the divergence this way (given an actual distribution (<strong>P</strong>) and an estimated distribution (<strong>Q</strong>)):  </p>
<p><span class="math display">\[\begin{align}
\text{KL Divergence} = \underbrace{H(P,Q)}_{\text{cross entropy}} - \underbrace{H(P)}_{\text{entropy}}
\end{align}\]</span></p>
<p>Note that <strong>cross-entropy</strong> is used in <strong>machine learning</strong> as a loss function, generally replacing the common notation <span class="math inline">\(\mathcal{J}(\theta)\)</span> with <span class="math inline">\(\mathcal{H}_Q(P)\)</span>.</p>
<p>Note that the notation can be confused with a <strong>joint entropy</strong> notation. In our case, we use <span class="math inline">\(\mathcal{H}_Q(P)\)</span> to refer to <strong>cross-entropy</strong>.</p>
</div>
<div id="gini-index" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.5.3</span> Gini Index <a href="information-theory.html#gini-index" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Gini Index</strong>, also called <strong>Gini Impurity</strong>, is denoted by the symbol <span class="math inline">\(\mathcal{G}(X)\)</span> and is expressed as:</p>
<p><span class="math display">\[\begin{align}
\mathcal{G}(X)  = \underbrace{1 - \underbrace{\sum_x P_X(x)^2}_{\text{Gini}}}_{\text{Gini Impurity}}
\end{align}\]</span></p>
<p><strong>Gini Index</strong> (<span class="math inline">\(\mathcal{G}\)</span>) can be used to measure the impurity of information similar to <strong>Entropy</strong>. To illustrate, we use the same example we used for <strong>Entropy</strong>. Suppose we have four coins. The three coins have heads on both sides, and the last coin has tails on both sides. Compute for the Gini index of the set.</p>
<p><span class="math display">\[\begin{align}
\mathcal{G} {}&amp;= 1 - ( P(x_1)^2 + P(x_2)^2  \\
&amp;= 1 - \left[ \left(\frac{3}{4}\right)^2 + \left(\frac{1}{4}\right)^2  \right] \nonumber \\
&amp;= 1 - (0.5625 + 0.0625 ) = 1 - 0.625 \nonumber \\
&amp;= 0.375 \nonumber
\end{align}\]</span></p>
<p>Similarly, if we have a coin with one side head and another side also head, no matter how many times we flip the coin, there is always a 100% probability that it lands on the head. Thus, there are zero surprises right there.</p>
<p><span class="math display">\[\begin{align}
\mathcal{G} = 1- P(x)^2  = -1 \times (1)^2 = 0,\ \ \ \ \ \ where\ P(x) = 100\%
\end{align}\]</span></p>
<p>Therefore, similar to <strong>Entropy</strong>, the higher the <strong>Gini index</strong>, the more we see some information content.</p>
</div>
<div id="information-gain" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.5.4</span> Information Gain <a href="information-theory.html#information-gain" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Information Gain</strong> is denoted by the symbol <span class="math inline">\(\mathcal{I}(T,X)\)</span> and is expressed like so:</p>
<p>for <strong>Entropy</strong>:</p>
<p><span class="math display">\[\begin{align}
\mathcal{I}_{entropy}(T, X) {}&amp;= \mathcal{H}(parent) - \binom{weighted}{average} \mathcal{H}(children) \\
&amp;= \mathcal{H}(T) - \mathcal{H}(T,X)\ \ \ \ \ \ where\ \sum_x \mathcal{H}(x)
\end{align}\]</span></p>
<p>for <strong>Gini</strong>:</p>
<p><span class="math display">\[\begin{align}
\mathcal{I}_{gini}(T, X) {}&amp;= \mathcal{G}(parent) - \binom{weighted}{average} \mathcal{G}(children) \\
&amp;= \mathcal{G}(T) - \mathcal{G}(T,X)\ \ \ \ \ \ where\ \sum_x \mathcal{G}(x)
\end{align}\]</span></p>
<p><strong>Information Gain</strong> (<span class="math inline">\(\mathcal{I}\)</span>) measures the quality of <strong>split</strong>. So that if there are 15 red balls and five green balls in an urn and we split those 20 balls into two groups such that the 1st group has eight red balls and three green balls and the 2nd group has seven red balls and two green balls, then the information gain of the split is computed as such:</p>
<p><span class="math display">\[\begin{align*}
\mathcal{H}(T) {}&amp;= -\frac{15}{20}\ \log_e \left( \frac{15}{20} \right) - \frac{5}{20}\ \log_e \left( \frac{5}{20} \right) = 0.5623352 \\
\mathcal{H}(x_1) &amp;= -\frac{8}{11}\ \log_e \left( \frac{8}{11} \right) - \frac{3}{11}\ \log_e \left( \frac{3}{11} \right)=  0.5859526 \\
\mathcal{H}(x_2) &amp;= -\frac{7}{9}\ \log_e \left( \frac{7}{9} \right) - \frac{2}{9}\ \log_e \left( \frac{2}{9} \right) =  0.5297062 \\
\mathcal{H}(T, X) &amp;= \frac{11}{20}\ \times 0.5859526  + \frac{9}{20}\ 0.5297062 = 0.5606417\\
\\
\mathcal{I} &amp;= 0.5623352 - 0.5606417 = 0.0016935
\end{align*}\]</span></p>
<p>Note that the higher the Information Gain, the better the split. An Information Gain of zero means that the split is worst. For Information Gain using Gini, see <strong>Multi-Classification</strong> section in Chapter <strong>10</strong> (<strong>Computation Learning II</strong>).</p>
</div>
<div id="mutual-information" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.5.5</span> Mutual Information <a href="information-theory.html#mutual-information" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Mutual Information</strong> is denoted by the symbol <span class="math inline">\(\mathcal{I}(X; Y)\)</span> and is expressed as:</p>
<p><span class="math display">\[\begin{align}
\mathcal{I}(X; Y) = \sum_{x \in X} \sum_{y \in Y} P(x,y) \log_e \frac{P(x,y)}{P(x)P(y)}
\end{align}\]</span></p>
<p>To illustrate, suppose we have two coins (X and Y), and we toss them five times, choosing any of the two coins randomly so that we end up with the following data: X = (H, H, T), Y = ( T, H ). We then compute for the probabilities:</p>
<p><span class="math display">\[\begin{align}
\begin{array}{rrrr}
P(X) = \frac{3}{5} &amp; P(Y) = \frac{2}{5} &amp; P(H) = \frac{3}{5} &amp; P(T) = \frac{2}{5}\\
P(X,H) = \frac{2}{5} &amp; P(X,T) = \frac{1}{5} &amp; P(Y,H) = \frac{1}{5} &amp; P(Y,T) = \frac{1}{5}\\
\end{array} \label{eqn:eqnnumber315}
\end{align}\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[\begin{align}
\mathcal{I}(X; Y) {}&amp;= 
P(X, H) \log_e \frac{P(X, H)}{P(X)P(H)} + 
P(X, T) \log_e \frac{P(X, T)}{P(X)P(T)} \nonumber \\ 
&amp;+P(Y, H) \log_e \frac{P(Y, H)}{P(Y)P(H)}  + 
P(Y, T) \log_e \frac{P(Y, T)}{P(Y)P(T)} \\
&amp;= 0.40\ \log_e \frac{0.40}{0.60 \times 0.60} + 0.20\ \log_e \frac{0.20}{0.60 \times 0.40 } \nonumber \\
&amp;+ 0.20\ \log_e \frac{0.20}{0.40 \times 0.60}  + 0.20\ \log_e \frac{0.20}{0.40 \times 0.40}\nonumber \\
&amp;= 0.01384429 \nonumber
\end{align}\]</span></p>
<p>Below is a list of a few important properties of <strong>Mutual Information</strong> in relation to <strong>entropy</strong>.</p>
<p><span class="math display">\[\begin{align}
\mathcal{I}(X; Y) {}&amp;= \mathcal{H}(X) - \mathcal{H}(X|Y)\\
\mathcal{I}(X; Y) &amp;= \mathcal{H}(Y) - \mathcal{H}(Y|X)\\
\mathcal{I}(X; Y) &amp;= \mathcal{H}(X) + \mathcal{H}(Y) - \mathcal{H}(X,Y)
\end{align}\]</span></p>
</div>
<div id="kullback-leibler-divergence" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.5.6</span> Kullback-Leibler Divergence  <a href="information-theory.html#kullback-leibler-divergence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Kullback-Leibler (KL) Divergence</strong>, also called <strong>Relative Entropy</strong>, measures the <strong>closeness</strong> of an approximating distribution <span class="math inline">\(\mathcal{Q}(X)\)</span> to a true distribution <span class="math inline">\(P(X)\)</span>. <strong>KL divergence</strong> is intimately related to <strong>Cross-Entropy</strong> as we continue to deal with a true distribution <span class="math inline">\(P(X)\)</span> along with a new approximating distribution denoted as <span class="math inline">\(\mathcal{Q}(X)\)</span>. Note that the distribution Q(X) is also called <strong>reference</strong> distribution, <strong>approximate</strong> distribution, <strong>training</strong> distribution, etc.</p>
<p>The <strong>KL divergence</strong> equation comes either in the form of <strong>Forward KL divergence</strong>, also called <strong>mean-seeking</strong>, <strong>zero-avoiding</strong> method; or in the form of <strong>Reverse KL divergence</strong>, also called <strong>mode-seeking</strong>, <strong>zero-forcing</strong> method. See Figure .</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kldivergence"></span>
<img src="kldivergence.png" alt="KL Divergence" width="80%" />
<p class="caption">
Figure 3.10: KL Divergence
</p>
</div>

<p>There are two components (factors) of the <strong>KL divergence</strong> equation: the weighting function, namely <span class="math inline">\(w(x)\)</span>, and the penalty function, namely <span class="math inline">\(g(x)\)</span>. The <strong>penalty function</strong> is interpreted as a <strong>log-likelihood ratio</strong>, which measures the ratio of how likely an approximating distribution <span class="math inline">\(Q(X)\)</span> describes a true distribution <span class="math inline">\(P(X)\)</span> and is expressed as:</p>
<p><span class="math display">\[\begin{align}
\log_e \mathcal{LR} \approx \int_x \log_e \left(\frac{P(X)}{\mathcal{Q}(X)}\right)
\end{align}\]</span></p>
<p>The <strong>Forward KL divergence</strong> has the following formula in which the penalty function measures the likelihood ratio of the actual distribution <span class="math inline">\(P(X)\)</span> over <span class="math inline">\(\mathcal{Q}(X)\)</span>. It is written as:</p>
<p><span class="math display">\[\begin{align}
\mathcal{D}_{KL}(P || Q) \equiv \mathcal{KL}(\ P\ ||\ Q\ ) = \sum_{x} 
 \underbrace{P(x)}_\text{weight}\ 
 \underbrace{ \log_e \left[ \frac{P(x)}{\mathcal{Q}(x)}\right] }_\text{penalty} = 
- \sum_{x} 
 \underbrace{P(x)}_\text{weight}\ 
 \underbrace{ \log_e \left[ \frac{\mathcal{Q}(x)}{P(x)}\right] }_\text{penalty}
\end{align}\]</span></p>
<p>For unimodal, the <strong>Forward KL divergence</strong> moves the Q in the direction towards P for measuring the closeness of the approximating distribution to the actual distribution.</p>
<p>For multimodal, the divergence seeks to settle on the average (the mean) of an actual multimodal distribution. The entropy term allows the approximating distribution to control the spread, encouraging it to have broader coverage across the actual multimodal distribution; hence, this divergence is <strong>inclusive</strong>. Below is the <strong>continuous</strong> version of the <strong>forward KL divergence</strong> split into two terms: the relative entropy and the cross-entropy.</p>
<p><span class="math display">\[\begin{align}
\mathcal{KL}(\ P\ ||\ Q\ ) {}&amp;= \int_x P(x)\left(\log_e P(x) - \log_e \mathcal{Q}(x)\right) dx\\
&amp;=\underbrace{\int_x P(x) \log_e P(x) dx}_\text{relative entropy}
\underbrace{ - \int_x P(x) \log_e \mathcal{Q}(x) dx }_\text{cross-entropy}
\end{align}\]</span></p>
<p>On the other hand, the <strong>Reverse KL divergence</strong> has the following formula in which the penalty function measures the likelihood ratio of the approximating distribution <span class="math inline">\(\mathcal{Q}(X)\)</span> over <span class="math inline">\(P(X)\)</span>.</p>
<p><span class="math display">\[\begin{align}
\mathcal{D}_{KL}(Q || P) \equiv \mathcal{KL}(\ Q\ ||\ P\ ) = \sum_{x} 
 \underbrace{\mathcal{Q}(x)}_\text{weight}\ 
 \underbrace{ \log_e \left[ \frac{\mathcal{Q}(x)}{P(x)}\right] }_\text{penalty}
\end{align}\]</span></p>
<p>Here, the <strong>penalty</strong> term has higher influence to the divergence if P(x) &gt; 0.</p>
<p>The divergence seeks to settle on the mode (the most common value) of an actual multimodal distribution. Therefore, in a mixture distribution, it may prefer one with the higher probability - and a mode that has a higher scale (e.g., variance) gets to be one in which the <strong>approximating distribution</strong> may tend to follow; hence, the divergence is <strong>exclusive</strong>.</p>
<p>Also, note that <strong>KL divergence</strong> is non-symmetric; meaning, that <span class="math inline">\(\mathcal{KL}(\ P\ ||\ Q\ ) \ne \mathcal{KL}(\ Q\ ||\ P\ )\)</span>. Additionally, it has the following properties called the <strong>Gibbâs Inequality</strong>:</p>
<p><span class="math display">\[\begin{align}
\mathcal{KL}(P||Q) \ge 0\ \ and\ \ \mathcal{KL}(P||P) = 0
\end{align}\]</span></p>
<p>Below is an example implementation of <strong>Forward KL-divergence</strong> in R code:</p>

<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1">KL.divergence &lt;-<span class="st"> </span><span class="cf">function</span>(X, mu1, sd1, mu2, sd2) {</a>
<a class="sourceLine" id="cb42-2" data-line-number="2">  ln &lt;-<span class="st"> </span><span class="cf">function</span>(n) { <span class="kw">log</span>(n, <span class="kw">exp</span>(<span class="dv">1</span>)) } <span class="co"># exp(1) = 2.718282</span></a>
<a class="sourceLine" id="cb42-3" data-line-number="3">  P =<span class="st"> </span><span class="kw">dnorm</span>(<span class="dt">x =</span> X, <span class="dt">mean=</span>mu1, <span class="dt">sd=</span>sd1)</a>
<a class="sourceLine" id="cb42-4" data-line-number="4">  Q =<span class="st"> </span><span class="kw">dnorm</span>(<span class="dt">x =</span> X, <span class="dt">mean=</span>mu2, <span class="dt">sd=</span>sd2)</a>
<a class="sourceLine" id="cb42-5" data-line-number="5">  <span class="kw">sum</span>( P <span class="op">*</span><span class="st"> </span><span class="kw">ln</span>( P <span class="op">/</span><span class="st"> </span>Q) )</a>
<a class="sourceLine" id="cb42-6" data-line-number="6">}</a></code></pre></div>

<p>To illustrate, let us solve for the <strong>KL divergence</strong> by generating two distributions (P and R) whose means <span class="math inline">\(\mu\)</span> are ten apart. Note that we are not explaining mean-seeking and mode-seeking in this illustration; instead, how an approximating distribution estimates the mean parameter of an actual distribution.</p>
<p><span class="math display">\[\begin{align}
P\mid\mu, \sigma^2 \sim N(0, 1)\ \ \ \ \ \ \ \ \ R\mid\mu, \sigma^2 \sim N(10, 1.2)
\end{align}\]</span></p>
<p>We simulate the sampling distributions like so:</p>

<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb43-2" data-line-number="2">P =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb43-3" data-line-number="3">R =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">mean=</span><span class="dv">10</span>, <span class="dt">sd =</span> <span class="fl">1.2</span>)</a></code></pre></div>

<p>Let us show the two true distributions (P, R) in a graph - note that both distributions are independent and are not components of a bimodal mixture distribution. See Figure .</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kldivergence1"></span>
<img src="DS_files/figure-html/kldivergence1-1.png" alt="Model Distribution (P and R)" width="70%" />
<p class="caption">
Figure 3.11: Model Distribution (P and R)
</p>
</div>

<p>The goal is to approximate a distribution given a fixed variance by estimating the mean. In other words, we have an unknown mean, and we need to find the proper values of the corresponding parameters (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>) that best characterize one of the distributions in the figure (P or R). Let us call this approximating (moving) distribution as Q.</p>
<p>Let us plot the <strong>KL divergence</strong> between an approximating Q and a true P and between the same Q and another true R.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kldivergence2"></span>
<img src="DS_files/figure-html/kldivergence2-1.png" alt="Model Distribution (P and R)" width="70%" />
<p class="caption">
Figure 3.12: Model Distribution (P and R)
</p>
</div>

<p>In Figure , as the approximating distribution Q moves in the direction towards P (<span class="math inline">\(\mu\)</span> = 0, <span class="math inline">\(\sigma=1\)</span>), the <strong>KL divergence</strong> becomes zero as it gets a <span class="math inline">\(\mu=0\)</span> which completely matches the P distribution.</p>

<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" data-line-number="1">P.mu =<span class="st"> </span><span class="dv">0</span>;  P.sd =<span class="st"> </span><span class="dv">1</span>; Q.mu =<span class="st"> </span><span class="dv">1</span>; Q.sd =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb44-2" data-line-number="2">Q.sample =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">range</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>), <span class="dt">size=</span><span class="dv">20</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb44-3" data-line-number="3"><span class="kw">round</span>( <span class="kw">KL.divergence</span>(Q.sample, <span class="dt">mu1=</span>P.mu, <span class="dt">sd1=</span>P.sd, <span class="dt">mu2=</span>Q.mu, <span class="dt">sd2=</span>Q.sd), </a>
<a class="sourceLine" id="cb44-4" data-line-number="4">       <span class="fl">1e-10</span>)</a></code></pre></div>
<pre><code>## [1] 0</code></pre>

<p>Similarly, as Q moves towards R with (<span class="math inline">\(\mu\)</span> = 10, <span class="math inline">\(\sigma=1.2\)</span>), Q settles on <span class="math inline">\(\mu\)</span> with <strong>KL divergence</strong> equating to zero divergence.</p>

<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1">R.mu =<span class="st"> </span><span class="dv">10</span>;  R.sd =<span class="st"> </span><span class="fl">1.2</span>; Q.mu =<span class="st"> </span><span class="dv">10</span>; Q.sd =<span class="st"> </span><span class="fl">1.2</span></a>
<a class="sourceLine" id="cb46-2" data-line-number="2">Q.sample =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">range</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>), <span class="dt">size=</span><span class="dv">20</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb46-3" data-line-number="3"><span class="kw">round</span>( <span class="kw">KL.divergence</span>(Q.sample, <span class="dt">mu1=</span>R.mu, <span class="dt">sd1=</span>R.sd, <span class="dt">mu2=</span>Q.mu, <span class="dt">sd2=</span>Q.sd), </a>
<a class="sourceLine" id="cb46-4" data-line-number="4">       <span class="fl">1e-10</span>)</a></code></pre></div>
<pre><code>## [1] 0</code></pre>

<p>Assume that Q.mu = 5, then we get a high divergence for Q moving away from R:</p>

<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" data-line-number="1">R.mu =<span class="st"> </span><span class="dv">10</span>;  R.sd =<span class="st"> </span><span class="fl">1.2</span>; Q.mu =<span class="st"> </span><span class="dv">5</span>; Q.sd =<span class="st"> </span><span class="fl">1.2</span></a>
<a class="sourceLine" id="cb48-2" data-line-number="2">Q.sample =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">range</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>), <span class="dt">size=</span><span class="dv">20</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb48-3" data-line-number="3"><span class="kw">round</span>( <span class="kw">KL.divergence</span>(Q.sample, <span class="dt">mu1=</span>R.mu, <span class="dt">sd1=</span>R.sd, <span class="dt">mu2=</span>Q.mu, <span class="dt">sd2=</span>Q.sd), </a>
<a class="sourceLine" id="cb48-4" data-line-number="4">       <span class="fl">1e-10</span>)</a></code></pre></div>
<pre><code>## [1] 35</code></pre>

<p>For comparison, we leave readers to investigate <strong>Wasserstein distance</strong> as an alternative measurement to <strong>KL divergence</strong>.</p>
<p>Now that we have seen the capability of <strong>KL divergence</strong>, we show how <strong>KL divergence</strong> is minimized for <strong>Variational inference</strong> in a later discussion as a way to solve the computational challenge inherent in <strong>Markov Chain Monte Carlo</strong>.</p>
<p>We leave readers also to investigate <strong>Bregman Divergence</strong>, <strong>Jensen-Bregman Divergence</strong>, and <strong>Jensen-Shannon Divergence</strong>.</p>
</div>
<div id="jensens-inequality" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.5.7</span> Jensenâs Inequality<a href="information-theory.html#jensens-inequality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Jensenâs Inequality</strong> states that the secant line drawn between any two points on a convex curve is always above the convex curve. Similarly, a secant line drawn between two points on a concave curve is always below the concave curve. See Figure .</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:jensensinequality"></span>
<img src="jensensinequality.png" alt="Jensen's Inequality" width="70%" />
<p class="caption">
Figure 3.13: Jensenâs Inequality
</p>
</div>

<p>Mathematically, the average point in a secant line is always greater than a convex function; but lesser than a concave function.</p>
<p><span class="math display">\[\begin{align}
\underbrace{\overbrace{\mathbb{E}(f(x))}^\text{secant line} \ge \overbrace{ f(\mathbb{E}(x))}^\text{curve function}  }_\text{convex}
\ \ \ \ \ \ \ \ \ \ 
\underbrace{   \overbrace{\mathbb{E}(f(x))}^\text{secant line} \le \overbrace{f(\mathbb{E}(x))}^\text{curve function}
}_\text{concave}
\end{align}\]</span></p>
<p>In <strong>variational inference</strong>, we use <strong>Jensenâs inequality</strong> as a trick to be able to derive the upper/lower bound:</p>
<p><span class="math display">\[\begin{align}
\underbrace{\overbrace{\mathbb{E}(f(x))}^\text{secant line} = \overbrace{ f(\mathbb{E}(x))}^\text{curve function}  }_\text{convex (upper bound)}
\ \ \ \ \ \ \ \ \ \ 
\underbrace{   \overbrace{\mathbb{E}(f(x))}^\text{secant line} = \overbrace{f(\mathbb{E}(x))}^\text{curve function}
}_\text{concave (lower bound)}
\end{align}\]</span></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="conjugacy.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesianinference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
